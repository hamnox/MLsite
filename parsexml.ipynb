{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# code modified from http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from glob import glob\n",
    "\n",
    "filenames = glob(\"arxiv_metadata/*.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3D0812.4044%20OR%201103.1778%20OR%201104.2373%20OR%201107.3342%20OR%201112.5309%20OR%201206.1106%20OR%201206.5533%20OR%201207.0580%20OR%201209.1557%20OR%201210.7495%20OR%201211.5063%20OR%201211.6581%20OR%201211.6950%20OR%201212.3023%20OR%201301.0802%20OR%201301.1942%20OR%201301.2444%20OR%201303.5960%20OR%201303.7220%20OR%201304.0806%20OR%201305.0445%20OR%201305.1422%20OR%201305.4778%20OR%201305.5306%20OR%201306.0386%20OR%201306.1849%20OR%201306.4793%20OR%201306.5860%20OR%201307.0048%20OR%201310.5568%20OR%201310.5796%20OR%201311.0989%20OR%201311.2492%20OR%201311.2524%20OR%201311.2901%20OR%201311.3959%20OR%201311.6531%20OR%201312.6034%20OR%201312.6229%20OR%201312.6947%20OR%201312.7219%20OR%201402.0240%20OR%201402.1298%20OR%201402.1754%20OR%201402.3044%20OR%201402.3337%20OR%201402.4303%20OR%201402.4893%20OR%201402.5874%20OR%201402.5876%26id_list%3D%26start%3D0%26max_results%3D50\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=0812.4044 OR 1103.1778 OR 1104.2373 OR 1107.3342 OR 1112.5309 OR 1206.1106 OR 1206.5533 OR 1207.0580 OR 1209.1557 OR 1210.7495 OR 1211.5063 OR 1211.6581 OR 1211.6950 OR 1212.3023 OR 1301.0802 OR 1301.1942 OR 1301.2444 OR 1303.5960 OR 1303.7220 OR 1304.0806 OR 1305.0445 OR 1305.1422 OR 1305.4778 OR 1305.5306 OR 1306.0386 OR 1306.1849 OR 1306.4793 OR 1306.5860 OR 1307.0048 OR 1310.5568 OR 1310.5796 OR 1311.0989 OR 1311.2492 OR 1311.2524 OR 1311.2901 OR 1311.3959 OR 1311.6531 OR 1312.6034 OR 1312.6229 OR 1312.6947 OR 1312.7219 OR 1402.0240 OR 1402.1298 OR 1402.1754 OR 1402.3044 OR 1402.3337 OR 1402.4303 OR 1402.4893 OR 1402.5874 OR 1402.5876&amp;id_list=&amp;start=0&amp;max_results=50</title>\n",
      "  <id>http://arxiv.org/api/ZM0zqvA4YPZtEcf17N/wDcjfPXI</id>\n",
      "  <updated>2016-06-14T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">64</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">50</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1310.5568v1</id>\n",
      "    <updated>2013-10-21T14:31:20Z</updated>\n",
      "    <published>2013-10-21T14:31:20Z</published>\n",
      "    <title>Towards Application of the RBNK Model</title>\n",
      "    <summary>  The computational modeling of genetic regulatory networks is now common\n",
      "place, either by fitting a system to experimental data or by exploring the\n",
      "behaviour of abstract systems with the aim of identifying underlying\n",
      "principles. This paper presents an approach to the latter, considering the\n",
      "response to environmental changes of a well-known model placed upon tunable\n",
      "fitness landscapes. The effects on genome size and gene connectivity are\n",
      "explored.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Larry Bull</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1306.4793,\n",
      "  arXiv:1303.7220</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1310.5568v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.5568v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1603.01185v2</id>\n",
      "    <updated>2016-03-15T14:48:37Z</updated>\n",
      "    <published>2016-03-02T14:31:15Z</published>\n",
      "    <title>Evolving Boolean Regulatory Networks with Variable Gene Expression Times</title>\n",
      "    <summary>  The time taken for gene expression varies not least because proteins vary in\n",
      "length considerably. This paper uses an abstract, tuneable Boolean regulatory\n",
      "network model to explore gene expression time variation. In particular, it is\n",
      "shown how non-uniform expression times can emerge under certain conditions\n",
      "through simulated evolution. That is, gene expression time variance appears\n",
      "beneficial in the shaping of the dynamical behaviour of the regulatory network\n",
      "without explicit consideration of protein function.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Larry Bull</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1505.01980,\n",
      "  arXiv:1306.4793, arXiv:1303.7220, arXiv:1310.5568</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1603.01185v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.01185v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.BM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.MN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1306.4793v1</id>\n",
      "    <updated>2013-06-20T09:00:40Z</updated>\n",
      "    <published>2013-06-20T09:00:40Z</published>\n",
      "    <title>Evolving Boolean Regulatory Networks with Epigenetic Control</title>\n",
      "    <summary>  The significant role of epigenetic mechanisms within natural systems has\n",
      "become increasingly clear. This paper uses a recently presented abstract,\n",
      "tunable Boolean genetic regulatory network model to explore aspects of\n",
      "epigenetics. It is shown how dynamically controlling transcription via a DNA\n",
      "methylation-inspired mechanism can be selected for by simulated evolution under\n",
      "various single and multiple cell scenarios. Further, it is shown that the\n",
      "effects of such control can be inherited without detriment to fitness.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Larry Bull</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, 10 figures. arXiv admin note: substantial text overlap with\n",
      "  arXiv:1303.7220</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1306.4793v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.4793v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.MN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1505.01980v1</id>\n",
      "    <updated>2015-05-08T10:20:06Z</updated>\n",
      "    <published>2015-05-08T10:20:06Z</published>\n",
      "    <title>Evolving Boolean Networks with RNA Editing</title>\n",
      "    <summary>  The editing of transcribed RNA by other molecules such that the form of the\n",
      "final product differs from that specified in the corresponding DNA sequence is\n",
      "ubiquitous. This paper uses an abstract, tunable Boolean genetic regulatory\n",
      "network model to explore aspects of RNA editing. In particular, it is shown how\n",
      "dynamically altering expressed sequences via a guide RNA-inspired mechanism can\n",
      "be selected for by simulated evolution under various single and multicellular\n",
      "scenarios.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Larry Bull</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1306.4793,\n",
      "  arXiv:1303.7220</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1505.01980v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1505.01980v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.MN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.PE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1306.5860v1</id>\n",
      "    <updated>2013-06-25T07:02:20Z</updated>\n",
      "    <published>2013-06-25T07:02:20Z</published>\n",
      "    <title>Supersparse Linear Integer Models for Predictive Scoring Systems</title>\n",
      "    <summary>  We introduce Supersparse Linear Integer Models (SLIM) as a tool to create\n",
      "scoring systems for binary classification. We derive theoretical bounds on the\n",
      "true risk of SLIM scoring systems, and present experimental results to show\n",
      "that SLIM scoring systems are accurate, sparse, and interpretable\n",
      "classification models.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Berk Ustun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefano Traca</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cynthia Rudin</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Short version</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1306.5860v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.5860v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1603.01573v1</id>\n",
      "    <updated>2016-03-04T19:13:36Z</updated>\n",
      "    <published>2016-03-04T19:13:36Z</published>\n",
      "    <title>McCulloch-Pitts brains and pseudorandom functions</title>\n",
      "    <summary>  In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model\n",
      "of the central nervous system. Motivated by EEG recordings of normal brain\n",
      "activity, Chv\\'atal and Goldsmith asked whether or not these dynamical systems\n",
      "can be engineered to produce trajectories which are irregular, disorderly,\n",
      "apparently unpredictable. We show that they cannot build weak pseudorandom\n",
      "functions.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Va코ek Chv치tal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Goldsmith</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nan Yang</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Supersedes arXiv:1311.6531 [math.DS]</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1603.01573v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.01573v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"92B20, 65C10, 37B15, 62P10\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1206.1106v2</id>\n",
      "    <updated>2013-02-18T16:09:50Z</updated>\n",
      "    <published>2012-06-06T02:06:57Z</published>\n",
      "    <title>No More Pesky Learning Rates</title>\n",
      "    <summary>  The performance of stochastic gradient descent (SGD) depends critically on\n",
      "how learning rates are tuned and decreased over time. We propose a method to\n",
      "automatically adjust multiple learning rates so as to minimize the expected\n",
      "error at any one time. The method relies on local gradient variations across\n",
      "samples. In our approach, learning rates can increase as well as decrease,\n",
      "making it suitable for non-stationary problems. Using a number of convex and\n",
      "non-convex learning tasks, we show that the resulting algorithm matches the\n",
      "performance of SGD or other adaptive approaches with their best settings\n",
      "obtained through systematic search, and effectively removes the need for\n",
      "learning rate tuning.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Tom Schaul</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sixin Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yann LeCun</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1206.1106v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.1106v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1206.5533v2</id>\n",
      "    <updated>2012-09-16T17:49:12Z</updated>\n",
      "    <published>2012-06-24T19:17:35Z</published>\n",
      "    <title>Practical recommendations for gradient-based training of deep\n",
      "  architectures</title>\n",
      "    <summary>  Learning algorithms related to artificial neural networks and in particular\n",
      "for Deep Learning may seem to involve many bells and whistles, called\n",
      "hyper-parameters. This chapter is meant as a practical guide with\n",
      "recommendations for some of the most commonly used hyper-parameters, in\n",
      "particular in the context of learning algorithms based on back-propagated\n",
      "gradient and gradient-based optimization. It also discusses how to deal with\n",
      "the fact that more interesting results can be obtained when allowing one to\n",
      "adjust many hyper-parameters. Overall, it describes elements of the practice\n",
      "used to successfully and efficiently train and debug large-scale and often deep\n",
      "multi-layer neural networks. It closes with open questions about the training\n",
      "difficulties observed with deeper architectures.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yoshua Bengio</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1206.5533v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1206.5533v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1211.5063v2</id>\n",
      "    <updated>2013-02-16T00:35:48Z</updated>\n",
      "    <published>2012-11-21T15:40:11Z</published>\n",
      "    <title>On the difficulty of training Recurrent Neural Networks</title>\n",
      "    <summary>  There are two widely known issues with properly training Recurrent Neural\n",
      "Networks, the vanishing and the exploding gradient problems detailed in Bengio\n",
      "et al. (1994). In this paper we attempt to improve the understanding of the\n",
      "underlying issues by exploring these problems from an analytical, a geometric\n",
      "and a dynamical systems perspective. Our analysis is used to justify a simple\n",
      "yet effective solution. We propose a gradient norm clipping strategy to deal\n",
      "with exploding gradients and a soft constraint for the vanishing gradients\n",
      "problem. We validate empirically our hypothesis and proposed solutions in the\n",
      "experimental section.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Razvan Pascanu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomas Mikolov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yoshua Bengio</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Improved description of the exploding gradient problem and\n",
      "  description and analysis of the vanishing gradient problem</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1211.5063v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.5063v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1304.0806v3</id>\n",
      "    <updated>2016-04-17T19:56:30Z</updated>\n",
      "    <published>2013-04-02T22:10:00Z</published>\n",
      "    <title>IFP-Intuitionistic fuzzy soft set theory and its applications</title>\n",
      "    <summary>  In this work, we present definition of intuitionistic fuzzy parameterized\n",
      "(IFP) intuitionistic fuzzy soft set and its operations. Then we define\n",
      "IFP-aggregation operator to form IFP-intuitionistic fuzzy soft-decision-making\n",
      "method which allows constructing more efficient decision processes.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Faruk Karaaslan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Naim Cagman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Saban Yilmaz</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the author due to a crucial errors\n",
      "  in the notation and some problems in the algorithm</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1304.0806v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1304.0806v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"62C86 (Decision theory and fuzziness)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1310.5796v4</id>\n",
      "    <updated>2016-04-04T23:35:45Z</updated>\n",
      "    <published>2013-10-22T04:28:12Z</published>\n",
      "    <title>Relative Deviation Learning Bounds and Generalization with Unbounded\n",
      "  Loss Functions</title>\n",
      "    <summary>  We present an extensive analysis of relative deviation bounds, including\n",
      "detailed proofs of two-sided inequalities and their implications. We also give\n",
      "detailed proofs of two-sided generalization bounds that hold in the general\n",
      "case of unbounded loss functions, under the assumption that a moment of the\n",
      "loss is bounded. These bounds are useful in the analysis of importance\n",
      "weighting and other learning tasks such as unbounded regression.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Corinna Cortes</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Spencer Greenberg</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mehryar Mohri</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1310.5796v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1310.5796v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1212.3023v1</id>\n",
      "    <updated>2012-12-13T00:34:23Z</updated>\n",
      "    <published>2012-12-13T00:34:23Z</published>\n",
      "    <title>Keyword Extraction for Identifying Social Actors</title>\n",
      "    <summary>  Identifying the social actor has become one of tasks in Artificial\n",
      "Intelligence, whereby extracting keyword from Web snippets depend on the use of\n",
      "web is steadily gaining ground in this research. We develop therefore an\n",
      "approach based on overlap principle for utilizing a collection of features in\n",
      "web snippets, where use of keyword will eliminate the un-relevant web pages.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Mahyuddin K. M. Nasution</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shahrul Azman Mohd Noah</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, nothing, draft to ICOCSIM 2012</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1212.3023v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1212.3023v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1305.0445v2</id>\n",
      "    <updated>2013-06-07T02:35:21Z</updated>\n",
      "    <published>2013-05-02T14:33:28Z</published>\n",
      "    <title>Deep Learning of Representations: Looking Forward</title>\n",
      "    <summary>  Deep learning research aims at discovering learning algorithms that discover\n",
      "multiple levels of distributed representations, with higher levels representing\n",
      "more abstract concepts. Although the study of deep learning has already led to\n",
      "impressive theoretical results, learning algorithms and breakthrough\n",
      "experiments, several challenges lie ahead. This paper proposes to examine some\n",
      "of these challenges, centering on the questions of scaling deep learning\n",
      "algorithms to much larger models and datasets, reducing optimization\n",
      "difficulties due to ill-conditioning or local minima, designing more efficient\n",
      "and powerful inference and sampling procedures, and learning to disentangle the\n",
      "factors of variation underlying the observed data. It also proposes a few\n",
      "forward-looking research directions aimed at overcoming these challenges.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yoshua Bengio</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1305.0445v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.0445v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1311.6531v3</id>\n",
      "    <updated>2016-03-07T08:52:47Z</updated>\n",
      "    <published>2013-11-26T01:03:30Z</published>\n",
      "    <title>Brains and pseudorandom generators</title>\n",
      "    <summary>  In a pioneering classic, Warren McCulloch and Walter Pitts proposed a model\n",
      "of the central nervous system; motivated by EEG recordings of normal brain\n",
      "activity, Chv\\' atal and Goldsmith asked whether or not this model can be\n",
      "engineered to provide pseudorandom number generators. We supply evidence\n",
      "suggesting that the answer is negative.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Va코ek Chv치tal</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Goldsmith</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nan Yang</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper misinterprets the notion of a pseudorandom generator. For\n",
      "  this reason, it has been withdrawn by the authors. Its main result,\n",
      "  interpreted in terms of pseudorandom functions, reappears in arXiv:1603.01573\n",
      "  [math.DS]</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1311.6531v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.6531v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"92B20, 65C10, 37B15, 62P10\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1303.7220v1</id>\n",
      "    <updated>2013-03-26T09:03:35Z</updated>\n",
      "    <published>2013-03-26T09:03:35Z</published>\n",
      "    <title>On Mobile DNA in Artificial Regulatory Networks: Evolving Functional and\n",
      "  Structural Dynamism</title>\n",
      "    <summary>  There is a growing body of work considering the use of representations based\n",
      "upon genetic regulatory networks. This paper uses a recently presented\n",
      "abstract, tunable Boolean regulatory network model to explore aspects of mobile\n",
      "DNA, such as transposons, within these dynamical systems. The significant role\n",
      "of mobile DNA in the evolution of natural systems is becoming increasingly\n",
      "clear. Whilst operators loosely based upon transposons have previously been\n",
      "used within evolutionary computation, their use within regulatory network\n",
      "representations enables the potential exploitation of numerous new mechanisms.\n",
      "This paper shows how dynamically controlling network node connectivity and\n",
      "function via transposon-inspired mechanisms can be selected for under\n",
      "non-stationary and coevolutionary scenarios, including when such changes are\n",
      "heritable.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Larry Bull</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">24 pages, 11 figures</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1303.7220v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.7220v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1306.5533v1</id>\n",
      "    <updated>2013-06-24T07:53:25Z</updated>\n",
      "    <published>2013-06-24T07:53:25Z</published>\n",
      "    <title>Evolving Gene Regulatory Networks with Mobile DNA Mechanisms</title>\n",
      "    <summary>  This paper uses a recently presented abstract, tuneable Boolean regulatory\n",
      "network model extended to consider aspects of mobile DNA, such as transposons.\n",
      "The significant role of mobile DNA in the evolution of natural systems is\n",
      "becoming increasingly clear. This paper shows how dynamically controlling\n",
      "network node connectivity and function via transposon-inspired mechanisms can\n",
      "be selected for in computational intelligence tasks to give improved\n",
      "performance. The designs of dynamical networks intended for implementation\n",
      "within the slime mould Physarum polycephalum and for the distributed control of\n",
      "a smart surface are considered.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Larry Bull</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Adamatzky</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 8 figures. arXiv admin note: substantial text overlap with\n",
      "  arXiv:1303.7220</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1306.5533v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.5533v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"nlin.AO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.MN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1107.3342v3</id>\n",
      "    <updated>2016-03-11T18:28:34Z</updated>\n",
      "    <published>2011-07-17T23:54:09Z</published>\n",
      "    <title>Computing Strong Game-Theoretic Strategies in Jotto</title>\n",
      "    <summary>  We develop a new approach that computes approximate equilibrium strategies in\n",
      "Jotto, a popular word game. Jotto is an extremely large two-player game of\n",
      "imperfect information; its game tree has many orders of magnitude more states\n",
      "than games previously studied, including no-limit Texas hold 'em. To address\n",
      "the fact that the game is so large, we propose a novel strategy representation\n",
      "called oracular form, in which we do not explicitly represent a strategy, but\n",
      "rather appeal to an oracle that quickly outputs a sample move from the\n",
      "strategy's distribution. Our overall approach is based on an extension of the\n",
      "fictitious play algorithm to this oracular setting. We demonstrate the\n",
      "superiority of our computed strategies over the strategies computed by a\n",
      "benchmark algorithm, both in terms of head-to-head and worst-case performance.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Sam Ganzfried</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1107.3342v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1107.3342v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1307.0048v3</id>\n",
      "    <updated>2016-04-14T01:55:55Z</updated>\n",
      "    <published>2013-06-28T23:32:11Z</published>\n",
      "    <title>Simple one-pass algorithm for penalized linear regression with\n",
      "  cross-validation on MapReduce</title>\n",
      "    <summary>  In this paper, we propose a one-pass algorithm on MapReduce for penalized\n",
      "linear regression\n",
      "  \\[f_\\lambda(\\alpha, \\beta) = \\|Y - \\alpha\\mathbf{1} - X\\beta\\|_2^2 +\n",
      "p_{\\lambda}(\\beta)\\] where $\\alpha$ is the intercept which can be omitted\n",
      "depending on application; $\\beta$ is the coefficients and $p_{\\lambda}$ is the\n",
      "penalized function with penalizing parameter $\\lambda$. $f_\\lambda(\\alpha,\n",
      "\\beta)$ includes interesting classes such as Lasso, Ridge regression and\n",
      "Elastic-net. Compared to latest iterative distributed algorithms requiring\n",
      "multiple MapReduce jobs, our algorithm achieves huge performance improvement;\n",
      "moreover, our algorithm is exact compared to the approximate algorithms such as\n",
      "parallel stochastic gradient decent. Moreover, what our algorithm distinguishes\n",
      "with others is that it trains the model with cross validation to choose optimal\n",
      "$\\lambda$ instead of user specified one.\n",
      "  Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Kun Yang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1307.0048v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1307.0048v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/0812.4044v3</id>\n",
      "    <updated>2016-04-03T21:41:38Z</updated>\n",
      "    <published>2008-12-21T17:45:27Z</published>\n",
      "    <title>The Offset Tree for Learning with Partial Labels</title>\n",
      "    <summary>  We present an algorithm, called the Offset Tree, for learning to make\n",
      "decisions in situations where the payoff of only one choice is observed, rather\n",
      "than all choices. The algorithm reduces this setting to binary classification,\n",
      "allowing one to reuse of any existing, fully supervised binary classification\n",
      "algorithm in this partial information setting. We show that the Offset Tree is\n",
      "an optimal reduction to binary classification. In particular, it has regret at\n",
      "most $(k-1)$ times the regret of the binary classifier it uses (where $k$ is\n",
      "the number of choices), and no reduction to binary classification can do\n",
      "better. This reduction is also computationally optimal, both at training and\n",
      "test time, requiring just $O(\\log_2 k)$ work to train on an example or make a\n",
      "prediction.\n",
      "  Experiments with the Offset Tree show that it generally performs better than\n",
      "several alternative approaches.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Alina Beygelzimer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>John Langford</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/0812.4044v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0812.4044v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1104.2373v4</id>\n",
      "    <updated>2013-02-09T14:33:14Z</updated>\n",
      "    <published>2011-04-13T04:20:07Z</published>\n",
      "    <title>Hybrid Deterministic-Stochastic Methods for Data Fitting</title>\n",
      "    <summary>  Many structured data-fitting applications require the solution of an\n",
      "optimization problem involving a sum over a potentially large number of\n",
      "measurements. Incremental gradient algorithms offer inexpensive iterations by\n",
      "sampling a subset of the terms in the sum. These methods can make great\n",
      "progress initially, but often slow as they approach a solution. In contrast,\n",
      "full-gradient methods achieve steady convergence at the expense of evaluating\n",
      "the full objective and gradient on each iteration. We explore hybrid methods\n",
      "that exhibit the benefits of both approaches. Rate-of-convergence analysis\n",
      "shows that by controlling the sample size in an incremental gradient algorithm,\n",
      "it is possible to maintain the steady convergence rates of full-gradient\n",
      "methods. We detail a practical quasi-Newton implementation based on this\n",
      "approach. Numerical experiments illustrate its potential benefits.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Michael P. Friedlander</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mark Schmidt</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1137/110830629</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1137/110830629\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages. Revised proofs of Theorems 2.6 and 3.1, results unchanged</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1104.2373v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1104.2373v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1209.1557v4</id>\n",
      "    <updated>2016-01-27T13:14:52Z</updated>\n",
      "    <published>2012-09-07T14:46:49Z</published>\n",
      "    <title>Learning Model-Based Sparsity via Projected Gradient Descent</title>\n",
      "    <summary>  Several convex formulation methods have been proposed previously for\n",
      "statistical estimation with structured sparsity as the prior. These methods\n",
      "often require a carefully tuned regularization parameter, often a cumbersome or\n",
      "heuristic exercise. Furthermore, the estimate that these methods produce might\n",
      "not belong to the desired sparsity model, albeit accurately approximating the\n",
      "true parameter. Therefore, greedy-type algorithms could often be more desirable\n",
      "in estimating structured-sparse parameters. So far, these greedy methods have\n",
      "mostly focused on linear statistical models. In this paper we study the\n",
      "projected gradient descent with non-convex structured-sparse parameter model as\n",
      "the constraint set. Should the cost function have a Stable Model-Restricted\n",
      "Hessian the algorithm produces an approximation for the desired minimizer. As\n",
      "an example we elaborate on application of the main results to estimation in\n",
      "Generalized Linear Model.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Sohail Bahmani</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Petros T. Boufounos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bhiksha Raj</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TIT.2016.2515078</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TIT.2016.2515078\" rel=\"related\"/>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE Transactions on Information Theory 62(4):2092--2099, 2016</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/1209.1557v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1209.1557v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"62FXX, 65KXX\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1210.7495v3</id>\n",
      "    <updated>2016-02-27T14:45:52Z</updated>\n",
      "    <published>2012-10-28T19:37:33Z</published>\n",
      "    <title>Illustrating a neural model of logic computations: The case of Sherlock\n",
      "  Holmes' old maxim</title>\n",
      "    <summary>  Natural languages can express some logical propositions that humans are able\n",
      "to understand. We illustrate this fact with a famous text that Conan Doyle\n",
      "attributed to Holmes: 'It is an old maxim of mine that when you have excluded\n",
      "the impossible, whatever remains, however improbable, must be the truth'. This\n",
      "is a subtle logical statement usually felt as an evident truth. The problem we\n",
      "are trying to solve is the cognitive reason for such a feeling. We postulate\n",
      "here that we accept Holmes' maxim as true because our adult brains are equipped\n",
      "with neural modules that naturally perform modal logical computations.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Eduardo Mizraji</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1387/theoria.13959</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1387/theoria.13959\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Corrected version with new references</arxiv:comment>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">THEORIA 31/1 (2016): 7-25</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/1210.7495v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1210.7495v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"92B20, 68T05, 68T37\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1211.6950v1</id>\n",
      "    <updated>2012-11-29T15:26:23Z</updated>\n",
      "    <published>2012-11-29T15:26:23Z</published>\n",
      "    <title>Dynamic Network Cartography</title>\n",
      "    <summary>  Communication networks have evolved from specialized, research and tactical\n",
      "transmission systems to large-scale and highly complex interconnections of\n",
      "intelligent devices, increasingly becoming more commercial, consumer-oriented,\n",
      "and heterogeneous. Propelled by emergent social networking services and\n",
      "high-definition streaming platforms, network traffic has grown explosively\n",
      "thanks to the advances in processing speed and storage capacity of\n",
      "state-of-the-art communication technologies. As \"netizens\" demand a seamless\n",
      "networking experience that entails not only higher speeds, but also resilience\n",
      "and robustness to failures and malicious cyber-attacks, ample opportunities for\n",
      "signal processing (SP) research arise. The vision is for ubiquitous smart\n",
      "network devices to enable data-driven statistical learning algorithms for\n",
      "distributed, robust, and online network operation and management, adaptable to\n",
      "the dynamically-evolving network landscape with minimal need for human\n",
      "intervention. The present paper aims at delineating the analytical background\n",
      "and the relevance of SP tools to dynamic network monitoring, introducing the SP\n",
      "readership to the concept of dynamic network cartography -- a framework to\n",
      "construct maps of the dynamic network state in an efficient and scalable manner\n",
      "tailored to large-scale heterogeneous networks.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Gonzalo Mateos</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ketan Rajawat</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/MSP.2012.2232355</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/MSP.2012.2232355\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To appear in the IEEE Signal Processing Magazine - Special Issue on\n",
      "  Adaptation and Learning over Complex Networks</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1211.6950v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1211.6950v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.IT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1301.1942v2</id>\n",
      "    <updated>2016-01-10T16:01:22Z</updated>\n",
      "    <published>2013-01-09T18:26:56Z</published>\n",
      "    <title>Bayesian Optimization in a Billion Dimensions via Random Embeddings</title>\n",
      "    <summary>  Bayesian optimization techniques have been successfully applied to robotics,\n",
      "planning, sensor placement, recommendation, advertising, intelligent user\n",
      "interfaces and automatic algorithm configuration. Despite these successes, the\n",
      "approach is restricted to problems of moderate dimension, and several workshops\n",
      "on Bayesian optimization have identified its scaling to high-dimensions as one\n",
      "of the holy grails of the field. In this paper, we introduce a novel random\n",
      "embedding idea to attack this problem. The resulting Random EMbedding Bayesian\n",
      "Optimization (REMBO) algorithm is very simple, has important invariance\n",
      "properties, and applies to domains with both categorical and continuous\n",
      "variables. We present a thorough theoretical analysis of REMBO. Empirical\n",
      "results confirm that REMBO can effectively solve problems with billions of\n",
      "dimensions, provided the intrinsic dimensionality is low. They also show that\n",
      "REMBO achieves state-of-the-art performance in optimizing the 47 discrete\n",
      "parameters of a popular mixed integer linear programming solver.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Ziyu Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Frank Hutter</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Masrour Zoghi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Matheson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nando de Freitas</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">33 pages</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1301.1942v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.1942v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1301.2444v3</id>\n",
      "    <updated>2016-01-28T07:37:59Z</updated>\n",
      "    <published>2013-01-11T10:38:09Z</published>\n",
      "    <title>TEI and LMF crosswalks</title>\n",
      "    <summary>  The present paper explores various arguments in favour of making the Text\n",
      "Encoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO\n",
      "standard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the\n",
      "issues that would have to be resolved in order to reach an appropriate\n",
      "implementation of these ideas, in particular in terms of infor-mational\n",
      "coverage. We show how the customisation facilities offered by the TEI\n",
      "guidelines can provide an adequate background, not only to cover missing\n",
      "components within the current Dictionary chapter of the TEI guidelines, but\n",
      "also to allow specific lexical projects to deal with local constraints. We\n",
      "expect this proposal to be a basis for a future ISO project in the context of\n",
      "the on going revision of LMF.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Laurent Romary</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ALPAGE, CMB</arxiv:affiliation>\n",
      "    </author>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">JLCL - Journal for Language Technology and Computational\n",
      "  Linguistics, 2015, 30 (1)</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/1301.2444v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.2444v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1303.5960v3</id>\n",
      "    <updated>2016-01-21T20:23:31Z</updated>\n",
      "    <published>2013-03-24T15:27:51Z</published>\n",
      "    <title>SYNTAGMA. A Linguistic Approach to Parsing</title>\n",
      "    <summary>  SYNTAGMA is a rule-based parsing system, structured on two levels: a general\n",
      "parsing engine and a language specific grammar. The parsing engine is a\n",
      "language independent program, while grammar and language specific rules and\n",
      "resources are given as text files, consisting in a list of constituent\n",
      "structuresand a lexical database with word sense related features and\n",
      "constraints. Since its theoretical background is principally Tesniere's\n",
      "Elements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument\n",
      "structure (valency) in constraint satisfaction, and allows also horizontal\n",
      "bounds, for instance treating coordination. Notions such as Pro, traces, empty\n",
      "categories are derived from Generative Grammar and some solutions are close to\n",
      "Government&amp;Binding Theory, although they are the result of an autonomous\n",
      "research. These properties allow SYNTAGMA to manage complex syntactic\n",
      "configurations and well known weak points in parsing engineering. An important\n",
      "resource is the semantic network, which is used in disambiguation tasks.\n",
      "Parsing process follows a bottom-up, rule driven strategy. Its behavior can be\n",
      "controlled and fine-tuned.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Daniel Christen</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1303.5960v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1303.5960v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1305.1422v3</id>\n",
      "    <updated>2016-01-11T10:48:52Z</updated>\n",
      "    <published>2013-05-07T06:43:26Z</published>\n",
      "    <title>Somoclu: An Efficient Parallel Library for Self-Organizing Maps</title>\n",
      "    <summary>  Somoclu is a massively parallel tool for training self-organizing maps on\n",
      "large data sets written in C++. It builds on OpenMP for multicore execution,\n",
      "and on MPI for distributing the workload across the nodes in a cluster. It is\n",
      "also able to boost training by using CUDA if graphics processing units are\n",
      "available. A sparse kernel is included, which is useful for high-dimensional\n",
      "but sparse data, such as the vector spaces common in text mining workflows.\n",
      "Python, R and MATLAB interfaces facilitate interactive use. Apart from fast\n",
      "execution, memory use is highly optimized, enabling training large emergent\n",
      "maps even on a single computer.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Peter Wittek</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shi Chao Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ik Soo Lim</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Li Zhao</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">27 pages, 9 figures. The code is available at\n",
      "  https://peterwittek.github.io/somoclu/</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1305.1422v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.1422v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1305.4778v4</id>\n",
      "    <updated>2016-03-15T10:55:10Z</updated>\n",
      "    <published>2013-05-21T10:32:29Z</published>\n",
      "    <title>Zero-sum repeated games: Counterexamples to the existence of the\n",
      "  asymptotic value and the conjecture\n",
      "  $\\operatorname{maxmin}=\\operatorname{lim}v_n$</title>\n",
      "    <summary>  Mertens [In Proceedings of the International Congress of Mathematicians\n",
      "(Berkeley, Calif., 1986) (1987) 1528-1577 Amer. Math. Soc.] proposed two\n",
      "general conjectures about repeated games: the first one is that, in any\n",
      "two-person zero-sum repeated game, the asymptotic value exists, and the second\n",
      "one is that, when Player 1 is more informed than Player 2, in the long run\n",
      "Player 1 is able to guarantee the asymptotic value. We disprove these two\n",
      "long-standing conjectures by providing an example of a zero-sum repeated game\n",
      "with public signals and perfect observation of the actions, where the value of\n",
      "the $\\lambda$-discounted game does not converge when $\\lambda$ goes to 0. The\n",
      "aforementioned example involves seven states, two actions and two signals for\n",
      "each player. Remarkably, players observe the payoffs, and play in turn.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Bruno Ziliotto</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/14-AOP997</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/14-AOP997\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published at http://dx.doi.org/10.1214/14-AOP997 in the Annals of\n",
      "  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical\n",
      "  Statistics (http://www.imstat.org)</arxiv:comment>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Probability 2016, Vol. 44, No. 2, 1107-1133</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/1305.4778v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.4778v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"91A20 (Primary), 91A05, 91A15 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1311.2901v3</id>\n",
      "    <updated>2013-11-28T23:04:01Z</updated>\n",
      "    <published>2013-11-12T20:02:22Z</published>\n",
      "    <title>Visualizing and Understanding Convolutional Networks</title>\n",
      "    <summary>  Large Convolutional Network models have recently demonstrated impressive\n",
      "classification performance on the ImageNet benchmark. However there is no clear\n",
      "understanding of why they perform so well, or how they might be improved. In\n",
      "this paper we address both issues. We introduce a novel visualization technique\n",
      "that gives insight into the function of intermediate feature layers and the\n",
      "operation of the classifier. We also perform an ablation study to discover the\n",
      "performance contribution from different model layers. This enables us to find\n",
      "model architectures that outperform Krizhevsky \\etal on the ImageNet\n",
      "classification benchmark. We show our ImageNet model generalizes well to other\n",
      "datasets: when the softmax classifier is retrained, it convincingly beats the\n",
      "current state-of-the-art results on Caltech-101 and Caltech-256 datasets.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Matthew D Zeiler</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rob Fergus</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1311.2901v3\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.2901v3\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1312.6034v2</id>\n",
      "    <updated>2014-04-19T11:54:52Z</updated>\n",
      "    <published>2013-12-20T16:45:54Z</published>\n",
      "    <title>Deep Inside Convolutional Networks: Visualising Image Classification\n",
      "  Models and Saliency Maps</title>\n",
      "    <summary>  This paper addresses the visualisation of image classification models, learnt\n",
      "using deep Convolutional Networks (ConvNets). We consider two visualisation\n",
      "techniques, based on computing the gradient of the class score with respect to\n",
      "the input image. The first one generates an image, which maximises the class\n",
      "score [Erhan et al., 2009], thus visualising the notion of the class, captured\n",
      "by a ConvNet. The second technique computes a class saliency map, specific to a\n",
      "given image and class. We show that such maps can be employed for weakly\n",
      "supervised object segmentation using classification ConvNets. Finally, we\n",
      "establish the connection between the gradient-based ConvNet visualisation\n",
      "methods and deconvolutional networks [Zeiler et al., 2013].\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Karen Simonyan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrea Vedaldi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Zisserman</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1312.6034v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.6034v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1312.6229v4</id>\n",
      "    <updated>2014-02-24T03:38:17Z</updated>\n",
      "    <published>2013-12-21T09:52:33Z</published>\n",
      "    <title>OverFeat: Integrated Recognition, Localization and Detection using\n",
      "  Convolutional Networks</title>\n",
      "    <summary>  We present an integrated framework for using Convolutional Networks for\n",
      "classification, localization and detection. We show how a multiscale and\n",
      "sliding window approach can be efficiently implemented within a ConvNet. We\n",
      "also introduce a novel deep learning approach to localization by learning to\n",
      "predict object boundaries. Bounding boxes are then accumulated rather than\n",
      "suppressed in order to increase detection confidence. We show that different\n",
      "tasks can be learned simultaneously using a single shared network. This\n",
      "integrated framework is the winner of the localization task of the ImageNet\n",
      "Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very\n",
      "competitive results for the detection and classifications tasks. In\n",
      "post-competition work, we establish a new state of the art for the detection\n",
      "task. Finally, we release a feature extractor from our best model called\n",
      "OverFeat.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Pierre Sermanet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Eigen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michael Mathieu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rob Fergus</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yann LeCun</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1312.6229v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1312.6229v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1402.0240v4</id>\n",
      "    <updated>2016-03-26T22:52:52Z</updated>\n",
      "    <published>2014-02-02T20:03:19Z</published>\n",
      "    <title>Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\n",
      "  Algorithms</title>\n",
      "    <summary>  We study an extension of the classical graph cut problem, wherein we replace\n",
      "the modular (sum of edge weights) cost function by a submodular set function\n",
      "defined over graph edges. Special cases of this problem have appeared in\n",
      "different applications in signal processing, machine learning, and computer\n",
      "vision. In this paper, we connect these applications via the generic\n",
      "formulation of \"cooperative graph cuts\", for which we study complexity,\n",
      "algorithms, and connections to polymatroidal network flows. Finally, we compare\n",
      "the proposed algorithms empirically.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Stefanie Jegelka</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">MIT</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeff Bilmes</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Washington</arxiv:affiliation>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">46 pages</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1402.0240v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.0240v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.DM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1402.3044v2</id>\n",
      "    <updated>2016-01-08T19:51:00Z</updated>\n",
      "    <published>2014-02-13T07:14:28Z</published>\n",
      "    <title>Finding a Collective Set of Items: From Proportional Multirepresentation\n",
      "  to Group Recommendation</title>\n",
      "    <summary>  We consider the following problem: There is a set of items (e.g., movies) and\n",
      "a group of agents (e.g., passengers on a plane); each agent has some intrinsic\n",
      "utility for each of the items. Our goal is to pick a set of $K$ items that\n",
      "maximize the total derived utility of all the agents (i.e., in our example we\n",
      "are to pick $K$ movies that we put on the plane's entertainment system).\n",
      "However, the actual utility that an agent derives from a given item is only a\n",
      "fraction of its intrinsic one, and this fraction depends on how the agent ranks\n",
      "the item among the chosen, available, ones. We provide a formal specification\n",
      "of the model and provide concrete examples and settings where it is applicable.\n",
      "We show that the problem is hard in general, but we show a number of\n",
      "tractability results for its natural special cases.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Piotr Skowron</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Piotr Faliszewski</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jerome Lang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1402.3044v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.3044v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1402.3337v5</id>\n",
      "    <updated>2015-04-08T14:51:11Z</updated>\n",
      "    <published>2014-02-13T23:37:39Z</published>\n",
      "    <title>Zero-bias autoencoders and the benefits of co-adapting features</title>\n",
      "    <summary>  Regularized training of an autoencoder typically results in hidden unit\n",
      "biases that take on large negative values. We show that negative biases are a\n",
      "natural result of using a hidden layer whose responsibility is to both\n",
      "represent the input data and act as a selection mechanism that ensures sparsity\n",
      "of the representation. We then show that negative biases impede the learning of\n",
      "data distributions whose intrinsic dimensionality is high. We also propose a\n",
      "new activation function that decouples the two roles of the hidden layer and\n",
      "that allows us to learn representations on data with very high intrinsic\n",
      "dimensionality, where standard autoencoders typically fail. Since the decoupled\n",
      "activation function acts like an implicit regularizer, the model can be trained\n",
      "by minimizing the reconstruction error of training data, without requiring any\n",
      "additional regularization.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Kishore Konda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roland Memisevic</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Krueger</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1402.3337v5\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.3337v5\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1402.4303v2</id>\n",
      "    <updated>2016-03-02T17:01:37Z</updated>\n",
      "    <published>2014-02-18T11:31:08Z</published>\n",
      "    <title>Finding Preference Profiles of Condorcet Dimension $k$ via SAT</title>\n",
      "    <summary>  Condorcet winning sets are a set-valued generalization of the well-known\n",
      "concept of a Condorcet winner. As supersets of Condorcet winning sets are\n",
      "always Condorcet winning sets themselves, an interesting property of preference\n",
      "profiles is the size of the smallest Condorcet winning set they admit. This\n",
      "smallest size is called the Condorcet dimension of a preference profile. Since\n",
      "little is known about profiles that have a certain Condorcet dimension, we show\n",
      "in this paper how the problem of finding a preference profile that has a given\n",
      "Condorcet dimension can be encoded as a satisfiability problem and solved by a\n",
      "SAT solver. Initial results include a minimal example of a preference profile\n",
      "of Condorcet dimension 3, improving previously known examples both in terms of\n",
      "the number of agents as well as alternatives. Due to the high complexity of\n",
      "such problems it remains open whether a preference profile of Condorcet\n",
      "dimension 4 exists.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Christian Geist</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Corrected typos, updated references, and added conclusion</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1402.4303v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.4303v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1402.4893v4</id>\n",
      "    <updated>2016-03-29T19:10:35Z</updated>\n",
      "    <published>2014-02-20T05:15:22Z</published>\n",
      "    <title>Anisotropic Mesh Adaptation for Image Representation</title>\n",
      "    <summary>  Triangular meshes have gained much interest in image representation and have\n",
      "been widely used in image processing. This paper introduces a framework of\n",
      "anisotropic mesh adaptation (AMA) methods to image representation and proposes\n",
      "a GPRAMA method that is based on AMA and greedy-point removal (GPR) scheme.\n",
      "Different than many other methods that triangulate sample points to form the\n",
      "mesh, the AMA methods start directly with a triangular mesh and then adapt the\n",
      "mesh based on a user-defined metric tensor to represent the image. The AMA\n",
      "methods have clear mathematical framework and provides flexibility for both\n",
      "image representation and image reconstruction. A mesh patching technique is\n",
      "developed for the implementation of the GPRAMA method, which leads to an\n",
      "improved version of the popular GPRFS-ED method. The GPRAMA method can achieve\n",
      "better quality than the GPRFS-ED method but with lower computational cost.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Xianping Li</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">25 pages, 15 figures</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1402.4893v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.4893v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.NA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"I.4.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1402.5876v4</id>\n",
      "    <updated>2016-04-11T11:07:31Z</updated>\n",
      "    <published>2014-02-24T16:19:51Z</published>\n",
      "    <title>Manifold Gaussian Processes for Regression</title>\n",
      "    <summary>  Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness\n",
      "assumptions on the structure of the function to be modeled. To model complex\n",
      "and non-differentiable functions, these smoothness assumptions are often too\n",
      "restrictive. One way to alleviate this limitation is to find a different\n",
      "representation of the data by introducing a feature space. This feature space\n",
      "is often learned in an unsupervised way, which might lead to data\n",
      "representations that are not useful for the overall regression task. In this\n",
      "paper, we propose Manifold Gaussian Processes, a novel supervised method that\n",
      "jointly learns a transformation of the data into a feature space and a GP\n",
      "regression from the feature space to observed space. The Manifold GP is a full\n",
      "GP and allows to learn data representations, which are useful for the overall\n",
      "regression task. As a proof-of-concept, we evaluate our approach on complex\n",
      "non-smooth functions where standard GPs perform poorly, such as step functions\n",
      "and robotics tasks with contacts.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Roberto Calandra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Peters</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Carl Edward Rasmussen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marc Peter Deisenroth</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, accepted to IJCNN 2016</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1402.5876v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.5876v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1103.1778v1</id>\n",
      "    <updated>2011-03-09T13:33:23Z</updated>\n",
      "    <published>2011-03-09T13:33:23Z</published>\n",
      "    <title>Pituitary Adenoma Segmentation</title>\n",
      "    <summary>  Sellar tumors are approximately 10-15% among all intracranial neoplasms. The\n",
      "most common sellar lesion is the pituitary adenoma. Manual segmentation is a\n",
      "time-consuming process that can be shortened by using adequate algorithms. In\n",
      "this contribution, we present a segmentation method for pituitary adenoma. The\n",
      "method is based on an algorithm we developed recently in previous work where\n",
      "the novel segmentation scheme was successfully used for segmentation of\n",
      "glioblastoma multiforme and provided an average Dice Similarity Coefficient\n",
      "(DSC) of 77%. This scheme is used for automatic adenoma segmentation. In our\n",
      "experimental evaluation, neurosurgeons with strong experiences in the treatment\n",
      "of pituitary adenoma performed manual slice-by-slice segmentation of 10\n",
      "magnetic resonance imaging (MRI) cases. Afterwards, the segmentations were\n",
      "compared with the segmentation results of the proposed method via the DSC. The\n",
      "average DSC for all data sets was 77.49% +/- 4.52%. Compared with a manual\n",
      "segmentation that took, on the average, 3.91 +/- 0.54 minutes, the overall\n",
      "segmentation in our implementation required less than 4 seconds.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Jan Egger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Miriam H. A. Bauer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniela Kuhnt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bernd Freisleben</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher Nimsky</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages, 5 figures, BIOSIGNAL, Berlin, 2010</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1103.1778v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1103.1778v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"physics.med-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.TO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1207.0580v1</id>\n",
      "    <updated>2012-07-03T06:35:15Z</updated>\n",
      "    <published>2012-07-03T06:35:15Z</published>\n",
      "    <title>Improving neural networks by preventing co-adaptation of feature\n",
      "  detectors</title>\n",
      "    <summary>  When a large feedforward neural network is trained on a small training set,\n",
      "it typically performs poorly on held-out test data. This \"overfitting\" is\n",
      "greatly reduced by randomly omitting half of the feature detectors on each\n",
      "training case. This prevents complex co-adaptations in which a feature detector\n",
      "is only helpful in the context of several other specific feature detectors.\n",
      "Instead, each neuron learns to detect a feature that is generally helpful for\n",
      "producing the correct answer given the combinatorially large variety of\n",
      "internal contexts in which it must operate. Random \"dropout\" gives big\n",
      "improvements on many benchmark tasks and sets new records for speech and object\n",
      "recognition.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Geoffrey E. Hinton</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nitish Srivastava</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alex Krizhevsky</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ilya Sutskever</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ruslan R. Salakhutdinov</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1207.0580v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1207.0580v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1305.5306v1</id>\n",
      "    <updated>2013-05-23T03:35:31Z</updated>\n",
      "    <published>2013-05-23T03:35:31Z</published>\n",
      "    <title>A Supervised Neural Autoregressive Topic Model for Simultaneous Image\n",
      "  Classification and Annotation</title>\n",
      "    <summary>  Topic modeling based on latent Dirichlet allocation (LDA) has been a\n",
      "framework of choice to perform scene recognition and annotation. Recently, a\n",
      "new type of topic model called the Document Neural Autoregressive Distribution\n",
      "Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance\n",
      "for document modeling. In this work, we show how to successfully apply and\n",
      "extend this model to the context of visual scene modeling. Specifically, we\n",
      "propose SupDocNADE, a supervised extension of DocNADE, that increases the\n",
      "discriminative power of the hidden topic features by incorporating label\n",
      "information into the training objective of the model. We also describe how to\n",
      "leverage information about the spatial position of the visual words and how to\n",
      "embed additional image annotations, so as to simultaneously perform image\n",
      "classification and annotation. We test our model on the Scene15, LabelMe and\n",
      "UIUC-Sports datasets and show that it compares favorably to other topic models\n",
      "such as the supervised variant of LDA.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yin Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu-Jin Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hugo Larochelle</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 5 figures</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1305.5306v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1305.5306v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1311.0989v2</id>\n",
      "    <updated>2014-05-23T12:02:06Z</updated>\n",
      "    <published>2013-11-05T08:46:26Z</published>\n",
      "    <title>Large Margin Distribution Machine</title>\n",
      "    <summary>  Support vector machine (SVM) has been one of the most popular learning\n",
      "algorithms, with the central idea of maximizing the minimum margin, i.e., the\n",
      "smallest distance from the instances to the classification boundary. Recent\n",
      "theoretical results, however, disclosed that maximizing the minimum margin does\n",
      "not necessarily lead to better generalization performances, and instead, the\n",
      "margin distribution has been proven to be more crucial. In this paper, we\n",
      "propose the Large margin Distribution Machine (LDM), which tries to achieve a\n",
      "better generalization performance by optimizing the margin distribution. We\n",
      "characterize the margin distribution by the first- and second-order statistics,\n",
      "i.e., the margin mean and variance. The LDM is a general learning approach\n",
      "which can be used in any place where SVM can be applied, and its superiority is\n",
      "verified both theoretically and empirically in this paper.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Teng Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhi-Hua Zhou</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In: Proceedings of the 20th ACM SIGKDD Conference on Knowledge\n",
      "  Discovery and Data Mining (KDD'14), New York, NY, 2014</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1311.0989v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.0989v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1402.5874v2</id>\n",
      "    <updated>2016-03-21T10:56:40Z</updated>\n",
      "    <published>2014-02-24T16:16:17Z</published>\n",
      "    <title>Predictive Interval Models for Non-parametric Regression</title>\n",
      "    <summary>  Having a regression model, we are interested in finding two-sided intervals\n",
      "that are guaranteed to contain at least a desired proportion of the conditional\n",
      "distribution of the response variable given a specific combination of\n",
      "predictors. We name such intervals predictive intervals. This work presents a\n",
      "new method to find two-sided predictive intervals for non-parametric least\n",
      "squares regression without the homoscedasticity assumption. Our predictive\n",
      "intervals are built by using tolerance intervals on prediction errors in the\n",
      "query point's neighborhood. We proposed a predictive interval model test and we\n",
      "also used it as a constraint in our hyper-parameter tuning algorithm. This\n",
      "gives an algorithm that finds the smallest reliable predictive intervals for a\n",
      "given dataset. We also introduce a measure for comparing different interval\n",
      "prediction methods yielding intervals having different size and coverage. These\n",
      "experiments show that our methods are more reliable, effective and precise than\n",
      "other interval prediction methods.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Mohammad Ghasemi Hamed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mathieu Serrurier</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas Durand</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn by the authors due to multiple errors\n",
      "  in the formulations and equations</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1402.5874v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1402.5874v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1508.07096v1</id>\n",
      "    <updated>2015-08-28T05:24:06Z</updated>\n",
      "    <published>2015-08-28T05:24:06Z</published>\n",
      "    <title>Partitioning Large Scale Deep Belief Networks Using Dropout</title>\n",
      "    <summary>  Deep learning methods have shown great promise in many practical\n",
      "applications, ranging from speech recognition, visual object recognition, to\n",
      "text processing. However, most of the current deep learning methods suffer from\n",
      "scalability problems for large-scale applications, forcing researchers or users\n",
      "to focus on small-scale problems with fewer parameters.\n",
      "  In this paper, we consider a well-known machine learning model, deep belief\n",
      "networks (DBNs) that have yielded impressive classification performance on a\n",
      "large number of benchmark machine learning tasks. To scale up DBN, we propose\n",
      "an approach that can use the computing clusters in a distributed environment to\n",
      "train large models, while the dense matrix computations within a single machine\n",
      "are sped up using graphics processors (GPU). When training a DBN, each machine\n",
      "randomly drops out a portion of neurons in each hidden layer, for each training\n",
      "case, making the remaining neurons only learn to detect features that are\n",
      "generally helpful for producing the correct answer. Within our approach, we\n",
      "have developed four methods to combine outcomes from each machine to form a\n",
      "unified model. Our preliminary experiment on the mnst handwritten digit\n",
      "database demonstrates that our approach outperforms the state of the art test\n",
      "error rate.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yanping Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sai Zhang</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:1207.0580 by other authors</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1508.07096v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1508.07096v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1602.00104v1</id>\n",
      "    <updated>2016-01-30T10:53:05Z</updated>\n",
      "    <published>2016-01-30T10:53:05Z</published>\n",
      "    <title>Extracting Keyword for Disambiguating Name Based on the Overlap\n",
      "  Principle</title>\n",
      "    <summary>  Name disambiguation has become one of the main themes in the Semantic Web\n",
      "agenda. The semantic web is an extension of the current Web in which\n",
      "information is not only given well-defined meaning, but also has many purposes\n",
      "that contain the ambiguous naturally or a lot of thing came with the overlap,\n",
      "mainly deals with the persons name. Therefore, we develop an approach to\n",
      "extract keywords from web snippet with utilizing the overlap principle, a\n",
      "concept to understand things with ambiguous, whereby features of person are\n",
      "generated for dealing with the variety of web, the web is steadily gaining\n",
      "ground in the semantic research.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Mahyuddin K. M. Nasution</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, Proceeding of International Conference on Information\n",
      "  Technology and Engineering Application (4-th ICIBA), Book 1, 119-125,\n",
      "  February 20-21, 2015. arXiv admin note: text overlap with arXiv:1212.3023</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1602.00104v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.00104v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"F.2.2; I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1602.02022v1</id>\n",
      "    <updated>2016-02-05T14:08:21Z</updated>\n",
      "    <published>2016-02-05T14:08:21Z</published>\n",
      "    <title>Preoperative Volume Determination for Pituitary Adenoma</title>\n",
      "    <summary>  The most common sellar lesion is the pituitary adenoma, and sellar tumors are\n",
      "approximately 10-15% of all intracranial neoplasms. Manual slice-by-slice\n",
      "segmentation takes quite some time that can be reduced by using the appropriate\n",
      "algorithms. In this contribution, we present a segmentation method for\n",
      "pituitary adenoma. The method is based on an algorithm that we have applied\n",
      "recently to segmenting glioblastoma multiforme. A modification of this scheme\n",
      "is used for adenoma segmentation that is much harder to perform, due to lack of\n",
      "contrast-enhanced boundaries. In our experimental evaluation, neurosurgeons\n",
      "performed manual slice-by-slice segmentation of ten magnetic resonance imaging\n",
      "(MRI) cases. The segmentations were compared to the segmentation results of the\n",
      "proposed method using the Dice Similarity Coefficient (DSC). The average DSC\n",
      "for all datasets was 75.92% +/- 7.24%. A manual segmentation took about four\n",
      "minutes and our algorithm required about one second.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Dzenan Zukic</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Egger</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Miriam H. A. Bauer</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniela Kuhnt</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Barbara Carl</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bernd Freisleben</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andreas Kolb</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christopher Nimsky</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1117/12.877660</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1117/12.877660\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 6 figures, 1 table, 16 references in Proc. SPIE 7963,\n",
      "  Medical Imaging 2011: Computer-Aided Diagnosis, 79632T (9 March 2011). arXiv\n",
      "  admin note: text overlap with arXiv:1103.1778</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1602.02022v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1602.02022v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1603.05587v4</id>\n",
      "    <updated>2016-04-01T10:23:38Z</updated>\n",
      "    <published>2016-03-17T17:39:12Z</published>\n",
      "    <title>Reliable Prediction Intervals for Local Linear Regression</title>\n",
      "    <summary>  This paper introduces two methods for estimating reliable prediction\n",
      "intervals for local linear least-squares regressions, named Bounded Oscillation\n",
      "Prediction Intervals (BOPI). It also proposes a new measure for comparing\n",
      "interval prediction models named Equivalent Gaussian Standard Deviation (EGSD).\n",
      "The experimental results compare BOPI to other methods using coverage\n",
      "probability, Mean Interval Size and the introduced EGSD measure. The results\n",
      "were generally in favor of the BOPI on considered benchmark regression\n",
      "datasets. It also, reports simulation studies validating the BOPI method's\n",
      "reliability.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Mohammad Ghasemi Hamed</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Masoud Ebadi Kivaj</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">40 pages,11 figures, 14 tables and 1 algorithm. arXiv admin note:\n",
      "  text overlap with arXiv:1402.5874</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1603.05587v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1603.05587v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1604.03348v1</id>\n",
      "    <updated>2016-04-12T11:39:16Z</updated>\n",
      "    <published>2016-04-12T11:39:16Z</published>\n",
      "    <title>Optimal Margin Distribution Machine</title>\n",
      "    <summary>  Support vector machine (SVM) has been one of the most popular learning\n",
      "algorithms, with the central idea of maximizing the minimum margin, i.e., the\n",
      "smallest distance from the instances to the classification boundary. Recent\n",
      "theoretical results, however, disclosed that maximizing the minimum margin does\n",
      "not necessarily lead to better generalization performances, and instead, the\n",
      "margin distribution has been proven to be more crucial. Based on this idea, we\n",
      "propose a new method, named Optimal margin Distribution Machine (ODM), which\n",
      "tries to achieve a better generalization performance by optimizing the margin\n",
      "distribution. We characterize the margin distribution by the first- and\n",
      "second-order statistics, i.e., the margin mean and variance. The proposed\n",
      "method is a general learning approach which can be used in any place where SVM\n",
      "can be applied, and their superiority is verified both theoretically and\n",
      "empirically in this paper.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Teng Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhi-Hua Zhou</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: substantial text overlap with arXiv:1311.0989</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1604.03348v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1604.03348v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1301.0802v4</id>\n",
      "    <updated>2016-03-24T14:26:50Z</updated>\n",
      "    <published>2013-01-04T18:55:41Z</published>\n",
      "    <title>Borrowing strengh in hierarchical Bayes: Posterior concentration of the\n",
      "  Dirichlet base measure</title>\n",
      "    <summary>  This paper studies posterior concentration behavior of the base probability\n",
      "measure of a Dirichlet measure, given observations associated with the sampled\n",
      "Dirichlet processes, as the number of observations tends to infinity. The base\n",
      "measure itself is endowed with another Dirichlet prior, a construction known as\n",
      "the hierarchical Dirichlet processes (Teh et al. [J. Amer. Statist. Assoc. 101\n",
      "(2006) 1566-1581]). Convergence rates are established in transportation\n",
      "distances (i.e., Wasserstein metrics) under various conditions on the geometry\n",
      "of the support of the true base measure. As a consequence of the theory, we\n",
      "demonstrate the benefit of \"borrowing strength\" in the inference of multiple\n",
      "groups of data - a powerful insight often invoked to motivate hierarchical\n",
      "modeling. In certain settings, the gain in efficiency due to the latent\n",
      "hierarchy can be dramatic, improving from a standard nonparametric rate to a\n",
      "parametric rate of convergence. Tools developed include transportation\n",
      "distances for nonparametric Bayesian hierarchies of random measures, the\n",
      "existence of tests for Dirichlet measures, and geometric properties of the\n",
      "support of Dirichlet measures.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>XuanLong Nguyen</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.3150/15-BEJ703</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.3150/15-BEJ703\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published at http://dx.doi.org/10.3150/15-BEJ703 in the Bernoulli\n",
      "  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n",
      "  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</arxiv:comment>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Bernoulli 2016, Vol. 22, No. 3, 1535-1571</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/1301.0802v4\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1301.0802v4\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"math.PR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1306.1849v2</id>\n",
      "    <updated>2016-02-24T00:01:09Z</updated>\n",
      "    <published>2013-06-07T21:49:30Z</published>\n",
      "    <title>New Results on Equilibria in Strategic Candidacy</title>\n",
      "    <summary>  We consider a voting setting where candidates have preferences about the\n",
      "outcome of the election and are free to join or leave the election. The\n",
      "corresponding candidacy game, where candidates choose strategically to\n",
      "participate or not, has been studied %initially by Dutta et al., who showed\n",
      "that no non-dictatorial voting procedure satisfying unanimity is\n",
      "candidacy-strategyproof, that is, is such that the joint action where all\n",
      "candidates enter the election is always a pure strategy Nash equilibrium. Dutta\n",
      "et al. also showed that for some voting tree procedures, there are candidacy\n",
      "games with no pure Nash equilibria, and that for the rule that outputs the\n",
      "sophisticated winner of voting by successive elimination, all games have a pure\n",
      "Nash equilibrium. No results were known about other voting rules. Here we prove\n",
      "several such results. For four candidates, the message is, roughly, that most\n",
      "scoring rules (with the exception of Borda) do not guarantee the existence of a\n",
      "pure Nash equilibrium but that Condorcet-consistent rules, for an odd number of\n",
      "voters, do. For five candidates, most rules we study no longer have this\n",
      "guarantee. Finally, we identify one prominent rule that guarantees the\n",
      "existence of a pure Nash equilibrium for any number of candidates (and for an\n",
      "odd number of voters): the Copeland rule. We also show that under mild\n",
      "assumptions on the voting rule, the existence of strong equilibria cannot be\n",
      "guaranteed.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>J칠r칪me Lang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nicolas Maudet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Maria Polukarov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alice Cohen-Hadria</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1306.1849v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1306.1849v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1311.2524v5</id>\n",
      "    <updated>2014-10-22T17:23:20Z</updated>\n",
      "    <published>2013-11-11T18:43:49Z</published>\n",
      "    <title>Rich feature hierarchies for accurate object detection and semantic\n",
      "  segmentation</title>\n",
      "    <summary>  Object detection performance, as measured on the canonical PASCAL VOC\n",
      "dataset, has plateaued in the last few years. The best-performing methods are\n",
      "complex ensemble systems that typically combine multiple low-level image\n",
      "features with high-level context. In this paper, we propose a simple and\n",
      "scalable detection algorithm that improves mean average precision (mAP) by more\n",
      "than 30% relative to the previous best result on VOC 2012---achieving a mAP of\n",
      "53.3%. Our approach combines two key insights: (1) one can apply high-capacity\n",
      "convolutional neural networks (CNNs) to bottom-up region proposals in order to\n",
      "localize and segment objects and (2) when labeled training data is scarce,\n",
      "supervised pre-training for an auxiliary task, followed by domain-specific\n",
      "fine-tuning, yields a significant performance boost. Since we combine region\n",
      "proposals with CNNs, we call our method R-CNN: Regions with CNN features. We\n",
      "also compare R-CNN to OverFeat, a recently proposed sliding-window detector\n",
      "based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by\n",
      "a large margin on the 200-class ILSVRC2013 detection dataset. Source code for\n",
      "the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Ross Girshick</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jeff Donahue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Trevor Darrell</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jitendra Malik</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Extended version of our CVPR 2014 paper; latest update (v5) includes\n",
      "  results using deeper networks (see Appendix G. Changelog)</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/1311.2524v5\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1311.2524v5\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(filenames[0], \"r\") as f:\n",
    "    print(\"\".join(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesoups = []\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, \"r\") as f:\n",
    "        lesoups.append(BeautifulSoup(\"\".join(f.readlines()),\"xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'1602.03742': {'arxivid': u'1602.03742',\n",
       "  'authorsaffil': [[u'Carlos Palma', None],\n",
       "   [u'Augusto Salazar', None],\n",
       "   [u'Francisco Vargas', None]],\n",
       "  'categoryterms': [u'cs.HC', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03742v1',\n",
       "  'published': u'2016-02-11T14:22:26Z',\n",
       "  'summary': u'  Automatic recognition of the quality of movement in human beings is a\\nchallenging task, given the difficulty both in defining the constraints that\\nmake a movement correct, and the difficulty in using noisy data to determine if\\nthese constraints were satisfied. This paper presents a method for the\\ndetection of deviations from the correct form in movements from physical\\ntherapy routines based on Hidden Markov Models, which is compared to Dynamic\\nTime Warping. The activities studied include upper an lower limbs movements,\\nthe data used comes from a Kinect sensor. Correct repetitions of the activities\\nof interest were recorded, as well as deviations from these correct forms. The\\nability of the proposed approach to detect these deviations was studied.\\nResults show that a system based on HMM is much more likely to determine if a\\ncertain movement has deviated from the specification.\\n',\n",
       "  'title': u'HMM and DTW for evaluation of therapeutical gestures using kinect'},\n",
       " u'1509.08535': {'arxivid': u'1509.08535',\n",
       "  'authorsaffil': [[u'Siamak Ravanbakhsh', None],\n",
       "   [u'Barnabas Poczos', None],\n",
       "   [u'Russell Greiner', None]],\n",
       "  'categoryterms': [u'math.ST', u'cs.AI', u'cs.DM', u'stat.ML', u'stat.TH'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.08535v3',\n",
       "  'published': u'2015-09-28T23:11:16Z',\n",
       "  'summary': u'  Boolean matrix factorization and Boolean matrix completion from noisy\\nobservations are desirable unsupervised data-analysis methods due to their\\ninterpretability, but hard to perform due to their NP-hardness. We treat these\\nproblems as maximum a posteriori inference problems in a graphical model and\\npresent a message passing approach that scales linearly with the number of\\nobservations and factors. Our empirical study demonstrates that message passing\\nis able to recover low-rank Boolean matrices, in the boundaries of\\ntheoretically possible recovery and compares favorably with state-of-the-art in\\nreal-world applications, such collaborative filtering with large-scale Boolean\\ndata.\\n',\n",
       "  'title': u'Boolean Matrix Factorization and Noisy Completion via Message Passing'},\n",
       " u'1602.02343': {'arxivid': u'1602.02343',\n",
       "  'authorsaffil': [[u'Carlos Torres', None],\n",
       "   [u'Victor Fragoso', None],\n",
       "   [u'Scott D. Hammond', None],\n",
       "   [u'Jeffrey C. Fried', None],\n",
       "   [u'B. S. Manjunath', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Ten-page manuscript including references and ten figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02343v2',\n",
       "  'published': u'2016-02-07T06:33:08Z',\n",
       "  'summary': u'  Manual analysis of body poses of bed-ridden patients requires staff to\\ncontinuously track and record patient poses. Two limitations in the\\ndissemination of pose-related therapies are scarce human resources and\\nunreliable automated systems. This work addresses these issues by introducing a\\nnew method and a new system for robust automated classification of sleep poses\\nin an Intensive Care Unit (ICU) environment. The new method,\\ncoupled-constrained Least-Squares (cc-LS), uses multimodal and multiview (MM)\\ndata and finds the set of modality trust values that minimizes the difference\\nbetween expected and estimated labels. The new system, Eye-CU, is an affordable\\nmulti-sensor modular system for unobtrusive data collection and analysis in\\nhealthcare. Experimental results indicate that the performance of cc-LS matches\\nthe performance of existing methods in ideal scenarios. This method outperforms\\nthe latest techniques in challenging scenarios by 13% for those with poor\\nillumination and by 70% for those with both poor illumination and occlusions.\\nResults also show that a reduced Eye-CU configuration can classify poses\\nwithout pressure information with only a slight drop in its performance.\\n',\n",
       "  'title': u'Eye-CU: Sleep Pose Classification for Healthcare using Multimodal\\n  Multiview Data'},\n",
       " u'1502.04042': {'arxivid': u'1502.04042',\n",
       "  'authorsaffil': [[u'Andrew J. R. Simpson', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE', u'68Txx'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.04042v1',\n",
       "  'published': u'2015-02-13T16:09:41Z',\n",
       "  'summary': u'  Inspired by the brain, deep neural networks (DNN) are thought to learn\\nabstract representations through their hierarchical architecture. However, at\\npresent, how this happens is not well understood. Here, we demonstrate that DNN\\nlearn abstract representations by a process of demodulation. We introduce a\\nbiased sigmoid activation function and use it to show that DNN learn and\\nperform better when optimized for demodulation. Our findings constitute the\\nfirst unambiguous evidence that DNN perform abstract learning in practical use.\\nOur findings may also explain abstract learning in the human brain.\\n',\n",
       "  'title': u'Abstract Learning via Demodulation in a Deep Neural Network'},\n",
       " u'1510.08660': {'arxivid': u'1510.08660',\n",
       "  'authorsaffil': [[u'Samira Ebrahimi Kahou', None],\n",
       "   [u'Vincent Michalski', None],\n",
       "   [u'Roland Memisevic', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.08660v4',\n",
       "  'published': u'2015-10-29T12:06:56Z',\n",
       "  'summary': u'  We present an attention-based modular neural framework for computer vision.\\nThe framework uses a soft attention mechanism allowing models to be trained\\nwith gradient descent. It consists of three modules: a recurrent attention\\nmodule controlling where to look in an image or video frame, a\\nfeature-extraction module providing a representation of what is seen, and an\\nobjective module formalizing why the model learns its attentive behavior. The\\nattention module allows the model to focus computation on task-related\\ninformation in the input. We apply the framework to several object tracking\\ntasks and explore various design choices. We experiment with three data sets,\\nbouncing ball, moving digits and the real-world KTH data set. The proposed\\nRecurrent Attentive Tracking Model performs well on all three tasks and can\\ngeneralize to related but previously unseen sequences from a challenging\\ntracking data set.\\n',\n",
       "  'title': u'RATM: Recurrent Attentive Tracking Model'},\n",
       " u'1507.04540': {'arxivid': u'1507.04540',\n",
       "  'authorsaffil': [[u'Tianpei Xie', None],\n",
       "   [u'Nasser M. Nasrabadi', None],\n",
       "   [u'Alfred O. Hero', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IT', u'math.IT', u'stat.ML'],\n",
       "  'comment': u'13 pages, submitted to IEEE Transaction of Signal Processing, Feb\\n  2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.04540v3',\n",
       "  'published': u'2015-07-16T12:16:02Z',\n",
       "  'summary': u'  In this paper, we propose a general framework to learn a robust large-margin\\nbinary classifier when corrupt measurements, called anomalies, caused by sensor\\nfailure might be present in the training set. The goal is to minimize the\\ngeneralization error of the classifier on non-corrupted measurements while\\ncontrolling the false alarm rate associated with anomalous samples. By\\nincorporating a non-parametric regularizer based on an empirical entropy\\nestimator, we propose a Geometric-Entropy-Minimization regularized Maximum\\nEntropy Discrimination (GEM-MED) method to learn to classify and detect\\nanomalies in a joint manner. We demonstrate using simulated data and a real\\nmultimodal data set. Our GEM-MED method can yield improved performance over\\nprevious robust classification methods in terms of both classification accuracy\\nand anomaly detection rate.\\n',\n",
       "  'title': u'Learning to classify with possible sensor failures'},\n",
       " u'1602.02181': {'arxivid': u'1602.02181',\n",
       "  'authorsaffil': [[u'He He', None],\n",
       "   [u'Paul Mineiro', None],\n",
       "   [u'Nikos Karampatziakis', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02181v1',\n",
       "  'published': u'2016-02-05T22:32:50Z',\n",
       "  'summary': u'  We propose a general framework for sequential and dynamic acquisition of\\nuseful information in order to solve a particular task. While our goal could in\\nprinciple be tackled by general reinforcement learning, our particular setting\\nis constrained enough to allow more efficient algorithms. In this paper, we\\nwork under the Learning to Search framework and show how to formulate the goal\\nof finding a dynamic information acquisition policy in that framework. We apply\\nour formulation on two tasks, sentiment analysis and image recognition, and\\nshow that the learned policies exhibit good statistical performance. As an\\nemergent byproduct, the learned policies show a tendency to focus on the most\\nprominent parts of each instance and give harder instances more attention\\nwithout explicitly being trained to do so.\\n',\n",
       "  'title': u'Active Information Acquisition'},\n",
       " u'1506.02438': {'arxivid': u'1506.02438',\n",
       "  'authorsaffil': [[u'John Schulman', None],\n",
       "   [u'Philipp Moritz', None],\n",
       "   [u'Sergey Levine', None],\n",
       "   [u'Michael Jordan', None],\n",
       "   [u'Pieter Abbeel', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.RO', u'cs.SY'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.02438v4',\n",
       "  'published': u'2015-06-08T11:12:48Z',\n",
       "  'summary': u'  Policy gradient methods are an appealing approach in reinforcement learning\\nbecause they directly optimize the cumulative reward and can straightforwardly\\nbe used with nonlinear function approximators such as neural networks. The two\\nmain challenges are the large number of samples typically required, and the\\ndifficulty of obtaining stable and steady improvement despite the\\nnonstationarity of the incoming data. We address the first challenge by using\\nvalue functions to substantially reduce the variance of policy gradient\\nestimates at the cost of some bias, with an exponentially-weighted estimator of\\nthe advantage function that is analogous to TD(lambda). We address the second\\nchallenge by using trust region optimization procedure for both the policy and\\nthe value function, which are represented by neural networks.\\n  Our approach yields strong empirical results on highly challenging 3D\\nlocomotion tasks, learning running gaits for bipedal and quadrupedal simulated\\nrobots, and learning a policy for getting the biped to stand up from starting\\nout lying on the ground. In contrast to a body of prior work that uses\\nhand-crafted policy representations, our neural network policies map directly\\nfrom raw kinematics to joint torques. Our algorithm is fully model-free, and\\nthe amount of simulated experience required for the learning tasks on 3D bipeds\\ncorresponds to 1-2 weeks of real time.\\n',\n",
       "  'title': u'High-Dimensional Continuous Control Using Generalized Advantage\\n  Estimation'},\n",
       " u'1603.05631': {'arxivid': u'1603.05631',\n",
       "  'authorsaffil': [[u'Xiaolong Wang', None], [u'Abhinav Gupta', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05631v1',\n",
       "  'published': u'2016-03-17T19:33:20Z',\n",
       "  'summary': u'  Current generative frameworks use end-to-end learning and generate images by\\nsampling from uniform noise distribution. However, these approaches ignore the\\nmost basic principle of image formation: images are product of: (a) Structure:\\nthe underlying 3D model; (b) Style: the texture mapped onto structure. In this\\npaper, we factorize the image generation process and propose Style and\\nStructure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two\\ncomponents: the Structure-GAN generates a surface normal map; the Style-GAN\\ntakes the surface normal map as input and generates the 2D image. Apart from a\\nreal vs. generated loss function, we use an additional loss with computed\\nsurface normals from generated images. The two GANs are first trained\\nindependently, and then merged together via joint learning. We show our S^2-GAN\\nmodel is interpretable, generates more realistic images and can be used to\\nlearn unsupervised RGBD representations.\\n',\n",
       "  'title': u'Generative Image Modeling using Style and Structure Adversarial Networks'},\n",
       " u'1603.06503': {'arxivid': u'1603.06503',\n",
       "  'authorsaffil': [[u'Bernd Bohnet', None],\n",
       "   [u'Miguel Ballesteros', None],\n",
       "   [u'Ryan McDonald', None],\n",
       "   [u'Joakim Nivre', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06503v1',\n",
       "  'published': u'2016-03-21T17:20:34Z',\n",
       "  'summary': u'  We study the use of greedy feature selection methods for morphosyntactic\\ntagging under a number of different conditions. We compare a static ordering of\\nfeatures to a dynamic ordering based on mutual information statistics, and we\\napply the techniques to standalone taggers as well as joint systems for tagging\\nand parsing. Experiments on five languages show that feature selection can\\nresult in more compact models as well as higher accuracy under all conditions,\\nbut also that a dynamic ordering works better than a static ordering and that\\njoint systems benefit more than standalone taggers. We also show that the same\\ntechniques can be used to select which morphosyntactic categories to predict in\\norder to maximize syntactic accuracy in a joint system. Our final results\\nrepresent a substantial improvement of the state of the art for several\\nlanguages, while at the same time reducing both the number of features and the\\nrunning time by up to 80% in some cases.\\n',\n",
       "  'title': u'Static and Dynamic Feature Selection in Morphosyntactic Analyzers'},\n",
       " u'1603.04779': {'arxivid': u'1603.04779',\n",
       "  'authorsaffil': [[u'Yanghao Li', None],\n",
       "   [u'Naiyan Wang', None],\n",
       "   [u'Jianping Shi', None],\n",
       "   [u'Jiaying Liu', None],\n",
       "   [u'Xiaodi Hou', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04779v2',\n",
       "  'published': u'2016-03-15T17:44:32Z',\n",
       "  'summary': u'  Deep neural networks (DNN) have shown unprecedented success in various\\ncomputer vision applications such as image classification and object detection.\\nHowever, it is still a common (yet inconvenient) practice to prepare at least\\ntens of thousands of labeled image to fine-tune a network on every task before\\nthe model is ready to use. Recent study shows that a DNN has strong dependency\\ntowards the training dataset, and the learned features cannot be easily\\ntransferred to a different but relevant task without fine-tuning.\\n  In this paper, we propose a simple yet powerful remedy, called Adaptive Batch\\nNormalization(AdaBN), to increase the generalization ability of a DNN. Our\\napproach is based on the well-known Batch Normalization technique which has\\nbecome a standard component in modern deep learning. In contrary to other deep\\nlearning domain adaptation methods, our method does not require additional\\ncomponents, and is parameter-free. It archives state-of-the-art performance\\ndespite its surprising simplicity. Furthermore, we demonstrate that our method\\nis complementary with other existing methods. Combining AdaBN with existing\\ndomain adaptation treatments may further improve model performance.\\n',\n",
       "  'title': u'Revisiting Batch Normalization For Practical Domain Adaptation'},\n",
       " u'1601.07460': {'arxivid': u'1601.07460',\n",
       "  'authorsaffil': [[u'Asish Ghoshal', None], [u'Jean Honorio', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IT', u'math.IT', u'stat.ML'],\n",
       "  'comment': u'Added new results. Included important parameters in the bound',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07460v2',\n",
       "  'published': u'2016-01-27T17:41:05Z',\n",
       "  'summary': u'  In this paper, we study the information-theoretic limits of learning the\\nstructure of Bayesian networks, on discrete as well as continuous random\\nvariables, from a finite amount of data. We show that under certain\\nparameterizations of the Bayesian network --- the minimum number of samples\\nrequired to learn the \"true\" network grows as $\\\\mathcal{O}(M)$ and\\n$\\\\mathcal{O}(S\\\\log M)$ for non-sparse and sparse Bayesian networks respectively\\n--- where $M$ is the number of variables in the network and $S$ is the maximum\\nnumber of parents of any node in the network. We study various commonly used\\nBayesian networks, such as Conditional Probability Table (CPT) based networks,\\nNoisy-OR networks, Logistic regression (LR) networks, and Gaussian networks. We\\nidentify various important parameters of the conditional distributions that\\naffect the complexity of learning such models, like the maximum inverse\\nprobability for CPT networks, the failure probability for Noisy-OR networks,\\nthe $\\\\ell_2$ norm of weight vectors for LR networks and the signal and noise\\nparameters for Gaussian networks. We also show that an existing procedure calle\\nSparsityBoost, by Brenner and Sontag, for learning binary CPT networks is\\ninformation-theoretically optimal in the number of variables.\\n',\n",
       "  'title': u'Information-theoretic limits of learning the structure of Bayesian\\n  networks'},\n",
       " u'1501.01186': {'arxivid': u'1501.01186',\n",
       "  'authorsaffil': [[u'Vicky Kalogeiton', None],\n",
       "   [u'Vittorio Ferrari', None],\n",
       "   [u'Cordelia Schmid', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'8 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1501.01186v3',\n",
       "  'published': u'2015-01-06T14:13:07Z',\n",
       "  'summary': u'  Object detection is one of the most important challenges in computer vision.\\nObject detectors are usually trained on bounding-boxes from still images.\\nRecently, video has been used as an alternative source of data. Yet, for a\\ngiven test domain (image or video), the performance of the detector depends on\\nthe domain it was trained on. In this paper, we examine the reasons behind this\\nperformance gap. We define and evaluate different domain shift factors: spatial\\nlocation accuracy, appearance diversity, image quality and aspect distribution.\\nWe examine the impact of these factors by comparing performance before and\\nafter factoring them out. The results show that all four factors affect the\\nperformance of the detectors and their combined effect explains nearly the\\nwhole performance gap.\\n',\n",
       "  'title': u'Analysing domain shift factors between videos and images for object\\n  detection'},\n",
       " u'1603.04771': {'arxivid': u'1603.04771',\n",
       "  'authorsaffil': [[u'Ayan Chakrabarti', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Supplementary results are available at\\n  www.ttic.edu/chakrabarti/ndeblur/supp_results.pdf',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04771v1',\n",
       "  'published': u'2016-03-15T17:35:26Z',\n",
       "  'summary': u'  We present a new method for blind motion deblurring that uses a neural\\nnetwork trained to compute estimates of sharp image patches from observations\\nthat are blurred by an unknown motion kernel. Instead of regressing directly to\\npatch intensities, this network learns to predict the complex Fourier\\ncoefficients of a deconvolution filter to be applied to the input patch for\\nrestoration. For inference, we apply the network independently to all\\noverlapping patches in the observed image, and average its outputs to form an\\ninitial estimate of the sharp image. We then explicitly estimate a single\\nglobal blur kernel by relating this estimate to the observed image, and finally\\nperform non-blind deconvolution with this kernel. Our method exhibits accuracy\\nand robustness close to state-of-the-art iterative methods, while being much\\nfaster when parallelized on GPU hardware.\\n',\n",
       "  'title': u'A Neural Approach to Blind Motion Deblurring'},\n",
       " u'1509.07979': {'arxivid': u'1509.07979',\n",
       "  'authorsaffil': [[u'Yogesh Girdhar', None],\n",
       "   [u'Walter Cho', None],\n",
       "   [u'Matthew Campbell', None],\n",
       "   [u'Jesus Pineda', None],\n",
       "   [u'Elizabeth Clarke', None],\n",
       "   [u'Hanumant Singh', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.RO'],\n",
       "  'comment': u'6 pages, ICRA 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.07979v2',\n",
       "  'published': u'2015-09-26T13:51:39Z',\n",
       "  'summary': u'  This paper explores the use of a Bayesian non-parametric topic modeling\\ntechnique for the purpose of anomaly detection in video data. We present\\nresults from two experiments. The first experiment shows that the proposed\\ntechnique is automatically able characterize the underlying terrain, and detect\\nanomalous flora in image data collected by an underwater robot. The second\\nexperiment shows that the same technique can be used on images from a static\\ncamera in a dynamic unstructured environment. In the second dataset, consisting\\nof video data from a static seafloor camera capturing images of a busy coral\\nreef, the proposed technique was able to detect all three instances of an\\nunderwater vehicle passing in front of the camera, amongst many other\\nobservations of fishes, debris, lighting changes due to surface waves, and\\nbenthic flora.\\n',\n",
       "  'title': u'Anomaly Detection in Unstructured Environments using Bayesian\\n  Nonparametric Scene Modeling'},\n",
       " u'1601.07265': {'arxivid': u'1601.07265',\n",
       "  'authorsaffil': [[u'Siyu Huang', None],\n",
       "   [u'Xi Li', None],\n",
       "   [u'Zhongfei Zhang', None],\n",
       "   [u'Zhouzhou He', None],\n",
       "   [u'Fei Wu', None],\n",
       "   [u'Wei Liu', None],\n",
       "   [u'Jinhui Tang', None],\n",
       "   [u'Yueting Zhuang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07265v1',\n",
       "  'published': u'2016-01-27T05:04:31Z',\n",
       "  'summary': u\"  Capabilities of inference and prediction are significant components of visual\\nsystems. In this paper, we address an important and challenging task of them:\\nvisual path prediction. Its goal is to infer the future path for a visual\\nobject in a static scene. This task is complicated as it needs high-level\\nsemantic understandings of both the scenes and motion patterns underlying video\\nsequences. In practice, cluttered situations have also raised higher demands on\\nthe effectiveness and robustness of the considered models. Motivated by these\\nobservations, we propose a deep learning framework which simultaneously\\nperforms deep feature learning for visual representation in conjunction with\\nspatio-temporal context modeling. After that, we propose a unified path\\nplanning scheme to make accurate future path prediction based on the analytic\\nresults of the context models. The highly effective visual representation and\\ndeep context models ensure that our framework makes a deep semantic\\nunderstanding of the scene and motion pattern, consequently improving the\\nperformance of the visual path prediction task. In order to comprehensively\\nevaluate the model's performance on the visual path prediction task, we\\nconstruct two large benchmark datasets from the adaptation of video tracking\\ndatasets. The qualitative and quantitative experimental results show that our\\napproach outperforms the existing approaches and owns a better generalization\\ncapability.\\n\",\n",
       "  'title': u'Deep Learning Driven Visual Path Prediction from a Single Image'},\n",
       " u'1601.07267': {'arxivid': u'1601.07267',\n",
       "  'authorsaffil': [[u'Ioannis Avramopoulos', None]],\n",
       "  'categoryterms': [u'cs.GT', u'cs.LG', u'math.OC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07267v2',\n",
       "  'published': u'2016-01-27T05:30:22Z',\n",
       "  'summary': u'  We show that evolutionarily stable states in general (nonlinear) population\\ngames (which can be viewed as continuous vector fields constrained on a\\npolytope) are asymptotically stable under a multiplicative weights dynamic\\n(under appropriate choices of a parameter called the learning rate or step\\nsize, which we demonstrate to be crucial to achieve convergence, as otherwise\\neven chaotic behavior is possible to manifest). Our result implies that\\nevolutionary theories based on multiplicative weights are compatible (in\\nprinciple, more general) with those based on the notion of evolutionary\\nstability. However, our result further establishes multiplicative weights as a\\nnonlinear programming primitive (on par with standard nonlinear programming\\nmethods) since various nonlinear optimization problems, such as finding\\nNash/Wardrop equilibria in nonatomic congestion games, which are well-known to\\nbe equipped with a convex potential function, and finding strict local maxima\\nof quadratic programming problems, are special cases of the problem of\\ncomputing evolutionarily stable states in nonlinear population games.\\n',\n",
       "  'title': u'Evolutionary stability implies asymptotic stability under multiplicative\\n  weights'},\n",
       " u'1508.00506': {'arxivid': u'1508.00506',\n",
       "  'authorsaffil': [[u'Tobias Sutter', None],\n",
       "   [u'Arnab Ganguly', None],\n",
       "   [u'Heinz Koeppl', None]],\n",
       "  'categoryterms': [u'math.OC',\n",
       "   u'cs.LG',\n",
       "   u'cs.SY',\n",
       "   u'math.PR',\n",
       "   u'math.ST',\n",
       "   u'stat.TH',\n",
       "   u'62M05, 60J60, 60H10, 49J15'],\n",
       "  'comment': u'36 pages, 2 figures, minor changes',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.00506v2',\n",
       "  'published': u'2015-08-03T18:01:13Z',\n",
       "  'summary': u'  We consider a hidden Markov model, where the signal process, given by a\\ndiffusion, is only indirectly observed through some noisy measurements. The\\narticle develops a variational method for approximating the hidden states of\\nthe signal process given the full set of observations. This, in particular,\\nleads to systematic approximations of the smoothing densities of the signal\\nprocess. The paper then demonstrates how an efficient inference scheme, based\\non this variational approach to the approximation of the hidden states, can be\\ndesigned to estimate the unknown parameters of stochastic differential\\nequations. Two examples at the end illustrate the efficacy and the accuracy of\\nthe presented method.\\n',\n",
       "  'title': u'A variational approach to path estimation and parameter inference of\\n  hidden diffusion processes'},\n",
       " u'1602.08425': {'arxivid': u'1602.08425',\n",
       "  'authorsaffil': [[u'Florian Bernard', None],\n",
       "   [u'Luis Salamanca', None],\n",
       "   [u'Johan Thunberg', None],\n",
       "   [u'Alexander Tack', None],\n",
       "   [u'Dennis Jentsch', None],\n",
       "   [u'Hans Lamecker', None],\n",
       "   [u'Stefan Zachow', None],\n",
       "   [u'Frank Hertel', None],\n",
       "   [u'Jorge Goncalves', None],\n",
       "   [u'Peter Gemmar', None]],\n",
       "  'categoryterms': [u'cs.CV', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08425v1',\n",
       "  'published': u'2016-02-26T18:30:07Z',\n",
       "  'summary': u\"  The reconstruction of an object's shape or surface from a set of 3D points is\\na common topic in materials and life sciences, computationally handled in\\ncomputer graphics. Such points usually stem from optical or tactile 3D\\ncoordinate measuring equipment. Surface reconstruction also appears in medical\\nimage analysis, e.g. in anatomy reconstruction from tomographic measurements or\\nthe alignment of intra-operative navigation and preoperative planning data. In\\ncontrast to mere 3D point clouds, medical imaging yields contextual information\\non the 3D point data that can be used to adopt prior information on the shape\\nthat is to be reconstructed from the measurements. In this work we propose to\\nuse a statistical shape model (SSM) as a prior for surface reconstruction. The\\nprior knowledge is represented by a point distribution model (PDM) that is\\nassociated with a surface mesh. Using the shape distribution that is modelled\\nby the PDM, we reformulate the problem of surface reconstruction from a\\nprobabilistic perspective based on a Gaussian Mixture Model (GMM). In order to\\ndo so, the given measurements are interpreted as samples of the GMM. By using\\nmixture components with anisotropic covariances that are oriented according to\\nthe surface normals at the PDM points, a surface-based fitting is accomplished.\\nBy estimating the parameters of the GMM in a maximum a posteriori manner, the\\nreconstruction of the surface from the given measurements is achieved.\\nExtensive experiments suggest that our proposed approach leads to superior\\nsurface reconstructions compared to Iterative Closest Point (ICP) methods.\\n\",\n",
       "  'title': u'Shape-aware Surface Reconstruction from Sparse Data'},\n",
       " u'1506.08473': {'arxivid': u'1506.08473',\n",
       "  'authorsaffil': [[u'Majid Janzamin', None],\n",
       "   [u'Hanie Sedghi', None],\n",
       "   [u'Anima Anandkumar', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE', u'stat.ML'],\n",
       "  'comment': u'The tensor decomposition analysis is expanded, and the analysis of\\n  ridge regression is added for recovering the parameters of last layer of\\n  neural network',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.08473v3',\n",
       "  'published': u'2015-06-28T23:19:49Z',\n",
       "  'summary': u'  Training neural networks is a challenging non-convex optimization problem,\\nand backpropagation or gradient descent can get stuck in spurious local optima.\\nWe propose a novel algorithm based on tensor decomposition for guaranteed\\ntraining of two-layer neural networks. We provide risk bounds for our proposed\\nmethod, with a polynomial sample complexity in the relevant parameters, such as\\ninput dimension and number of neurons. While learning arbitrary target\\nfunctions is NP-hard, we provide transparent conditions on the function and the\\ninput for learnability. Our training method is based on tensor decomposition,\\nwhich provably converges to the global optimum, under a set of mild\\nnon-degeneracy conditions. It consists of simple embarrassingly parallel linear\\nand multi-linear operations, and is competitive with standard stochastic\\ngradient descent (SGD), in terms of computational complexity. Thus, we propose\\na computationally efficient method with guaranteed risk bounds for training\\nneural networks with one hidden layer.\\n',\n",
       "  'title': u'Beating the Perils of Non-Convexity: Guaranteed Training of Neural\\n  Networks using Tensor Methods'},\n",
       " u'1602.07119': {'arxivid': u'1602.07119',\n",
       "  'authorsaffil': [[u'Pascal Mettes', None],\n",
       "   [u'Dennis C. Koelma', None],\n",
       "   [u'Cees G. M. Snoek', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1145/2911996.2912036',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07119v1',\n",
       "  'published': u'2016-02-23T11:12:55Z',\n",
       "  'summary': u'  This paper strives for video event detection using a representation learned\\nfrom deep convolutional neural networks. Different from the leading approaches,\\nwho all learn from the 1,000 classes defined in the ImageNet Large Scale Visual\\nRecognition Challenge, we investigate how to leverage the complete ImageNet\\nhierarchy for pre-training deep networks. To deal with the problems of\\nover-specific classes and classes with few images, we introduce a bottom-up and\\ntop-down approach for reorganization of the ImageNet hierarchy based on all its\\n21,814 classes and more than 14 million images. Experiments on the TRECVID\\nMultimedia Event Detection 2013 and 2015 datasets show that video\\nrepresentations derived from the layers of a deep neural network pre-trained\\nwith our reorganized hierarchy i) improves over standard pre-training, ii) is\\ncomplementary among different reorganizations, iii) maintains the benefits of\\nfusion with other modalities, and iv) leads to state-of-the-art event detection\\nresults. The reorganized hierarchies and their derived Caffe models are\\npublicly available at http://tinyurl.com/imagenetshuffle.\\n',\n",
       "  'title': u'The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection'},\n",
       " u'1601.07483': {'arxivid': u'1601.07483',\n",
       "  'authorsaffil': [[u'Shashank Shekhar', None], [u'Deepak Khemani', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'AAAI format, (9 pages), (1 figure), (4 tables)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07483v3',\n",
       "  'published': u'2016-01-27T18:23:24Z',\n",
       "  'summary': u'  In recent years, the planning community has observed that techniques for\\nlearning heuristic functions have yielded improvements in performance. One\\napproach is to use offline learning to learn predictive models from existing\\nheuristics in a domain dependent manner. These learned models are deployed as\\nnew heuristic functions. The learned models can in turn be tuned online using a\\ndomain independent error correction approach to further enhance their\\ninformativeness. The online tuning approach is domain independent but instance\\nspecific, and contributes to improved performance for individual instances as\\nplanning proceeds. Consequently it is more effective in larger problems.\\n  In this paper, we mention two approaches applicable in Partial Order Causal\\nLink (POCL) Planning that is also known as Plan Space Planning. First, we\\nendeavor to enhance the performance of a POCL planner by giving an algorithm\\nfor supervised learning. Second, we then discuss an online error minimization\\napproach in POCL framework to minimize the step-error associated with the\\noffline learned models thus enhancing their informativeness. Our evaluation\\nshows that the learning approaches scale up the performance of the planner over\\nstandard benchmarks, specially for larger problems.\\n',\n",
       "  'title': u'Learning and Tuning Meta-heuristics in Plan Space Planning'},\n",
       " u'1602.03105': {'arxivid': u'1602.03105',\n",
       "  'authorsaffil': [[u'Branislav Kveton', None],\n",
       "   [u'Hung Bui', None],\n",
       "   [u'Mohammad Ghavamzadeh', None],\n",
       "   [u'Georgios Theocharous', None],\n",
       "   [u'S. Muthukrishnan', None],\n",
       "   [u'Siqi Sun', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03105v1',\n",
       "  'published': u'2016-02-09T18:07:51Z',\n",
       "  'summary': u'  Structured high-cardinality data arises in many domains and poses a major\\nchallenge for both modeling and inference, which is beyond current graphical\\nmodel frameworks. We view these data as a stream $(x^{(t)})_{t = 1}^n$ of $n$\\nobservations from an unknown distribution $P$, where $x^{(t)} \\\\in [M]^K$ is a\\n$K$-dimensional vector and $M$ is the cardinality of its entries, which is very\\nlarge. Suppose that the graphical model $\\\\mathcal{G}$ of $P$ is known, and let\\n$\\\\bar{P}$ be the maximum-likelihood estimate (MLE) of $P$ from $(x^{(t)})_{t =\\n1}^n$ conditioned on $\\\\mathcal{G}$. In this work, we design and analyze\\nalgorithms that approximate $\\\\bar{P}$ with $\\\\hat{P}$, such that $\\\\hat{P}(x)\\n\\\\approx \\\\bar{P}(x)$ for any $x \\\\in [M]^K$ with a high probability, and\\ncrucially in the space independent of $M$. The key idea of our approximations\\nis to use the structure of $\\\\mathcal{G}$ and approximately estimate its factors\\nby \"sketches\". The sketches hash high-cardinality variables using random\\nprojections. Our approximations are computationally and space efficient, being\\nindependent of $M$. Our error bounds are multiplicative and provably improve\\nupon those of the count-min (CM) sketch, a state-of-the-art approach to\\nestimating the frequency of values in a stream, in a class of naive Bayes\\nmodels. We evaluate our algorithms on synthetic and real-world problems, and\\nreport an order of magnitude improvements over the CM sketch.\\n',\n",
       "  'title': u'Graphical Model Sketch'},\n",
       " u'1603.01633': {'arxivid': u'1603.01633',\n",
       "  'authorsaffil': [[u'Ulugbek S. Kamilov', None],\n",
       "   [u'Petros T. Boufounos', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01633v1',\n",
       "  'published': u'2016-03-04T21:16:21Z',\n",
       "  'summary': u'  Spatial resolution of depth sensors is often significantly lower compared to\\nthat of conventional optical cameras. Recent work has explored the idea of\\nimproving the resolution of depth using higher resolution intensity as a side\\ninformation. In this paper, we demonstrate that further incorporating temporal\\ninformation in videos can significantly improve the results. In particular, we\\npropose a novel approach that improves depth resolution, exploiting the\\nspace-time redundancy in the depth and intensity using motion-adaptive low-rank\\nregularization. Experiments confirm that the proposed approach substantially\\nimproves the quality of the estimated high-resolution depth. Our approach can\\nbe a first component in systems using vision techniques that rely on high\\nresolution depth information.\\n',\n",
       "  'title': u'Depth Superresolution using Motion Adaptive Regularization'},\n",
       " u'1603.01520': {'arxivid': u'1603.01520',\n",
       "  'authorsaffil': [[u'Daniel Rubio Bonilla', None],\n",
       "   [u'Colin W. Glass', None],\n",
       "   [u'Jan Kuper', None]],\n",
       "  'categoryterms': [u'cs.PL', u'cs.CL', u'B.1.4'],\n",
       "  'comment': u'Part of the Program Transformation for Programmability in\\n  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March\\n  2016, 7 pages, LaTeX, 4 PNG figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01520v3',\n",
       "  'published': u'2016-03-04T16:13:24Z',\n",
       "  'summary': u'  In this paper we discuss how semantic annotations can be used to introduce\\nmathematical algorithmic information of the underlying imperative code to\\nenable compilers to produce code transformations that will enable better\\nperformance. By using this approaches not only good performance is achieved,\\nbut also better programmability, maintainability and portability across\\ndifferent hardware architectures. To exemplify this we will use polynomial\\nequations of different degrees.\\n',\n",
       "  'title': u'Optimized Polynomial Evaluation with Semantic Annotations'},\n",
       " u'1502.03121': {'arxivid': u'1502.03121',\n",
       "  'authorsaffil': [[u'Qi Wei', None],\n",
       "   [u'Nicolas Dobigeon', None],\n",
       "   [u'Jean-Yves Tourneret', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1109/TIP.2015.2458572',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.03121v1',\n",
       "  'published': u'2015-02-10T21:18:54Z',\n",
       "  'summary': u'  This paper proposes a fast multi-band image fusion algorithm, which combines\\na high-spatial low-spectral resolution image and a low-spatial high-spectral\\nresolution image. The well admitted forward model is explored to form the\\nlikelihoods of the observations. Maximizing the likelihoods leads to solving a\\nSylvester equation. By exploiting the properties of the circulant and\\ndownsampling matrices associated with the fusion problem, a closed-form\\nsolution for the corresponding Sylvester equation is obtained explicitly,\\ngetting rid of any iterative update step. Coupled with the alternating\\ndirection method of multipliers and the block coordinate descent method, the\\nproposed algorithm can be easily generalized to incorporate prior information\\nfor the fusion problem, allowing a Bayesian estimator. Simulation results show\\nthat the proposed algorithm achieves the same performance as existing\\nalgorithms with the advantage of significantly decreasing the computational\\ncomplexity of these algorithms.\\n',\n",
       "  'title': u'Fast Fusion of Multi-Band Images Based on Solving a Sylvester Equation'},\n",
       " u'1603.01185': {'arxivid': u'1603.01185',\n",
       "  'authorsaffil': [[u'Larry Bull', None]],\n",
       "  'categoryterms': [u'q-bio.BM', u'cs.NE', u'q-bio.MN'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1505.01980,\\n  arXiv:1306.4793, arXiv:1303.7220, arXiv:1310.5568',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01185v2',\n",
       "  'published': u'2016-03-02T14:31:15Z',\n",
       "  'summary': u'  The time taken for gene expression varies not least because proteins vary in\\nlength considerably. This paper uses an abstract, tuneable Boolean regulatory\\nnetwork model to explore gene expression time variation. In particular, it is\\nshown how non-uniform expression times can emerge under certain conditions\\nthrough simulated evolution. That is, gene expression time variance appears\\nbeneficial in the shaping of the dynamical behaviour of the regulatory network\\nwithout explicit consideration of protein function.\\n',\n",
       "  'title': u'Evolving Boolean Regulatory Networks with Variable Gene Expression Times'},\n",
       " u'1602.02617': {'arxivid': u'1602.02617',\n",
       "  'authorsaffil': [[u'Zhun-Ga Liu', u'Palaiseau'],\n",
       "   [u'Quan Pan', u'Palaiseau'],\n",
       "   [u'Jean Dezert', u'Palaiseau'],\n",
       "   [u'Arnaud Martin', u'DRUID']],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1016/j.patcog.2015.10.001',\n",
       "  'journalref': u'Pattern Recognition, Elsevier, 2016, 52',\n",
       "  'link': u'http://arxiv.org/abs/1602.02617v1',\n",
       "  'published': u'2016-02-08T15:52:08Z',\n",
       "  'summary': u'  In classification of incomplete pattern, the missing values can either play a\\ncrucial role in the class determination, or have only little influence (or\\neventually none) on the classification results according to the context. We\\npropose a credal classification method for incomplete pattern with adaptive\\nimputation of missing values based on belief function theory. At first, we try\\nto classify the object (incomplete pattern) based only on the available\\nattribute values. As underlying principle, we assume that the missing\\ninformation is not crucial for the classification if a specific class for the\\nobject can be found using only the available information. In this case, the\\nobject is committed to this particular class. However, if the object cannot be\\nclassified without ambiguity, it means that the missing values play a main role\\nfor achieving an accurate classification. In this case, the missing values will\\nbe imputed based on the K-nearest neighbor (K-NN) and self-organizing map (SOM)\\ntechniques, and the edited pattern with the imputation is then classified. The\\n(original or edited) pattern is respectively classified according to each\\ntraining class, and the classification results represented by basic belief\\nassignments are fused with proper combination rules for making the credal\\nclassification. The object is allowed to belong with different masses of belief\\nto the specific classes and meta-classes (which are particular disjunctions of\\nseveral single classes). The credal classification captures well the\\nuncertainty and imprecision of classification, and reduces effectively the rate\\nof misclassifications thanks to the introduction of meta-classes. The\\neffectiveness of the proposed method with respect to other classical methods is\\ndemonstrated based on several experiments using artificial and real data sets.\\n',\n",
       "  'title': u'Adaptive imputation of missing values for incomplete pattern\\n  classification'},\n",
       " u'1506.05011': {'arxivid': u'1506.05011',\n",
       "  'authorsaffil': [[u'Theofanis Karaletsos', None],\n",
       "   [u'Serge Belongie', None],\n",
       "   [u'Gunnar R\\xe4tsch', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'16 pages, publishes in ICLR 16',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.05011v4',\n",
       "  'published': u'2015-06-16T15:54:59Z',\n",
       "  'summary': u'  Representation learning systems typically rely on massive amounts of labeled\\ndata in order to be trained to high accuracy. Recently, high-dimensional\\nparametric models like neural networks have succeeded in building rich\\nrepresentations using either compressive, reconstructive or supervised\\ncriteria. However, the semantic structure inherent in observations is\\noftentimes lost in the process. Human perception excels at understanding\\nsemantics but cannot always be expressed in terms of labels. Thus,\\n\\\\emph{oracles} or \\\\emph{human-in-the-loop systems}, for example crowdsourcing,\\nare often employed to generate similarity constraints using an implicit\\nsimilarity function encoded in human perception. In this work we propose to\\ncombine \\\\emph{generative unsupervised feature learning} with a\\n\\\\emph{probabilistic treatment of oracle information like triplets} in order to\\ntransfer implicit privileged oracle knowledge into explicit nonlinear Bayesian\\nlatent factor models of the observations. We use a fast variational algorithm\\nto learn the joint model and demonstrate applicability to a well-known image\\ndataset. We show how implicit triplet information can provide rich information\\nto learn representations that outperform previous metric learning approaches as\\nwell as generative models without this side-information in a variety of\\npredictive tasks. In addition, we illustrate that the proposed approach\\ncompartmentalizes the latent spaces semantically which allows interpretation of\\nthe latent variables.\\n',\n",
       "  'title': u'Bayesian representation learning with oracle constraints'},\n",
       " u'1403.6348': {'arxivid': u'1403.6348',\n",
       "  'authorsaffil': [[u'Blaz Sovdat', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'I.2.6'],\n",
       "  'comment': u'Added directions future work; more consistent notation; fixed the\\n  errors in the updating algorithms for entropy; fixed an error in the\\n  statement of theorem 5; added two references to related work; fixed a few\\n  typos',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1403.6348v5',\n",
       "  'published': u'2014-03-25T14:07:21Z',\n",
       "  'summary': u'  Despite growing interest in data stream mining the most successful\\nincremental learners, such as VFDT, still use periodic recomputation to update\\nattribute information gains and Gini indices. This note provides simple\\nincremental formulas and algorithms for computing entropy and Gini index from\\ntime-changing data streams.\\n',\n",
       "  'title': u'Updating Formulas and Algorithms for Computing Entropy and Gini Index\\n  from Time-Changing Data Streams'},\n",
       " u'1604.00466': {'arxivid': u'1604.00466',\n",
       "  'authorsaffil': [[u'Mohamed Elhoseiny', None],\n",
       "   [u'Scott Cohen', None],\n",
       "   [u'Walter Chang', None],\n",
       "   [u'Brian Price', None],\n",
       "   [u'Ahmed Elgammal', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00466v3',\n",
       "  'published': u'2016-04-02T06:35:45Z',\n",
       "  'summary': u'  Motivated by the application of fact-level image understanding, we present an\\nautomatic method for data collection of structured visual facts from images\\nwith captions. Example structured facts include attributed objects (e.g.,\\n<flower, red>), actions (e.g., <baby, smile>), interactions (e.g., <man,\\nwalking, dog>), and positional information (e.g., <vase, on, table>). The\\ncollected annotations are in the form of fact-image pairs (e.g.,<man, walking,\\ndog> and an image region containing this fact). With a language approach, the\\nproposed method is able to collect hundreds of thousands of visual fact\\nannotations with accuracy of 83% according to human judgment. Our method\\nautomatically collected more than 380,000 visual fact annotations and more than\\n110,000 unique visual facts from images with captions and localized them in\\nimages in less than one day of processing time on standard CPU platforms.\\n',\n",
       "  'title': u'Automatic Annotation of Structured Facts in Images'},\n",
       " u'1604.00462': {'arxivid': u'1604.00462',\n",
       "  'authorsaffil': [[u'Wenlian Lu', None],\n",
       "   [u'Ren Zheng', None],\n",
       "   [u'Tianping Chen', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.SY', u'math.CA'],\n",
       "  'comment': u'13 pages, 2 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00462v1',\n",
       "  'published': u'2016-04-02T05:04:12Z',\n",
       "  'summary': u'  In this paper, we discuss the outer-synchronization of the asymmetrically\\nconnected recurrent time-varying neural networks. By both centralized and\\ndecentralized discretization data sampling principles, we derive several\\nsufficient conditions based on diverse vector norms that guarantee that any two\\ntrajectories from different initial values of the identical neural network\\nsystem converge together. The lower bounds of the common time intervals between\\ndata samples in centralized and decentralized principles are proved to be\\npositive, which guarantees exclusion of Zeno behavior. A numerical example is\\nprovided to illustrate the efficiency of the theoretical results.\\n',\n",
       "  'title': u'Centralized and Decentralized Global Outer-synchronization of Asymmetric\\n  Recurrent Time-varying Neural Network by Data-sampling'},\n",
       " u'1604.00461': {'arxivid': u'1604.00461',\n",
       "  'authorsaffil': [[u'Mo Yu', None],\n",
       "   [u'Mark Dredze', None],\n",
       "   [u'Raman Arora', None],\n",
       "   [u'Matthew Gormley', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'Accepted by NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00461v1',\n",
       "  'published': u'2016-04-02T04:59:21Z',\n",
       "  'summary': u'  Modern NLP models rely heavily on engineered features, which often combine\\nword and contextual information into complex lexical features. Such combination\\nresults in large numbers of features, which can lead to over-fitting. We\\npresent a new model that represents complex lexical features---comprised of\\nparts for words, contextual information and labels---in a tensor that captures\\nconjunction information among these parts. We apply low-rank tensor\\napproximations to the corresponding parameter tensors to reduce the parameter\\nspace and improve prediction speed. Furthermore, we investigate two methods for\\nhandling features that include $n$-grams of mixed lengths. Our model achieves\\nstate-of-the-art results on tasks in relation extraction, PP-attachment, and\\npreposition disambiguation.\\n',\n",
       "  'title': u'Embedding Lexical Features via Low-Rank Tensors'},\n",
       " u'1508.06904': {'arxivid': u'1508.06904',\n",
       "  'authorsaffil': [[u'Markus Thom', None], [u'Franz Gritschneder', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.06904v2',\n",
       "  'published': u'2015-08-27T15:50:26Z',\n",
       "  'summary': u'  We introduce and analyze a rigorous formulation of the dynamics of a signal\\nprocessing scheme aimed at exact dense signal scanning. Related methods\\nproposed in the recent past lack a satisfactory analysis whether they actually\\nfulfill any exactness constraints. We improve on this through an exact\\ncharacterization of the requirements for a sound sliding window approach. The\\ntools developed in this paper are especially beneficial if Convolutional Neural\\nNetworks are employed, but can also be used as a more general framework to\\nvalidate related approaches to signal scanning. The contributed theory helps to\\neliminate redundant computations and renders special case treatment\\nunnecessary, resulting in a dramatic boost in efficiency particularly on\\nmassively parallel processors. This is demonstrated both theoretically in a\\ncomputational complexity analysis and empirically on modern parallel\\nprocessors.\\n',\n",
       "  'title': u'Rapid Exact Signal Scanning with Deep Convolutional Neural Networks'},\n",
       " u'1603.08907': {'arxivid': u'1603.08907',\n",
       "  'authorsaffil': [[u'Punarjay Chakravarty', None],\n",
       "   [u'Tinne Tuytelaars', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'16 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08907v1',\n",
       "  'published': u'2016-03-29T19:47:46Z',\n",
       "  'summary': u'  In this paper, we show how to use audio to supervise the learning of active\\nspeaker detection in video. Voice Activity Detection (VAD) guides the learning\\nof the vision-based classifier in a weakly supervised manner. The classifier\\nuses spatio-temporal features to encode upper body motion - facial expressions\\nand gesticulations associated with speaking. We further improve a generic model\\nfor active speaker detection by learning person specific models. Finally, we\\ndemonstrate the online adaptation of generic models learnt on one dataset, to\\npreviously unseen people in a new dataset, again using audio (VAD) for weak\\nsupervision. The use of temporal continuity overcomes the lack of clean\\ntraining data. We are the first to present an active speaker detection system\\nthat learns on one audio-visual dataset and automatically adapts to speakers in\\na new dataset. This work can be seen as an example of how the availability of\\nmulti-modal data allows us to learn a model without the need for supervision,\\nby transferring knowledge from one modality to another.\\n',\n",
       "  'title': u'Cross-modal Supervision for Learning Active Speaker Detection in Video'},\n",
       " u'1602.03585': {'arxivid': u'1602.03585',\n",
       "  'authorsaffil': [[u'Yangmuzi Zhang', None],\n",
       "   [u'Zhuolin Jiang', None],\n",
       "   [u'Xi Chen', None],\n",
       "   [u'Larry S. Davis', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03585v1',\n",
       "  'published': u'2016-02-11T00:50:17Z',\n",
       "  'summary': u'  A multi-scale greedy-based object proposal generation approach is presented.\\nBased on the multi-scale nature of objects in images, our approach is built on\\ntop of a hierarchical segmentation. We first identify the representative and\\ndiverse exemplar clusters within each scale by using a diversity ranking\\nalgorithm. Object proposals are obtained by selecting a subset from the\\nmulti-scale segment pool via maximizing a submodular objective function, which\\nconsists of a weighted coverage term, a single-scale diversity term and a\\nmulti-scale reward term. The weighted coverage term forces the selected set of\\nobject proposals to be representative and compact; the single-scale diversity\\nterm encourages choosing segments from different exemplar clusters so that they\\nwill cover as many object patterns as possible; the multi-scale reward term\\nencourages the selected proposals to be discriminative and selected from\\nmultiple layers generated by the hierarchical image segmentation. The\\nexperimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012\\nsegmentation dataset demonstrate the accuracy and efficiency of our object\\nproposal model. Additionally, we validate our object proposals in simultaneous\\nsegmentation and detection and outperform the state-of-art performance.\\n',\n",
       "  'title': u'Generating Discriminative Object Proposals via Submodular Ranking'},\n",
       " u'1603.06430': {'arxivid': u'1603.06430',\n",
       "  'authorsaffil': [[u'Seonwoo Min', None],\n",
       "   [u'Byunghan Lee', None],\n",
       "   [u'Sungroh Yoon', None]],\n",
       "  'categoryterms': [u'cs.LG', u'q-bio.GN'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06430v4',\n",
       "  'published': u'2016-03-21T13:55:02Z',\n",
       "  'summary': u'  As we are living in the era of big data, transforming biomedical big data\\ninto valuable knowledge has been one of the most important problems in\\nbioinformatics. At the same time, deep learning has advanced rapidly since\\nearly 2000s and is recently showing a state-of-the-art performance in various\\nfields. So naturally, applying deep learning in bioinformatics to gain insights\\nfrom data is under the spotlight of both the academia and the industry. This\\narticle reviews some research of deep learning in bioinformatics. To provide a\\nbig picture, we categorized the research by both bioinformatics domains (i.e.,\\nomics, biomedical imaging, biomedical signal processing) and deep learning\\narchitectures (i.e., deep neural network, convolutional neural network,\\nrecurrent neural network, modified neural network) as well as present brief\\ndescriptions of each work. Additionally, we introduce a few issues of deep\\nlearning in bioinformatics such as problems of class imbalance data and suggest\\nfuture research directions such as multimodal deep learning. We believe that\\nthis paper could provide valuable insights and be a starting point for\\nresearchers to apply deep learning in their bioinformatics studies.\\n',\n",
       "  'title': u'Deep Learning in Bioinformatics'},\n",
       " u'1603.06433': {'arxivid': u'1603.06433',\n",
       "  'authorsaffil': [[u'Wolfgang Konen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'8 pages, 1 figure',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06433v1',\n",
       "  'published': u'2016-03-21T14:23:00Z',\n",
       "  'summary': u\"  This technical report describes an improved image mosaicking algorithm. It is\\nbased on Jain's logarithmic search algorithm [Jain 1981] which is coupled to\\nthe method of Kourogi (1999} for matching images in a video sequence.\\nLogarithmic search has a better invariance against illumination changes than\\nthe original optical-flow-based method of Kourogi.\\n\",\n",
       "  'title': u'Illumination-invariant image mosaic calculation based on logarithmic\\n  search'},\n",
       " u'1603.06432': {'arxivid': u'1603.06432',\n",
       "  'authorsaffil': [[u'Artem Rozantsev', None],\n",
       "   [u'Mathieu Salzmann', None],\n",
       "   [u'Pascal Fua', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06432v1',\n",
       "  'published': u'2016-03-21T14:20:41Z',\n",
       "  'summary': u'  Deep Neural Networks have demonstrated outstanding performance in many\\nComputer Vision tasks but typically require large amounts of labeled training\\ndata to achieve it. This is a serious limitation when such data is difficult to\\nobtain. In traditional Machine Learning, Domain Adaptation is an approach to\\novercoming this problem by leveraging annotated data from a source domain, in\\nwhich it is abundant, to train a classifier to operate in a target domain, in\\nwhich labeled data is either sparse or even lacking altogether. In the Deep\\nLearning case, most existing methods use the same architecture with the same\\nweights for both source and target data, which essentially amounts to learning\\ndomain invariant features. Here, we show that it is more effective to\\nexplicitly model the shift from one domain to the other. To this end, we\\nintroduce a two-stream architecture, one of which operates in the source domain\\nand the other in the target domain. In contrast to other approaches, the\\nweights in corresponding layers are related but not shared to account for\\ndifferences between the two domains. We demonstrate that this both yields\\nhigher accuracy than state-of-the-art methods on several object recognition and\\ndetection tasks and consistently outperforms networks with shared weights in\\nboth supervised and unsupervised settings.\\n',\n",
       "  'title': u'Beyond Sharing Weights for Deep Domain Adaptation'},\n",
       " u'1512.09170': {'arxivid': u'1512.09170',\n",
       "  'authorsaffil': [[u'Vitaly Feldman', None],\n",
       "   [u'Cristobal Guzman', None],\n",
       "   [u'Santosh Vempala', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DS'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.09170v1',\n",
       "  'published': u'2015-12-30T22:29:23Z',\n",
       "  'summary': u'  Stochastic convex optimization, where the objective is the expectation of a\\nrandom convex function, is an important and widely used method with numerous\\napplications in machine learning, statistics, operations research and other\\nareas. We study the complexity of stochastic convex optimization given only\\nstatistical query (SQ) access to the objective function. We show that\\nwell-known and popular methods, including first-order iterative methods and\\npolynomial-time methods, can be implemented using only statistical queries. For\\nmany cases of interest we derive nearly matching upper and lower bounds on the\\nestimation (sample) complexity including linear optimization in the most\\ngeneral setting. We then present several consequences for machine learning,\\ndifferential privacy and proving concrete lower bounds on the power of convex\\noptimization based methods.\\n  A new technical ingredient of our work is SQ algorithms for estimating the\\nmean vector of a distribution over vectors in $\\\\mathbb{R}^d$ with optimal\\nestimation complexity. This is a natural problem and we show that our solutions\\ncan be used to get substantially improved SQ versions of Perceptron and other\\nonline algorithms for learning halfspaces.\\n',\n",
       "  'title': u'Statistical Query Algorithms for Stochastic Convex Optimization'},\n",
       " u'1604.03286': {'arxivid': u'1604.03286',\n",
       "  'authorsaffil': [[u'Th\\xe9odore Bluche', None],\n",
       "   [u'J\\xe9r\\xf4me Louradour', None],\n",
       "   [u'Ronaldo Messina', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03286v2',\n",
       "  'published': u'2016-04-12T08:11:20Z',\n",
       "  'summary': u'  We present an attention-based model for end-to-end handwriting recognition.\\nOur system does not require any segmentation of the input paragraph. The model\\nis inspired by the differentiable attention models presented recently for\\nspeech recognition, image captioning or translation. The main difference is the\\ncovert and overt attention, implemented as a multi-dimensional LSTM network.\\nOur principal contribution towards handwriting recognition lies in the\\nautomatic transcription without a prior segmentation into lines, which was\\ncrucial in previous approaches. To the best of our knowledge this is the first\\nsuccessful attempt of end-to-end multi-line handwriting recognition. We carried\\nout experiments on the well-known IAM Database. The results are encouraging and\\nbring hope to perform full paragraph transcription in the near future.\\n',\n",
       "  'title': u'Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with\\n  MDLSTM Attention'},\n",
       " u'1512.09176': {'arxivid': u'1512.09176',\n",
       "  'authorsaffil': [[u'Jie Xu', None],\n",
       "   [u'Tianwei Xing', None],\n",
       "   [u'Mihaela van der Schaar', None]],\n",
       "  'categoryterms': [u'cs.CY', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.09176v2',\n",
       "  'published': u'2015-12-30T22:57:13Z',\n",
       "  'summary': u'  Given the variability in student learning it is becoming increasingly\\nimportant to tailor courses as well as course sequences to student needs. This\\npaper presents a systematic methodology for offering personalized course\\nsequence recommendations to students. First, a forward-search\\nbackward-induction algorithm is developed that can optimally select course\\nsequences to decrease the time required for a student to graduate. The\\nalgorithm accounts for prerequisite requirements (typically present in higher\\nlevel education) and course availability. Second, using the tools of\\nmulti-armed bandits, an algorithm is developed that can optimally recommend a\\ncourse sequence that both reduces the time to graduate while also increasing\\nthe overall GPA of the student. The algorithm dynamically learns how students\\nwith different contextual backgrounds perform for given course sequences and\\nthen recommends an optimal course sequence for new students. Using real-world\\nstudent data from the UCLA Mechanical and Aerospace Engineering department, we\\nillustrate how the proposed algorithms outperform other methods that do not\\ninclude student contextual information when making course sequence\\nrecommendations.\\n',\n",
       "  'title': u'Personalized Course Sequence Recommendations'},\n",
       " u'1602.07807': {'arxivid': u'1602.07807',\n",
       "  'authorsaffil': [[u'Michael Bloodgood', None], [u'Benjamin Strauss', None]],\n",
       "  'categoryterms': [u'cs.DB',\n",
       "   u'cs.CL',\n",
       "   u'stat.ML',\n",
       "   u'I.5.1; I.5.4; G.3; I.2.7; I.2.6'],\n",
       "  'comment': u'8 pages, 4 figures, 5 tables; published in Proceedings of the 2016\\n  IEEE Tenth International Conference on Semantic Computing (ICSC), Laguna\\n  Hills, CA, USA, pages 79-86, February 2016',\n",
       "  'doi': u'10.1109/ICSC.2016.38',\n",
       "  'journalref': u'In Proceedings of the 2016 IEEE Tenth International Conference on\\n  Semantic Computing (ICSC), pages 79-86, Laguna Hills, CA, USA, February 2016.\\n  IEEE',\n",
       "  'link': u'http://arxiv.org/abs/1602.07807v2',\n",
       "  'published': u'2016-02-25T05:49:36Z',\n",
       "  'summary': u\"  Many important forms of data are stored digitally in XML format. Errors can\\noccur in the textual content of the data in the fields of the XML. Fixing these\\nerrors manually is time-consuming and expensive, especially for large amounts\\nof data. There is increasing interest in the research, development, and use of\\nautomated techniques for assisting with data cleaning. Electronic dictionaries\\nare an important form of data frequently stored in XML format that frequently\\nhave errors introduced through a mixture of manual typographical entry errors\\nand optical character recognition errors. In this paper we describe methods for\\nflagging statistical anomalies as likely errors in electronic dictionaries\\nstored in XML format. We describe six systems based on different sources of\\ninformation. The systems detect errors using various signals in the data\\nincluding uncommon characters, text length, character-based language models,\\nword-based language models, tied-field length ratios, and tied-field\\ntransliteration models. Four of the systems detect errors based on expectations\\nautomatically inferred from content within elements of a single field type. We\\ncall these single-field systems. Two of the systems detect errors based on\\ncorrespondence expectations automatically inferred from content within elements\\nof multiple related field types. We call these tied-field systems. For each\\nsystem, we provide an intuitive analysis of the type of error that it is\\nsuccessful at detecting. Finally, we describe two larger-scale evaluations\\nusing crowdsourcing with Amazon's Mechanical Turk platform and using the\\nannotations of a domain expert. The evaluations consistently show that the\\nsystems are useful for improving the efficiency with which errors in XML\\nelectronic dictionaries can be detected.\\n\",\n",
       "  'title': u'Data Cleaning for XML Electronic Dictionaries via Statistical Anomaly\\n  Detection'},\n",
       " u'1603.04319': {'arxivid': u'1603.04319',\n",
       "  'authorsaffil': [[u'Jalal Etesami', None],\n",
       "   [u'Negar Kiyavash', None],\n",
       "   [u'Kun Zhang', None],\n",
       "   [u'Kushagra Singhal', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04319v1',\n",
       "  'published': u'2016-03-14T16:08:26Z',\n",
       "  'summary': u'  Learning the influence structure of multiple time series data is of great\\ninterest to many disciplines. This paper studies the problem of recovering the\\ncausal structure in network of multivariate linear Hawkes processes. In such\\nprocesses, the occurrence of an event in one process affects the probability of\\noccurrence of new events in some other processes. Thus, a natural notion of\\ncausality exists between such processes captured by the support of the\\nexcitation matrix. We show that the resulting causal influence network is\\nequivalent to the Directed Information graph (DIG) of the processes, which\\nencodes the causal factorization of the joint distribution of the processes.\\nFurthermore, we present an algorithm for learning the support of excitation\\nmatrix (or equivalently the DIG). The performance of the algorithm is evaluated\\non synthesized multivariate Hawkes networks as well as a stock market and\\nMemeTracker real-world dataset.\\n',\n",
       "  'title': u'Learning Network of Multivariate Hawkes Processes: A Time Series\\n  Approach'},\n",
       " u'1602.07803': {'arxivid': u'1602.07803',\n",
       "  'authorsaffil': [[u'Md. Masudul Haque', None],\n",
       "   [u'Md. Tarek Habib', None],\n",
       "   [u'Md. Mokhlesur Rahman', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'in International Journal in Foundations of Computer Science &\\n  Technology (IJFCST) Vol.5, No.6, November 2015',\n",
       "  'doi': u'10.5121/ijfcst.2015.5607',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07803v1',\n",
       "  'published': u'2016-02-25T05:35:16Z',\n",
       "  'summary': u'  Word completion and word prediction are two important phenomena in typing\\nthat benefit users who type using keyboard or other similar devices. They can\\nhave profound impact on the typing of disable people. Our work is based on word\\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\\nauto completing a sentence by predicting a correct word in a sentence which\\nsaves time and keystrokes of typing and also reduces misspelling. We use large\\ndata corpus of Bangla language of different word types to predict correct word\\nwith the accuracy as much as possible. We have found promising results. We hope\\nthat our work will impact on the baseline for automated Bangla typing.\\n',\n",
       "  'title': u'Automated Word Prediction in Bangla Language Using Stochastic Language\\n  Models'},\n",
       " u'1602.03368': {'arxivid': u'1602.03368',\n",
       "  'authorsaffil': [[u'Aydin Demircioglu', None],\n",
       "   [u'Daniel Horn', None],\n",
       "   [u'Tobias Glasmachers', None],\n",
       "   [u'Bernd Bischl', None],\n",
       "   [u'Claus Weihs', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03368v1',\n",
       "  'published': u'2016-02-10T13:34:30Z',\n",
       "  'summary': u'  Kernelized Support Vector Machines (SVMs) are among the best performing\\nsupervised learning methods. But for optimal predictive performance,\\ntime-consuming parameter tuning is crucial, which impedes application. To\\ntackle this problem, the classic model selection procedure based on grid-search\\nand cross-validation was refined, e.g. by data subsampling and direct search\\nheuristics. Here we focus on a different aspect, the stopping criterion for SVM\\ntraining. We show that by limiting the training time given to the SVM solver\\nduring parameter tuning we can reduce model selection times by an order of\\nmagnitude.\\n',\n",
       "  'title': u'Fast model selection by limiting SVM training times'},\n",
       " u'1504.03083': {'arxivid': u'1504.03083',\n",
       "  'authorsaffil': [[u'Xiaodong He', None],\n",
       "   [u'Rupesh Srivastava', None],\n",
       "   [u'Jianfeng Gao', None],\n",
       "   [u'Li Deng', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u\"This is a previous tech report of a part of the work of\\n  arXiv:1411.4952. In order to avoid confusion, we'd like to withdraw this\\n  report from arXiv\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.03083v2',\n",
       "  'published': u'2015-04-13T07:36:08Z',\n",
       "  'summary': u'  This technical report provides extra details of the deep multimodal\\nsimilarity model (DMSM) which was proposed in (Fang et al. 2015,\\narXiv:1411.4952). The model is trained via maximizing global semantic\\nsimilarity between images and their captions in natural language using the\\npublic Microsoft COCO database, which consists of a large set of images and\\ntheir corresponding captions. The learned representations attempt to capture\\nthe combination of various visual concepts and cues.\\n',\n",
       "  'title': u'Joint Learning of Distributed Representations for Images and Texts'},\n",
       " u'1603.00106': {'arxivid': u'1603.00106',\n",
       "  'authorsaffil': [[u'Saurav Ghosh', None],\n",
       "   [u'Prithwish Chakraborty', None],\n",
       "   [u'Emily Cohn', None],\n",
       "   [u'John S. Brownstein', None],\n",
       "   [u'Naren Ramakrishnan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL', u'stat.ML'],\n",
       "  'comment': u'this paper has been submitted to a conference',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00106v2',\n",
       "  'published': u'2016-03-01T00:45:18Z',\n",
       "  'summary': u'  Traditional disease surveillance can be augmented with a wide variety of\\nreal-time sources such as, news and social media. However, these sources are in\\ngeneral unstructured and, construction of surveillance tools such as\\ntaxonomical correlations and trace mapping involves considerable human\\nsupervision. In this paper, we motivate a disease vocabulary driven word2vec\\nmodel (Dis2Vec) to model diseases and constituent attributes as word embeddings\\nfrom the HealthMap news corpus. We use these word embeddings to automatically\\ncreate disease taxonomies and evaluate our model against corresponding human\\nannotated taxonomies. We compare our model accuracies against several\\nstate-of-the art word2vec methods. Our results demonstrate that Dis2Vec\\noutperforms traditional distributed vector representations in its ability to\\nfaithfully capture taxonomical attributes across different class of diseases\\nsuch as endemic, emerging and rare.\\n',\n",
       "  'title': u'Characterizing Diseases from Unstructured Text: A Vocabulary Driven\\n  Word2vec Approach'},\n",
       " u'1602.07064': {'arxivid': u'1602.07064',\n",
       "  'authorsaffil': [[u'Jorge Martinez-Gil', None]],\n",
       "  'categoryterms': [u'cs.DB', u'cs.AI', u'68T30'],\n",
       "  'comment': u'12 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07064v1',\n",
       "  'published': u'2016-02-23T07:33:02Z',\n",
       "  'summary': u'  In this work we present SIFT, a 3-step algorithm for the analysis of the\\nstructural information represented by means of a taxonomy. The major advantage\\nof this algorithm is the capability to leverage the information inherent to the\\nhierarchical structures of taxonomies to infer correspondences which can allow\\nto merge them in a later step. This method is particular relevant in scenarios\\nwhere taxonomy alignment techniques exploiting textual information from\\ntaxonomy nodes cannot operate successfully.\\n',\n",
       "  'title': u'SIFT: An Algorithm for Extracting Structural Information From Taxonomies'},\n",
       " u'1603.01987': {'arxivid': u'1603.01987',\n",
       "  'authorsaffil': [[u'Vittoria Cozza', None],\n",
       "   [u'Marinella Petrocchi', None],\n",
       "   [u'Angelo Spognardi', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01987v1',\n",
       "  'published': u'2016-03-07T09:54:11Z',\n",
       "  'summary': u'  Automatic quality evaluation of Web information is a task with many fields of\\napplications and of great relevance, especially in critical domains like the\\nmedical one. We move from the intuition that the quality of content of medical\\nWeb documents is affected by features related with the specific domain. First,\\nthe usage of a specific vocabulary (Domain Informativeness); then, the adoption\\nof specific codes (like those used in the infoboxes of Wikipedia articles) and\\nthe type of document (e.g., historical and technical ones). In this paper, we\\npropose to leverage specific domain features to improve the results of the\\nevaluation of Wikipedia medical articles. In particular, we evaluate the\\narticles adopting an \"actionable\" model, whose features are related to the\\ncontent of the articles, so that the model can also directly suggest strategies\\nfor improving a given article quality. We rely on Natural Language Processing\\n(NLP) and dictionaries-based techniques in order to extract the bio-medical\\nconcepts in a text. We prove the effectiveness of our approach by classifying\\nthe medical articles of the Wikipedia Medicine Portal, which have been\\npreviously manually labeled by the Wiki Project team. The results of our\\nexperiments confirm that, by considering domain-oriented features, it is\\npossible to obtain sensible improvements with respect to existing solutions,\\nmainly for those articles that other approaches have less correctly classified.\\nOther than being interesting by their own, the results call for further\\nresearch in the area of domain specific features suitable for Web data quality\\nassessment.\\n',\n",
       "  'title': u'A matter of words: NLP for quality evaluation of Wikipedia medical\\n  articles'},\n",
       " u'1603.01450': {'arxivid': u'1603.01450',\n",
       "  'authorsaffil': [[u'Hossein Vahabi', None],\n",
       "   [u'Paul Lagr\\xe9e', None],\n",
       "   [u'Claire Vernade', None],\n",
       "   [u'Olivier Capp\\xe9', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.LG'],\n",
       "  'comment': u'This submission has been withdrawn by arXiv administrators due to\\n  irreconcilable authorship dispute',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01450v2',\n",
       "  'published': u'2016-03-04T13:25:56Z',\n",
       "  'summary': u'  In many web applications, a recommendation is not a single item suggested to\\na user but a list of possibly interesting contents that may be ranked in some\\ncontexts. The combinatorial bandit problem has been studied quite extensively\\nthese last two years and many theoretical results now exist : lower bounds on\\nthe regret or asymptotically optimal algorithms. However, because of the\\nvariety of situations that can be considered, results are designed to solve the\\nproblem for a specific reward structure such as the Cascade Model. The present\\nwork focuses on the problem of ranking items when the user is allowed to click\\non several items while scanning the list from top to bottom.\\n',\n",
       "  'title': u'Sequential ranking under random semi-bandit feedback'},\n",
       " u'1602.06518': {'arxivid': u'1602.06518',\n",
       "  'authorsaffil': [[u'Anastasia Pentina', None],\n",
       "   [u'Christoph H. Lampert', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06518v2',\n",
       "  'published': u'2016-02-21T11:18:10Z',\n",
       "  'summary': u'  In this paper we consider the problem of multi-task learning, in which a\\nlearner is given a collection of prediction tasks that need to be solved. In\\ncontrast to previous work, we give up on the assumption that labeled training\\ndata is available for all tasks. Instead, we propose an active task selection\\nframework, where based only on the unlabeled data, the learner can choose a,\\ntypically small, subset of tasks for which he gets some labeled examples. For\\nthe remaining tasks, which have no available annotation, solutions are found by\\ntransferring information from the selected tasks. We analyze two transfer\\nstrategies and develop generalization bounds for each of them. Based on this\\ntheoretical analysis we propose two algorithms for making the choice of labeled\\ntasks in a principled way and show their effectiveness on synthetic and real\\ndata.\\n',\n",
       "  'title': u'Active Task Selection for Multi-Task Learning'},\n",
       " u'1506.08350': {'arxivid': u'1506.08350',\n",
       "  'authorsaffil': [[u'Yadong Mu', None],\n",
       "   [u'Wei Liu', None],\n",
       "   [u'Wei Fan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NA', u'90C06'],\n",
       "  'comment': u'14 pages, 9 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.08350v2',\n",
       "  'published': u'2015-06-28T03:33:38Z',\n",
       "  'summary': u'  Stochastic gradient descent (SGD) holds as a classical method to build large\\nscale machine learning models over big data. A stochastic gradient is typically\\ncalculated from a limited number of samples (known as mini-batch), so it\\npotentially incurs a high variance and causes the estimated parameters bounce\\naround the optimal solution. To improve the stability of stochastic gradient,\\nrecent years have witnessed the proposal of several semi-stochastic gradient\\ndescent algorithms, which distinguish themselves from standard SGD by\\nincorporating global information into gradient computation. In this paper we\\ncontribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm\\nto this nascent research area, accelerating the optimization of a large family\\nof composite convex functions. Though theoretically converging faster, prior\\nsemi-stochastic algorithms are found to suffer from high iteration complexity,\\nwhich makes them even slower than SGD in practice on many datasets. In our\\nproposed S3GD, the semi-stochastic gradient is calculated based on efficient\\nmanifold propagation, which can be numerically accomplished by sparse matrix\\nmultiplications. This way S3GD is able to generate a highly-accurate estimate\\nof the exact gradient from each mini-batch with largely-reduced computational\\ncomplexity. Theoretic analysis reveals that the proposed S3GD elegantly\\nbalances the geometric algorithmic convergence rate against the space and time\\ncomplexities during the optimization. The efficacy of S3GD is also\\nexperimentally corroborated on several large-scale benchmark datasets.\\n',\n",
       "  'title': u'Stochastic Gradient Made Stable: A Manifold Propagation Approach for\\n  Large-Scale Optimization'},\n",
       " u'1511.04401': {'arxivid': u'1511.04401',\n",
       "  'authorsaffil': [[u'Federico Raue', None],\n",
       "   [u'Thomas M. Breuel', None],\n",
       "   [u'Andreas Dengel', None],\n",
       "   [u'Marcus Liwicki', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'Under review as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04401v3',\n",
       "  'published': u'2015-11-13T18:59:36Z',\n",
       "  'summary': u'  In this paper, we extend a symbolic association framework to being able to\\nhandle missing elements in multimodal sequences. The general scope of the work\\nis the symbolic associations of object-word mappings as it happens in language\\ndevelopment on infants. This scenario has been long interested by Artificial\\nIntelligence, Psychology and Neuroscience. In this work, we extend a recent\\napproach for multimodal sequences (visual and audio) to also cope with missing\\nelements in one or both modalities. Our approach uses two parallel Long\\nShort-Term Memory (LSTM) networks with a learning rule based on EM-algorithm.\\nIt aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to\\ninclude an extra step for the combination with max and mean operations for\\nhandling missing elements in the sequences. The intuition behind is that the\\ncombination acts as a condition selector for choosing the best representation\\nfrom both LSTMs. We evaluated the proposed extension in three different\\nscenarios: audio sequences with missing elements, visual sequences with missing\\nelements, and sequences with missing elements in both modalities. The\\nperformance of our extension reaches better results than the original model and\\nsimilar results to a unique LSTM trained in one modality, i.e., where the\\nlearning problem is less difficult.\\n',\n",
       "  'title': u'Symbol Grounding Association in Multimodal Sequences with Missing\\n  Elements'},\n",
       " u'1511.08198': {'arxivid': u'1511.08198',\n",
       "  'authorsaffil': [[u'John Wieting', None],\n",
       "   [u'Mohit Bansal', None],\n",
       "   [u'Kevin Gimpel', None],\n",
       "   [u'Karen Livescu', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'Published as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.08198v3',\n",
       "  'published': u'2015-11-25T20:52:15Z',\n",
       "  'summary': u'  We consider the problem of learning general-purpose, paraphrastic sentence\\nembeddings based on supervision from the Paraphrase Database (Ganitkevitch et\\nal., 2013). We compare six compositional architectures, evaluating them on\\nannotated textual similarity datasets drawn both from the same distribution as\\nthe training data and from a wide range of other domains. We find that the most\\ncomplex architectures, such as long short-term memory (LSTM) recurrent neural\\nnetworks, perform best on the in-domain data. However, in out-of-domain\\nscenarios, simple architectures such as word averaging vastly outperform LSTMs.\\nOur simplest averaging model is even competitive with systems tuned for the\\nparticular tasks while also being extremely efficient and easy to use.\\n  In order to better understand how these architectures compare, we conduct\\nfurther experiments on three supervised NLP tasks: sentence similarity,\\nentailment, and sentiment classification. We again find that the word averaging\\nmodels perform well for sentence similarity and entailment, outperforming\\nLSTMs. However, on sentiment classification, we find that the LSTM performs\\nvery strongly-even recording new state-of-the-art performance on the Stanford\\nSentiment Treebank.\\n  We then demonstrate how to combine our pretrained sentence embeddings with\\nthese supervised tasks, using them both as a prior and as a black box feature\\nextractor. This leads to performance rivaling the state of the art on the SICK\\nsimilarity and entailment tasks. We release all of our resources to the\\nresearch community with the hope that they can serve as the new baseline for\\nfurther work on universal sentence embeddings.\\n',\n",
       "  'title': u'Towards Universal Paraphrastic Sentence Embeddings'},\n",
       " u'1602.06042': {'arxivid': u'1602.06042',\n",
       "  'authorsaffil': [[u'Prateek Jain', None],\n",
       "   [u'Nikhil Rao', None],\n",
       "   [u'Inderjit Dhillon', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06042v2',\n",
       "  'published': u'2016-02-19T04:28:50Z',\n",
       "  'summary': u'  Several learning applications require solving high-dimensional regression\\nproblems where the relevant features belong to a small number of (overlapping)\\ngroups. For very large datasets and under standard sparsity constraints, hard\\nthresholding methods have proven to be extremely efficient, but such methods\\nrequire NP hard projections when dealing with overlapping groups. In this\\npaper, we show that such NP-hard projections can not only be avoided by\\nappealing to submodular optimization, but such methods come with strong\\ntheoretical guarantees even in the presence of poorly conditioned data (i.e.\\nsay when two features have correlation $\\\\geq 0.99$), which existing analyses\\ncannot handle. These methods exhibit an interesting computation-accuracy\\ntrade-off and can be extended to significantly harder problems such as sparse\\noverlapping groups. Experiments on both real and synthetic data validate our\\nclaims and demonstrate that the proposed methods are orders of magnitude faster\\nthan other greedy and convex relaxation techniques for learning with\\ngroup-structured sparsity.\\n',\n",
       "  'title': u'Structured Sparse Regression via Greedy Hard-Thresholding'},\n",
       " u'1507.04777': {'arxivid': u'1507.04777',\n",
       "  'authorsaffil': [[u'Stephan Mandt', None],\n",
       "   [u'Florian Wenzel', None],\n",
       "   [u'Shinichi Nakajima', None],\n",
       "   [u'John P. Cunningham', None],\n",
       "   [u'Christoph Lippert', None],\n",
       "   [u'Marius Kloft', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'18 pages + appendix',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.04777v2',\n",
       "  'published': u'2015-07-16T21:33:48Z',\n",
       "  'summary': u'  Among the goals of statistical genetics is to find associations between\\ngenetic data and binary phenotypes, such as heritable diseases. Often, the data\\nare obfuscated by confounders such as age, ethnicity, or population structure.\\nLinear mixed models are linear regression models that correct for confounding\\nby means of correlated label noise; they are widely appreciated in the field of\\nstatistical genetics. We generalize this modeling paradigm to binary\\nclassification, where we face the problem that marginalizing over the noise\\nleads to an intractable, high-dimensional integral. We present a scalable,\\napproximate inference algorithm that lets us fit the model to high-dimensional\\ndata sets. The algorithm selects features based on an $\\\\ell_1$ norm regularizer\\nwhich are up to 40% less confounded compared to the outcomes of uncorrected\\nfeature selection, as we show. The proposed method also outperforms Gaussian\\nprocess classification and uncorrelated probit regression in terms of\\nprediction performance.\\n',\n",
       "  'title': u'Sparse Estimation in a Correlated Probit Model'},\n",
       " u'1511.05933': {'arxivid': u'1511.05933',\n",
       "  'authorsaffil': [[u'Sayantan Dasgupta', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05933v6',\n",
       "  'published': u'2015-11-18T20:26:42Z',\n",
       "  'summary': u'  K-means is one of the most widely used algorithms for clustering in Data\\nMining applications, which attempts to minimize the sum of square of Euclidean\\ndistance of the points in the clusters from the respective means of the\\nclusters. The simplicity and scalability of K-means makes it very appealing.\\nHowever, K-means suffers from local minima problem, and comes with no guarantee\\nto converge to the optimal cost. K-means++ tries to address the problem by\\nseeding the means using a distance based sampling scheme. However, seeding the\\nmeans in K-means++ needs $O\\\\left(K\\\\right)$ passes through the entire dataset.\\nThis could be very costly in large amount of dataset. Here we propose a method\\nof seeding initial means based on factorizations of higher order moments for\\nbounded data. Our method takes O(1) passes through the entire dataset to\\nextract the initial set of means, and its final cost can be proven to be within\\n$O(\\\\sqrt{K})$ of the optimal cost. We demonstrate the performance of our\\nalgorithm in comparison with the existing algorithms on various benchmark\\ndatasets.\\n',\n",
       "  'title': u'Towards O(1) Seeding of K-Means'},\n",
       " u'1507.04811': {'arxivid': u'1507.04811',\n",
       "  'authorsaffil': [[u'Jian Xu', None],\n",
       "   [u'Xuhui Shao', None],\n",
       "   [u'Jianjie Ma', None],\n",
       "   [u'Kuang-chih Lee', None],\n",
       "   [u'Hang Qi', None],\n",
       "   [u'Quan Lu', None]],\n",
       "  'categoryterms': [u'cs.GT', u'cs.AI'],\n",
       "  'comment': u'AAAI 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.04811v3',\n",
       "  'published': u'2015-07-17T01:28:54Z',\n",
       "  'summary': u'  Real-time bidding (RTB) has become one of the largest online advertising\\nmarkets in the world. Today the bid price per ad impression is typically\\ndecided by the expected value of how it can lead to a desired action event\\n(e.g., registering an account or placing a purchase order) to the advertiser.\\nHowever, this industry standard approach to decide the bid price does not\\nconsider the actual effect of the ad shown to the user, which should be\\nmeasured based on the performance lift among users who have been or have not\\nbeen exposed to a certain treatment of ads. In this paper, we propose a new\\nbidding strategy and prove that if the bid price is decided based on the\\nperformance lift rather than absolute performance value, advertisers can\\nactually gain more action events. We describe the modeling methodology to\\npredict the performance lift and demonstrate the actual performance gain\\nthrough blind A/B test with real ad campaigns in an industry-leading\\nDemand-Side Platform (DSP). We also discuss the relationship between\\nattribution models and bidding strategies. We prove that, to move the DSPs to\\nbid based on performance lift, they should be rewarded according to the\\nrelative performance lift they contribute.\\n',\n",
       "  'title': u'Lift-Based Bidding in Ad Selection'},\n",
       " u'1603.04588': {'arxivid': u'1603.04588',\n",
       "  'authorsaffil': [[u'Hawren Fang', None]],\n",
       "  'categoryterms': [u'cs.CV', u'I.5.2; I.4.10'],\n",
       "  'comment': u'25 pages, 8 figures, 6 tables, unpublished manuscript',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04588v1',\n",
       "  'published': u'2016-03-15T08:31:04Z',\n",
       "  'summary': u'  We consider dimensionality reduction methods for face recognition in a\\nsupervised setting, using an image-as-matrix representation. A common procedure\\nis to project image matrices into a smaller space in which the recognition is\\nperformed. These methods are often called \"two-dimensional\" in the literature\\nand there exist counterparts that use an image-as-vector representation. When\\ntwo face images are close to each other in the input space they may remain\\nclose after projection - but this is not desirable in the situation when these\\ntwo images are from different classes, and this often affects the recognition\\nperformance. We extend a previously developed `repulsion Laplacean\\' technique\\nbased on adding terms to the objective function with the goal or creation a\\nrepulsion energy between such images in the projected space. This scheme, which\\nrelies on a repulsion graph, is generic and can be incorporated into various\\ntwo-dimensional methods. It can be regarded as a multilinear generalization of\\nthe repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42 (2009), pp.\\n2392--2402]. Experimental results demonstrate that the proposed methodology\\noffers significant recognition improvement relative to the underlying\\ntwo-dimensional methods.\\n',\n",
       "  'title': u'Classification with Repulsion Tensors: A Case Study on Face Recognition'},\n",
       " u'1603.04586': {'arxivid': u'1603.04586',\n",
       "  'authorsaffil': [[u'Mikko Lauri', None], [u'Risto Ritala', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.RO', u'cs.SY'],\n",
       "  'comment': u'6 pages, 2 figures',\n",
       "  'doi': u'10.1109/ICRA.2015.7139867',\n",
       "  'journalref': u'Proc. IEEE Intl. Conf. on Robotics and Automation (ICRA), pp.\\n  4807-4812, 2015',\n",
       "  'link': u'http://arxiv.org/abs/1603.04586v1',\n",
       "  'published': u'2016-03-15T08:12:52Z',\n",
       "  'summary': u'  Sequential decision making under uncertainty is studied in a mixed\\nobservability domain. The goal is to maximize the amount of information\\nobtained on a partially observable stochastic process under constraints imposed\\nby a fully observable internal state. An upper bound for the optimal value\\nfunction is derived by relaxing constraints. We identify conditions under which\\nthe relaxed problem is a multi-armed bandit whose optimal policy is easily\\ncomputable. The upper bound is applied to prune the search space in the\\noriginal problem, and the effect on solution quality is assessed via simulation\\nexperiments. Empirical results show effective pruning of the search space in a\\ntarget monitoring domain.\\n',\n",
       "  'title': u'Optimal Sensing via Multi-armed Bandit Relaxations in Mixed\\n  Observability Domains'},\n",
       " u'1511.05939': {'arxivid': u'1511.05939',\n",
       "  'authorsaffil': [[u'Oren Rippel', None],\n",
       "   [u'Manohar Paluri', None],\n",
       "   [u'Piotr Dollar', None],\n",
       "   [u'Lubomir Bourdev', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05939v2',\n",
       "  'published': u'2015-11-18T20:41:05Z',\n",
       "  'summary': u'  Distance metric learning (DML) approaches learn a transformation to a\\nrepresentation space where distance is in correspondence with a predefined\\nnotion of similarity. While such models offer a number of compelling benefits,\\nit has been difficult for these to compete with modern classification\\nalgorithms in performance and even in feature extraction.\\n  In this work, we propose a novel approach explicitly designed to address a\\nnumber of subtle yet important issues which have stymied earlier DML\\nalgorithms. It maintains an explicit model of the distributions of the\\ndifferent classes in representation space. It then employs this knowledge to\\nadaptively assess similarity, and achieve local discrimination by penalizing\\nclass distribution overlap.\\n  We demonstrate the effectiveness of this idea on several tasks. Our approach\\nachieves state-of-the-art classification results on a number of fine-grained\\nvisual recognition datasets, surpassing the standard softmax classifier and\\noutperforming triplet loss by a relative margin of 30-40%. In terms of\\ncomputational performance, it alleviates training inefficiencies in the\\ntraditional triplet loss, reaching the same error in 5-30 times fewer\\niterations. Beyond classification, we further validate the saliency of the\\nlearnt representations via their attribute concentration and hierarchy recovery\\nproperties, achieving 10-25% relative gains on the softmax classifier and\\n25-50% on triplet loss in these tasks.\\n',\n",
       "  'title': u'Metric Learning with Adaptive Density Discrimination'},\n",
       " u'1603.09095': {'arxivid': u'1603.09095',\n",
       "  'authorsaffil': [[u'Nenad Marku\\u0161', None],\n",
       "   [u'Igor S. Pand\\u017ei\\u0107', None],\n",
       "   [u'J\\xf6rgen Ahlberg', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09095v2',\n",
       "  'published': u'2016-03-30T09:24:40Z',\n",
       "  'summary': u'  Current best local descriptors are learned on a large dataset of matching and\\nnon-matching keypoint pairs. However, data of this kind is not always available\\nsince detailed keypoint correspondences can be hard to establish. On the other\\nhand, we can often obtain labels for pairs of keypoint bags. For example,\\nkeypoint bags extracted from two images of the same object under different\\nviews form a matching pair, and keypoint bags extracted from images of\\ndifferent objects form a non-matching pair. On average, matching pairs should\\ncontain more corresponding keypoints than non-matching pairs. We describe an\\nend-to-end differentiable architecture that enables the learning of local\\nkeypoint descriptors from such weakly-labeled data.\\n',\n",
       "  'title': u'Learning Local Descriptors by Optimizing the Keypoint-Correspondence\\n  Criterion'},\n",
       " u'1512.07336': {'arxivid': u'1512.07336',\n",
       "  'authorsaffil': [[u'Pengtao Xie', None],\n",
       "   [u'Yuntian Deng', None],\n",
       "   [u'Eric Xing', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07336v1',\n",
       "  'published': u'2015-12-23T02:29:39Z',\n",
       "  'summary': u'  Latent Variable Models (LVMs) are a large family of machine learning models\\nproviding a principled and effective way to extract underlying patterns,\\nstructure and knowledge from observed data. Due to the dramatic growth of\\nvolume and complexity of data, several new challenges have emerged and cannot\\nbe effectively addressed by existing LVMs: (1) How to capture long-tail\\npatterns that carry crucial information when the popularity of patterns is\\ndistributed in a power-law fashion? (2) How to reduce model complexity and\\ncomputational cost without compromising the modeling power of LVMs? (3) How to\\nimprove the interpretability and reduce the redundancy of discovered patterns?\\nTo addresses the three challenges discussed above, we develop a novel\\nregularization technique for LVMs, which controls the geometry of the latent\\nspace during learning to enable the learned latent components of LVMs to be\\ndiverse in the sense that they are favored to be mutually different from each\\nother, to accomplish long-tail coverage, low redundancy, and better\\ninterpretability. We propose a mutual angular regularizer (MAR) to encourage\\nthe components in LVMs to have larger mutual angles. The MAR is non-convex and\\nnon-smooth, entailing great challenges for optimization. To cope with this\\nissue, we derive a smooth lower bound of the MAR and optimize the lower bound\\ninstead. We show that the monotonicity of the lower bound is closely aligned\\nwith the MAR to qualify the lower bound as a desirable surrogate of the MAR.\\nUsing neural network (NN) as an instance, we analyze how the MAR affects the\\ngeneralization performance of NN. On two popular latent variable models ---\\nrestricted Boltzmann machine and distance metric learning, we demonstrate that\\nMAR can effectively capture long-tail patterns, reduce model complexity without\\nsacrificing expressivity and improve interpretability.\\n',\n",
       "  'title': u'Latent Variable Modeling with Diversity-Inducing Mutual Angular\\n  Regularization'},\n",
       " u'1601.07649': {'arxivid': u'1601.07649',\n",
       "  'authorsaffil': [[u'Fayao Liu', None],\n",
       "   [u'Guosheng Lin', None],\n",
       "   [u'Chunhua Shen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07649v1',\n",
       "  'published': u'2016-01-28T05:30:50Z',\n",
       "  'summary': u\"  Recent works on deep conditional random fields (CRF) have set new records on\\nmany vision tasks involving structured predictions. Here we propose a\\nfully-connected deep continuous CRF model for both discrete and continuous\\nlabelling problems. We exemplify the usefulness of the proposed model on\\nmulti-class semantic labelling (discrete) and the robust depth estimation\\n(continuous) problems.\\n  In our framework, we model both the unary and the pairwise potential\\nfunctions as deep convolutional neural networks (CNN), which are jointly\\nlearned in an end-to-end fashion. The proposed method possesses the main\\nadvantage of continuously-valued CRF, which is a closed-form solution for the\\nMaximum a posteriori (MAP) inference.\\n  To better adapt to different tasks, instead of using the commonly employed\\nmaximum likelihood CRF parameter learning protocol, we propose task-specific\\nloss functions for learning the CRF parameters.\\n  It enables direct optimization of the quality of the MAP estimates during the\\ncourse of learning.\\n  Specifically, we optimize the multi-class classification loss for the\\nsemantic labelling task and the Turkey's biweight loss for the robust depth\\nestimation problem.\\n  Experimental results on the semantic labelling and robust depth estimation\\ntasks demonstrate that the proposed method compare favorably against both\\nbaseline and state-of-the-art methods.\\n  In particular, we show that although the proposed deep CRF model is\\ncontinuously valued, with the equipment of task-specific loss, it achieves\\nimpressive results even on discrete labelling tasks.\\n\",\n",
       "  'title': u'Discriminative Training of Deep Fully-connected Continuous CRF with\\n  Task-specific Loss'},\n",
       " u'1601.06733': {'arxivid': u'1601.06733',\n",
       "  'authorsaffil': [[u'Jianpeng Cheng', None],\n",
       "   [u'Li Dong', None],\n",
       "   [u'Mirella Lapata', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.NE'],\n",
       "  'comment': u'Fixed the incomparable parameters in experiments to previous work;\\n  fixed a few typos',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06733v6',\n",
       "  'published': u'2016-01-25T19:25:48Z',\n",
       "  'summary': u'  Machine reading, the automatic understanding of text, remains a challenging\\ntask of great value for NLP applications. We propose a machine reader which\\nprocesses text incrementally from left to right, while linking the current word\\nto previous words stored in memory and implicitly discovering lexical\\ndependencies facilitating understanding. The reader is equipped with a Long\\nShort-Term Memory architecture, which differs from previous work in that it has\\na memory tape (instead of a memory cell) for adaptively storing past\\ninformation without severe information compression. We also integrate our\\nreader with a new attention mechanism in encoder-decoder architecture.\\nExperiments on language modeling, sentiment analysis, and natural language\\ninference show that our model matches or outperforms the state of the art.\\n',\n",
       "  'title': u'Long Short-Term Memory-Networks for Machine Reading'},\n",
       " u'1508.03865': {'arxivid': u'1508.03865',\n",
       "  'authorsaffil': [[u'Yannick Meier', None],\n",
       "   [u'Jie Xu', None],\n",
       "   [u'Onur Atan', None],\n",
       "   [u'Mihaela van der Schaar', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'15 pages, 15 figures',\n",
       "  'doi': u'10.1109/TSP.2015.2496278',\n",
       "  'journalref': u'IEEE Transactions on Signal Processing, vol. 64, no. 4, pp.\\n  959-972, Feb.15, 2016',\n",
       "  'link': u'http://arxiv.org/abs/1508.03865v2',\n",
       "  'published': u'2015-08-16T20:53:09Z',\n",
       "  'summary': u\"  To increase efficacy in traditional classroom courses as well as in Massive\\nOpen Online Courses (MOOCs), automated systems supporting the instructor are\\nneeded. One important problem is to automatically detect students that are\\ngoing to do poorly in a course early enough to be able to take remedial\\nactions. Existing grade prediction systems focus on maximizing the accuracy of\\nthe prediction while overseeing the importance of issuing timely and\\npersonalized predictions. This paper proposes an algorithm that predicts the\\nfinal grade of each student in a class. It issues a prediction for each student\\nindividually, when the expected accuracy of the prediction is sufficient. The\\nalgorithm learns online what is the optimal prediction and time to issue a\\nprediction based on past history of students' performance in a course. We\\nderive a confidence estimate for the prediction accuracy and demonstrate the\\nperformance of our algorithm on a dataset obtained based on the performance of\\napproximately 700 UCLA undergraduate students who have taken an introductory\\ndigital signal processing over the past 7 years. We demonstrate that for 85% of\\nthe students we can predict with 76% accuracy whether they are going do well or\\npoorly in the class after the 4th course week. Using data obtained from a pilot\\ncourse, our methodology suggests that it is effective to perform early in-class\\nassessments such as quizzes, which result in timely performance prediction for\\neach student, thereby enabling timely interventions by the instructor (at the\\nstudent or class level) when necessary.\\n\",\n",
       "  'title': u'Predicting Grades'},\n",
       " u'1503.00488': {'arxivid': u'1503.00488',\n",
       "  'authorsaffil': [[u'Chunlei Peng', None],\n",
       "   [u'Xinbo Gao', None],\n",
       "   [u'Nannan Wang', None],\n",
       "   [u'Jie Li', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'13 pages, 10 figures, TPAMI 2016 accepted',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.00488v3',\n",
       "  'published': u'2015-03-02T11:50:42Z',\n",
       "  'summary': u'  Heterogeneous face recognition (HFR) refers to matching face images acquired\\nfrom different sources (i.e., different sensors or different wavelengths) for\\nidentification. HFR plays an important role in both biometrics research and\\nindustry. In spite of promising progresses achieved in recent years, HFR is\\nstill a challenging problem due to the difficulty to represent two\\nheterogeneous images in a homogeneous manner. Existing HFR methods either\\nrepresent an image ignoring the spatial information, or rely on a\\ntransformation procedure which complicates the recognition task. Considering\\nthese problems, we propose a novel graphical representation based HFR method\\n(G-HFR) in this paper. Markov networks are employed to represent heterogeneous\\nimage patches separately, which takes the spatial compatibility between\\nneighboring image patches into consideration. A coupled representation\\nsimilarity metric (CRSM) is designed to measure the similarity between obtained\\ngraphical representations. Extensive experiments conducted on multiple HFR\\nscenarios (viewed sketch, forensic sketch, near infrared image, and thermal\\ninfrared image) show that the proposed method outperforms state-of-the-art\\nmethods.\\n',\n",
       "  'title': u'Graphical Representation for Heterogeneous Face Recognition'},\n",
       " u'1603.03007': {'arxivid': u'1603.03007',\n",
       "  'authorsaffil': [[u'Alexander Tchitchigin', None],\n",
       "   [u'Max Talanov', None],\n",
       "   [u'Larisa Safina', None],\n",
       "   [u'Manuel Mazzara', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.AI'],\n",
       "  'comment': u'keywords: robotics, spiking neural networks, artificial emotions,\\n  affective computing',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03007v1',\n",
       "  'published': u'2016-03-09T19:14:27Z',\n",
       "  'summary': u'  In this position paper we present a novel approach to neurobiologically\\nplausible implementation of emotional reactions and behaviors for real-time\\nautonomous robotic systems. The working metaphor we use is the \"day\" and\\n\"night\" phases of mammalian life. During the \"day\" phase a robotic system\\nstores the inbound information and is controlled by a light-weight rule-based\\nsystem in real time. In contrast to that, during the \"night\" phase the stored\\ninformation is been transferred to the supercomputing system to update the\\nrealistic neural network: emotional and behavioral strategies.\\n',\n",
       "  'title': u'Robot Dream'},\n",
       " u'1511.07289': {'arxivid': u'1511.07289',\n",
       "  'authorsaffil': [[u'Djork-Arn\\xe9 Clevert', None],\n",
       "   [u'Thomas Unterthiner', None],\n",
       "   [u'Sepp Hochreiter', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Published as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07289v5',\n",
       "  'published': u'2015-11-23T15:58:05Z',\n",
       "  'summary': u'  We introduce the \"exponential linear unit\" (ELU) which speeds up learning in\\ndeep neural networks and leads to higher classification accuracies. Like\\nrectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs\\n(PReLUs), ELUs alleviate the vanishing gradient problem via the identity for\\npositive values. However, ELUs have improved learning characteristics compared\\nto the units with other activation functions. In contrast to ReLUs, ELUs have\\nnegative values which allows them to push mean unit activations closer to zero\\nlike batch normalization but with lower computational complexity. Mean shifts\\ntoward zero speed up learning by bringing the normal gradient closer to the\\nunit natural gradient because of a reduced bias shift effect. While LReLUs and\\nPReLUs have negative values, too, they do not ensure a noise-robust\\ndeactivation state. ELUs saturate to a negative value with smaller inputs and\\nthereby decrease the forward propagated variation and information. Therefore,\\nELUs code the degree of presence of particular phenomena in the input, while\\nthey do not quantitatively model the degree of their absence. In experiments,\\nELUs lead not only to faster learning, but also to significantly better\\ngeneralization performance than ReLUs and LReLUs on networks with more than 5\\nlayers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with\\nbatch normalization while batch normalization does not improve ELU networks.\\nELU networks are among the top 10 reported CIFAR-10 results and yield the best\\npublished result on CIFAR-100, without resorting to multi-view evaluation or\\nmodel averaging. On ImageNet, ELU networks considerably speed up learning\\ncompared to a ReLU network with the same architecture, obtaining less than 10%\\nclassification error for a single crop, single model network.\\n',\n",
       "  'title': u'Fast and Accurate Deep Network Learning by Exponential Linear Units\\n  (ELUs)'},\n",
       " u'1604.00730': {'arxivid': u'1604.00730',\n",
       "  'authorsaffil': [[u'Shaodi You', None],\n",
       "   [u'Robby T. Tan', None],\n",
       "   [u'Rei Kawakami', None],\n",
       "   [u'Yasuhiro Mukaigawa', None],\n",
       "   [u'Katsushi Ikeuchi', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'12 pages, 15figues',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00730v1',\n",
       "  'published': u'2016-04-04T03:16:40Z',\n",
       "  'summary': u'  This paper introduces depth estimation from water drops. The key idea is that\\na single water drop adhered to window glass is totally transparent and convex,\\nand thus optically acts like a fisheye lens. If we have more than one water\\ndrop in a single image, then through each of them we can see the environment\\nwith different view points, similar to stereo. To realize this idea, we need to\\nrectify every water drop imagery to make radially distorted planar surfaces\\nlook flat. For this rectification, we consider two physical properties of water\\ndrops: (1) A static water drop has constant volume, and its geometric convex\\nshape is determined by the balance between the tension force and gravity. This\\nimplies that the 3D geometric shape can be obtained by minimizing the overall\\npotential energy, which is the sum of the tension energy and the gravitational\\npotential energy. (2) The imagery inside a water-drop is determined by the\\nwater-drop 3D shape and total reflection at the boundary. This total reflection\\ngenerates a dark band commonly observed in any adherent water drops. Hence,\\nonce the 3D shape of water drops are recovered, we can rectify the water drop\\nimages through backward raytracing. Subsequently, we can compute depth using\\nstereo. In addition to depth estimation, we can also apply image refocusing.\\nExperiments on real images and a quantitative evaluation show the effectiveness\\nof our proposed method. To our best knowledge, never before have adherent water\\ndrops been used to estimate depth.\\n',\n",
       "  'title': u'Waterdrop Stereo'},\n",
       " u'1511.06807': {'arxivid': u'1511.06807',\n",
       "  'authorsaffil': [[u'Arvind Neelakantan', None],\n",
       "   [u'Luke Vilnis', None],\n",
       "   [u'Quoc V. Le', None],\n",
       "   [u'Ilya Sutskever', None],\n",
       "   [u'Lukasz Kaiser', None],\n",
       "   [u'Karol Kurach', None],\n",
       "   [u'James Martens', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06807v1',\n",
       "  'published': u'2015-11-21T01:11:29Z',\n",
       "  'summary': u'  Deep feedforward and recurrent networks have achieved impressive results in\\nmany perception and language processing applications. This success is partially\\nattributed to architectural innovations such as convolutional and long\\nshort-term memory networks. The main motivation for these architectural\\ninnovations is that they capture better domain knowledge, and importantly are\\neasier to optimize than more basic architectures. Recently, more complex\\narchitectures such as Neural Turing Machines and Memory Networks have been\\nproposed for tasks including question answering and general computation,\\ncreating a new set of optimization challenges. In this paper, we discuss a\\nlow-overhead and easy-to-implement technique of adding gradient noise which we\\nfind to be surprisingly effective when training these very deep architectures.\\nThe technique not only helps to avoid overfitting, but also can result in lower\\ntraining loss. This method alone allows a fully-connected 20-layer deep network\\nto be trained with standard gradient descent, even starting from a poor\\ninitialization. We see consistent improvements for many complex models,\\nincluding a 72% relative reduction in error rate over a carefully-tuned\\nbaseline on a challenging question-answering task, and a doubling of the number\\nof accurate binary multiplication models learned across 7,000 random restarts.\\nWe encourage further application of this technique to additional complex modern\\narchitectures.\\n',\n",
       "  'title': u'Adding Gradient Noise Improves Learning for Very Deep Networks'},\n",
       " u'1601.04468': {'arxivid': u'1601.04468',\n",
       "  'authorsaffil': [[u'Artem Sokolov', None],\n",
       "   [u'Stefan Riezler', None],\n",
       "   [u'Tanguy Urvoy', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'In Proceedings of MT Summit XV, 2015. Miami, FL',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04468v1',\n",
       "  'published': u'2016-01-18T11:09:02Z',\n",
       "  'summary': u'  We present an approach to structured prediction from bandit feedback, called\\nBandit Structured Prediction, where only the value of a task loss function at a\\nsingle predicted point, instead of a correct structure, is observed in\\nlearning. We present an application to discriminative reranking in Statistical\\nMachine Translation (SMT) where the learning algorithm only has access to a\\n1-BLEU loss evaluation of a predicted translation instead of obtaining a gold\\nstandard reference translation. In our experiment bandit feedback is obtained\\nby evaluating BLEU on reference translations without revealing them to the\\nalgorithm. This can be thought of as a simulation of interactive machine\\ntranslation where an SMT system is personalized by a user who provides single\\npoint feedback to predicted translations. Our experiments show that our\\napproach improves translation quality and is comparable to approaches that\\nemploy more informative feedback in learning.\\n',\n",
       "  'title': u'Bandit Structured Prediction for Learning from Partial Feedback in\\n  Statistical Machine Translation'},\n",
       " u'1604.00734': {'arxivid': u'1604.00734',\n",
       "  'authorsaffil': [[u'Matthew Francis-Landau', None],\n",
       "   [u'Greg Durrett', None],\n",
       "   [u'Dan Klein', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Accepted at NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00734v1',\n",
       "  'published': u'2016-04-04T03:58:31Z',\n",
       "  'summary': u\"  A key challenge in entity linking is making effective use of contextual\\ninformation to disambiguate mentions that might refer to different entities in\\ndifferent contexts. We present a model that uses convolutional neural networks\\nto capture semantic correspondence between a mention's context and a proposed\\ntarget entity. These convolutional networks operate at multiple granularities\\nto exploit various kinds of topic information, and their rich parameterization\\ngives them the capacity to learn which n-grams characterize different topics.\\nWe combine these networks with a sparse linear model to achieve\\nstate-of-the-art performance on multiple entity linking datasets, outperforming\\nthe prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).\\n\",\n",
       "  'title': u'Capturing Semantic Similarity for Entity Linking with Convolutional\\n  Neural Networks'},\n",
       " u'1603.07076': {'arxivid': u'1603.07076',\n",
       "  'authorsaffil': [[u'Albert Haque', None],\n",
       "   [u'Boya Peng', None],\n",
       "   [u'Zelun Luo', None],\n",
       "   [u'Alexandre Alahi', None],\n",
       "   [u'Serena Yeung', None],\n",
       "   [u'Li Fei-Fei', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07076v2',\n",
       "  'published': u'2016-03-23T06:24:19Z',\n",
       "  'summary': u'  We propose a viewpoint invariant model for 3D human pose estimation from a\\nsingle depth image. To achieve viewpoint invariance, our deep discriminative\\nmodel embeds local regions into a learned viewpoint invariant feature space.\\nFormulated as a multi-task learning problem, our model is able to selectively\\npredict partial poses in the presence of noise and occlusion. Our approach\\nleverages a convolutional and recurrent network with a top-down error feedback\\nmechanism to self-correct previous pose estimates in an end-to-end manner. We\\nevaluate our model on a previously published depth dataset and a newly\\ncollected human pose dataset containing 100K annotated depth images from\\nextreme viewpoints. Experiments show that our model achieves competitive\\nperformance on frontal views while achieving state-of-the-art performance on\\nalternate viewpoints.\\n',\n",
       "  'title': u'Viewpoint Invariant 3D Human Pose Estimation with Recurrent Error\\n  Feedback'},\n",
       " u'1602.00269': {'arxivid': u'1602.00269',\n",
       "  'authorsaffil': [[u'Sarath P R', None],\n",
       "   [u'Sunil Mandhan', None],\n",
       "   [u'Yoshiki Niwa', None]],\n",
       "  'categoryterms': [u'cs.AI', u'68T50'],\n",
       "  'comment': u'6 Pages',\n",
       "  'doi': u'10.13140/RG.2.1.4763.3365',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00269v1',\n",
       "  'published': u'2016-01-31T15:58:51Z',\n",
       "  'summary': u'  This paper describes about information extraction system, which is an\\nextension of the system developed by team Hitachi for \"Disease/Disorder\\nTemplate filling\" task organized by ShARe/CLEF eHealth Evolution Lab 2014. In\\nthis extension module we focus on extraction of numerical attributes and values\\nfrom discharge summary records and associating correct relation between\\nattributes and values. We solve the problem in two steps. First step is\\nextraction of numerical attributes and values, which is developed as a Named\\nEntity Recognition (NER) model using Stanford NLP libraries. Second step is\\ncorrectly associating the attributes to values, which is developed as a\\nrelation extraction module in Apache cTAKES framework. We integrated Stanford\\nNER model as cTAKES pipeline component and used in relation extraction module.\\nConditional Random Field (CRF) algorithm is used for NER and Support Vector\\nMachines (SVM) for relation extraction. For attribute value relation\\nextraction, we observe 95% accuracy using NER alone and combined accuracy of\\n87% with NER and SVM.\\n',\n",
       "  'title': u'Numerical Atrribute Extraction from Clinical Texts'},\n",
       " u'1601.02852': {'arxivid': u'1601.02852',\n",
       "  'authorsaffil': [[u'Jinsoo Choi', None],\n",
       "   [u'Tae-Hyun Oh', None],\n",
       "   [u'In So Kweon', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.HC', u'cs.MM'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02852v1',\n",
       "  'published': u'2016-01-12T13:31:38Z',\n",
       "  'summary': u\"  Photo collections and its applications today attempt to reflect user\\ninteractions in various forms. Moreover, photo collections aim to capture the\\nusers' intention with minimum effort through applications capturing user\\nintentions. Human interest regions in an image carry powerful information about\\nthe user's behavior and can be used in many photo applications. Research on\\nhuman visual attention has been conducted in the form of gaze tracking and\\ncomputational saliency models in the computer vision community, and has shown\\nconsiderable progress. This paper presents an integration between implicit gaze\\nestimation and computational saliency model to effectively estimate human\\nattention regions in images on the fly. Furthermore, our method estimates human\\nattention via implicit calibration and incremental model updating without any\\nactive participation from the user. We also present extensive analysis and\\npossible applications for personal photo collections.\\n\",\n",
       "  'title': u'Human Attention Estimation for Natural Images: An Automatic Gaze\\n  Refinement Approach'},\n",
       " u'1604.00119': {'arxivid': u'1604.00119',\n",
       "  'authorsaffil': [[u'Krish Perumal', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.IR', u'cs.LG', u'cs.SI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00119v3',\n",
       "  'published': u'2016-04-01T03:32:03Z',\n",
       "  'summary': u'  Web discussion forums are used by millions of people worldwide to share\\ninformation belonging to a variety of domains such as automotive vehicles,\\npets, sports, etc. They typically contain posts that fall into different\\ncategories such as problem, solution, feedback, spam, etc. Automatic\\nidentification of these categories can aid information retrieval that is\\ntailored for specific user requirements. Previously, a number of supervised\\nmethods have attempted to solve this problem; however, these depend on the\\navailability of abundant training data. A few existing unsupervised and\\nsemi-supervised approaches are either focused on identifying a single category\\nor do not report category-specific performance. In contrast, this work proposes\\nunsupervised and semi-supervised methods that require no or minimal training\\ndata to achieve this objective without compromising on performance. A\\nfine-grained analysis is also carried out to discuss their limitations. The\\nproposed methods are based on sequence models (specifically, Hidden Markov\\nModels) that can model language for each category using word and part-of-speech\\nprobability distributions, and manually specified features. Empirical\\nevaluations across domains demonstrate that the proposed methods are better\\nsuited for this task than existing ones.\\n',\n",
       "  'title': u'Semi-supervised and Unsupervised Methods for Categorizing Posts in Web\\n  Discussion Forums'},\n",
       " u'1602.08447': {'arxivid': u'1602.08447',\n",
       "  'authorsaffil': [[u'Mumtaz Ali', None],\n",
       "   [u'Nguyen Van Minh', None],\n",
       "   [u'Le Hoang Son', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'Keywords: Medical diagnosis, neutrosophic set, neutrosophic\\n  recommender system, non-linear regression model',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08447v1',\n",
       "  'published': u'2016-02-25T03:20:00Z',\n",
       "  'summary': u'  Neutrosophic set has the ability to handle uncertain, incomplete,\\ninconsistent, indeterminate information in a more accurate way. In this paper,\\nwe proposed a neutrosophic recommender system to predict the diseases based on\\nneutrosophic set which includes single-criterion neutrosophic recommender\\nsystem (SC-NRS) and multi-criterion neutrosophic recommender system (MC-NRS).\\nFurther, we investigated some algebraic operations of neutrosophic recommender\\nsystem such as union, complement, intersection, probabilistic sum, bold sum,\\nbold intersection, bounded difference, symmetric difference, convex linear sum\\nof min and max operators, Cartesian product, associativity, commutativity and\\ndistributive. Based on these operations, we studied the algebraic structures\\nsuch as lattices, Kleen algebra, de Morgan algebra, Brouwerian algebra, BCK\\nalgebra, Stone algebra and MV algebra. In addition, we introduced several types\\nof similarity measures based on these algebraic operations and studied some of\\ntheir theoretic properties. Moreover, we accomplished a prediction formula\\nusing the proposed algebraic similarity measure. We also proposed a new\\nalgorithm for medical diagnosis based on neutrosophic recommender system.\\nFinally to check the validity of the proposed methodology, we made experiments\\non the datasets Heart, RHC, Breast cancer, Diabetes and DMD. At the end, we\\npresented the MSE and computational time by comparing the proposed algorithm\\nwith the relevant ones such as ICSM, DSM, CARE, CFMD, as well as other variants\\nnamely Variant 67, Variant 69, and Varian 71 both in tabular and graphical form\\nto analyze the efficiency and accuracy. Finally we analyzed the strength of all\\n8 algorithms by ANOVA statistical tool.\\n',\n",
       "  'title': u'A Neutrosophic Recommender System for Medical Diagnosis Based on\\n  Algebraic Neutrosophic Measures'},\n",
       " u'1603.08161': {'arxivid': u'1603.08161',\n",
       "  'authorsaffil': [[u'Matthias Innmann', None],\n",
       "   [u'Michael Zollh\\xf6fer', None],\n",
       "   [u'Matthias Nie\\xdfner', None],\n",
       "   [u'Christian Theobalt', None],\n",
       "   [u'Marc Stamminger', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08161v1',\n",
       "  'published': u'2016-03-27T02:09:03Z',\n",
       "  'summary': u\"  We present a novel approach for the reconstruction of dynamic geometric\\nshapes using a single hand-held consumer-grade RGB-D sensor at real-time rates.\\nOur method does not require a pre-defined shape template to start with and\\nbuilds up the scene model from scratch during the scanning process. Geometry\\nand motion are parameterized in a unified manner by a volumetric representation\\nthat encodes a distance field of the surface geometry as well as the non-rigid\\nspace deformation. Motion tracking is based on a set of extracted sparse color\\nfeatures in combination with a dense depth-based constraint formulation. This\\nenables accurate tracking and drastically reduces drift inherent to standard\\nmodel-to-depth alignment. We cast finding the optimal deformation of space as a\\nnon-linear regularized variational optimization problem by enforcing local\\nsmoothness and proximity to the input constraints. The problem is tackled in\\nreal-time at the camera's capture rate using a data-parallel flip-flop\\noptimization strategy. Our results demonstrate robust tracking even for fast\\nmotion and scenes that lack geometric features.\\n\",\n",
       "  'title': u'VolumeDeform: Real-time Volumetric Non-rigid Reconstruction'},\n",
       " u'1604.00147': {'arxivid': u'1604.00147',\n",
       "  'authorsaffil': [[u'Lijuan Zhou', None],\n",
       "   [u'Wanqing Li', None],\n",
       "   [u'Philip Ogunbona', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted by the 2016 IEEE International Conference on Multimedia and\\n  Expo (ICME 2016). 6 pages paper and 4 pages supplementary material',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00147v1',\n",
       "  'published': u'2016-04-01T06:24:31Z',\n",
       "  'summary': u'  This paper presents a novel method for learning a pose lexicon comprising\\nsemantic poses defined by textual instructions and their associated visual\\nposes defined by visual features. The proposed method simultaneously takes two\\ninput streams, semantic poses and visual pose candidates, and statistically\\nlearns a mapping between them to construct the lexicon. With the learned\\nlexicon, action recognition can be cast as the problem of finding the maximum\\ntranslation probability of a sequence of semantic poses given a stream of\\nvisual pose candidates. Experiments evaluating pre-trained and zero-shot action\\nrecognition conducted on MSRC-12 gesture and WorkoutSu-10 exercise datasets\\nwere used to verify the efficacy of the proposed method.\\n',\n",
       "  'title': u'Learning a Pose Lexicon for Semantic Action Recognition'},\n",
       " u'1602.08448': {'arxivid': u'1602.08448',\n",
       "  'authorsaffil': [[u'Daniel Russo', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08448v2',\n",
       "  'published': u'2016-02-26T19:39:01Z',\n",
       "  'summary': u'  This paper considers the optimal adaptive allocation of measurement effort\\nfor identifying the best among a finite set of options or designs. An\\nexperimenter sequentially chooses designs to measure and observes noisy signals\\nof their quality with the goal of confidently identifying the best design after\\na small number of measurements. I propose three simple Bayesian algorithms for\\nadaptively allocating measurement effort. One is Top-Two Probability sampling,\\nwhich computes the two designs with the highest posterior probability of being\\noptimal, and then randomizes to select among these two. One is a variant a\\ntop-two sampling which considers not only the probability a design is optimal,\\nbut the expected amount by which it exceeds other designs. The final algorithm\\nis a modified version of Thompson sampling that is tailored for identifying the\\nbest design. I prove that these simple algorithms satisfy a strong optimality\\nproperty. In a frequestist setting where the true quality of the designs is\\nfixed, the posterior is said to be consistent if it correctly identifies the\\noptimal design, in the sense that that the posterior probability assigned to\\nthe event that some other design is optimal converges to zero as measurements\\nare collected. I show that under the proposed algorithms this convergence\\noccurs at an exponential rate, and the corresponding exponent is the best\\npossible among all allocation\\n',\n",
       "  'title': u'Simple Bayesian Algorithms for Best Arm Identification'},\n",
       " u'1603.01855': {'arxivid': u'1603.01855',\n",
       "  'authorsaffil': [[u'Sougata Chaudhuri', None], [u'Ambuj Tewari', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Appearing in AISTATS 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01855v1',\n",
       "  'published': u'2016-03-06T18:43:54Z',\n",
       "  'summary': u'  We consider an online learning to rank setting in which, at each round, an\\noblivious adversary generates a list of $m$ documents, pertaining to a query,\\nand the learner produces scores to rank the documents. The adversary then\\ngenerates a relevance vector and the learner updates its ranker according to\\nthe feedback received. We consider the setting where the feedback is restricted\\nto be the relevance levels of only the top $k$ documents in the ranked list for\\n$k \\\\ll m$. However, the performance of learner is judged based on the\\nunrevealed full relevance vectors, using an appropriate learning to rank loss\\nfunction. We develop efficient algorithms for well known losses in the\\npointwise, pairwise and listwise families. We also prove that no online\\nalgorithm can have sublinear regret, with top-1 feedback, for any loss that is\\ncalibrated with respect to NDCG. We apply our algorithms on benchmark datasets\\ndemonstrating efficient online learning of a ranking function from highly\\nrestricted feedback.\\n',\n",
       "  'title': u'Online Learning to Rank with Feedback at the Top'},\n",
       " u'1601.06608': {'arxivid': u'1601.06608',\n",
       "  'authorsaffil': [[u'Mrinal Haloi', None],\n",
       "   [u'Samarendra Dandapat', None],\n",
       "   [u'Rohit Sinha', None]],\n",
       "  'categoryterms': [u'cs.CV', u'68T45'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06608v1',\n",
       "  'published': u'2016-01-25T14:05:36Z',\n",
       "  'summary': u'  In this work, we have presented a novel method for detection of retinal image\\nfeatures, the optic disc and the fovea, from colour fundus photographs of\\ndilated eyes for Computer-aided Diagnosis(CAD) system. A saliency map based\\nmethod was used to detect the optic disc followed by an unsupervised\\nprobabilistic Latent Semantic Analysis for detection validation. The validation\\nconcept is based on distinct vessels structures in the optic disc. By using the\\nclinical information of standard location of the fovea with respect to the\\noptic disc, the macula region is estimated. Accuracy of 100\\\\% detection is\\nachieved for the optic disc and the macula on MESSIDOR and DIARETDB1 and 98.8\\\\%\\ndetection accuracy on STARE dataset.\\n',\n",
       "  'title': u'An Unsupervised Method for Detection and Validation of The Optic Disc\\n  and The Fovea'},\n",
       " u'1603.08148': {'arxivid': u'1603.08148',\n",
       "  'authorsaffil': [[u'Caglar Gulcehre', None],\n",
       "   [u'Sungjin Ahn', None],\n",
       "   [u'Ramesh Nallapati', None],\n",
       "   [u'Bowen Zhou', None],\n",
       "   [u'Yoshua Bengio', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08148v2',\n",
       "  'published': u'2016-03-26T22:31:57Z',\n",
       "  'summary': u'  The problem of rare and unknown words is an important issue that can\\npotentially effect the performance of many NLP systems, including both\\ntraditional count based and deep learning models. We propose a novel way to\\ndeal with the rare and unseen words for the neural network models with\\nattention. Our model uses two softmax layers in order to predict the next word\\nin conditional language models: one of the softmax layers predicts the location\\nof a word in the source sentence, and the other softmax layer predicts a word\\nin the shortlist vocabulary. The decision of which softmax layer to use at each\\ntimestep is adaptively made by an MLP which is conditioned on the context. We\\nmotivate this work from a psychological evidence that humans naturally have a\\ntendency to point towards objects in the context or the environment when the\\nname of an object is not known. Using our proposed model, we observe\\nimprovements in two tasks, neural machine translation on the Europarl English\\nto French parallel corpora and text summarization on the Gigaword dataset.\\n',\n",
       "  'title': u'Pointing the Unknown Words'},\n",
       " u'1601.01887': {'arxivid': u'1601.01887',\n",
       "  'authorsaffil': [[u'Rustam Tagiew', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.DL'],\n",
       "  'comment': u'5 pages, 2 figure',\n",
       "  'doi': u'10.13140/RG.2.1.1619.1847',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01887v1',\n",
       "  'published': u'2016-01-08T14:29:44Z',\n",
       "  'summary': u\"  The number of scientific papers grows exponentially in many disciplines. The\\nshare of online available papers grows as well. At the same time, the period of\\ntime for a paper to loose at chance to be cited anymore shortens. The decay of\\nthe citing rate shows similarity to ultradiffusional processes as for other\\nonline contents in social networks. The distribution of papers per author shows\\nsimilarity to the distribution of posts per user in social networks. The rate\\nof uncited papers for online available papers grows while some papers 'go\\nviral' in terms of being cited. Summarized, the practice of scientific\\npublishing moves towards the domain of social networks. The goal of this\\nproject is to create a text engineering tool, which can semi-automatically\\ncategorize a paper according to its type of contribution and extract\\nrelationships between them into an ontological database. Semi-automatic\\ncategorization means that the mistakes made by automatic pre-categorization and\\nrelationship-extraction will be corrected through a wikipedia-like front-end by\\nvolunteers from general public. This tool should not only help researchers and\\nthe general public to find relevant supplementary material and peers faster,\\nbut also provide more information for research funding agencies.\\n\",\n",
       "  'title': u'Research Project: Text Engineering Tool for Ontological Scientometry'},\n",
       " u'1601.01885': {'arxivid': u'1601.01885',\n",
       "  'authorsaffil': [[u'Anguelos Nicolaou', None],\n",
       "   [u'Andrew Bagdanov', None],\n",
       "   [u'Lluis Gomez-Bigorda', None],\n",
       "   [u'Dimosthenis Karatzas', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01885v1',\n",
       "  'published': u'2016-01-08T14:25:20Z',\n",
       "  'summary': u'  In this paper we introduce a script identification method based on\\nhand-crafted texture features and an artificial neural network. The proposed\\npipeline achieves near state-of-the-art performance for script identification\\nof video-text and state-of-the-art performance on visual language\\nidentification of handwritten text. More than using the deep network as a\\nclassifier, the use of its intermediary activations as a learned metric\\ndemonstrates remarkable results and allows the use of discriminative models on\\nunknown classes. Comparative experiments in video-text and text in the wild\\ndatasets provide insights on the internals of the proposed deep network.\\n',\n",
       "  'title': u'Visual Script and Language Identification'},\n",
       " u'1604.00647': {'arxivid': u'1604.00647',\n",
       "  'authorsaffil': [[u'Lucas Drumond', None],\n",
       "   [u'Ernesto Diaz-Aviles', None],\n",
       "   [u'Lars Schmidt-Thieme', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'Keywords: Multi-Relational Learning, Distributed Learning,\\n  Factorization Models, ADMM',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00647v1',\n",
       "  'published': u'2016-04-03T15:42:36Z',\n",
       "  'summary': u\"  Learning from multiple-relational data which contains noise, ambiguities, or\\nduplicate entities is essential to a wide range of applications such as\\nstatistical inference based on Web Linked Data, recommender systems,\\ncomputational biology, and natural language processing. These tasks usually\\nrequire working with very large and complex datasets - e.g., the Web graph -\\nhowever, current approaches to multi-relational learning are not practical for\\nsuch scenarios due to their high computational complexity and poor scalability\\non large data.\\n  In this paper, we propose a novel and scalable approach for multi-relational\\nfactorization based on consensus optimization. Our model, called ConsMRF, is\\nbased on the Alternating Direction Method of Multipliers (ADMM) framework,\\nwhich enables us to optimize each target relation using a smaller set of\\nparameters than the state-of-the-art competitors in this task.\\n  Due to ADMM's nature, ConsMRF can be easily parallelized which makes it\\nsuitable for large multi-relational data. Experiments on large Web datasets -\\nderived from DBpedia, Wikipedia and YAGO - show the efficiency and performance\\nimprovement of ConsMRF over strong competitors. In addition, ConsMRF\\nnear-linear scalability indicates great potential to tackle Web-scale problem\\nsizes.\\n\",\n",
       "  'title': u'Multi-Relational Learning at Scale with ADMM'},\n",
       " u'1604.00133': {'arxivid': u'1604.00133',\n",
       "  'authorsaffil': [[u'Liang Zheng', None],\n",
       "   [u'Yali Zhao', None],\n",
       "   [u'Shengjin Wang', None],\n",
       "   [u'Jingdong Wang', None],\n",
       "   [u'Qi Tian', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages. It will be submitted to an appropriate journal',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00133v1',\n",
       "  'published': u'2016-04-01T05:31:57Z',\n",
       "  'summary': u'  The objective of this paper is the effective transfer of the Convolutional\\nNeural Network (CNN) feature in image search and classification.\\nSystematically, we study three facts in CNN transfer. 1) We demonstrate the\\nadvantage of using images with a properly large size as input to CNN instead of\\nthe conventionally resized one. 2) We benchmark the performance of different\\nCNN layers improved by average/max pooling on the feature maps. Our observation\\nsuggests that the Conv5 feature yields very competitive accuracy under such\\npooling step. 3) We find that the simple combination of pooled features\\nextracted across various CNN layers is effective in collecting evidences from\\nboth low and high level descriptors. Following these good practices, we are\\ncapable of improving the state of the art on a number of benchmarks to a large\\nmargin.\\n',\n",
       "  'title': u'Good Practice in CNN Feature Transfer'},\n",
       " u'1604.00644': {'arxivid': u'1604.00644',\n",
       "  'authorsaffil': [[u'Karine da Silva Miras de Ara\\xfajo', None],\n",
       "   [u'Fabr\\xedcio Olivetti de Fran\\xe7a', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.AI'],\n",
       "  'comment': u'This paper is a translation of \\\\cite{karine2015}, published in\\n  Portuguese at Brazilian Congress on Computational Intelligence, 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00644v2',\n",
       "  'published': u'2016-04-03T14:57:24Z',\n",
       "  'summary': u'  One of the common artificial intelligence applications in electronic games\\nconsists of making an artificial agent learn how to execute some determined\\ntask successfully in a game environment. One way to perform this task is\\nthrough machine learning algorithms capable of learning the sequence of actions\\nrequired to win in a given game environment. There are several supervised\\nlearning techniques able to learn the correct answer for a problem through\\nexamples. However, when learning how to play electronic games, the correct\\nanswer might only be known by the end of the game, after all the actions were\\nalready taken. Thus, not being possible to measure the accuracy of each\\nindividual action to be taken at each time step. A way for dealing with this\\nproblem is through Neuroevolution, a method which trains Artificial Neural\\nNetworks using evolutionary algorithms. In this article, we introduce a\\nframework for testing optimization algorithms with artificial agent controllers\\nin electronic games, called EvoMan, which is inspired in the action-platformer\\ngame Mega Man II. The environment can be configured to run in different\\nexperiment modes, as single evolution, coevolution and others. To demonstrate\\nsome challenges regarding the proposed platform, as initial experiments we\\napplied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the\\ncontext of competitively coevolving two distinct agents in this game.\\n',\n",
       "  'title': u'An electronic-game framework for evaluating coevolutionary algorithms'},\n",
       " u'1511.06973': {'arxivid': u'1511.06973',\n",
       "  'authorsaffil': [[u'Qi Wu', None],\n",
       "   [u'Peng Wang', None],\n",
       "   [u'Chunhua Shen', None],\n",
       "   [u'Anthony Dick', None],\n",
       "   [u'Anton van den Hengel', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted to IEEE Conf. Computer Vision and Pattern Recognition',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06973v2',\n",
       "  'published': u'2015-11-22T07:08:14Z',\n",
       "  'summary': u'  We propose a method for visual question answering which combines an internal\\nrepresentation of the content of an image with information extracted from a\\ngeneral knowledge base to answer a broad range of image-based questions. This\\nallows more complex questions to be answered using the predominant neural\\nnetwork-based approach than has previously been possible. It particularly\\nallows questions to be asked about the contents of an image, even when the\\nimage itself does not contain the whole answer. The method constructs a textual\\nrepresentation of the semantic content of an image, and merges it with textual\\ninformation sourced from a knowledge base, to develop a deeper understanding of\\nthe scene viewed. Priming a recurrent neural network with this combined\\ninformation, and the submitted question, leads to a very flexible visual\\nquestion answering approach. We are specifically able to answer questions posed\\nin natural language, that refer to information not contained in the image. We\\ndemonstrate the effectiveness of our model on two publicly available datasets,\\nToronto COCO-QA and MS COCO-VQA and show that it produces the best reported\\nresults in both cases.\\n',\n",
       "  'title': u'Ask Me Anything: Free-form Visual Question Answering Based on Knowledge\\n  from External Sources'},\n",
       " u'1604.00136': {'arxivid': u'1604.00136',\n",
       "  'authorsaffil': [[u'Pia Bideau', None], [u'Erik Learned-Miller', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00136v1',\n",
       "  'published': u'2016-04-01T05:37:26Z',\n",
       "  'summary': u'  The human ability to detect and segment moving objects works in the presence\\nof multiple objects, complex background geometry, motion of the observer, and\\neven camouflage. In addition to all of this, the ability to detect motion is\\nnearly instantaneous. While there has been much recent progress in motion\\nsegmentation, it still appears we are far from human capabilities. In this\\nwork, we derive from first principles a new likelihood function for assessing\\nthe probability of an optical flow vector given the 3D motion direction of an\\nobject. This likelihood uses a novel combination of the angle and magnitude of\\nthe optical flow to maximize the information about the true motions of objects.\\nUsing this new likelihood and several innovations in initialization, we develop\\na motion segmentation algorithm that beats current state-of-the-art methods by\\na large margin. We compare to five state-of-the-art methods on two established\\nbenchmarks, and a third new data set of camouflaged animals, which we introduce\\nto push motion segmentation to the next level.\\n',\n",
       "  'title': u\"It's Moving! A Probabilistic Model for Causal Motion Segmentation in\\n  Moving Camera Videos\"},\n",
       " u'1603.03915': {'arxivid': u'1603.03915',\n",
       "  'authorsaffil': [[u'Baoguang Shi', None],\n",
       "   [u'Xinggang Wang', None],\n",
       "   [u'Pengyuan Lyu', None],\n",
       "   [u'Cong Yao', None],\n",
       "   [u'Xiang Bai', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted by CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03915v2',\n",
       "  'published': u'2016-03-12T13:58:27Z',\n",
       "  'summary': u'  Recognizing text in natural images is a challenging task with many unsolved\\nproblems. Different from those in documents, words in natural images often\\npossess irregular shapes, which are caused by perspective distortion, curved\\ncharacter placement, etc. We propose RARE (Robust text recognizer with\\nAutomatic REctification), a recognition model that is robust to irregular text.\\nRARE is a specially-designed deep neural network, which consists of a Spatial\\nTransformer Network (STN) and a Sequence Recognition Network (SRN). In testing,\\nan image is firstly rectified via a predicted Thin-Plate-Spline (TPS)\\ntransformation, into a more \"readable\" image for the following SRN, which\\nrecognizes text through a sequence recognition approach. We show that the model\\nis able to recognize several types of irregular text, including perspective\\ntext and curved text. RARE is end-to-end trainable, requiring only images and\\nassociated text labels, making it convenient to train and deploy the model in\\npractical systems. State-of-the-art or highly-competitive performance achieved\\non several benchmarks well demonstrates the effectiveness of the proposed\\nmodel.\\n',\n",
       "  'title': u'Robust Scene Text Recognition with Automatic Rectification'},\n",
       " u'1603.08212': {'arxivid': u'1603.08212',\n",
       "  'authorsaffil': [[u'Ita Lifshitz', None],\n",
       "   [u'Ethan Fetaya', None],\n",
       "   [u'Shimon Ullman', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08212v1',\n",
       "  'published': u'2016-03-27T12:45:33Z',\n",
       "  'summary': u'  In this paper we consider the problem of human pose estimation from a single\\nstill image. We propose a novel approach where each location in the image votes\\nfor the position of each keypoint using a convolutional neural net. The voting\\nscheme allows us to utilize information from the whole image, rather than rely\\non a sparse set of keypoint locations. Using dense, multi-target votes, not\\nonly produces good keypoint predictions, but also enables us to compute\\nimage-dependent joint keypoint probabilities by looking at consensus voting.\\nThis differs from most previous methods where joint probabilities are learned\\nfrom relative keypoint locations and are independent of the image. We finally\\ncombine the keypoints votes and joint probabilities in order to identify the\\noptimal pose configuration. We show our competitive performance on the MPII\\nHuman Pose and Leeds Sports Pose datasets.\\n',\n",
       "  'title': u'Human Pose Estimation using Deep Consensus Voting'},\n",
       " u'1603.03911': {'arxivid': u'1603.03911',\n",
       "  'authorsaffil': [[u'Laura Sevilla-Lara', None],\n",
       "   [u'Deqing Sun', None],\n",
       "   [u'Varun Jampani', None],\n",
       "   [u'Michael J. Black', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03911v2',\n",
       "  'published': u'2016-03-12T13:34:09Z',\n",
       "  'summary': u'  Existing optical flow methods make generic, spatially homogeneous,\\nassumptions about the spatial structure of the flow. In reality, optical flow\\nvaries across an image depending on object class. Simply put, different objects\\nmove differently. Here we exploit recent advances in static semantic scene\\nsegmentation to segment the image into objects of different types. We define\\ndifferent models of image motion in these regions depending on the type of\\nobject. For example, we model the motion on roads with homographies, vegetation\\nwith spatially smooth flow, and independently moving objects like cars and\\nplanes with affine motion plus deviations. We then pose the flow estimation\\nproblem using a novel formulation of localized layers, which addresses\\nlimitations of traditional layered models for dealing with complex scene\\nmotion. Our semantic flow method achieves the lowest error of any published\\nmonocular method in the KITTI-2015 flow benchmark and produces qualitatively\\nbetter flow and segmentation than recent top methods on a wide range of natural\\nvideos.\\n',\n",
       "  'title': u'Optical Flow with Semantic Segmentation and Localized Layers'},\n",
       " u'1603.07294': {'arxivid': u'1603.07294',\n",
       "  'authorsaffil': [[u'James Foulds', None],\n",
       "   [u'Joseph Geumlek', None],\n",
       "   [u'Max Welling', None],\n",
       "   [u'Kamalika Chaudhuri', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.CR', u'stat.ML'],\n",
       "  'comment': u'Updated to match the accepted UAI version. Generalized the ARE result\\n  and included a more detailed proof. Improved some figures, etc',\n",
       "  'doi': None,\n",
       "  'journalref': u'Proceedings of the 32nd Conference on Uncertainty in Artificial\\n  Intelligence (UAI), 2016',\n",
       "  'link': u'http://arxiv.org/abs/1603.07294v2',\n",
       "  'published': u'2016-03-23T18:31:05Z',\n",
       "  'summary': u'  Bayesian inference has great promise for the privacy-preserving analysis of\\nsensitive data, as posterior sampling automatically preserves differential\\nprivacy, an algorithmic notion of data privacy, under certain conditions\\n(Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample\\n(OPS) approach elegantly provides privacy \"for free,\" it is data inefficient in\\nthe sense of asymptotic relative efficiency (ARE). We show that a simple\\nalternative based on the Laplace mechanism, the workhorse of differential\\nprivacy, is as asymptotically efficient as non-private posterior inference,\\nunder general assumptions. This technique also has practical advantages\\nincluding efficient use of the privacy budget for MCMC. We demonstrate the\\npracticality of our approach on a time-series analysis of sensitive military\\nrecords from the Afghanistan and Iraq wars disclosed by the Wikileaks\\norganization.\\n',\n",
       "  'title': u'On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis'},\n",
       " u'1603.06805': {'arxivid': u'1603.06805',\n",
       "  'authorsaffil': [[u'Dieter Hendricks', None]],\n",
       "  'categoryterms': [u'q-fin.TR', u'cs.LG', u'q-fin.CP'],\n",
       "  'comment': u'17 pages, 6 figures, 3 tables, under consideration at Pattern\\n  Recognition Letters',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06805v1',\n",
       "  'published': u'2016-03-22T14:23:42Z',\n",
       "  'summary': u'  We present a scheme for online, unsupervised state discovery and detection\\nfrom streaming, multi-featured, asynchronous data in high-frequency financial\\nmarkets. Online feature correlations are computed using an unbiased, lossless\\nFourier estimator. A high-speed maximum likelihood clustering algorithm is then\\nused to find the feature cluster configuration which best explains the\\nstructure in the correlation matrix. We conjecture that this feature\\nconfiguration is a candidate descriptor for the temporal state of the system.\\nUsing a simple cluster configuration similarity metric, we are able to\\nenumerate the state space based on prevailing feature configurations. The\\nproposed state representation removes the need for human-driven data\\npre-processing for state attribute specification, allowing a learning agent to\\nfind structure in streaming data, discern changes in the system, enumerate its\\nperceived state space and learn suitable action-selection policies.\\n',\n",
       "  'title': u'Using real-time cluster configurations of streaming asynchronous\\n  features as online state descriptors in financial markets'},\n",
       " u'1603.03170': {'arxivid': u'1603.03170',\n",
       "  'authorsaffil': [[u'Laurent Romary', u'CMB, ALPAGE'],\n",
       "   [u'Mike Mertens', u'CMB, ALPAGE'],\n",
       "   [u'Anne Baillot', u'CMB, ALPAGE']],\n",
       "  'categoryterms': [u'cs.CY', u'cs.CL', u'cs.DL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': u'BIBLIOTHEK Forschung und Praxis, De Gruyter, 2016, 39 (3),\\n  pp.350-357',\n",
       "  'link': u'http://arxiv.org/abs/1603.03170v2',\n",
       "  'published': u'2016-03-10T07:43:15Z',\n",
       "  'summary': u'  This paper provides both an update concerning the setting up of the European\\nDARIAH infrastructure and a series of strong action lines related to the\\ndevelopment of a data centred strategy for the humanities in the coming years.\\nIn particular we tackle various aspect of data management: data hosting, the\\nsetting up of a DARIAH seal of approval, the establishment of a charter between\\ncultural heritage institutions and scholars and finally a specific view on\\ncertification mechanisms for data.\\n',\n",
       "  'title': u'Data fluidity in DARIAH -- pushing the agenda forward'},\n",
       " u'1508.06013': {'arxivid': u'1508.06013',\n",
       "  'authorsaffil': [[u'Zeinab Bahmani', None],\n",
       "   [u'Leopoldo Bertossi', None],\n",
       "   [u'Nikolaos Vasiloglou', None]],\n",
       "  'categoryterms': [u'cs.DB', u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'To appear in Proc. SUM, 2015',\n",
       "  'doi': None,\n",
       "  'journalref': u\"Proc. SUM'15, 2015, Springer LNAI 9310, pp. 399-414\",\n",
       "  'link': u'http://arxiv.org/abs/1508.06013v1',\n",
       "  'published': u'2015-08-25T02:35:58Z',\n",
       "  'summary': u'  Entity resolution (ER), an important and common data cleaning problem, is\\nabout detecting data duplicate representations for the same external entities,\\nand merging them into single representations. Relatively recently, declarative\\nrules called matching dependencies (MDs) have been proposed for specifying\\nsimilarity conditions under which attribute values in database records are\\nmerged. In this work we show the process and the benefits of integrating three\\ncomponents of ER: (a) Classifiers for duplicate/non-duplicate record pairs\\nbuilt using machine learning (ML) techniques, (b) MDs for supporting both the\\nblocking phase of ML and the merge itself; and (c) The use of the declarative\\nlanguage LogiQL -an extended form of Datalog supported by the LogicBlox\\nplatform- for data processing, and the specification and enforcement of MDs.\\n',\n",
       "  'title': u'ERBlox: Combining Matching Dependencies with Machine Learning for Entity\\n  Resolution'},\n",
       " u'1602.02995': {'arxivid': u'1602.02995',\n",
       "  'authorsaffil': [[u'Colin Lea', None],\n",
       "   [u'Austin Reiter', None],\n",
       "   [u'Rene Vidal', None],\n",
       "   [u'Gregory D. Hager', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.RO'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02995v2',\n",
       "  'published': u'2016-02-09T14:28:44Z',\n",
       "  'summary': u'  Joint segmentation and classification of fine-grained actions is important\\nfor applications in human-robot interaction, video surveillance, and human\\nskill evaluation. However, despite substantial recent progress in large scale\\naction classification, the performance of state-of-the-art fine-grained action\\nrecognition approaches remains low. In this paper, we propose a new\\nspatio-temporal CNN model for fine-grained action classification and\\nsegmentation, which combines (1) a spatial CNN to represent objects in the\\nscene and their spatial relationships; (2) a temporal CNN that captures how\\nobject relationships within an action change over time; and (3) a semi-Markov\\nmodel that captures transitions from one action to another. In addition, we\\nintroduce an efficient segmental inference algorithm for joint segmentation and\\nclassification of actions that is orders of magnitude faster than\\nstate-of-the-art approaches. We highlight the effectiveness of our approach on\\ncooking and surgical action datasets for which we observe substantially\\nimproved performance relative to recent baseline methods.\\n',\n",
       "  'title': u'Segmental Spatio-Temporal CNNs for Fine-grained Action Segmentation and\\n  Classification'},\n",
       " u'1602.02990': {'arxivid': u'1602.02990',\n",
       "  'authorsaffil': [[u'Ralf Der', None], [u'Georg Martius', None]],\n",
       "  'categoryterms': [u'cs.RO',\n",
       "   u'cs.LG',\n",
       "   u'cs.SY',\n",
       "   u'37N35, 68T05, 68T40, 93C40',\n",
       "   u'I.2.9; I.2.6'],\n",
       "  'comment': u'13 pages, 2 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02990v1',\n",
       "  'published': u'2016-02-09T14:16:26Z',\n",
       "  'summary': u\"  With the accelerated development of robot technologies, optimal control\\nbecomes one of the central themes of research. In traditional approaches, the\\ncontroller, by its internal functionality, finds appropriate actions on the\\nbasis of the history of sensor values, guided by the goals, intentions,\\nobjectives, learning schemes, and so on planted into it. The idea is that the\\ncontroller controls the world---the body plus its environment---as reliably as\\npossible. This paper advocates for a new paradigm of control, obtained by\\nmaking the world control its controller in the first place. The paper presents\\na solution with a controller that is devoid of any functionalities of its own,\\ngiven by a fixed, explicit and context-free function of the recent history of\\nthe sensor values. When applying this controller to a muscle-tendon driven\\narm-shoulder system from the Myorobotics toolkit, we observe a vast variety of\\nself-organized behavior patterns: when left alone, the arm realizes\\npseudo-random sequences of different poses but one can also manipulate the\\nsystem into definite motion patterns. But most interestingly, after attaching\\nan object, the controller gets in a functional resonance with the object's\\ninternal dynamics: when given a half-filled bottle, the system spontaneously\\nstarts shaking the bottle so that maximum response from the dynamics of the\\nwater is being generated. After attaching a pendulum to the arm, the controller\\ndrives the pendulum into a circular mode. In this way, the robot discovers\\naffordances of objects its body is interacting with. We also discuss\\nperspectives for using this controller paradigm for intention driven behavior\\ngeneration.\\n\",\n",
       "  'title': u'The world as its own best controller: a case study with anthropomimetic\\n  robots'},\n",
       " u'1509.04186': {'arxivid': u'1509.04186',\n",
       "  'authorsaffil': [[u'Gaurav Sharma', None],\n",
       "   [u'Frederic Jurie', None],\n",
       "   [u'Cordelia Schmid', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted for publication in IEEE Transactions on Pattern Analysis and\\n  Machine Intelligence (TPAMI)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.04186v2',\n",
       "  'published': u'2015-09-14T16:33:04Z',\n",
       "  'summary': u\"  We introduce an Expanded Parts Model (EPM) for recognizing human attributes\\n(e.g. young, short hair, wearing suit) and actions (e.g. running, jumping) in\\nstill images. An EPM is a collection of part templates which are learnt\\ndiscriminatively to explain specific scale-space regions in the images (in\\nhuman centric coordinates). This is in contrast to current models which consist\\nof a relatively few (i.e. a mixture of) 'average' templates. EPM uses only a\\nsubset of the parts to score an image and scores the image sparsely in space,\\ni.e. it ignores redundant and random background in an image. To learn our\\nmodel, we propose an algorithm which automatically mines parts and learns\\ncorresponding discriminative templates together with their respective locations\\nfrom a large number of candidate parts. We validate our method on three recent\\nchallenging datasets of human attributes and actions. We obtain convincing\\nqualitative and state-of-the-art quantitative results on the three datasets.\\n\",\n",
       "  'title': u'Expanded Parts Model for Semantic Description of Humans in Still Images'},\n",
       " u'1512.00965': {'arxivid': u'1512.00965',\n",
       "  'authorsaffil': [[u'Pengcheng Yin', None],\n",
       "   [u'Zhengdong Lu', None],\n",
       "   [u'Hang Li', None],\n",
       "   [u'Ben Kao', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.00965v2',\n",
       "  'published': u'2015-12-03T06:46:27Z',\n",
       "  'summary': u'  We proposed Neural Enquirer as a neural network architecture to execute a\\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\\nNeural Enquirer finds the distributed representation of a query and then\\nexecutes it on knowledge-base tables to obtain the answer as one of the values\\nin the tables. Unlike similar efforts in end-to-end training of semantic\\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\\ndistributional representation of the query and the knowledge-base, but also\\nrealizes the execution of compositional queries as a series of differentiable\\noperations, with intermediate results (consisting of annotations of the tables\\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\\ntrained with gradient descent, with which not only the parameters of the\\ncontrolling components and semantic parsing component, but also the embeddings\\nof the tables and query words can be learned from scratch. The training can be\\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\\nstep-by-step supervision for complicated queries, and benefit from it. Neural\\nEnquirer is one step towards building neural network systems which seek to\\nunderstand language by executing it on real-world. Our experiments show that\\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\\nwith rich structures.\\n',\n",
       "  'title': u'Neural Enquirer: Learning to Query Tables with Natural Language'},\n",
       " u'1603.09630': {'arxivid': u'1603.09630',\n",
       "  'authorsaffil': [[u'Pawel Swietojanski', None], [u'Steve Renals', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'Submitted to IEEE/ACM Transactions on Audio, Speech and Language\\n  Processing',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09630v1',\n",
       "  'published': u'2016-03-31T15:10:40Z',\n",
       "  'summary': u'  We present a deep neural network (DNN) acoustic model including parametrised\\nand differentiable pooling operators. Unsupervised acoustic model adaptation is\\ncast as the problem of updating the decision boundaries implemented by each\\npooling operator. In particular, we experiment with two types of pooling\\nparametrisations: learned $L_p$-norm pooling and weighted Gaussian pooling, in\\nwhich the weights of both operators are treated as speaker-dependent. We\\nperform investigations using three different large vocabulary speech\\nrecognition corpora: AMI meetings, TED talks and Switchboard conversational\\ntelephone speech. We demonstrate that differentiable pooling operators provide\\na robust and relatively low-dimensional way to adapt acoustic models, with word\\nerror rates reductions ranging from 5--20\\\\% with respect to unadapted systems,\\nwhich themselves are better than the baseline fully-connected DNN-based\\nacoustic models. We also investigate how the proposed techniques work under\\nvarious adaptation conditions including the quality of adaptation data and\\ncomplementarity to other feature- and model-space adaptation methods, as well\\nas providing an analysis of the characteristics of each of the proposed\\napproaches.\\n',\n",
       "  'title': u'Differentiable Pooling for Unsupervised Acoustic Model Adaptation'},\n",
       " u'1512.02895': {'arxivid': u'1512.02895',\n",
       "  'authorsaffil': [[u'Xiaofan Zhang', None],\n",
       "   [u'Feng Zhou', None],\n",
       "   [u'Yuanqing Lin', None],\n",
       "   [u'Shaoting Zhang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.02895v2',\n",
       "  'published': u'2015-12-09T15:22:26Z',\n",
       "  'summary': u'  Recent algorithms in convolutional neural networks (CNN) considerably advance\\nthe fine-grained image classification, which aims to differentiate subtle\\ndifferences among subordinate classes. However, previous studies have rarely\\nfocused on learning a fined-grained and structured feature representation that\\nis able to locate similar images at different levels of relevance, e.g.,\\ndiscovering cars from the same make or the same model, both of which require\\nhigh precision. In this paper, we propose two main contributions to tackle this\\nproblem. 1) A multi-task learning framework is designed to effectively learn\\nfine-grained feature representations by jointly optimizing both classification\\nand similarity constraints. 2) To model the multi-level relevance, label\\nstructures such as hierarchy or shared attributes are seamlessly embedded into\\nthe framework by generalizing the triplet loss. Extensive and thorough\\nexperiments have been conducted on three fine-grained datasets, i.e., the\\nStanford car, the car-333, and the food datasets, which contain either\\nhierarchical labels or shared attributes. Our proposed method has achieved very\\ncompetitive performance, i.e., among state-of-the-art classification accuracy.\\nMore importantly, it significantly outperforms previous fine-grained feature\\nrepresentations for image retrieval at different levels of relevance.\\n',\n",
       "  'title': u'Embedding Label Structures for Fine-Grained Feature Representation'},\n",
       " u'1403.8144': {'arxivid': u'1403.8144',\n",
       "  'authorsaffil': [[u'Ping Li', None],\n",
       "   [u'Michael Mitzenmacher', None],\n",
       "   [u'Anshumali Shrivastava', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DB', u'cs.DS', u'stat.CO'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1403.8144v1',\n",
       "  'published': u'2014-03-31T19:43:53Z',\n",
       "  'summary': u'  This technical note compares two coding (quantization) schemes for random\\nprojections in the context of sub-linear time approximate near neighbor search.\\nThe first scheme is based on uniform quantization while the second scheme\\nutilizes a uniform quantization plus a uniformly random offset (which has been\\npopular in practice). The prior work compared the two schemes in the context of\\nsimilarity estimation and training linear classifiers, with the conclusion that\\nthe step of random offset is not necessary and may hurt the performance\\n(depending on the similarity level). The task of near neighbor search is\\nrelated to similarity estimation with importance distinctions and requires own\\nstudy. In this paper, we demonstrate that in the context of near neighbor\\nsearch, the step of random offset is not needed either and may hurt the\\nperformance (sometimes significantly so, depending on the similarity and other\\nparameters).\\n',\n",
       "  'title': u'Coding for Random Projections and Approximate Near Neighbor Search'},\n",
       " u'1602.02999': {'arxivid': u'1602.02999',\n",
       "  'authorsaffil': [[u'Bappaditya Mandal', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'11 pages, 4 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02999v1',\n",
       "  'published': u'2016-02-09T14:31:46Z',\n",
       "  'summary': u'  In this paper, we analyze some of our real-world deployment of face\\nrecognition (FR) systems for various applications and discuss the gaps between\\nexpectations of the user and what the system can deliver. We evaluate some of\\nour proposed algorithms with ad-hoc modifications for applications such as FR\\non wearable devices (like Google Glass), monitoring of elderly people in senior\\ncitizens centers, FR of children in child care centers and face matching\\nbetween a scanned IC/passport face image and a few live webcam images for\\nautomatic hotel/resort checkouts. We describe each of these applications, the\\nchallenges involved and proposed solutions. Since FR is intuitive in nature and\\nwe human beings use it for interactions with the outside world, people have\\nhigh expectations of its performance in real-world scenarios. However, we\\nanalyze and discuss here that it is not the case, machine recognition of faces\\nfor each of these applications poses unique challenges and demands specific\\nresearch components so as to adapt in the actual sites.\\n',\n",
       "  'title': u'Face Recognition: Perspectives from the Real-World'},\n",
       " u'1603.07453': {'arxivid': u'1603.07453',\n",
       "  'authorsaffil': [[u'Bruno Woltzenlogel Paleo', None]],\n",
       "  'categoryterms': [u'cs.LO', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07453v1',\n",
       "  'published': u'2016-03-24T07:00:33Z',\n",
       "  'summary': u'  This paper argues that a combined treatment of probabilities, time and\\nactions is essential for an appropriate logical account of the notion of\\nprobability; and, based on this intuition, describes an expressive\\nprobabilistic temporal logic for reasoning about actions with uncertain\\noutcomes. The logic is modal and higher-order: modalities annotated by actions\\nare used to express possibility and necessity of propositions in the next\\nstates resulting from the actions, and a higher-order function is needed to\\nexpress the probability operator. The proposed logic is shown to be an adequate\\nextension of classical mathematical probability theory, and its expressiveness\\nis illustrated through the formalization of the Monty Hall problem.\\n',\n",
       "  'title': u'An Expressive Probabilistic Temporal Logic'},\n",
       " u'1601.05644': {'arxivid': u'1601.05644',\n",
       "  'authorsaffil': [[u'Weilong Peng',\n",
       "    u'School of Computer Science, Tianjin University'],\n",
       "   [u'Zhiyong Feng', u'School of Computer Science, Tianjin University'],\n",
       "   [u'Chao Xu', u'School of Software, Tianjin University']],\n",
       "  'categoryterms': [u'cs.CV', u'cs.GR'],\n",
       "  'comment': u'9 pages, 6 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05644v1',\n",
       "  'published': u'2016-01-21T14:11:40Z',\n",
       "  'summary': u'  Recently, many methods have been proposed for face reconstruction from\\nmultiple images, most of which involve fundamental principles of Shape from\\nShading and Structure from motion. However, a majority of the methods just\\ngenerate discrete surface model of face. In this paper, B-spline Shape from\\nMotion and Shading (BsSfMS) is proposed to reconstruct continuous B-spline\\nsurface for multi-view face images, according to an assumption that shading and\\nmotion information in the images contain 1st- and 0th-order derivative of\\nB-spline face respectively. Face surface is expressed as a B-spline surface\\nthat can be reconstructed by optimizing B-spline control points. Therefore,\\nnormals and 3D feature points computed from shading and motion of images\\nrespectively are used as the 1st- and 0th- order derivative information, to be\\njointly applied in optimizing the B-spline face. Additionally, an IMLS\\n(iterative multi-least-square) algorithm is proposed to handle the difficult\\ncontrol point optimization. Furthermore, synthetic samples and LFW dataset are\\nintroduced and conducted to verify the proposed approach, and the experimental\\nresults demonstrate the effectiveness with different poses, illuminations,\\nexpressions etc., even with wild images.\\n',\n",
       "  'title': u'B-spline Shape from Motion & Shading: An Automatic Free-form Surface\\n  Modeling for Face Reconstruction'},\n",
       " u'1601.05647': {'arxivid': u'1601.05647',\n",
       "  'authorsaffil': [[u'Milos Cernak', None],\n",
       "   [u'Afsaneh Asaei', None],\n",
       "   [u'Herv\\xe9 Bourlard', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05647v2',\n",
       "  'published': u'2016-01-21T14:15:41Z',\n",
       "  'summary': u'  The speech signal conveys information on different time scales from short\\ntime scale or segmental, associated to phonological and phonetic information to\\nlong time scale or supra segmental, associated to syllabic and prosodic\\ninformation. Linguistic and neurocognitive studies recognize the phonological\\nclasses at segmental level as the essential and invariant representations used\\nin speech temporal organization. In the context of speech processing, a deep\\nneural network (DNN) is an effective computational method to infer the\\nprobability of individual phonological classes from a short segment of speech\\nsignal. A vector of all phonological class probabilities is referred to as\\nphonological posterior. There are only very few classes comprising a short term\\nspeech signal; hence, the phonological posterior is a sparse vector. Although\\nthe phonological posteriors are estimated at segmental level, we claim that\\nthey convey supra-segmental information. Specifically, we demonstrate that\\nphonological posteriors are indicative of syllabic and prosodic events.\\nBuilding on findings from converging linguistic evidence on the gestural model\\nof Articulatory Phonology as well as the neural basis of speech perception, we\\nhypothesize that phonological posteriors convey properties of linguistic\\nclasses at multiple time scales, and this information is embedded in their\\nsupport (index) of active coefficients. To verify this hypothesis, we obtain a\\nbinary representation of phonological posteriors at the segmental level which\\nis referred to as first-order sparsity structure; the high-order structures are\\nobtained by the concatenation of first-order binary vectors. It is then\\nconfirmed that the classification of supra-segmental linguistic events, the\\nproblem known as linguistic parsing, can be achieved with high accuracy using\\nasimple binary pattern matching of first-order or high-order structures.\\n',\n",
       "  'title': u'On Structured Sparsity of Phonological Posteriors for Linguistic Parsing'},\n",
       " u'1511.06485': {'arxivid': u'1511.06485',\n",
       "  'authorsaffil': [[u'Pratik Chaudhari', None], [u'Stefano Soatto', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06485v4',\n",
       "  'published': u'2015-11-20T04:31:05Z',\n",
       "  'summary': u'  We analyze the regularization properties of additive gradient noise in the\\ntraining of deep networks by posing it as finding the ground state of the\\nHamiltonian of a spherical spin glass in an external magnetic field. We show\\nthat depending upon the magnitude of the magnetic field, the Hamiltonian\\nchanges dramatically from a highly non-convex energy landscape with\\nexponentially many critical points to a regime with polynomially many critical\\npoints and finally, \"trivializes\"\\' to exactly one minimum. This phenomenon,\\nknown as topology trivialization in the physics literature, can be leveraged to\\ndevise annealing schemes for additive noise such that the training starts in\\nthe polynomial regime but gradually morphs the energy landscape into the\\noriginal one as training progresses. We demonstrate through experiments on\\nfully-connected and convolutional neural networks that annealing schemes based\\non trivialization lead to accelerated training and also improve generalization\\nerror.\\n',\n",
       "  'title': u'The Effect of Gradient Noise on the Energy Landscape of Deep Networks'},\n",
       " u'1511.06481': {'arxivid': u'1511.06481',\n",
       "  'authorsaffil': [[u'Guillaume Alain', None],\n",
       "   [u'Alex Lamb', None],\n",
       "   [u'Chinnadhurai Sankar', None],\n",
       "   [u'Aaron Courville', None],\n",
       "   [u'Yoshua Bengio', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06481v7',\n",
       "  'published': u'2015-11-20T03:09:43Z',\n",
       "  'summary': u'  Humans are able to accelerate their learning by selecting training materials\\nthat are the most informative and at the appropriate level of difficulty. We\\npropose a framework for distributing deep learning in which one set of workers\\nsearch for the most informative examples in parallel while a single worker\\nupdates the model on examples selected by importance sampling. This leads the\\nmodel to update using an unbiased estimate of the gradient which also has\\nminimum variance when the sampling proposal is proportional to the L2-norm of\\nthe gradient. We show experimentally that this method reduces gradient variance\\neven in a context where the cost of synchronization across machines cannot be\\nignored, and where the factors for importance sampling are not updated\\ninstantly across the training set.\\n',\n",
       "  'title': u'Variance Reduction in SGD by Distributed Importance Sampling'},\n",
       " u'1602.07641': {'arxivid': u'1602.07641',\n",
       "  'authorsaffil': [[u'Nick DePalma', None], [u'Cynthia Breazeal', None]],\n",
       "  'categoryterms': [u'cs.RO'],\n",
       "  'comment': u'Presented at \"2nd Workshop on Cognitive Architectures for Social\\n  Human-Robot Interaction 2016 (arXiv:1602.01868)\"',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07641v1',\n",
       "  'published': u'2016-02-24T19:17:11Z',\n",
       "  'summary': u'  Robotic architectures that incorporate cloud-based resources are just now\\ngaining popularity. However, researchers have very few investigations into\\ntheir capabilities to support claims of their feasibility. We propose a novel\\nmethod to exchange quality for speed of response. Further, we back this\\nassertion with empirical findings from experiments performed with Amazon\\nMechanical Turk and find that our method improves quality in exchange for\\nresponse time in our cognitive architecture.\\n',\n",
       "  'title': u'NIMBUS: A Hybrid Cloud-Crowd Realtime Architecture for Visual Learning\\n  in Interactive Domains'},\n",
       " u'1511.06488': {'arxivid': u'1511.06488',\n",
       "  'authorsaffil': [[u'Wonyong Sung', None],\n",
       "   [u'Sungho Shin', None],\n",
       "   [u'Kyuyeon Hwang', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06488v3',\n",
       "  'published': u'2015-11-20T04:55:46Z',\n",
       "  'summary': u\"  The complexity of deep neural network algorithms for hardware implementation\\ncan be much lowered by optimizing the word-length of weights and signals.\\nDirect quantization of floating-point weights, however, does not show good\\nperformance when the number of bits assigned is small. Retraining of quantized\\nnetworks has been developed to relieve this problem. In this work, the effects\\nof retraining are analyzed for a feedforward deep neural network (FFDNN) and a\\nconvolutional neural network (CNN). The network complexity is controlled to\\nknow their effects on the resiliency of quantized networks by retraining. The\\ncomplexity of the FFDNN is controlled by varying the unit size in each hidden\\nlayer and the number of layers, while that of the CNN is done by modifying the\\nfeature map configuration. We find that the performance gap between the\\nfloating-point and the retrain-based ternary (+1, 0, -1) weight neural networks\\nexists with a fair amount in 'complexity limited' networks, but the discrepancy\\nalmost vanishes in fully complex networks whose capability is limited by the\\ntraining data, rather than by the number of connections. This research shows\\nthat highly complex DNNs have the capability of absorbing the effects of severe\\nweight quantization through retraining, but connection limited networks are\\nless resilient. This paper also presents the effective compression ratio to\\nguide the trade-off between the network size and the precision when the\\nhardware resource is limited.\\n\",\n",
       "  'title': u'Resiliency of Deep Neural Networks under Quantization'},\n",
       " u'1603.08328': {'arxivid': u'1603.08328',\n",
       "  'authorsaffil': [[u'Tatsunori Taniai', None],\n",
       "   [u'Yasuyuki Matsushita', None],\n",
       "   [u'Yoichi Sato', None],\n",
       "   [u'Takeshi Naemura', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'14 pages. An extended version of our preliminary conference paper\\n  [30], Taniai et al. \"Graph Cut based Continuous Stereo Matching using Locally\\n  Shared Labels\" in the proceedings of IEEE Conference on Computer Vision and\\n  Pattern Recognition (CVPR 2014). Our benchmark scores were submitted to\\n  Middlebury Stereo Evaluation (Version 2) on April 22th, 2015. See\\n  http://vision.middlebury.edu/stereo/eval/',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08328v1',\n",
       "  'published': u'2016-03-28T07:27:49Z',\n",
       "  'summary': u'  We present an accurate and efficient stereo matching method using local\\nexpansion moves, a new move making scheme using graph cuts. The local expansion\\nmoves are presented as many alpha-expansions defined for small grid regions.\\nThe local expansion moves extend the traditional expansion moves by two ways:\\nlocalization and spatial propagation. By localization, we use different\\ncandidate alpha-labels according to the locations of local alpha-expansions. By\\nspatial propagation, we design our local alpha-expansions to propagate\\ncurrently assigned labels for nearby regions. With this localization and\\nspatial propagation, our method can efficiently infer Markov random field\\nmodels with a huge or continuous label space using a randomized search scheme.\\nOur local expansion move method has several advantages over previous approaches\\nthat are based on fusion moves or belief propagation; it produces submodular\\nmoves deriving a subproblem optimality; it helps find good, smooth, piecewise\\nlinear disparity maps; it is suitable for parallelization; it can use\\ncost-volume filtering techniques for accelerating the matching cost\\ncomputations. Our method is evaluated using the Middlebury stereo benchmark and\\nshown to have the best performance in sub-pixel accuracy.\\n',\n",
       "  'title': u'Continuous Stereo Matching using Local Expansion Moves'},\n",
       " u'1603.07584': {'arxivid': u'1603.07584',\n",
       "  'authorsaffil': [[u'Rodrigo Pena', None],\n",
       "   [u'Xavier Bresson', None],\n",
       "   [u'Pierre Vandergheynst', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07584v1',\n",
       "  'published': u'2016-03-24T14:15:17Z',\n",
       "  'summary': u'  We cast the problem of source localization on graphs as the simultaneous\\nproblem of sparse recovery and diffusion ker- nel learning. An l1\\nregularization term enforces the sparsity constraint while we recover the\\nsources of diffusion from a single snapshot of the diffusion process. The\\ndiffusion ker- nel is estimated by assuming the process to be as generic as the\\nstandard heat diffusion. We show with synthetic data that we can concomitantly\\nlearn the diffusion kernel and the sources, given an estimated initialization.\\nWe validate our model with cholera mortality and atmospheric tracer diffusion\\ndata, showing also that the accuracy of the solution depends on the\\nconstruction of the graph from the data points.\\n',\n",
       "  'title': u'Source Localization on Graphs via l1 Recovery and Spectral Graph Theory'},\n",
       " u'1603.08323': {'arxivid': u'1603.08323',\n",
       "  'authorsaffil': [[u'Micha\\u0142 Spytkowski', None],\n",
       "   [u'\\u0141ukasz P. Olech', None],\n",
       "   [u'Halina Kwa\\u015bnicka', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Presented on ACIIDS2016 conference https://aciids.pwr.edu.pl/. The\\n  final publication is available at Springer via\\n  http://dx.doi.org/10.1007/978-3-662-49381-6_63',\n",
       "  'doi': u'10.1007/978-3-662-49381-6_63',\n",
       "  'journalref': u'ACIIDS 2016, Da Nang, Vietnam, March 14-16, 2016, pp. 654\\n  (Springer Berlin Heidelberg)',\n",
       "  'link': u'http://arxiv.org/abs/1603.08323v1',\n",
       "  'published': u'2016-03-28T06:38:56Z',\n",
       "  'summary': u\"  The paper presents a cursory examination of clustering, focusing on a rarely\\nexplored field of hierarchy of clusters. Based on this, a short discussion of\\nclustering quality measures is presented and the F-score measure is examined\\nmore deeply. As there are no attempts to assess the quality for hierarchies of\\nclusters, three variants of the F-Score based index are presented: classic,\\nhierarchical and partial order. The partial order index is the authors'\\napproach to the subject. Conducted experiments show the properties of the\\nconsidered measures. In conclusions, the strong and weak sides of each variant\\nare presented.\\n\",\n",
       "  'title': u'Hierarchy of Groups Evaluation Using Different F-score Variants'},\n",
       " u'1603.06859': {'arxivid': u'1603.06859',\n",
       "  'authorsaffil': [[u'Andr\\xe9 L. V. Coelho', None],\n",
       "   [u'Fabr\\xedcio O. de Fran\\xe7a', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'article under review by Neural Computing and Applications, Springer',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06859v1',\n",
       "  'published': u'2016-03-22T16:32:26Z',\n",
       "  'summary': u'  Perceptrons are neuronal devices capable of fully discriminating linearly\\nseparable classes. Although straightforward to implement and train, their\\napplicability is usually hindered by non-trivial requirements imposed by\\nreal-world classification problems. Therefore, several approaches, such as\\nkernel perceptrons, have been conceived to counteract such difficulties. In\\nthis paper, we investigate an enhanced perceptron model based on the notion of\\ncontrastive biclusters. From this perspective, a good discriminative bicluster\\ncomprises a subset of data instances belonging to one class that show high\\ncoherence across a subset of features and high differentiation from nearest\\ninstances of the other class under the same features (referred to as its\\ncontrastive bicluster). Upon each local subspace associated with a pair of\\ncontrastive biclusters a perceptron is trained and the model with highest area\\nunder the receiver operating characteristic curve (AUC) value is selected as\\nthe final classifier. Experiments conducted on a range of data sets, including\\nthose related to a difficult biosignal classification problem, show that the\\nproposed variant can be indeed very useful, prevailing in most of the cases\\nupon standard and kernel perceptrons in terms of accuracy and AUC measures.\\n',\n",
       "  'title': u'Enhanced perceptrons using contrastive biclusters'},\n",
       " u'1508.02933': {'arxivid': u'1508.02933',\n",
       "  'authorsaffil': [[u'Robert Nishihara', None],\n",
       "   [u'David Lopez-Paz', None],\n",
       "   [u'L\\xe9on Bottou', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'math.OC', u'math.ST', u'stat.TH'],\n",
       "  'comment': u'11 pages, International Conference on Artificial Intelligence and\\n  Statistics, 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.02933v3',\n",
       "  'published': u'2015-08-12T14:31:49Z',\n",
       "  'summary': u'  Algorithms for hyperparameter optimization abound, all of which work well\\nunder different and often unverifiable assumptions. Motivated by the general\\nchallenge of sequentially choosing which algorithm to use, we study the more\\nspecific task of choosing among distributions to use for random hyperparameter\\noptimization. This work is naturally framed in the extreme bandit setting,\\nwhich deals with sequentially choosing which distribution from a collection to\\nsample in order to minimize (maximize) the single best cost (reward). Whereas\\nthe distributions in the standard bandit setting are primarily characterized by\\ntheir means, a number of subtleties arise when we care about the minimal cost\\nas opposed to the average cost. For example, there may not be a well-defined\\n\"best\" distribution as there is in the standard bandit setting. The best\\ndistribution depends on the rewards that have been obtained and on the\\nremaining time horizon. Whereas in the standard bandit setting, it is sensible\\nto compare policies with an oracle which plays the single best arm, in the\\nextreme bandit setting, there are multiple sensible oracle models. We define a\\nsensible notion of \"extreme regret\" in the extreme bandit setting, which\\nparallels the concept of regret in the standard bandit setting. We then prove\\nthat no policy can asymptotically achieve no extreme regret.\\n',\n",
       "  'title': u'No Regret Bound for Extreme Bandits'},\n",
       " u'1604.00869': {'arxivid': u'1604.00869',\n",
       "  'authorsaffil': [[u'Sundong Kim', None]],\n",
       "  'categoryterms': [u'cs.AI', u'I.2.6'],\n",
       "  'comment': u'11 pages, submitted to International Semantic Web Conference 2014\\n  (Rejected), Revising(2016-04-04~)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00869v1',\n",
       "  'published': u'2016-04-04T14:23:25Z',\n",
       "  'summary': u'  Knowledge base is the way to store structured and unstructured data\\nthroughout the web. Since the size of the web is increasing rapidly, there are\\nhuge needs to structure the knowledge in a fully automated way. However\\nfully-automated knowledge-base evolution on the Semantic Web is a major\\nchallenges, although there are many ontology evolution techniques available.\\nTherefore learning ontology automatically can contribute to the semantic web\\nsociety significantly. In this paper, we propose full-automated ontology\\nlearning algorithm to generate refined knowledge base from incomplete knowledge\\nbase and rdf-triples. Our algorithm is data-driven approach which is based on\\nthe property of each instance. Ontology class is being elaborated by\\ngeneralizing frequent property of its instances. By using that developed class\\ninformation, each instance can find its most relatively matching class. By\\nrepeating these two steps, we achieve fully-automated ontology evolution from\\nincomplete basic knowledge base.\\n',\n",
       "  'title': u'Automatic Knowledge Base Evolution by Learning Instances'},\n",
       " u'1603.04117': {'arxivid': u'1603.04117',\n",
       "  'authorsaffil': [[u'Prateek Singhal', None],\n",
       "   [u'Ruffin White', None],\n",
       "   [u'Henrik Christensen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Submitted to IROS 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04117v1',\n",
       "  'published': u'2016-03-14T03:06:39Z',\n",
       "  'summary': u'  We present an on-line 3D visual object tracking framework for monocular\\ncameras by incorporating spatial knowledge and uncertainty from semantic\\nmapping along with high frequency measurements from visual odometry. Using a\\ncombination of vision and odometry that are tightly integrated we can increase\\nthe overall performance of object based tracking for semantic mapping. We\\npresent a framework for integration of the two data-sources into a coherent\\nframework through information based fusion/arbitration. We demonstrate the\\nframework in the context of OmniMapper[1] and present results on 6 challenging\\nsequences over multiple objects compared to data obtained from a motion capture\\nsystems. We are able to achieve a mean error of 0.23m for per frame tracking\\nshowing 9% relative error less than state of the art tracker.\\n',\n",
       "  'title': u'Multi-modal Tracking for Object based SLAM'},\n",
       " u'1604.00861': {'arxivid': u'1604.00861',\n",
       "  'authorsaffil': [[u'Giambattista Parascandolo', None],\n",
       "   [u'Heikki Huttunen', None],\n",
       "   [u'Tuomas Virtanen', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'To appean in Proceedings of IEEE ICASSP 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00861v1',\n",
       "  'published': u'2016-04-04T13:54:09Z',\n",
       "  'summary': u'  In this paper we present an approach to polyphonic sound event detection in\\nreal life recordings based on bi-directional long short term memory (BLSTM)\\nrecurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to\\nmap acoustic features of a mixture signal consisting of sounds from multiple\\nclasses, to binary activity indicators of each event class. Our method is\\ntested on a large database of real-life recordings, with 61 classes (e.g.\\nmusic, car, speech) from 10 different everyday contexts. The proposed method\\noutperforms previous approaches by a large margin, and the results are further\\nimproved using data augmentation techniques. Overall, our system reports an\\naverage F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a\\nrelative improvement over previous state-of-the-art approach of 6.8% and 15.1%\\nrespectively.\\n',\n",
       "  'title': u'Recurrent Neural Networks for Polyphonic Sound Event Detection in Real\\n  Life Recordings'},\n",
       " u'1603.06180': {'arxivid': u'1603.06180',\n",
       "  'authorsaffil': [[u'Ronghang Hu', None],\n",
       "   [u'Marcus Rohrbach', None],\n",
       "   [u'Trevor Darrell', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06180v1',\n",
       "  'published': u'2016-03-20T04:10:53Z',\n",
       "  'summary': u'  In this paper we approach the novel problem of segmenting an image based on a\\nnatural language expression. This is different from traditional semantic\\nsegmentation over a predefined set of semantic classes, as e.g., the phrase\\n\"two men sitting on the right bench\" requires segmenting only the two people on\\nthe right bench and no one standing or sitting on another bench. Previous\\napproaches suitable for this task were limited to a fixed set of categories\\nand/or rectangular regions. To produce pixelwise segmentation for the language\\nexpression, we propose an end-to-end trainable recurrent and convolutional\\nnetwork model that jointly learns to process visual and linguistic information.\\nIn our model, a recurrent LSTM network is used to encode the referential\\nexpression into a vector representation, and a fully convolutional network is\\nused to a extract a spatial feature map from the image and output a spatial\\nresponse map for the target object. We demonstrate on a benchmark dataset that\\nour model can produce quality segmentation output from the natural language\\nexpression, and outperforms baseline methods by a large margin.\\n',\n",
       "  'title': u'Segmentation from Natural Language Expressions'},\n",
       " u'1406.5301': {'arxivid': u'1406.5301',\n",
       "  'authorsaffil': [[u'Borko Bo\\u0161kovi\\u0107', None],\n",
       "   [u'Franc Brglez', None],\n",
       "   [u'Janez Brest', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.AI', u'68T20', u'I.2.8; G.1.6'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1406.5301v5',\n",
       "  'published': u'2014-06-20T08:00:15Z',\n",
       "  'summary': u'  The search for binary sequences with a high figure of merit, known as the low\\nautocorrelation binary sequence ($labs$}) problem, represents a formidable\\ncomputational challenge. To mitigate the computational constraints of the\\nproblem, we consider solvers that accept odd values of sequence length $L$ and\\nreturn solutions for skew-symmetric binary sequences only -- with the\\nconsequence that not all best solutions under this constraint will be optimal\\nfor each $L$. In order to improve both, the search for best merit factor $and$\\nthe asymptotic runtime performance, we instrumented three stochastic solvers,\\nthe first two are state-of-the-art solvers that rely on variants of memetic and\\ntabu search ($lssMAts$ and $lssRRts$), the third solver ($lssOrel$) organizes\\nthe search as a sequence of independent contiguous self-avoiding walk segments.\\nBy adapting a rigorous statistical methodology to performance testing of all\\nthree combinatorial solvers, experiments show that the solver with the best\\nasymptotic average-case performance, $lssOrel\\\\_8 = 0.000032*1.1504^L$, has the\\nbest chance of finding solutions that improve, as $L$ increases, figures of\\nmerit reported to date. The same methodology can be applied to engineering new\\n$labs$ solvers that may return merit factors even closer to the conjectured\\nasymptotic value of 12.3248.\\n',\n",
       "  'title': u'Low-Autocorrelation Binary Sequences: On Improved Merit Factors and\\n  Runtime Predictions to Achieve Them'},\n",
       " u'1603.02754': {'arxivid': u'1603.02754',\n",
       "  'authorsaffil': [[u'Tianqi Chen', None], [u'Carlos Guestrin', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u\"KDD'16 changed all figures to type1\",\n",
       "  'doi': u'10.1145/2939672.2939785',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02754v3',\n",
       "  'published': u'2016-03-09T01:11:51Z',\n",
       "  'summary': u'  Tree boosting is a highly effective and widely used machine learning method.\\nIn this paper, we describe a scalable end-to-end tree boosting system called\\nXGBoost, which is used widely by data scientists to achieve state-of-the-art\\nresults on many machine learning challenges. We propose a novel sparsity-aware\\nalgorithm for sparse data and weighted quantile sketch for approximate tree\\nlearning. More importantly, we provide insights on cache access patterns, data\\ncompression and sharding to build a scalable tree boosting system. By combining\\nthese insights, XGBoost scales beyond billions of examples using far fewer\\nresources than existing systems.\\n',\n",
       "  'title': u'XGBoost: A Scalable Tree Boosting System'},\n",
       " u'1503.06250': {'arxivid': u'1503.06250',\n",
       "  'authorsaffil': [[u'Talayeh Razzaghi', None],\n",
       "   [u'Oleg Roderick', None],\n",
       "   [u'Ilya Safro', None],\n",
       "   [u'Nick Marko', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.06250v1',\n",
       "  'published': u'2015-03-21T00:13:54Z',\n",
       "  'summary': u'  In medical domain, data features often contain missing values. This can\\ncreate serious bias in the predictive modeling. Typical standard data mining\\nmethods often produce poor performance measures. In this paper, we propose a\\nnew method to simultaneously classify large datasets and reduce the effects of\\nmissing values. The proposed method is based on a multilevel framework of the\\ncost-sensitive SVM and the expected maximization imputation method for missing\\nvalues, which relies on iterated regression analyses. We compare classification\\nresults of multilevel SVM-based algorithms on public benchmark datasets with\\nimbalanced classes and missing values as well as real data in health\\napplications, and show that our multilevel SVM-based method produces fast, and\\nmore accurate and robust classification results.\\n',\n",
       "  'title': u'Fast Imbalanced Classification of Healthcare Data with Missing Values'},\n",
       " u'1603.02752': {'arxivid': u'1603.02752',\n",
       "  'authorsaffil': [[u'Max Simchowitz', None],\n",
       "   [u'Kevin Jamieson', None],\n",
       "   [u'Benjamin Recht', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02752v2',\n",
       "  'published': u'2016-03-09T00:55:58Z',\n",
       "  'summary': u'  This paper studies the Best-of-K Bandit game: At each time the player chooses\\na subset S among all N-choose-K possible options and observes reward max(X(i) :\\ni in S) where X is a random vector drawn from a joint distribution. The\\nobjective is to identify the subset that achieves the highest expected reward\\nwith high probability using as few queries as possible. We present\\ndistribution-dependent lower bounds based on a particular construction which\\nforce a learner to consider all N-choose-K subsets, and match naive extensions\\nof known upper bounds in the bandit setting obtained by treating each subset as\\na separate arm. Nevertheless, we present evidence that exhaustive search may be\\navoided for certain, favorable distributions because the influence of\\nhigh-order order correlations may be dominated by lower order statistics.\\nFinally, we present an algorithm and analysis for independent arms, which\\nmitigates the surprising non-trivial information occlusion that occurs due to\\nonly observing the max in the subset. This may inform strategies for more\\ngeneral dependent measures, and we complement these result with independent-arm\\nlower bounds.\\n',\n",
       "  'title': u'Best-of-K Bandits'},\n",
       " u'1502.01710': {'arxivid': u'1502.01710',\n",
       "  'authorsaffil': [[u'Xiang Zhang', None], [u'Yann LeCun', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL'],\n",
       "  'comment': u'This technical report is superseded by a paper entitled\\n  \"Character-level Convolutional Networks for Text Classification\",\\n  arXiv:1509.01626. It has considerably more experimental results and a\\n  rewritten introduction',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.01710v5',\n",
       "  'published': u'2015-02-05T20:45:19Z',\n",
       "  'summary': u'  This article demontrates that we can apply deep learning to text\\nunderstanding from character-level inputs all the way up to abstract text\\nconcepts, using temporal convolutional networks (ConvNets). We apply ConvNets\\nto various large-scale datasets, including ontology classification, sentiment\\nanalysis, and text categorization. We show that temporal ConvNets can achieve\\nastonishing performance without the knowledge of words, phrases, sentences and\\nany other syntactic or semantic structures with regards to a human language.\\nEvidence shows that our models can work for both English and Chinese.\\n',\n",
       "  'title': u'Text Understanding from Scratch'},\n",
       " u'1602.00734': {'arxivid': u'1602.00734',\n",
       "  'authorsaffil': [[u'Yen-Huan Li', None], [u'Volkan Cevher', None]],\n",
       "  'categoryterms': [u'cs.IT', u'cs.LG', u'math.IT', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00734v1',\n",
       "  'published': u'2016-02-01T22:43:55Z',\n",
       "  'summary': u'  The standard approach to compressive sampling considers recovering an unknown\\ndeterministic signal with certain known structure, and designing the\\nsub-sampling pattern and recovery algorithm based on the known structure. This\\napproach requires looking for a good representation that reveals the signal\\nstructure, and solving a non-smooth convex minimization problem (e.g., basis\\npursuit). In this paper, another approach is considered: We learn a good\\nsub-sampling pattern based on available training signals, without knowing the\\nsignal structure in advance, and reconstruct an accordingly sub-sampled signal\\nby computationally much cheaper linear reconstruction. We provide a theoretical\\nguarantee on the recovery error, and show via experiments on real-world MRI\\ndata the effectiveness of the proposed compressive MRI scheme.\\n',\n",
       "  'title': u'Learning Data Triage: Linear Decoding Works for Compressive MRI'},\n",
       " u'1601.08003': {'arxivid': u'1601.08003',\n",
       "  'authorsaffil': [[u'Erik Jonsson', None], [u'Michael Felsberg', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Presented at the SSBA Symposium 2005, Malm\\\\\"o, Sweden',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.08003v1',\n",
       "  'published': u'2016-01-29T08:54:22Z',\n",
       "  'summary': u'  A robust mean value is often a good alternative to the standard mean value\\nwhen dealing with data containing many outliers. An efficient method for\\nsamples of one-dimensional features and the truncated quadratic error norm is\\npresented and compared to the method of channel averaging (soft histograms).\\n',\n",
       "  'title': u'Efficient Robust Mean Value Calculation of 1D Features'},\n",
       " u'1603.04110': {'arxivid': u'1603.04110',\n",
       "  'authorsaffil': [[u'Seyed Morteza Mousavi', None],\n",
       "   [u'Aaron Harwood', None],\n",
       "   [u'Shanika Karunasekera', None],\n",
       "   [u'Mojtaba Maghrebi', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'A version of this technical report has been submitted to the Springer\\n  Journal of Ambient Intelligence and Humanized Computing and it is under\\n  review',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04110v2',\n",
       "  'published': u'2016-03-14T01:52:28Z',\n",
       "  'summary': u'  Nowadays large amounts of GPS trajectory data is being continuously collected\\nby GPS-enabled devices such as vehicles navigation systems and mobile phones.\\nGPS trajectory data is useful for applications such as traffic management,\\nlocation forecasting, and itinerary planning. Such applications often need to\\nextract the time-stamped Sequence of Visited Locations (SVLs) of the mobile\\nobjects. The nearest neighbor query (NNQ) is the most applied method for\\nlabeling the visited locations based on the IDs of the POIs in the process of\\nSVL generation. NNQ in some scenarios is not accurate enough. To improve the\\nquality of the extracted SVLs, instead of using NNQ, we label the visited\\nlocations as the IDs of the POIs which geometrically intersect with the GPS\\nobservations. Intersection operator requires the accurate geometry of the\\npoints of interest which we refer to them as the Geometries of Interest (GOIs).\\nIn some application domains (e.g. movement trajectories of animals), adequate\\ninformation about the POIs and their GOIs may not be available a priori, or\\nthey may not be publicly accessible and, therefore, they need to be derived\\nfrom GPS trajectory data. In this paper we propose a novel method for\\nestimating the POIs and their GOIs, which consists of three phases: (i)\\nextracting the geometries of the stay regions; (ii) constructing the geometry\\nof destination regions based on the extracted stay regions; and (iii)\\nconstructing the GOIs based on the geometries of the destination regions. Using\\nthe geometric similarity to known GOIs as the major evaluation criterion, the\\nexperiments we performed using long-term GPS trajectory data show that our\\nmethod outperforms the existing approaches.\\n',\n",
       "  'title': u'Geometry of Interest (GOI): Spatio-Temporal Destination Extraction and\\n  Partitioning in GPS Trajectory Data'},\n",
       " u'1604.02647': {'arxivid': u'1604.02647',\n",
       "  'authorsaffil': [[u'Shunsuke Saito', None],\n",
       "   [u'Tianye Li', None],\n",
       "   [u'Hao Li', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02647v1',\n",
       "  'published': u'2016-04-10T07:04:47Z',\n",
       "  'summary': u'  We introduce the concept of unconstrained real-time 3D facial performance\\ncapture through explicit semantic segmentation in the RGB input. To ensure\\nrobustness, cutting edge supervised learning approaches rely on large training\\ndatasets of face images captured in the wild. While impressive tracking quality\\nhas been demonstrated for faces that are largely visible, any occlusion due to\\nhair, accessories, or hand-to-face gestures would result in significant visual\\nartifacts and loss of tracking accuracy. The modeling of occlusions has been\\nmostly avoided due to its immense space of appearance variability. To address\\nthis curse of high dimensionality, we perform tracking in unconstrained images\\nassuming non-face regions can be fully masked out. Along with recent\\nbreakthroughs in deep learning, we demonstrate that pixel-level facial\\nsegmentation is possible in real-time by repurposing convolutional neural\\nnetworks designed originally for general semantic segmentation. We develop an\\nefficient architecture based on a two-stream deconvolution network with\\ncomplementary characteristics, and introduce carefully designed training\\nsamples and data augmentation strategies for improved segmentation accuracy and\\nrobustness. We adopt a state-of-the-art regression-based facial tracking\\nframework with segmented face images as training, and demonstrate accurate and\\nuninterrupted facial performance capture in the presence of extreme occlusion\\nand even side views. Furthermore, the resulting segmentation can be directly\\nused to composite partial 3D face models on the input images and enable\\nseamless facial manipulation tasks, such as virtual make-up or face\\nreplacement.\\n',\n",
       "  'title': u'Real-Time Facial Segmentation and Performance Capture from RGB Input'},\n",
       " u'1604.02646': {'arxivid': u'1604.02646',\n",
       "  'authorsaffil': [[u'Biswajit Paria', None],\n",
       "   [u'Anirban Santara', None],\n",
       "   [u'Pabitra Mitra', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02646v2',\n",
       "  'published': u'2016-04-10T07:02:40Z',\n",
       "  'summary': u'  The success of deep neural networks is mostly due their ability to learn\\nmeaningful features from the data. Features learned in the hidden layers of\\ndeep neural networks trained in computer vision tasks have been shown to be\\nsimilar to mid-level vision features. We leverage this fact in this work and\\npropose the visualization regularizer for image tasks. The proposed\\nregularization technique enforces smoothness of the features learned by hidden\\nnodes and turns out to be a special case of Tikhonov regularization. We achieve\\nhigher classification accuracy as compared to existing regularizers such as the\\nL2 norm regularizer and dropout, on benchmark datasets with no change in the\\ntraining computational complexity.\\n',\n",
       "  'title': u'Visualization Regularizers for Neural Network based Image Recognition'},\n",
       " u'1505.00487': {'arxivid': u'1505.00487',\n",
       "  'authorsaffil': [[u'Subhashini Venugopalan', None],\n",
       "   [u'Marcus Rohrbach', None],\n",
       "   [u'Jeff Donahue', None],\n",
       "   [u'Raymond Mooney', None],\n",
       "   [u'Trevor Darrell', None],\n",
       "   [u'Kate Saenko', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'ICCV 2015 camera-ready. Includes code, project page and LSMDC\\n  challenge results',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.00487v3',\n",
       "  'published': u'2015-05-03T22:32:00Z',\n",
       "  'summary': u'  Real-world videos often have complex dynamics; and methods for generating\\nopen-domain video descriptions should be sensitive to temporal structure and\\nallow both input (sequence of frames) and output (sequence of words) of\\nvariable length. To approach this problem, we propose a novel end-to-end\\nsequence-to-sequence model to generate captions for videos. For this we exploit\\nrecurrent neural networks, specifically LSTMs, which have demonstrated\\nstate-of-the-art performance in image caption generation. Our LSTM model is\\ntrained on video-sentence pairs and learns to associate a sequence of video\\nframes to a sequence of words in order to generate a description of the event\\nin the video clip. Our model naturally is able to learn the temporal structure\\nof the sequence of frames as well as the sequence model of the generated\\nsentences, i.e. a language model. We evaluate several variants of our model\\nthat exploit different visual features on a standard set of YouTube videos and\\ntwo movie description datasets (M-VAD and MPII-MD).\\n',\n",
       "  'title': u'Sequence to Sequence -- Video to Text'},\n",
       " u'1511.06392': {'arxivid': u'1511.06392',\n",
       "  'authorsaffil': [[u'Karol Kurach', None],\n",
       "   [u'Marcin Andrychowicz', None],\n",
       "   [u'Ilya Sutskever', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'ICLR submission, 17 pages, 9 figures, 6 tables (with bibliography and\\n  appendix)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06392v3',\n",
       "  'published': u'2015-11-19T21:36:28Z',\n",
       "  'summary': u'  In this paper, we propose and investigate a new neural network architecture\\ncalled Neural Random Access Machine. It can manipulate and dereference pointers\\nto an external variable-size random-access memory. The model is trained from\\npure input-output examples using backpropagation.\\n  We evaluate the new model on a number of simple algorithmic tasks whose\\nsolutions require pointer manipulation and dereferencing. Our results show that\\nthe proposed model can learn to solve algorithmic tasks of such type and is\\ncapable of operating on simple data structures like linked-lists and binary\\ntrees. For easier tasks, the learned solutions generalize to sequences of\\narbitrary length. Moreover, memory access during inference can be done in a\\nconstant time under some assumptions.\\n',\n",
       "  'title': u'Neural Random-Access Machines'},\n",
       " u'1511.06393': {'arxivid': u'1511.06393',\n",
       "  'authorsaffil': [[u'Darryl D. Lin', None],\n",
       "   [u'Sachin S. Talathi', None],\n",
       "   [u'V. Sreekanth Annapureddy', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'ICML 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06393v3',\n",
       "  'published': u'2015-11-19T21:37:06Z',\n",
       "  'summary': u'  In recent years increasingly complex architectures for deep convolution\\nnetworks (DCNs) have been proposed to boost the performance on image\\nrecognition tasks. However, the gains in performance have come at a cost of\\nsubstantial increase in computation and model storage resources. Fixed point\\nimplementation of DCNs has the potential to alleviate some of these\\ncomplexities and facilitate potential deployment on embedded hardware. In this\\npaper, we propose a quantizer design for fixed point implementation of DCNs. We\\nformulate and solve an optimization problem to identify optimal fixed point\\nbit-width allocation across DCN layers. Our experiments show that in comparison\\nto equal bit-width settings, the fixed point DCNs with optimized bit width\\nallocation offer >20% reduction in the model size without any loss in accuracy\\non CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance\\nthe accuracy of fixed point DCNs beyond that of the original floating point\\nmodel. In doing so, we report a new state-of-the-art fixed point performance of\\n6.78% error-rate on CIFAR-10 benchmark.\\n',\n",
       "  'title': u'Fixed Point Quantization of Deep Convolutional Networks'},\n",
       " u'1511.06390': {'arxivid': u'1511.06390',\n",
       "  'authorsaffil': [[u'Jost Tobias Springenberg', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06390v2',\n",
       "  'published': u'2015-11-19T21:26:58Z',\n",
       "  'summary': u'  In this paper we present a method for learning a discriminative classifier\\nfrom unlabeled or partially labeled data. Our approach is based on an objective\\nfunction that trades-off mutual information between observed examples and their\\npredicted categorical class distribution, against robustness of the classifier\\nto an adversarial generative model. The resulting algorithm can either be\\ninterpreted as a natural generalization of the generative adversarial networks\\n(GAN) framework or as an extension of the regularized information maximization\\n(RIM) framework to robust classification against an optimal adversary. We\\nempirically evaluate our method - which we dub categorical generative\\nadversarial networks (or CatGAN) - on synthetic data as well as on challenging\\nimage classification tasks, demonstrating the robustness of the learned\\nclassifiers. We further qualitatively assess the fidelity of samples generated\\nby the adversarial generator that is learned alongside the discriminative\\nclassifier, and identify links between the CatGAN objective and discriminative\\nclustering algorithms (such as RIM).\\n',\n",
       "  'title': u'Unsupervised and Semi-supervised Learning with Categorical Generative\\n  Adversarial Networks'},\n",
       " u'1511.06391': {'arxivid': u'1511.06391',\n",
       "  'authorsaffil': [[u'Oriol Vinyals', None],\n",
       "   [u'Samy Bengio', None],\n",
       "   [u'Manjunath Kudlur', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'Accepted as a conference paper at ICLR 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06391v4',\n",
       "  'published': u'2015-11-19T21:31:26Z',\n",
       "  'summary': u'  Sequences have become first class citizens in supervised learning thanks to\\nthe resurgence of recurrent neural networks. Many complex tasks that require\\nmapping from or to a sequence of observations can now be formulated with the\\nsequence-to-sequence (seq2seq) framework which employs the chain rule to\\nefficiently represent the joint probability of sequences. In many cases,\\nhowever, variable sized inputs and/or outputs might not be naturally expressed\\nas sequences. For instance, it is not clear how to input a set of numbers into\\na model where the task is to sort them; similarly, we do not know how to\\norganize outputs when they correspond to random variables and the task is to\\nmodel their unknown joint probability. In this paper, we first show using\\nvarious examples that the order in which we organize input and/or output data\\nmatters significantly when learning an underlying model. We then discuss an\\nextension of the seq2seq framework that goes beyond sequences and handles input\\nsets in a principled way. In addition, we propose a loss which, by searching\\nover possible orders during training, deals with the lack of structure of\\noutput sets. We show empirical evidence of our claims regarding ordering, and\\non the modifications to the seq2seq framework on benchmark language modeling\\nand parsing tasks, as well as two artificial tasks -- sorting numbers and\\nestimating the joint probability of unknown graphical models.\\n',\n",
       "  'title': u'Order Matters: Sequence to sequence for sets'},\n",
       " u'1511.06396': {'arxivid': u'1511.06396',\n",
       "  'authorsaffil': [[u'Patrick Verga', None],\n",
       "   [u'David Belanger', None],\n",
       "   [u'Emma Strubell', None],\n",
       "   [u'Benjamin Roth', None],\n",
       "   [u'Andrew McCallum', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'Accepted to NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06396v2',\n",
       "  'published': u'2015-11-19T21:42:23Z',\n",
       "  'summary': u\"  Universal schema builds a knowledge base (KB) of entities and relations by\\njointly embedding all relation types from input KBs as well as textual patterns\\nexpressing relations from raw text. In most previous applications of universal\\nschema, each textual pattern is represented as a single embedding, preventing\\ngeneralization to unseen patterns. Recent work employs a neural network to\\ncapture patterns' compositional semantics, providing generalization to all\\npossible input text. In response, this paper introduces significant further\\nimprovements to the coverage and flexibility of universal schema relation\\nextraction: predictions for entities unseen in training and multilingual\\ntransfer learning to domains with no annotation. We evaluate our model through\\nextensive experiments on the English and Spanish TAC KBP benchmark,\\noutperforming the top system from TAC 2013 slot-filling using no handwritten\\npatterns or additional annotation. We also consider a multilingual setting in\\nwhich English training data entities overlap with the seed KB, but Spanish text\\ndoes not. Despite having no annotation for Spanish data, we train an accurate\\npredictor, with additional improvements obtained by tying word embeddings\\nacross languages. Furthermore, we find that multilingual training improves\\nEnglish relation extraction accuracy. Our approach is thus suited to\\nbroad-coverage automated knowledge base construction in a variety of languages\\nand domains.\\n\",\n",
       "  'title': u'Multilingual Relation Extraction using Compositional Universal Schema'},\n",
       " u'1511.06394': {'arxivid': u'1511.06394',\n",
       "  'authorsaffil': [[u'Olivier J. H\\xe9naff', None],\n",
       "   [u'Eero P. Simoncelli', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'Published as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06394v4',\n",
       "  'published': u'2015-11-19T21:40:13Z',\n",
       "  'summary': u'  We develop a new method for visualizing and refining the invariances of\\nlearned representations. Specifically, we test for a general form of\\ninvariance, linearization, in which the action of a transformation is confined\\nto a low-dimensional subspace. Given two reference images (typically, differing\\nby some transformation), we synthesize a sequence of images lying on a path\\nbetween them that is of minimal length in the space of the representation (a\\n\"representational geodesic\"). If the transformation relating the two reference\\nimages is linearized by the representation, this sequence should follow the\\ngradual evolution of this transformation. We use this method to assess the\\ninvariance properties of a state-of-the-art image classification network and\\nfind that geodesics generated for image pairs differing by translation,\\nrotation, and dilation do not evolve according to their associated\\ntransformations. Our method also suggests a remedy for these failures, and\\nfollowing this prescription, we show that the modified representation is able\\nto linearize a variety of geometric image transformations.\\n',\n",
       "  'title': u'Geodesics of learned representations'},\n",
       " u'1503.05724': {'arxivid': u'1503.05724',\n",
       "  'authorsaffil': [[u'Sebastian Urban', None],\n",
       "   [u'Patrick van der Smagt', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.05724v3',\n",
       "  'published': u'2015-03-19T11:48:14Z',\n",
       "  'summary': u'  Existing approaches to combine both additive and multiplicative neural units\\neither use a fixed assignment of operations or require discrete optimization to\\ndetermine what function a neuron should perform. This leads either to an\\ninefficient distribution of computational resources or an extensive increase in\\nthe computational complexity of the training procedure.\\n  We present a novel, parameterizable transfer function based on the\\nmathematical concept of non-integer functional iteration that allows the\\noperation each neuron performs to be smoothly and, most importantly,\\ndifferentiablely adjusted between addition and multiplication. This allows the\\ndecision between addition and multiplication to be integrated into the standard\\nbackpropagation training procedure.\\n',\n",
       "  'title': u'A Neural Transfer Function for a Smooth and Differentiable Transition\\n  Between Additive and Multiplicative Interactions'},\n",
       " u'1509.01007': {'arxivid': u'1509.01007',\n",
       "  'authorsaffil': [[u'Dominique Osborne', None],\n",
       "   [u'Shashi Narayan', None],\n",
       "   [u'Shay B. Cohen', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.01007v2',\n",
       "  'published': u'2015-09-03T09:39:36Z',\n",
       "  'summary': u'  Canonical correlation analysis (CCA) is a method for reducing the dimension\\nof data represented using two views. It has been previously used to derive word\\nembeddings, where one view indicates a word, and the other view indicates its\\ncontext. We describe a way to incorporate prior knowledge into CCA, give a\\ntheoretical justification for it, and test it by deriving word embeddings and\\nevaluating them on a myriad of datasets.\\n',\n",
       "  'title': u'Encoding Prior Knowledge with Eigenword Embeddings'},\n",
       " u'1602.02665': {'arxivid': u'1602.02665',\n",
       "  'authorsaffil': [[u'Johan Bollen', None],\n",
       "   [u'Bruno Gon\\xe7alves', None],\n",
       "   [u'Ingrid van de Leemput', None],\n",
       "   [u'Guangchen Ruan', None]],\n",
       "  'categoryterms': [u'cs.SI', u'cs.CL', u'cs.HC', u'physics.soc-ph'],\n",
       "  'comment': u'15 pages, 3 figures, 2 tables',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02665v1',\n",
       "  'published': u'2016-02-08T17:46:18Z',\n",
       "  'summary': u'  Most individuals in social networks experience a so-called Friendship\\nParadox: they are less popular than their friends on average. This effect may\\nexplain recent findings that widespread social network media use leads to\\nreduced happiness. However the relation between popularity and happiness is\\npoorly understood. A Friendship paradox does not necessarily imply a Happiness\\nparadox where most individuals are less happy than their friends. Here we\\nreport the first direct observation of a significant Happiness Paradox in a\\nlarge-scale online social network of $39,110$ Twitter users. Our results reveal\\nthat popular individuals are indeed happier and that a majority of individuals\\nexperience a significant Happiness paradox. The magnitude of the latter effect\\nis shaped by complex interactions between individual popularity, happiness, and\\nthe fact that users cluster assortatively by level of happiness. Our results\\nindicate that the topology of online social networks and the distribution of\\nhappiness in some populations can cause widespread psycho-social effects that\\naffect the well-being of billions of individuals.\\n',\n",
       "  'title': u'The happiness paradox: your friends are happier than you'},\n",
       " u'1603.07120': {'arxivid': u'1603.07120',\n",
       "  'authorsaffil': [[u'Amir Shahroudy', None],\n",
       "   [u'Tian-Tsong Ng', None],\n",
       "   [u'Yihong Gong', None],\n",
       "   [u'Gang Wang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07120v1',\n",
       "  'published': u'2016-03-23T10:22:12Z',\n",
       "  'summary': u'  Single modality action recognition on RGB or depth sequences has been\\nextensively explored recently. It is generally accepted that each of these two\\nmodalities has different strengths and limitations for the task of action\\nrecognition. Therefore, analysis of the RGB+D videos can help us to better\\nstudy the complementary properties of these two types of modalities and achieve\\nhigher levels of performance. In this paper, we propose a new deep autoencoder\\nbased shared-specific feature factorization network to separate input\\nmultimodal signals into a hierarchy of components. Further, based on the\\nstructure of the features, a structured sparsity learning machine is proposed\\nwhich utilizes mixed norms to apply regularization within components and group\\nselection between them for better classification performance. Our experimental\\nresults show the effectiveness of our cross-modality feature analysis framework\\nby achieving state-of-the-art accuracy for action classification on four\\nchallenging benchmark datasets, for which we reduce the error rate by more than\\n40% in three datasets and saturating the benchmark with perfect accuracy for\\nthe other one.\\n',\n",
       "  'title': u'Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos'},\n",
       " u'1603.04015': {'arxivid': u'1603.04015',\n",
       "  'authorsaffil': [[u'Jia-xin Cai', None],\n",
       "   [u'Xin Tang', None],\n",
       "   [u'Lifang Zhang', None],\n",
       "   [u'Guocan Feng', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted by the 23rd IEEE International Conference on Image\\n  Processing (ICIP2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04015v2',\n",
       "  'published': u'2016-03-13T10:20:25Z',\n",
       "  'summary': u'  In this paper, a discriminative two-phase dictionary learning framework is\\nproposed for classifying human action by sparse shape representations, in which\\nthe first-phase dictionary is learned on the selected discriminative frames and\\nthe second-phase dictionary is built for recognition using reconstruction\\nerrors of the first-phase dictionary as input features. We propose a \"zeroth\\nclass\" trick for detecting undiscriminating frames of the test video and\\neliminating them before voting on the action categories. Experimental results\\non benchmarks demonstrate the effectiveness of our method.\\n',\n",
       "  'title': u'Learning zeroth class dictionary for human action recognition'},\n",
       " u'1505.01197': {'arxivid': u'1505.01197',\n",
       "  'authorsaffil': [[u'Georgia Gkioxari', None],\n",
       "   [u'Ross Girshick', None],\n",
       "   [u'Jitendra Malik', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.01197v3',\n",
       "  'published': u'2015-05-05T21:56:10Z',\n",
       "  'summary': u'  There are multiple cues in an image which reveal what action a person is\\nperforming. For example, a jogger has a pose that is characteristic for\\njogging, but the scene (e.g. road, trail) and the presence of other joggers can\\nbe an additional source of information. In this work, we exploit the simple\\nobservation that actions are accompanied by contextual cues to build a strong\\naction recognition system. We adapt RCNN to use more than one region for\\nclassification while still maintaining the ability to localize the action. We\\ncall our system R*CNN. The action-specific models and the feature maps are\\ntrained jointly, allowing for action specific representations to emerge. R*CNN\\nachieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other\\napproaches in the field by a significant margin. Last, we show that R*CNN is\\nnot limited to action recognition. In particular, R*CNN can also be used to\\ntackle fine-grained tasks such as attribute classification. We validate this\\nclaim by reporting state-of-the-art performance on the Berkeley Attributes of\\nPeople dataset.\\n',\n",
       "  'title': u'Contextual Action Recognition with R*CNN'},\n",
       " u'1602.08741': {'arxivid': u'1602.08741',\n",
       "  'authorsaffil': [[u'Nikolay N. Vasiliev', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08741v1',\n",
       "  'published': u'2016-02-28T16:58:01Z',\n",
       "  'summary': u'  The most studied and most successful language models were developed and\\nevaluated mainly for English and other close European languages, such as\\nFrench, German, etc. It is important to study applicability of these models to\\nother languages. The use of vector space models for Russian was recently\\nstudied for multiple corpora, such as Wikipedia, RuWac, lib.ru. These models\\nwere evaluated against word semantic similarity task. For our knowledge Twitter\\nwas not considered as a corpus for this task, with this work we fill the gap.\\nResults for vectors trained on Twitter corpus are comparable in accuracy with\\nother single-corpus trained models, although the best performance is currently\\nachieved by combination of multiple corpora.\\n',\n",
       "  'title': u'Gibberish Semantics: How Good is Russian Twitter in Word Semantic\\n  Similarity Task?'},\n",
       " u'1509.05909': {'arxivid': u'1509.05909',\n",
       "  'authorsaffil': [[u'Alex Kendall', None], [u'Roberto Cipolla', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.RO'],\n",
       "  'comment': u'ICRA 2016; Fixed numerical error with rotation results',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.05909v2',\n",
       "  'published': u'2015-09-19T16:01:05Z',\n",
       "  'summary': u\"  We present a robust and real-time monocular six degree of freedom visual\\nrelocalization system. We use a Bayesian convolutional neural network to\\nregress the 6-DOF camera pose from a single RGB image. It is trained in an\\nend-to-end manner with no need of additional engineering or graph optimisation.\\nThe algorithm can operate indoors and outdoors in real time, taking under 6ms\\nto compute. It obtains approximately 2m and 6 degrees accuracy for very large\\nscale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian\\nconvolutional neural network implementation we obtain an estimate of the\\nmodel's relocalization uncertainty and improve state of the art localization\\naccuracy on a large scale outdoor dataset. We leverage the uncertainty measure\\nto estimate metric relocalization error and to detect the presence or absence\\nof the scene in the input image. We show that the model's uncertainty is caused\\nby images being dissimilar to the training dataset in either pose or\\nappearance.\\n\",\n",
       "  'title': u'Modelling Uncertainty in Deep Learning for Camera Relocalization'},\n",
       " u'1604.03655': {'arxivid': u'1604.03655',\n",
       "  'authorsaffil': [[u'Haris Aziz', None], [u'Simon Mackenzie', None]],\n",
       "  'categoryterms': [u'cs.DS',\n",
       "   u'cs.AI',\n",
       "   u'cs.GT',\n",
       "   u'91A12, 68Q15',\n",
       "   u'F.2; J.4'],\n",
       "  'comment': u'19 pages, 9 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03655v5',\n",
       "  'published': u'2016-04-13T05:06:29Z',\n",
       "  'summary': u'  We consider the well-studied cake cutting problem in which the goal is to\\nfind an envy-free allocation based on queries from the agents. The problem has\\nreceived attention in computer science, mathematics, and economics. It has been\\na major open problem whether there exists a bounded and discrete envy-free\\nprotocol. We resolve the problem by proposing a discrete and bounded envy-free\\nprotocol for any number of agents.\\n',\n",
       "  'title': u'A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of\\n  Agents'},\n",
       " u'1603.02532': {'arxivid': u'1603.02532',\n",
       "  'authorsaffil': [[u'Otte Hein\\xe4vaara', None],\n",
       "   [u'Janne Lepp\\xe4-aho', None],\n",
       "   [u'Jukka Corander', None],\n",
       "   [u'Antti Honkela', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.CO', u'stat.ML'],\n",
       "  'comment': u'9 pages, 10 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02532v1',\n",
       "  'published': u'2016-03-08T14:24:11Z',\n",
       "  'summary': u'  Various $\\\\ell_1$-penalised estimation methods such as graphical lasso and\\nCLIME are widely used for sparse precision matrix estimation. Many of these\\nmethods have been shown to be consistent under various quantitative assumptions\\nabout the underlying true covariance matrix. Intuitively, these conditions are\\nrelated to situations where the penalty term will dominate the optimisation. In\\nthis paper, we explore the consistency of $\\\\ell_1$-based methods for a class of\\nsparse latent variable -like models, which are strongly motivated by several\\ntypes of applications. We show that all $\\\\ell_1$-based methods fail\\ndramatically for models with nearly linear dependencies between the variables.\\nWe also study the consistency on models derived from real gene expression data\\nand note that the assumptions needed for consistency never hold even for modest\\nsized gene networks and $\\\\ell_1$-based methods also become unreliable in\\npractice for larger networks.\\n',\n",
       "  'title': u'On the inconsistency of $\\\\ell_1$-penalised sparse precision matrix\\n  estimation'},\n",
       " u'1506.02158': {'arxivid': u'1506.02158',\n",
       "  'authorsaffil': [[u'Yarin Gal', None], [u'Zoubin Ghahramani', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'12 pages, 3 figures, ICLR format, updated with reviewer comments',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.02158v6',\n",
       "  'published': u'2015-06-06T14:43:40Z',\n",
       "  'summary': u\"  Convolutional neural networks (CNNs) work well on large datasets. But\\nlabelled data is hard to collect, and in some applications larger amounts of\\ndata are not available. The problem then is how to use CNNs with small data --\\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\\nrobustness to over-fitting on small data than traditional approaches. This is\\nby placing a probability distribution over the CNN's kernels. We approximate\\nour model's intractable posterior with Bernoulli variational distributions,\\nrequiring no additional model parameters.\\n  On the theoretical side, we cast dropout network training as approximate\\ninference in Bayesian neural networks. This allows us to implement our model\\nusing existing tools in deep learning with no increase in time complexity,\\nwhile highlighting a negative result in the field. We show a considerable\\nimprovement in classification accuracy compared to standard techniques and\\nimprove on published state-of-the-art results for CIFAR-10.\\n\",\n",
       "  'title': u'Bayesian Convolutional Neural Networks with Bernoulli Approximate\\n  Variational Inference'},\n",
       " u'1603.01739': {'arxivid': u'1603.01739',\n",
       "  'authorsaffil': [[u'Viswanath P Sudarshan', None],\n",
       "   [u'Tobias Weiser', None],\n",
       "   [u'Phalgun Chintala', None],\n",
       "   [u'Subhamoy Mandal', None],\n",
       "   [u'Rahul Dutta', None]],\n",
       "  'categoryterms': [u'cs.CV', u'physics.med-ph'],\n",
       "  'comment': u'IEEE BHI 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01739v1',\n",
       "  'published': u'2016-03-05T16:27:43Z',\n",
       "  'summary': u'  Visual observation of Cumulus Oocyte Complexes provides only limited\\ninformation about its functional competence, whereas the molecular evaluations\\nmethods are cumbersome or costly. Image analysis of mammalian oocytes can\\nprovide attractive alternative to address this challenge. However, it is\\ncomplex, given the huge number of oocytes under inspection and the subjective\\nnature of the features inspected for identification. Supervised machine\\nlearning methods like random forest with annotations from expert biologists can\\nmake the analysis task standardized and reduces inter-subject variability. We\\npresent a semi-automatic framework for predicting the class an oocyte belongs\\nto, based on multi-object parametric segmentation on the acquired microscopic\\nimage followed by a feature based classification using random forests.\\n',\n",
       "  'title': u'Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for\\n  in Vitro Embryo Culture'},\n",
       " u'1603.03590': {'arxivid': u'1603.03590',\n",
       "  'authorsaffil': [[u'Till Kroeger', None],\n",
       "   [u'Radu Timofte', None],\n",
       "   [u'Dengxin Dai', None],\n",
       "   [u'Luc Van Gool', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.RO'],\n",
       "  'comment': u'9 pages main paper + 16 pages supplementary material',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03590v1',\n",
       "  'published': u'2016-03-11T10:55:07Z',\n",
       "  'summary': u\"  Most recent works in optical flow extraction focus on the accuracy and\\nneglect the time complexity. However, in real-life visual applications, such as\\ntracking, activity detection and recognition, the time complexity is critical.\\n  We propose a solution with very low time complexity and competitive accuracy\\nfor the computation of dense optical flow. It consists of three parts: 1)\\ninverse search for patch correspondences; 2) dense displacement field creation\\nthrough patch aggregation along multiple scales; 3) variational refinement. At\\nthe core of our Dense Inverse Search-based method (DIS) is the efficient search\\nof correspondences inspired by the inverse compositional image alignment\\nproposed by Baker and Matthews in 2001.\\n  DIS is competitive on standard optical flow benchmarks with large\\ndisplacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching the\\ntemporal resolution of human's biological vision system. It is order(s) of\\nmagnitude faster than state-of-the-art methods in the same range of accuracy,\\nmaking DIS ideal for visual applications.\\n\",\n",
       "  'title': u'Fast Optical Flow using Dense Inverse Search'},\n",
       " u'1604.02703': {'arxivid': u'1604.02703',\n",
       "  'authorsaffil': [[u'Wenzheng Chen', None],\n",
       "   [u'Huan Wang', None],\n",
       "   [u'Yangyan Li', None],\n",
       "   [u'Hao Su', None],\n",
       "   [u'Changhe Tu', None],\n",
       "   [u'Dani Lischinski', None],\n",
       "   [u'Daniel Cohen-Or', None],\n",
       "   [u'Baoquan Chen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02703v2',\n",
       "  'published': u'2016-04-10T15:22:04Z',\n",
       "  'summary': u'  Human 3D pose estimation from a single image is a challenging task with\\nnumerous applications. Convolutional Neural Networks (CNNs) have recently\\nachieved superior performance on the task of 2D pose estimation from a single\\nimage, by training on images with 2D annotations collected by crowd sourcing.\\nThis suggests that similar success could be achieved for direct estimation of\\n3D poses. However, 3D poses are much harder to annotate, and the lack of\\nsuitable annotated training images hinders attempts towards end-to-end\\nsolutions.\\n  To address this issue, we opt to automatically synthesize training images\\nwith ground truth pose annotations. We find that pose space coverage and\\ntexture diversity are the key ingredients for the effectiveness of synthetic\\ntraining data. We present a fully automatic, scalable approach that samples the\\nhuman pose space for guiding the synthesis procedure and extracts clothing\\ntextures from real images. We demonstrate that CNNs trained with our synthetic\\nimages out-perform those trained with real photos on 3D pose estimation tasks.\\n',\n",
       "  'title': u'Synthesizing Training Images for Boosting Human 3D Pose Estimation'},\n",
       " u'1603.04506': {'arxivid': u'1603.04506',\n",
       "  'authorsaffil': [[u'Paolo Toccacheli', None],\n",
       "   [u'Ilia Nouretdinov', None],\n",
       "   [u'Alexander Gammerman', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'17 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04506v1',\n",
       "  'published': u'2016-03-14T23:37:37Z',\n",
       "  'summary': u'  The paper presents an application of Conformal Predictors to a\\nchemoinformatics problem of identifying activities of chemical compounds. The\\npaper addresses some specific challenges of this domain: a large number of\\ncompounds (training examples), high-dimensionality of feature space, sparseness\\nand a strong class imbalance. A variant of conformal predictors called\\nInductive Mondrian Conformal Predictor is applied to deal with these\\nchallenges. Results are presented for several non-conformity measures (NCM)\\nextracted from underlying algorithms and different kernels. A number of\\nperformance measures are used in order to demonstrate the flexibility of\\nInductive Mondrian Conformal Predictors in dealing with such a complex set of\\ndata.\\n  Keywords: Conformal Prediction, Confidence Estimation, Chemoinformatics,\\nNon-Conformity Measure.\\n',\n",
       "  'title': u'Conformal Predictors for Compound Activity Prediction'},\n",
       " u'1512.01563': {'arxivid': u'1512.01563',\n",
       "  'authorsaffil': [[u'Yitao Liang', None],\n",
       "   [u'Marlos C. Machado', None],\n",
       "   [u'Erik Talvitie', None],\n",
       "   [u'Michael Bowling', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'A shorter version of this paper appears in the Proceedings of the\\n  15th International Conference on Autonomous Agents and Multiagent Systems\\n  (AAMAS 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.01563v2',\n",
       "  'published': u'2015-12-04T21:06:04Z',\n",
       "  'summary': u\"  The recently introduced Deep Q-Networks (DQN) algorithm has gained attention\\nas one of the first successful combinations of deep neural networks and\\nreinforcement learning. Its promise was demonstrated in the Arcade Learning\\nEnvironment (ALE), a challenging framework composed of dozens of Atari 2600\\ngames used to evaluate general competency in AI. It achieved dramatically\\nbetter results than earlier approaches, showing that its ability to learn good\\nrepresentations is quite robust and general. This paper attempts to understand\\nthe principles that underlie DQN's impressive performance and to better\\ncontextualize its success. We systematically evaluate the importance of key\\nrepresentational biases encoded by DQN's network by proposing simple linear\\nrepresentations that make use of these concepts. Incorporating these\\ncharacteristics, we obtain a computationally practical feature set that\\nachieves competitive performance to DQN in the ALE. Besides offering insight\\ninto the strengths and weaknesses of DQN, we provide a generic representation\\nfor the ALE, significantly reducing the burden of learning a representation for\\neach game. Moreover, we also provide a simple, reproducible benchmark for the\\nsake of comparison to future work in the ALE.\\n\",\n",
       "  'title': u'State of the Art Control of Atari Games Using Shallow Reinforcement\\n  Learning'},\n",
       " u'1510.04189': {'arxivid': u'1510.04189',\n",
       "  'authorsaffil': [[u'Arild N\\xf8kland', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.04189v2',\n",
       "  'published': u'2015-10-14T16:27:28Z',\n",
       "  'summary': u'  The back-propagation algorithm is widely used for learning in artificial\\nneural networks. A challenge in machine learning is to create models that\\ngeneralize to new data samples not seen in the training data. Recently, a\\ncommon flaw in several machine learning algorithms was discovered: small\\nperturbations added to the input data lead to consistent misclassification of\\ndata samples. Samples that easily mislead the model are called adversarial\\nexamples. Training a \"maxout\" network on adversarial examples has shown to\\ndecrease this vulnerability, but also increase classification performance. This\\npaper shows that adversarial training has a regularizing effect also in\\nnetworks with logistic, hyperbolic tangent and rectified linear units. A simple\\nextension to the back-propagation method is proposed, that adds an adversarial\\ngradient to the training. The extension requires an additional forward and\\nbackward pass to calculate a modified input sample, or mini batch, used as\\ninput for standard back-propagation learning. The first experimental results on\\nMNIST show that the \"adversarial back-propagation\" method increases the\\nresistance to adversarial examples and boosts the classification performance.\\nThe extension reduces the classification error on the permutation invariant\\nMNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a\\nnetwork with rectified linear units. Results on CIFAR-10 indicate that the\\nmethod has a regularizing effect similar to dropout in fully connected\\nnetworks. Based on these promising results, adversarial back-propagation is\\nproposed as a stand-alone regularizing method that should be further\\ninvestigated.\\n',\n",
       "  'title': u'Improving Back-Propagation by Adding an Adversarial Gradient'},\n",
       " u'1602.08127': {'arxivid': u'1602.08127',\n",
       "  'authorsaffil': [[u'Xiping Fu', None],\n",
       "   [u'Brendan McCane', None],\n",
       "   [u'Steven Mills', None],\n",
       "   [u'Michael Albert', None],\n",
       "   [u'Lech Szymanski', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'Submitting to journal (TPAMI). 17 pages, 11 figures. The Matlab codes\\n  for AutoJacoBin and NOKMeans are available:\\n  https://bitbucket.org/fxpfxp/autojacobin\\n  https://bitbucket.org/fxpfxp/nokmeans The SIFT10M dataset is available at:\\n  http://archive.ics.uci.edu/ml/datasets/SIFT10M',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08127v2',\n",
       "  'published': u'2016-02-25T21:47:16Z',\n",
       "  'summary': u'  Binary codes can be used to speed up nearest neighbor search tasks in large\\nscale data sets as they are efficient for both storage and retrieval. In this\\npaper, we propose a robust auto-encoder model that preserves the geometric\\nrelationships of high-dimensional data sets in Hamming space. This is done by\\nconsidering a noise-removing function in a region surrounding the manifold\\nwhere the training data points lie. This function is defined with the property\\nthat it projects the data points near the manifold into the manifold wisely,\\nand we approximate this function by its first order approximation. Experimental\\nresults show that the proposed method achieves better than state-of-the-art\\nresults on three large scale high dimensional data sets.\\n',\n",
       "  'title': u'Auto-JacoBin: Auto-encoder Jacobian Binary Hashing'},\n",
       " u'1602.00134': {'arxivid': u'1602.00134',\n",
       "  'authorsaffil': [[u'Shih-En Wei', None],\n",
       "   [u'Varun Ramakrishna', None],\n",
       "   [u'Takeo Kanade', None],\n",
       "   [u'Yaser Sheikh', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'camera ready',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00134v4',\n",
       "  'published': u'2016-01-30T16:15:28Z',\n",
       "  'summary': u'  Pose Machines provide a sequential prediction framework for learning rich\\nimplicit spatial models. In this work we show a systematic design for how\\nconvolutional networks can be incorporated into the pose machine framework for\\nlearning image features and image-dependent spatial models for the task of pose\\nestimation. The contribution of this paper is to implicitly model long-range\\ndependencies between variables in structured prediction tasks such as\\narticulated pose estimation. We achieve this by designing a sequential\\narchitecture composed of convolutional networks that directly operate on belief\\nmaps from previous stages, producing increasingly refined estimates for part\\nlocations, without the need for explicit graphical model-style inference. Our\\napproach addresses the characteristic difficulty of vanishing gradients during\\ntraining by providing a natural learning objective function that enforces\\nintermediate supervision, thereby replenishing back-propagated gradients and\\nconditioning the learning procedure. We demonstrate state-of-the-art\\nperformance and outperform competing methods on standard benchmarks including\\nthe MPII, LSP, and FLIC datasets.\\n',\n",
       "  'title': u'Convolutional Pose Machines'},\n",
       " u'1511.02274': {'arxivid': u'1511.02274',\n",
       "  'authorsaffil': [[u'Zichao Yang', None],\n",
       "   [u'Xiaodong He', None],\n",
       "   [u'Jianfeng Gao', None],\n",
       "   [u'Li Deng', None],\n",
       "   [u'Alex Smola', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL', u'cs.CV', u'cs.NE'],\n",
       "  'comment': u'test-dev/standard results added',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.02274v2',\n",
       "  'published': u'2015-11-07T00:43:32Z',\n",
       "  'summary': u'  This paper presents stacked attention networks (SANs) that learn to answer\\nnatural language questions from images. SANs use semantic representation of a\\nquestion as query to search for the regions in an image that are related to the\\nanswer. We argue that image question answering (QA) often requires multiple\\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\\nimage multiple times to infer the answer progressively. Experiments conducted\\non four image QA data sets demonstrate that the proposed SANs significantly\\noutperform previous state-of-the-art approaches. The visualization of the\\nattention layers illustrates the progress that the SAN locates the relevant\\nvisual clues that lead to the answer of the question layer-by-layer.\\n',\n",
       "  'title': u'Stacked Attention Networks for Image Question Answering'},\n",
       " u'1603.03758': {'arxivid': u'1603.03758',\n",
       "  'authorsaffil': [[u'Dane Bell', None],\n",
       "   [u'Gus Hahn-Powell', None],\n",
       "   [u'Marco A. Valenzuela-Esc\\xe1rcega', None],\n",
       "   [u'Mihai Surdeanu', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'This paper will appear in LREC 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03758v1',\n",
       "  'published': u'2016-03-11T20:48:49Z',\n",
       "  'summary': u'  We describe challenges and advantages unique to coreference resolution in the\\nbiomedical domain, and a sieve-based architecture that leverages domain\\nknowledge for both entity and event coreference resolution. Domain-general\\ncoreference resolution algorithms perform poorly on biomedical documents,\\nbecause the cues they rely on such as gender are largely absent in this domain,\\nand because they do not encode domain-specific knowledge such as the number and\\ntype of participants required in chemical reactions. Moreover, it is difficult\\nto directly encode this knowledge into most coreference resolution algorithms\\nbecause they are not rule-based. Our rule-based architecture uses sequentially\\napplied hand-designed \"sieves\", with the output of each sieve informing and\\nconstraining subsequent sieves. This architecture provides a 3.2% increase in\\nthroughput to our Reach event extraction system with precision parallel to that\\nof the stricter system that relies solely on syntactic patterns for extraction.\\n',\n",
       "  'title': u'An investigation of coreference phenomena in the biomedical domain'},\n",
       " u'1602.08128': {'arxivid': u'1602.08128',\n",
       "  'authorsaffil': [[u'Zhenhao Ge', None],\n",
       "   [u'Sudhendu R. Sharma', None],\n",
       "   [u'Mark J. T. Smith', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'SPIE Defense, Security, and Sensing',\n",
       "  'doi': u'10.1117/12.884155',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08128v1',\n",
       "  'published': u'2016-02-25T21:48:56Z',\n",
       "  'summary': u'  This paper presents a method for detecting mispronunciations with the aim of\\nimproving Computer Assisted Language Learning (CALL) tools used by foreign\\nlanguage learners. The algorithm is based on Principle Component Analysis\\n(PCA). It is hierarchical with each successive step refining the estimate to\\nclassify the test word as being either mispronounced or correct. Preprocessing\\nbefore detection, like normalization and time-scale modification, is\\nimplemented to guarantee uniformity of the feature vectors input to the\\ndetection system. The performance using various features including spectrograms\\nand Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.\\nBest results were obtained using MFCCs, achieving up to 99% accuracy in word\\nverification and 93% in native/non-native classification. Compared with Hidden\\nMarkov Models (HMMs) which are used pervasively in recognition application,\\nthis particular approach is computational efficient and effective when training\\ndata is limited.\\n',\n",
       "  'title': u'PCA Method for Automated Detection of Mispronounced Words'},\n",
       " u'1601.04114': {'arxivid': u'1601.04114',\n",
       "  'authorsaffil': [[u'Hossein Mobahi', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04114v2',\n",
       "  'published': u'2016-01-16T02:24:17Z',\n",
       "  'summary': u'  This work presents a new algorithm for training recurrent neural networks\\n(although ideas are applicable to feedforward networks as well). The algorithm\\nis derived from a theory in nonconvex optimization related to the diffusion\\nequation. The contributions made in this work are two fold. First, we show how\\nsome seemingly disconnected mechanisms used in deep learning such as smart\\ninitialization, annealed learning rate, layerwise pretraining, and noise\\ninjection (as done in dropout and SGD) arise naturally and automatically from\\nthis framework, without manually crafting them into the algorithms. Second, we\\npresent some preliminary results on comparing the proposed method against SGD.\\nIt turns out that the new algorithm can achieve similar level of generalization\\naccuracy of SGD in much fewer number of epochs.\\n',\n",
       "  'title': u'Training Recurrent Neural Networks by Diffusion'},\n",
       " u'1502.01400': {'arxivid': u'1502.01400',\n",
       "  'authorsaffil': [[u'Marcelo Pereyra', None], [u'Steve McLaughlin', None]],\n",
       "  'categoryterms': [u'stat.CO', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.01400v3',\n",
       "  'published': u'2015-02-05T00:35:08Z',\n",
       "  'summary': u'  This paper presents a new Bayesian estimation technique for hidden\\nPotts-Markov random fields with unknown regularisation parameters, with\\napplication to fast unsupervised K-class image segmentation. The technique is\\nderived by first removing the regularisation parameter from the Bayesian model\\nby marginalisation, followed by a small-variance-asymptotic (SVA) analysis in\\nwhich the spatial regularisation and the integer-constrained terms of the Potts\\nmodel are decoupled. The evaluation of this SVA Bayesian estimator is then\\nrelaxed into a problem that can be computed efficiently by iteratively solving\\na convex total-variation denoising problem and a least-squares clustering\\n(K-means) problem, both of which can be solved straightforwardly, even in\\nhigh-dimensions, and with parallel computing techniques. This leads to a fast\\nfully unsupervised Bayesian image segmentation methodology in which the\\nstrength of the spatial regularisation is adapted automatically to the observed\\nimage during the inference procedure, and that can be easily applied in large\\n2D and 3D scenarios or in applications requiring low computing times.\\nExperimental results on real images, as well as extensive comparisons with\\nstate-of-the-art algorithms, confirm that the proposed methodology offer\\nextremely fast convergence and produces accurate segmentation results, with the\\nimportant additional advantage of self-adjusting regularisation parameters.\\n',\n",
       "  'title': u'Fast unsupervised Bayesian image segmentation with adaptive spatial\\n  regularisation'},\n",
       " u'1603.07745': {'arxivid': u'1603.07745',\n",
       "  'authorsaffil': [[u'Ganesh Sundaramoorthi', None],\n",
       "   [u'Naeemullah Khan', None],\n",
       "   [u'Byung-Woo Hong', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07745v1',\n",
       "  'published': u'2016-03-24T20:39:24Z',\n",
       "  'summary': u'  We formulate a general energy and method for segmentation that is designed to\\nhave preference for segmenting the coarse structure over the fine structure of\\nthe data, without smoothing across boundaries of regions. The energy is\\nformulated by considering data terms at a continuum of scales from the scale\\nspace computed from the Heat Equation within regions, and integrating these\\nterms over all time. We show that the energy may be approximately optimized\\nwithout solving for the entire scale space, but rather solving time-independent\\nlinear equations at the native scale of the image, making the method\\ncomputationally feasible. We provide a multi-region scheme, and apply our\\nmethod to motion segmentation. Experiments on a benchmark dataset shows that\\nour method is less sensitive to clutter or other undesirable fine-scale\\nstructure, and leads to better performance in motion segmentation.\\n',\n",
       "  'title': u'Coarse-to-Fine Segmentation With Shape-Tailored Scale Spaces'},\n",
       " u'1510.08628': {'arxivid': u'1510.08628',\n",
       "  'authorsaffil': [[u'Jianfei Chen', None],\n",
       "   [u'Kaiwei Li', None],\n",
       "   [u'Jun Zhu', None],\n",
       "   [u'Wenguang Chen', None]],\n",
       "  'categoryterms': [u'stat.ML',\n",
       "   u'cs.DC',\n",
       "   u'cs.IR',\n",
       "   u'cs.LG',\n",
       "   u'H.3.4; G.3; G.4'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.08628v2',\n",
       "  'published': u'2015-10-29T10:33:20Z',\n",
       "  'summary': u'  Developing efficient and scalable algorithms for Latent Dirichlet Allocation\\n(LDA) is of wide interest for many applications. Previous work has developed an\\nO(1) Metropolis-Hastings sampling method for each token. However, the\\nperformance is far from being optimal due to random accesses to the parameter\\nmatrices and frequent cache misses.\\n  In this paper, we first carefully analyze the memory access efficiency of\\nexisting algorithms for LDA by the scope of random access, which is the size of\\nthe memory region in which random accesses fall, within a short period of time.\\nWe then develop WarpLDA, an LDA sampler which achieves both the best O(1) time\\ncomplexity per token and the best O(K) scope of random access. Our empirical\\nresults in a wide range of testing conditions demonstrate that WarpLDA is\\nconsistently 5-15x faster than the state-of-the-art Metropolis-Hastings based\\nLightLDA, and is comparable or faster than the sparsity aware F+LDA. With\\nWarpLDA, users can learn up to one million topics from hundreds of millions of\\ndocuments in a few hours, at an unprecedentedly throughput of 11G tokens per\\nsecond.\\n',\n",
       "  'title': u'WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet\\n  Allocation'},\n",
       " u'1603.00536': {'arxivid': u'1603.00536',\n",
       "  'authorsaffil': [[u'C\\xe9sar A. Mu\\xf1oz', u'NASA Langley Research Center'],\n",
       "   [u'Jorge A. P\\xe9rez', u'University of Groningen']],\n",
       "  'categoryterms': [u'cs.LO', u'cs.PL'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.4204/EPTCS.204',\n",
       "  'journalref': u'EPTCS 204, 2016',\n",
       "  'link': u'http://arxiv.org/abs/1603.00536v1',\n",
       "  'published': u'2016-03-02T00:49:28Z',\n",
       "  'summary': u'  This volume contains the proceedings of DCM 2015, the 11th International\\nWorkshop on Developments in Computational Models held on October 28, 2015 in\\nCali, Colombia. DCM 2015 was organized as a one-day satellite event of the 12th\\nInternational Colloquium on Theoretical Aspects of Computing (ICTAC 2015).\\n  Several new models of computation have emerged in the last few years, and\\nmany developments of traditional computational models have been proposed with\\nthe aim of taking into account the new demands of computer systems users and\\nthe new capabilities of computation engines. A new computational model, or a\\nnew feature in a traditional one, usually is reflected in a new family of\\nprogramming languages, and new paradigms of software development.\\n  The aim of the DCM workshop series is to bring together researchers who are\\ncurrently developing new computational models or new features for traditional\\ncomputational models, in order to foster their interaction, to provide a forum\\nfor presenting new ideas and work in progress, and to enable newcomers to learn\\nabout current activities in this area. Topics of interest include all abstract\\nmodels of computation and their applications to the development of programming\\nlanguages and systems.\\n',\n",
       "  'title': u'Proceedings of the Eleventh International Workshop on Developments in\\n  Computational Models'},\n",
       " u'1511.02014': {'arxivid': u'1511.02014',\n",
       "  'authorsaffil': [[u'Alexander Koplenig', None],\n",
       "   [u'Carolin Mueller-Spitzer', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1371/journal.pone.0150771',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.02014v2',\n",
       "  'published': u'2015-11-06T09:39:24Z',\n",
       "  'summary': u'  In order to demonstrate why it is important to correctly account for the\\n(serial dependent) structure of temporal data, we document an apparently\\nspectacular relationship between population size and lexical diversity: for\\nfive out of seven investigated languages, there is a strong relationship\\nbetween population size and lexical diversity of the primary language in this\\ncountry. We show that this relationship is the result of a misspecified model\\nthat does not consider the temporal aspect of the data by presenting a similar\\nbut nonsensical relationship between the global annual mean sea level and\\nlexical diversity. Given the fact that in the recent past, several studies were\\npublished that present surprising links between different economic, cultural,\\npolitical and (socio-)demographical variables on the one hand and cultural or\\nlinguistic characteristics on the other hand, but seem to suffer from exactly\\nthis problem, we explain the cause of the misspecification and show that it has\\nprofound consequences. We demonstrate how simple transformation of the time\\nseries can often solve problems of this type and argue that the evaluation of\\nthe plausibility of a relationship is important in this context. We hope that\\nour paper will help both researchers and reviewers to understand why it is\\nimportant to use special models for the analysis of data with a natural\\ntemporal ordering.\\n',\n",
       "  'title': u'Population size predicts lexical diversity, but so does the mean sea\\n  level - why it is important to correctly account for the structure of\\n  temporal data'},\n",
       " u'1603.04908': {'arxivid': u'1603.04908',\n",
       "  'authorsaffil': [[u'Gedas Bertasius', None],\n",
       "   [u'Hyun Soo Park', None],\n",
       "   [u'Stella X. Yu', None],\n",
       "   [u'Jianbo Shi', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04908v1',\n",
       "  'published': u'2016-03-15T22:29:03Z',\n",
       "  'summary': u\"  Objects afford visual sensation and motor actions. A first person camera,\\nplaced at the person's head, captures unscripted moments of our visual\\nsensorimotor object interactions. Can a single first person image tell us about\\nour momentary visual attention and motor action with objects, without a gaze\\ntracking device or tactile sensors? To study the holistic correlation of visual\\nattention with motor action, we introduce the concept of\\naction-objects---objects associated with seeing and touching actions, which\\nexhibit characteristic 3D spatial distance and orientation with respect to the\\nperson. A predictive action-object model is designed to re-organize the space\\nof interactions in terms of visual and tactile sensations, which is realized by\\nour proposed EgoNet network. EgoNet is composed of two convolutional neural\\nnetworks: 1) Semantic Gaze Pathway that learns 2D appearance cues with first\\nperson coordinate embedding, and 2) 3D Spatial Pathway that focuses on 3D depth\\nand height measurements relative to the person with brightness reflectance\\nattached. Retaining two distinct pathways enables effective learning from a\\nlimited number of examples, diversified prediction from complementary visual\\nsignals, and flexible architecture that is functional with RGB image without\\ndepth information. We show that our model correctly predicts action-objects in\\na first person image where we outperform the existing approaches across\\ndifferent datasets.\\n\",\n",
       "  'title': u'First Person Action-Object Detection with EgoNet'},\n",
       " u'1506.09016': {'arxivid': u'1506.09016',\n",
       "  'authorsaffil': [[u'Guillaume Bouchard', None],\n",
       "   [u'Th\\xe9o Trouillon', None],\n",
       "   [u'Julien Perez', None],\n",
       "   [u'Adrien Gaidon', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'cs.NA', u'math.OC', u'stat.ML'],\n",
       "  'comment': u'Update: removed convergence theorem and proof as there is an error.\\n  Submitted to UAI 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.09016v2',\n",
       "  'published': u'2015-06-30T10:08:35Z',\n",
       "  'summary': u'  Stochastic Gradient Descent (SGD) is one of the most widely used techniques\\nfor online optimization in machine learning. In this work, we accelerate SGD by\\nadaptively learning how to sample the most useful training examples at each\\ntime step. First, we show that SGD can be used to learn the best possible\\nsampling distribution of an importance sampling estimator. Second, we show that\\nthe sampling distribution of an SGD algorithm can be estimated online by\\nincrementally minimizing the variance of the gradient. The resulting algorithm\\n- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to\\noptimize, as well as a set of parameters to sample learning examples. We show\\nthat AWSGD yields faster convergence in three different applications: (i) image\\nclassification with deep features, where the sampling of images depends on\\ntheir labels, (ii) matrix factorization, where rows and columns are not sampled\\nuniformly, and (iii) reinforcement learning, where the optimized and\\nexploration policies are estimated at the same time, where our approach\\ncorresponds to an off-policy gradient algorithm.\\n',\n",
       "  'title': u'Online Learning to Sample'},\n",
       " u'1602.04906': {'arxivid': u'1602.04906',\n",
       "  'authorsaffil': [[u'Junyan Wang', None],\n",
       "   [u'Sai-kit Yeung', None],\n",
       "   [u'Jue Wang', None],\n",
       "   [u'Kun Zhou', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.GR', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04906v1',\n",
       "  'published': u'2016-02-16T04:31:20Z',\n",
       "  'summary': u'  Recent works on interactive video object cutout mainly focus on designing\\ndynamic foreground-background (FB) classifiers for segmentation propagation.\\nHowever, the research on optimally removing errors from the FB classification\\nis sparse, and the errors often accumulate rapidly, causing significant errors\\nin the propagated frames. In this work, we take the initial steps to addressing\\nthis problem, and we call this new task \\\\emph{segmentation rectification}. Our\\nkey observation is that the possibly asymmetrically distributed false positive\\nand false negative errors were handled equally in the conventional methods. We,\\nalternatively, propose to optimally remove these two types of errors. To this\\neffect, we propose a novel bilayer Markov Random Field (MRF) model for this new\\ntask. We also adopt the well-established structured learning framework to learn\\nthe optimal model from data. Additionally, we propose a novel one-class\\nstructured SVM (OSSVM) which greatly speeds up the structured learning process.\\nOur method naturally extends to RGB-D videos as well. Comprehensive experiments\\non both RGB and RGB-D data demonstrate that our simple and effective method\\nsignificantly outperforms the segmentation propagation methods adopted in the\\nstate-of-the-art video cutout systems, and the results also suggest the\\npotential usefulness of our method in image cutout system.\\n',\n",
       "  'title': u'Segmentation Rectification for Video Cutout via One-Class Structured\\n  Learning'},\n",
       " u'1504.06103': {'arxivid': u'1504.06103',\n",
       "  'authorsaffil': [[u'Tomas Vojir', None],\n",
       "   [u'Jiri Matas', None],\n",
       "   [u'Jana Noskova', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'27 pages, 9 figures, submitted to CVIU journal',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.06103v2',\n",
       "  'published': u'2015-04-23T09:34:59Z',\n",
       "  'summary': u'  In this paper, we propose a novel method for visual object tracking called\\nHMMTxD. The method fuses observations from complementary out-of-the box\\ntrackers and a detector by utilizing a hidden Markov model whose latent states\\ncorrespond to a binary vector expressing the failure of individual trackers.\\nThe Markov model is trained in an unsupervised way, relying on an online\\nlearned detector to provide a source of tracker-independent information for a\\nmodified Baum- Welch algorithm that updates the model w.r.t. the partially\\nannotated data.\\n  We show the effectiveness of the proposed method on combination of two and\\nthree tracking algorithms. The performance of HMMTxD is evaluated on two\\nstandard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publicly\\navailable sequences. The HMMTxD outperforms the state-of-the-art, often\\nsignificantly, on all datasets in almost all criteria.\\n',\n",
       "  'title': u'Online Adaptive Hidden Markov Model for Multi-Tracker Fusion'},\n",
       " u'1603.09509': {'arxivid': u'1603.09509',\n",
       "  'authorsaffil': [[u'Zhenyao Zhu', None],\n",
       "   [u'Jesse H. Engel', None],\n",
       "   [u'Awni Hannun', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE', u'cs.SD'],\n",
       "  'comment': u'\"fix typo in the title\"',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09509v2',\n",
       "  'published': u'2016-03-31T09:54:44Z',\n",
       "  'summary': u'  Deep learning has dramatically improved the performance of speech recognition\\nsystems through learning hierarchies of features optimized for the task at\\nhand. However, true end-to-end learning, where features are learned directly\\nfrom waveforms, has only recently reached the performance of hand-tailored\\nrepresentations based on the Fourier transform. In this paper, we detail an\\napproach to use convolutional filters to push past the inherent tradeoff of\\ntemporal and frequency resolution that exists for spectral representations. At\\nincreased computational cost, we show that increasing temporal resolution via\\nreduced stride and increasing frequency resolution via additional filters\\ndelivers significant performance improvements. Further, we find more efficient\\nrepresentations by simultaneously learning at multiple scales, leading to an\\noverall decrease in word error rate on a difficult internal speech test set by\\n20.7% relative to networks with the same number of parameters trained on\\nspectrograms.\\n',\n",
       "  'title': u'Learning Multiscale Features Directly From Waveforms'},\n",
       " u'1603.04904': {'arxivid': u'1603.04904',\n",
       "  'authorsaffil': [[u'Wei Li', None],\n",
       "   [u'Melvin Gauci', None],\n",
       "   [u'Roderich Gross', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'submitted',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04904v1',\n",
       "  'published': u'2016-03-15T22:20:52Z',\n",
       "  'summary': u\"  We propose Turing Learning, a novel system identification method for\\ninferring behavior. Turing Learning simultaneously optimizes models and\\nclassifiers. The classifiers are provided with data samples from both an agent\\nand models under observation, and are rewarded for discriminating between them.\\nConversely, the models are rewarded for 'tricking' the classifiers into\\ncategorizing them as the agent. Unlike other methods for system identification,\\nTuring Learning does not require predefined metrics to quantify the difference\\nbetween the agent and models. We present two case studies with swarms of\\nsimulated robots that show that Turing Learning outperforms a metric-based\\nsystem identification method in terms of model accuracy. The classifiers\\nperform well collectively and could be used to detect abnormal behavior in the\\nswarm. Moreover, we show that Turing Learning also successfully infers the\\nbehavior of physical robot swarms. The results show that collective behaviors\\ncan be directly inferred from motion trajectories of a single agent in the\\nswarm, which may have significant implications for the study of animal\\ncollectives.\\n\",\n",
       "  'title': u'Turing learning: a metric-free approach to inferring behavior and its\\n  application to swarms'},\n",
       " u'1405.4604': {'arxivid': u'1405.4604',\n",
       "  'authorsaffil': [[u'Razvan Pascanu', None],\n",
       "   [u'Yann N. Dauphin', None],\n",
       "   [u'Surya Ganguli', None],\n",
       "   [u'Yoshua Bengio', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'11 pages, 8 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1405.4604v2',\n",
       "  'published': u'2014-05-19T04:56:30Z',\n",
       "  'summary': u'  A central challenge to many fields of science and engineering involves\\nminimizing non-convex error functions over continuous, high dimensional spaces.\\nGradient descent or quasi-Newton methods are almost ubiquitously used to\\nperform such minimizations, and it is often thought that a main source of\\ndifficulty for the ability of these local methods to find the global minimum is\\nthe proliferation of local minima with much higher error than the global\\nminimum. Here we argue, based on results from statistical physics, random\\nmatrix theory, and neural network theory, that a deeper and more profound\\ndifficulty originates from the proliferation of saddle points, not local\\nminima, especially in high dimensional problems of practical interest. Such\\nsaddle points are surrounded by high error plateaus that can dramatically slow\\ndown learning, and give the illusory impression of the existence of a local\\nminimum. Motivated by these arguments, we propose a new algorithm, the\\nsaddle-free Newton method, that can rapidly escape high dimensional saddle\\npoints, unlike gradient descent and quasi-Newton methods. We apply this\\nalgorithm to deep neural network training, and provide preliminary numerical\\nevidence for its superior performance.\\n',\n",
       "  'title': u'On the saddle point problem for non-convex optimization'},\n",
       " u'1604.02416': {'arxivid': u'1604.02416',\n",
       "  'authorsaffil': [[u'Mohammad Khajah', None],\n",
       "   [u'Robert V. Lindsey', None],\n",
       "   [u'Michael C. Mozer', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.NE'],\n",
       "  'comment': u'8 pages, 2 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02416v1',\n",
       "  'published': u'2016-03-14T04:20:55Z',\n",
       "  'summary': u\"  In theoretical cognitive science, there is a tension between highly\\nstructured models whose parameters have a direct psychological interpretation\\nand highly complex, general-purpose models whose parameters and representations\\nare difficult to interpret. The former typically provide more insight into\\ncognition but the latter often perform better. This tension has recently\\nsurfaced in the realm of educational data mining, where a deep learning\\napproach to predicting students' performance as they work through a series of\\nexercises---termed deep knowledge tracing or DKT---has demonstrated a stunning\\nperformance advantage over the mainstay of the field, Bayesian knowledge\\ntracing or BKT. In this article, we attempt to understand the basis for DKT's\\nadvantage by considering the sources of statistical regularity in the data that\\nDKT can leverage but which BKT cannot. We hypothesize four forms of regularity\\nthat BKT fails to exploit: recency effects, the contextualized trial sequence,\\ninter-skill similarity, and individual variation in ability. We demonstrate\\nthat when BKT is extended to allow it more flexibility in modeling statistical\\nregularities---using extensions previously proposed in the literature---BKT\\nachieves a level of performance indistinguishable from that of DKT. We argue\\nthat while DKT is a powerful, useful, general-purpose framework for modeling\\nstudent learning, its gains do not come from the discovery of novel\\nrepresentations---the fundamental advantage of deep learning. To answer the\\nquestion posed in our title, knowledge tracing may be a domain that does not\\nrequire `depth'; shallow models like BKT can perform just as well and offer us\\ngreater interpretability and explanatory power.\\n\",\n",
       "  'title': u'How deep is knowledge tracing?'},\n",
       " u'1601.04075': {'arxivid': u'1601.04075',\n",
       "  'authorsaffil': [[u'Igor A. Podgorny', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.CL', u'cs.SI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04075v1',\n",
       "  'published': u'2016-01-15T21:01:12Z',\n",
       "  'summary': u'  TurboTax AnswerXchange is a social Q&A system supporting users working on\\nfederal and state tax returns. Using 2015 data, we demonstrate that content\\npopularity (or number of views per AnswerXchange question) can be predicted\\nwith reasonable accuracy based on attributes of the question alone. We also\\nemploy probabilistic topic analysis and uplift modeling to identify question\\nfeatures with the highest impact on popularity. We demonstrate that content\\npopularity is driven by behavioral attributes of AnswerXchange users and\\ndepends on complex interactions between search ranking algorithms,\\npsycholinguistic factors and question writing style. Our findings provide a\\nrationale for employing popularity predictions to guide the users into\\nformulating better questions and editing the existing ones. For example,\\nstarting question title with a question word or adding details to the question\\nincrease number of views per question. Similar approach can be applied to\\npromoting AnswerXchange content indexed by Google to drive organic traffic to\\nTurboTax.\\n',\n",
       "  'title': u'Modification of Question Writing Style Influences Content Popularity in\\n  a Social Q&A System'},\n",
       " u'1602.07620': {'arxivid': u'1602.07620',\n",
       "  'authorsaffil': [[u'Ashutosh Mishra', None],\n",
       "   [u'Sudipta Mahapatra', None],\n",
       "   [u'Swapna Banerjee', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Submitting to journal',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07620v1',\n",
       "  'published': u'2015-09-15T13:25:30Z',\n",
       "  'summary': u\"  Due to the confined focal length of optical sensors, focusing all objects in\\na scene with a single sensor is a difficult task. To handle such a situation,\\nimage fusion methods are used in multi-focus environment. Discrete Cosine\\nTransform (DCT) is a widely used image compression transform, image fusion in\\nDCT domain is an efficient method. This paper presents a low complexity\\napproach for multi-focus image fusion and its VLSI implementation using DCT.\\nThe proposed method is evaluated using reference/non-reference fusion measure\\ncriteria and the obtained results asserts it's effectiveness. The maximum\\nsynthesized frequency on FPGA is found to be 221 MHz and consumes 42% of FPGA\\nresources. The proposed method consumes very less power and can process 4K\\nresolution images at the rate of 60 frames per second which makes the hardware\\nsuitable for handheld portable devices such as camera module and wireless image\\nsensors.\\n\",\n",
       "  'title': u'A Low Complexity VLSI Architecture for Multi-Focus Image Fusion in DCT\\n  Domain'},\n",
       " u'1512.05300': {'arxivid': u'1512.05300',\n",
       "  'authorsaffil': [[u'Evgeniya Ustinova', None],\n",
       "   [u'Yaroslav Ganin', None],\n",
       "   [u'Victor Lempitsky', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05300v2',\n",
       "  'published': u'2015-12-16T19:45:37Z',\n",
       "  'summary': u'  In this work we explore the applicability of the recently proposed CNN\\narchitecture, called Bilinear CNN, and its new modification that we call\\nmulti-region Bilinear CNN to the person re-identification problem. Originally,\\nBilinear CNNs were introduced for fine-grained classification and proved to be\\nboth simple and high-performing architectures. Bilinear CNN allows to build an\\norderless descriptor for an image using outer product of features outputted\\nfrom two separate feature extractors. Based on this approach, Multiregion\\nBilinear CNN, apply bilinear pooling over multiple regions for extracting rich\\nand useful descriptors that retain some spatial information.\\n  We show than when embedded into a standard \"siamese\" type learning, bilinear\\nCNNs and in particular their multi-region variants can improve\\nre-identification performance compared to standard CNNs and achieve\\nstate-of-the-art accuracy on the largest person re-identification datasets\\navailable at the moment, namely CUHK03 and Market-1501.\\n',\n",
       "  'title': u'Multiregion Bilinear Convolutional Neural Networks for Person\\n  Re-Identification'},\n",
       " u'1511.05616': {'arxivid': u'1511.05616',\n",
       "  'authorsaffil': [[u'Hexiang Hu', None],\n",
       "   [u'Guang-Tong Zhou', None],\n",
       "   [u'Zhiwei Deng', None],\n",
       "   [u'Zicheng Liao', None],\n",
       "   [u'Greg Mori', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05616v3',\n",
       "  'published': u'2015-11-17T23:22:25Z',\n",
       "  'summary': u'  Images of scenes have various objects as well as abundant attributes, and\\ndiverse levels of visual categorization are possible. A natural image could be\\nassigned with fine-grained labels that describe major components,\\ncoarse-grained labels that depict high level abstraction or a set of labels\\nthat reveal attributes. Such categorization at different concept layers can be\\nmodeled with label graphs encoding label information. In this paper, we exploit\\nthis rich information with a state-of-art deep learning framework, and propose\\na generic structured model that leverages diverse label relations to improve\\nimage classification performance. Our approach employs a novel stacked label\\nprediction neural network, capturing both inter-level and intra-level label\\nsemantics. We evaluate our method on benchmark image datasets, and empirical\\nresults illustrate the efficacy of our model.\\n',\n",
       "  'title': u'Learning Structured Inference Neural Networks with Label Relations'},\n",
       " u'1601.00770': {'arxivid': u'1601.00770',\n",
       "  'authorsaffil': [[u'Makoto Miwa', None], [u'Mohit Bansal', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'Accepted for publication at the Association for Computational\\n  Linguistics (ACL), 2016. 13 pages, 1 figure, 6 tables',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00770v3',\n",
       "  'published': u'2016-01-05T08:53:05Z',\n",
       "  'summary': u'  We present a novel end-to-end neural model to extract entities and relations\\nbetween them. Our recurrent neural network based model captures both word\\nsequence and dependency tree substructure information by stacking bidirectional\\ntree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows\\nour model to jointly represent both entities and relations with shared\\nparameters in a single model. We further encourage detection of entities during\\ntraining and use of entity information in relation extraction via entity\\npretraining and scheduled sampling. Our model improves over the\\nstate-of-the-art feature-based model on end-to-end relation extraction,\\nachieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and\\nACE2004, respectively. We also show that our LSTM-RNN based model compares\\nfavorably to the state-of-the-art CNN based model (in F1-score) on nominal\\nrelation classification (SemEval-2010 Task 8). Finally, we present an extensive\\nablation analysis of several model components.\\n',\n",
       "  'title': u'End-to-End Relation Extraction using LSTMs on Sequences and Tree\\n  Structures'},\n",
       " u'1603.04513': {'arxivid': u'1603.04513',\n",
       "  'authorsaffil': [[u'Wenpeng Yin', None], [u'Hinrich Sch\\xfctze', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'in Proceeding of CoNLL2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04513v1',\n",
       "  'published': u'2016-03-15T00:25:02Z',\n",
       "  'summary': u'  We propose MVCNN, a convolution neural network (CNN) architecture for\\nsentence classification. It (i) combines diverse versions of pretrained word\\nembeddings and (ii) extracts features of multigranular phrases with\\nvariable-size convolution filters. We also show that pretraining MVCNN is\\ncritical for good performance. MVCNN achieves state-of-the-art performance on\\nfour tasks: on small-scale binary, small-scale multi-class and largescale\\nTwitter sentiment prediction and on subjectivity classification.\\n',\n",
       "  'title': u'Multichannel Variable-Size Convolution for Sentence Classification'},\n",
       " u'1512.08301': {'arxivid': u'1512.08301',\n",
       "  'authorsaffil': [[u'Shiliang Zhang', None],\n",
       "   [u'Cong Liu', None],\n",
       "   [u'Hui Jiang', None],\n",
       "   [u'Si Wei', None],\n",
       "   [u'Lirong Dai', None],\n",
       "   [u'Yu Hu', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'11 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.08301v2',\n",
       "  'published': u'2015-12-28T02:07:48Z',\n",
       "  'summary': u'  In this paper, we propose a novel neural network structure, namely\\n\\\\emph{feedforward sequential memory networks (FSMN)}, to model long-term\\ndependency in time series without using recurrent feedback. The proposed FSMN\\nis a standard fully-connected feedforward neural network equipped with some\\nlearnable memory blocks in its hidden layers. The memory blocks use a\\ntapped-delay line structure to encode the long context information into a\\nfixed-size representation as short-term memory mechanism. We have evaluated the\\nproposed FSMNs in several standard benchmark tasks, including speech\\nrecognition and language modelling. Experimental results have shown FSMNs\\nsignificantly outperform the conventional recurrent neural networks (RNN),\\nincluding LSTMs, in modeling sequential signals like speech or language.\\nMoreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMs\\ndue to the inherent non-recurrent model structure.\\n',\n",
       "  'title': u'Feedforward Sequential Memory Networks: A New Structure to Learn\\n  Long-term Dependency'},\n",
       " u'1503.08909': {'arxivid': u'1503.08909',\n",
       "  'authorsaffil': [[u'Joe Yue-Hei Ng', None],\n",
       "   [u'Matthew Hausknecht', None],\n",
       "   [u'Sudheendra Vijayanarasimhan', None],\n",
       "   [u'Oriol Vinyals', None],\n",
       "   [u'Rajat Monga', None],\n",
       "   [u'George Toderici', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.08909v2',\n",
       "  'published': u'2015-03-31T04:34:12Z',\n",
       "  'summary': u'  Convolutional neural networks (CNNs) have been extensively applied for image\\nrecognition problems giving state-of-the-art results on recognition, detection,\\nsegmentation and retrieval. In this work we propose and evaluate several deep\\nneural network architectures to combine image information across a video over\\nlonger time periods than previously attempted. We propose two methods capable\\nof handling full length videos. The first method explores various convolutional\\ntemporal feature pooling architectures, examining the various design choices\\nwhich need to be made when adapting a CNN for this task. The second proposed\\nmethod explicitly models the video as an ordered sequence of frames. For this\\npurpose we employ a recurrent neural network that uses Long Short-Term Memory\\n(LSTM) cells which are connected to the output of the underlying CNN. Our best\\nnetworks exhibit significant performance improvements over previously published\\nresults on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101\\ndatasets with (88.6% vs. 88.0%) and without additional optical flow information\\n(82.6% vs. 72.8%).\\n',\n",
       "  'title': u'Beyond Short Snippets: Deep Networks for Video Classification'},\n",
       " u'1601.00917': {'arxivid': u'1601.00917',\n",
       "  'authorsaffil': [[u'Jie Fu', None],\n",
       "   [u'Hongyin Luo', None],\n",
       "   [u'Jiashi Feng', None],\n",
       "   [u'Kian Hsiang Low', None],\n",
       "   [u'Tat-Seng Chua', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'International Joint Conference on Artificial Intelligence, IJCAI,\\n  2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00917v5',\n",
       "  'published': u'2016-01-05T17:43:15Z',\n",
       "  'summary': u'  The performance of deep neural networks is well-known to be sensitive to the\\nsetting of their hyperparameters. Recent advances in reverse-mode automatic\\ndifferentiation allow for optimizing hyperparameters with gradients. The\\nstandard way of computing these gradients involves a forward and backward pass\\nof computations. However, the backward pass usually needs to consume\\nunaffordable memory to store all the intermediate variables to exactly reverse\\nthe forward training procedure. In this work we propose a simple but effective\\nmethod, DrMAD, to distill the knowledge of the forward pass into a shortcut\\npath, through which we approximately reverse the training trajectory.\\nExperiments on several image benchmark datasets show that DrMAD is at least 45\\ntimes faster and consumes 100 times less memory compared to state-of-the-art\\nmethods for optimizing hyperparameters with minimal compromise to its\\neffectiveness. To the best of our knowledge, DrMAD is the first research\\nattempt to make it practical to automatically tune thousands of hyperparameters\\nof deep neural networks. The code can be downloaded from\\nhttps://github.com/bigaidream-projects/drmad\\n',\n",
       "  'title': u'DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing\\n  Hyperparameters of Deep Neural Networks'},\n",
       " u'1511.05432': {'arxivid': u'1511.05432',\n",
       "  'authorsaffil': [[u'Uri Shaham', None],\n",
       "   [u'Yutaro Yamada', None],\n",
       "   [u'Sahand Negahban', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05432v3',\n",
       "  'published': u'2015-11-17T15:14:57Z',\n",
       "  'summary': u'  We propose a general framework for increasing local stability of Artificial\\nNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through an\\nalternating minimization-maximization procedure, in which the loss of the\\nnetwork is minimized over perturbed examples that are generated at each\\nparameter update. We show that adversarial training of ANNs is in fact\\nrobustification of the network optimization, and that our proposed framework\\ngeneralizes previous approaches for increasing local stability of ANNs.\\nExperimental results reveal that our approach increases the robustness of the\\nnetwork to existing adversarial examples, while making it harder to generate\\nnew ones. Furthermore, our algorithm improves the accuracy of the network also\\non the original test data.\\n',\n",
       "  'title': u'Understanding Adversarial Training: Increasing Local Stability of Neural\\n  Nets through Robust Optimization'},\n",
       " u'1507.05956': {'arxivid': u'1507.05956',\n",
       "  'authorsaffil': [[u'Thomas W. Lynch', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.PL'],\n",
       "  'comment': u'6 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.05956v6',\n",
       "  'published': u'2015-07-21T14:24:28Z',\n",
       "  'summary': u\"  This paper describes the IBM 704 architecture and the genesis of the names\\nfor CAR, and CDR, which, as it turns out, probably don't quite make sense. The\\npaper suggests that this may not be all bad, as the names lend themselves to\\ncompounding. Indeed that the compound function names , such as CADR, or even\\nCADADR, etc. may be read as little access programs.\\n\",\n",
       "  'title': u'Towards a Better Understanding of CAR, CDR, CADR and the Others'},\n",
       " u'1409.4327': {'arxivid': u'1409.4327',\n",
       "  'authorsaffil': [[u'Dinesh Jayaraman', None], [u'Kristen Grauman', None]],\n",
       "  'categoryterms': [u'cs.CV', u'stat.ML'],\n",
       "  'comment': u'NIPS 2014',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1409.4327v2',\n",
       "  'published': u'2014-09-15T16:56:07Z',\n",
       "  'summary': u\"  In principle, zero-shot learning makes it possible to train a recognition\\nmodel simply by specifying the category's attributes. For example, with\\nclassifiers for generic attributes like \\\\emph{striped} and \\\\emph{four-legged},\\none can construct a classifier for the zebra category by enumerating which\\nproperties it possesses---even without providing zebra training images. In\\npractice, however, the standard zero-shot paradigm suffers because attribute\\npredictions in novel images are hard to get right. We propose a novel random\\nforest approach to train zero-shot models that explicitly accounts for the\\nunreliability of attribute predictions. By leveraging statistics about each\\nattribute's error tendencies, our method obtains more robust discriminative\\nmodels for the unseen classes. We further devise extensions to handle the\\nfew-shot scenario and unreliable attribute descriptions. On three datasets, we\\ndemonstrate the benefit for visual category learning with zero or few training\\nexamples, a critical domain for rare categories or categories defined on the\\nfly.\\n\",\n",
       "  'title': u'Zero Shot Recognition with Unreliable Attributes'},\n",
       " u'1603.06220': {'arxivid': u'1603.06220',\n",
       "  'authorsaffil': [[u'Pejman Khadivi', None],\n",
       "   [u'Ravi Tandon', None],\n",
       "   [u'Naren Ramakrishnan', None]],\n",
       "  'categoryterms': [u'cs.IT', u'cs.LG', u'math.IT'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06220v1',\n",
       "  'published': u'2016-03-20T14:39:27Z',\n",
       "  'summary': u'  Feed-forward deep neural networks have been used extensively in various\\nmachine learning applications. Developing a precise understanding of the\\nunderling behavior of neural networks is crucial for their efficient\\ndeployment. In this paper, we use an information theoretic approach to study\\nthe flow of information in a neural network and to determine how entropy of\\ninformation changes between consecutive layers. Moreover, using the Information\\nBottleneck principle, we develop a constrained optimization problem that can be\\nused in the training process of a deep neural network. Furthermore, we\\ndetermine a lower bound for the level of data representation that can be\\nachieved in a deep neural network with an acceptable level of distortion.\\n',\n",
       "  'title': u'Flow of Information in Feed-Forward Deep Neural Networks'},\n",
       " u'1601.02733': {'arxivid': u'1601.02733',\n",
       "  'authorsaffil': [[u'Ehsan Hosseini-Asl', None],\n",
       "   [u'Jacek M. Zurada', None],\n",
       "   [u'Olfa Nasraoui', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Accepted for publication in IEEE Transactions of Neural Networks and\\n  Learning Systems',\n",
       "  'doi': u'10.1109/TNNLS.2015.2479223',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02733v1',\n",
       "  'published': u'2016-01-12T05:33:03Z',\n",
       "  'summary': u'  We demonstrate a new deep learning autoencoder network, trained by a\\nnonnegativity constraint algorithm (NCAE), that learns features which show\\npart-based representation of data. The learning algorithm is based on\\nconstraining negative weights. The performance of the algorithm is assessed\\nbased on decomposing data into parts and its prediction performance is tested\\non three standard image data sets and one text dataset. The results indicate\\nthat the nonnegativity constraint forces the autoencoder to learn features that\\namount to a part-based representation of data, while improving sparsity and\\nreconstruction quality in comparison with the traditional sparse autoencoder\\nand Nonnegative Matrix Factorization. It is also shown that this newly acquired\\nrepresentation improves the prediction performance of a deep neural network.\\n',\n",
       "  'title': u'Deep Learning of Part-based Representation of Data Using Sparse\\n  Autoencoders with Nonnegativity Constraints'},\n",
       " u'1603.05189': {'arxivid': u'1603.05189',\n",
       "  'authorsaffil': [[u'Adam J. Last', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.SY'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05189v1',\n",
       "  'published': u'2016-03-16T17:47:57Z',\n",
       "  'summary': u'  This paper investigates the use of artificial neural networks (ANNs) to\\nreplace traditional algorithms and manual review for identifying anomalies in\\nvehicle run data. The specific data used for this study is from undersea\\nvehicle qualification tests. Such data is highly non-linear, therefore\\ntraditional algorithms are not adequate and manual review is time consuming. By\\nusing ANNs to predict nominal vehicle performance based solely on information\\navailable pre-run, vehicle deviation from expected performance can be\\nautomatically identified in the post-run data. Such capability is only now\\nbecoming available due to the rapid increase in understanding of ANN framework\\nand available computing power in the past decade. The ANN trained for the\\npurpose of this investigation is relatively simple, to keep the computing\\nrequirements within the parameters of a modern desktop PC. This ANN showed\\npotential in predicting vehicle performance, particularly during transient\\nevents within the run data. However, there were also several performance cases,\\nsuch as steady state operation and cases which did not have sufficient training\\ndata, where the ANN showed deficiencies. It is expected that as computational\\npower becomes more readily available, ANN understanding matures, and more\\ntraining data is acquired from real world tests, the performance predictions of\\nthe ANN will surpass traditional algorithms and manual human review.\\n',\n",
       "  'title': u'Applying Artifical Neural Networks To Predict Nominal Vehicle\\n  Performance'},\n",
       " u'1603.07285': {'arxivid': u'1603.07285',\n",
       "  'authorsaffil': [[u'Vincent Dumoulin', None], [u'Francesco Visin', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07285v1',\n",
       "  'published': u'2016-03-23T17:52:21Z',\n",
       "  'summary': u'  We introduce a guide to help deep learning practitioners understand and\\nmanipulate convolutional neural network architectures. The guide clarifies the\\nrelationship between various properties (input shape, kernel shape, zero\\npadding, strides and output shape) of convolutional, pooling and transposed\\nconvolutional layers, as well as the relationship between convolutional and\\ntransposed convolutional layers. Relationships are derived for various cases,\\nand are illustrated in order to make them intuitive.\\n',\n",
       "  'title': u'A guide to convolution arithmetic for deep learning'},\n",
       " u'1512.08949': {'arxivid': u'1512.08949',\n",
       "  'authorsaffil': [[u'Nihar B. Shah', None], [u'Martin J. Wainwright', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.IT', u'math.IT', u'stat.ML'],\n",
       "  'comment': u'Changes in version 2: In addition to recovery in the exact and\\n  Hamming metrics, v2 analyzes a general, abstract recovery criterion based on\\n  a notion of \"allowed sets\"',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.08949v2',\n",
       "  'published': u'2015-12-30T14:25:23Z',\n",
       "  'summary': u'  We consider data in the form of pairwise comparisons of n items, with the\\ngoal of precisely identifying the top k items for some value of k < n, or\\nalternatively, recovering a ranking of all the items. We analyze the Copeland\\ncounting algorithm that ranks the items in order of the number of pairwise\\ncomparisons won, and show it has three attractive features: (a) its\\ncomputational efficiency leads to speed-ups of several orders of magnitude in\\ncomputation time as compared to prior work; (b) it is robust in that\\ntheoretical guarantees impose no conditions on the underlying matrix of\\npairwise-comparison probabilities, in contrast to some prior work that applies\\nonly to the BTL parametric model; and (c) it is an optimal method up to\\nconstant factors, meaning that it achieves the information-theoretic limits for\\nrecovering the top k-subset. We extend our results to obtain sharp guarantees\\nfor approximate recovery under the Hamming distortion metric, and more\\ngenerally, to any arbitrary error requirement that satisfies a simple and\\nnatural monotonicity condition.\\n',\n",
       "  'title': u'Simple, Robust and Optimal Ranking from Pairwise Comparisons'},\n",
       " u'1504.00284': {'arxivid': u'1504.00284',\n",
       "  'authorsaffil': [[u'Adrian Calma', None],\n",
       "   [u'Tobias Reitmaier', None],\n",
       "   [u'Bernhard Sick', None],\n",
       "   [u'Paul Lukowicz', None],\n",
       "   [u'Mark Embrechts', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'16 pages, 6 Figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.00284v3',\n",
       "  'published': u'2015-04-01T16:39:26Z',\n",
       "  'summary': u'  Active learning (AL) is a learning paradigm where an active learner has to\\ntrain a model (e.g., a classifier) which is in principal trained in a\\nsupervised way, but in AL it has to be done by means of a data set with\\ninitially unlabeled samples. To get labels for these samples, the active\\nlearner has to ask an oracle (e.g., a human expert) for labels. The goal is to\\nmaximize the performance of the model and to minimize the number of queries at\\nthe same time. In this article, we first briefly discuss the state of the art\\nand own, preliminary work in the field of AL. Then, we propose the concept of\\ncollaborative active learning (CAL). With CAL, we will overcome some of the\\nharsh limitations of current AL. In particular, we envision scenarios where an\\nexpert may be wrong for various reasons, there might be several or even many\\nexperts with different expertise, the experts may label not only samples but\\nalso knowledge at a higher level such as rules, and we consider that the\\nlabeling costs depend on many conditions. Moreover, in a CAL process human\\nexperts will profit by improving their own knowledge, too.\\n',\n",
       "  'title': u'A New Vision of Collaborative Active Learning'},\n",
       " u'1512.00486': {'arxivid': u'1512.00486',\n",
       "  'authorsaffil': [[u'Maksim Lapin', None],\n",
       "   [u'Matthias Hein', None],\n",
       "   [u'Bernt Schiele', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'In Computer Vision and Pattern Recognition (CVPR), 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.00486v2',\n",
       "  'published': u'2015-12-01T21:22:35Z',\n",
       "  'summary': u'  In order to push the performance on realistic computer vision tasks, the\\nnumber of classes in modern benchmark datasets has significantly increased in\\nrecent years. This increase in the number of classes comes along with increased\\nambiguity between the class labels, raising the question if top-1 error is the\\nright performance measure. In this paper, we provide an extensive comparison\\nand evaluation of established multiclass methods comparing their top-k\\nperformance both from a practical as well as from a theoretical perspective.\\nMoreover, we introduce novel top-k loss functions as modifications of the\\nsoftmax and the multiclass SVM losses and provide efficient optimization\\nschemes for them. In the experiments, we compare on various datasets all of the\\nproposed and established methods for top-k error optimization. An interesting\\ninsight of this paper is that the softmax loss yields competitive top-k\\nperformance for all k simultaneously. For a specific top-k error, our new top-k\\nlosses lead typically to further improvements while being faster to train than\\nthe softmax.\\n',\n",
       "  'title': u'Loss Functions for Top-k Error: Analysis and Insights'},\n",
       " u'1504.06852': {'arxivid': u'1504.06852',\n",
       "  'authorsaffil': [[u'Philipp Fischer', None],\n",
       "   [u'Alexey Dosovitskiy', None],\n",
       "   [u'Eddy Ilg', None],\n",
       "   [u'Philip H\\xe4usser', None],\n",
       "   [u'Caner Haz\\u0131rba\\u015f', None],\n",
       "   [u'Vladimir Golkov', None],\n",
       "   [u'Patrick van der Smagt', None],\n",
       "   [u'Daniel Cremers', None],\n",
       "   [u'Thomas Brox', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'I.2.6; I.4.8'],\n",
       "  'comment': u'Added supplementary material',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.06852v2',\n",
       "  'published': u'2015-04-26T17:30:32Z',\n",
       "  'summary': u'  Convolutional neural networks (CNNs) have recently been very successful in a\\nvariety of computer vision tasks, especially on those linked to recognition.\\nOptical flow estimation has not been among the tasks where CNNs were\\nsuccessful. In this paper we construct appropriate CNNs which are capable of\\nsolving the optical flow estimation problem as a supervised learning task. We\\npropose and compare two architectures: a generic architecture and another one\\nincluding a layer that correlates feature vectors at different image locations.\\n  Since existing ground truth data sets are not sufficiently large to train a\\nCNN, we generate a synthetic Flying Chairs dataset. We show that networks\\ntrained on this unrealistic data still generalize very well to existing\\ndatasets such as Sintel and KITTI, achieving competitive accuracy at frame\\nrates of 5 to 10 fps.\\n',\n",
       "  'title': u'FlowNet: Learning Optical Flow with Convolutional Networks'},\n",
       " u'1604.03073': {'arxivid': u'1604.03073',\n",
       "  'authorsaffil': [[u'Ashley Prater', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'10 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03073v1',\n",
       "  'published': u'2016-04-11T19:14:05Z',\n",
       "  'summary': u\"  Reservoir computing is a recently introduced machine learning paradigm that\\nhas been shown to be well-suited for the processing of spatiotemporal data.\\nRather than training the network node connections and weights via\\nbackpropagation in traditional recurrent neural networks, reservoirs instead\\nhave fixed connections and weights among the `hidden layer' nodes, and\\ntraditionally only the weights to the output layer of neurons are trained using\\nlinear regression. We claim that for signal classification tasks, one may forgo\\nthe weight training step entirely and instead use a simple supervised\\nclustering method. The proposed method is analyzed theoretically and explored\\nthrough numerical experiments on real-world data. The examples demonstrate that\\nthe proposed clustering method outperforms the traditional trained output\\nweight approach in terms of speed, accuracy, and sensitivity to reservoir\\nparameters.\\n\",\n",
       "  'title': u'Reservoir computing for spatiotemporal signal classification without\\n  trained output weights'},\n",
       " u'1604.03075': {'arxivid': u'1604.03075',\n",
       "  'authorsaffil': [[u'Gary B. Huang', None],\n",
       "   [u'Louis K. Scheffer', None],\n",
       "   [u'Stephen M. Plaza', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03075v1',\n",
       "  'published': u'2016-04-11T19:25:44Z',\n",
       "  'summary': u'  Extracting a connectome from an electron microscopy (EM) data set requires\\nidentification of neurons and determination of synapses between neurons. As\\nmanual extraction of this information is very time-consuming, there has been\\nextensive research effort to automatically segment the neurons to help guide\\nand eventually replace manual tracing. Until recently, there has been\\ncomparatively less research on automatically detecting the actual synapses\\nbetween neurons. This discrepancy can, in part, be attributed to several\\nfactors: obtaining neuronal shapes is a prerequisite first step in extracting a\\nconnectome, manual tracing is much more time-consuming than annotating\\nsynapses, and neuronal contact area can be used as a proxy for synapses in\\ndetermining connections.\\n  However, recent research has demonstrated that contact area alone is not a\\nsufficient predictor of synaptic connection. Moreover, as segmentation has\\nimproved, we have observed that synapse annotation is consuming a more\\nsignificant fraction of overall reconstruction time. This ratio will only get\\nworse as segmentation improves, gating overall possible speed-up. Therefore, we\\naddress this problem by developing algorithms that automatically detect\\npre-synaptic neurons and their post-synaptic partners. In particular,\\npre-synaptic structures are detected using a Deep and Wide Multiscale Recursive\\nNetwork, and post-synaptic partners are detected using a MLP with features\\nconditioned on the local segmentation.\\n  This work is novel because it requires minimal amount of training, leverages\\nadvances in image segmentation directly, and provides a complete solution for\\npolyadic synapse detection. We further introduce novel metrics to evaluate our\\nalgorithm on connectomes of meaningful size. These metrics demonstrate that\\ncomplete automatic prediction can be used to effectively characterize most\\nconnectivity correctly.\\n',\n",
       "  'title': u'Fully-Automatic Synapse Prediction and Validation on a Large Data Set'},\n",
       " u'1508.01722': {'arxivid': u'1508.01722',\n",
       "  'authorsaffil': [[u'Jun-Cheng Chen', None],\n",
       "   [u'Vishal M. Patel', None],\n",
       "   [u'Rama Chellappa', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.01722v2',\n",
       "  'published': u'2015-08-07T15:21:19Z',\n",
       "  'summary': u'  In this paper, we present an algorithm for unconstrained face verification\\nbased on deep convolutional features and evaluate it on the newly released\\nIARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world\\nunconstrained faces from 500 subjects with full pose and illumination\\nvariations which are much harder than the traditional Labeled Face in the Wild\\n(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network\\n(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the\\nIJB-A dataset are provided.\\n',\n",
       "  'title': u'Unconstrained Face Verification using Deep CNN Features'},\n",
       " u'1504.08291': {'arxivid': u'1504.08291',\n",
       "  'authorsaffil': [[u'Raja Giryes', None],\n",
       "   [u'Guillermo Sapiro', None],\n",
       "   [u'Alex M. Bronstein', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'stat.ML', u'62M45', u'I.5.1'],\n",
       "  'comment': u'14 pages, 13 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.08291v5',\n",
       "  'published': u'2015-04-30T16:14:52Z',\n",
       "  'summary': u'  Three important properties of a classification machinery are: (i) the system\\npreserves the core information of the input data; (ii) the training examples\\nconvey information about unseen data; and (iii) the system is able to treat\\ndifferently points from different classes. In this work we show that these\\nfundamental properties are satisfied by the architecture of deep neural\\nnetworks. We formally prove that these networks with random Gaussian weights\\nperform a distance-preserving embedding of the data, with a special treatment\\nfor in-class and out-of-class data. Similar points at the input of the network\\nare likely to have a similar output. The theoretical analysis of deep networks\\nhere presented exploits tools used in the compressed sensing and dictionary\\nlearning literature, thereby making a formal connection between these important\\ntopics. The derived results allow drawing conclusions on the metric learning\\nproperties of the network and their relation to its structure, as well as\\nproviding bounds on the required size of the training set such that the\\ntraining examples would represent faithfully the unseen data. The results are\\nvalidated with state-of-the-art trained networks.\\n',\n",
       "  'title': u'Deep Neural Networks with Random Gaussian Weights: A Universal\\n  Classification Strategy?'},\n",
       " u'1604.00092': {'arxivid': u'1604.00092',\n",
       "  'authorsaffil': [[u'Paul Vernaza', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00092v1',\n",
       "  'published': u'2016-04-01T01:04:31Z',\n",
       "  'summary': u'  A novel global energy model for multi-class semantic image segmentation is\\nproposed that admits very efficient exact inference and derivative calculations\\nfor learning. Inference in this model is equivalent to MAP inference in a\\nparticular kind of vector-valued Gaussian Markov random field, and ultimately\\nreduces to solving a linear system of linear PDEs known as a reaction-diffusion\\nsystem. Solving this system can be achieved in time scaling near-linearly in\\nthe number of image pixels by reducing it to sequential FFTs, after a linear\\nchange of basis. The efficiency and differentiability of the model make it\\nespecially well-suited for integration with convolutional neural networks, even\\nallowing it to be used in interior, feature-generating layers and stacked\\nmultiple times. Experimental results are shown demonstrating that the model can\\nbe employed profitably in conjunction with different convolutional net\\narchitectures, and that doing so compares favorably to joint training of a\\nfully-connected CRF with a convolutional net.\\n',\n",
       "  'title': u'Variational reaction-diffusion systems for semantic segmentation'},\n",
       " u'1601.00599': {'arxivid': u'1601.00599',\n",
       "  'authorsaffil': [[u'Matthias Zeppelzauer', None],\n",
       "   [u'Daniel Schopfhauser', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.IR', u'cs.MM'],\n",
       "  'comment': u'Preprint of accepted manuscript for the Elsevier Image and Vision\\n  Computing Journal (IMAVIS). The paper will be published by IMAVIS under DOI\\n  10.1016/j.imavis.2015.12.004',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00599v1',\n",
       "  'published': u'2016-01-04T18:29:33Z',\n",
       "  'summary': u'  A large amount of social media hosted on platforms like Flickr and Instagram\\nis related to social events. The task of social event classification refers to\\nthe distinction of event and non-event-related content as well as the\\nclassification of event types (e.g. sports events, concerts, etc.). In this\\npaper, we provide an extensive study of textual, visual, as well as multimodal\\nrepresentations for social event classification. We investigate strengths and\\nweaknesses of the modalities and study synergy effects between the modalities.\\nExperimental results obtained with our multimodal representation outperform\\nstate-of-the-art methods and provide a new baseline for future research.\\n',\n",
       "  'title': u'Multimodal Classification of Events in Social Media'},\n",
       " u'1602.04128': {'arxivid': u'1602.04128',\n",
       "  'authorsaffil': [[u'Francesco Orabona', None], [u'D\\xe1vid P\\xe1l', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04128v1',\n",
       "  'published': u'2016-02-12T17:11:42Z',\n",
       "  'summary': u\"  In the recent years a number of parameter-free algorithms for online linear\\noptimization over Hilbert spaces and for learning with expert advice have been\\ndeveloped. While these two families of algorithms might seem different to a\\ndistract eye, the proof methods are indeed very similar, making the reader\\nwonder if such a connection is only accidental.\\n  In this paper, we unify these two families, showing that both can be\\ninstantiated from online coin betting algorithms. We present two new reductions\\nfrom online coin betting to online linear optimization over Hilbert spaces and\\nto learning with expert advice. We instantiate our framework using a betting\\nalgorithm based on the Krichevsky-Trofimov estimator. We obtain a simple\\nalgorithm for online linear optimization over any Hilbert space with\\n$O(\\\\norm{u}\\\\sqrt{T \\\\log(1+T \\\\norm{u}}))$ regret with respect to any competitor\\n$u$. For learning with expert advice we obtain an algorithm that has $O(\\\\sqrt{T\\n(1 + \\\\KL{u}{\\\\pi})})$ regret against any competitor $u$ and where $\\\\KL{u}{\\\\pi}$\\nis the Kullback-Leibler divergence between algorithm's prior distribution $\\\\pi$\\nand the competitor. In both cases, no parameters need to be tuned.\\n\",\n",
       "  'title': u'From Coin Betting to Parameter-Free Online Learning'},\n",
       " u'1601.03481': {'arxivid': u'1601.03481',\n",
       "  'authorsaffil': [[u'Tirtharaj Dash', None], [u'H. S. Behera', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'The final version of this paper has been published in \"International\\n  Conference on Communication and Computing (ICC-2014)\"\\n  [http://www.elsevierst.com/conference_book_download_chapter.php?cbid=86#chapter41]',\n",
       "  'doi': None,\n",
       "  'journalref': u'In Proc: K.R. Venugopal, S.C. Lingareddy (eds.) International\\n  Conference on Communication and Computing (ICC- 2014), Bangalore, India (June\\n  12-14, 2014), Computer Networks and Security, 314-323',\n",
       "  'link': u'http://arxiv.org/abs/1601.03481v1',\n",
       "  'published': u'2015-09-19T12:45:19Z',\n",
       "  'summary': u\"  In case of decision making problems, classification of pattern is a complex\\nand crucial task. Pattern classification using multilayer perceptron (MLP)\\ntrained with back propagation learning becomes much complex with increase in\\nnumber of layers, number of nodes and number of epochs and ultimate increases\\ncomputational time [31]. In this paper, an attempt has been made to use fuzzy\\nMLP and its learning algorithm for pattern classification. The time and space\\ncomplexities of the algorithm have been analyzed. A training performance\\ncomparison has been carried out between MLP and the proposed fuzzy-MLP model by\\nconsidering six cases. Results are noted against different learning rates\\nranging from 0 to 1. A new performance evaluation factor 'convergence gain' has\\nbeen introduced. It is observed that the number of epochs drastically reduced\\nand performance increased compared to MLP. The average and minimum gain has\\nbeen found to be 93% and 75% respectively. The best gain is found to be 95% and\\nis obtained by setting the learning rate to 0.55.\\n\",\n",
       "  'title': u'A Fuzzy MLP Approach for Non-linear Pattern Classification'},\n",
       " u'1603.01182': {'arxivid': u'1603.01182',\n",
       "  'authorsaffil': [[u'Filipe Alves Neto Verri', None],\n",
       "   [u'Paulo Roberto Urio', None],\n",
       "   [u'Liang Zhao', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'14 pages, 7 figures, 1 appendix, submitted to IEEE Transactions on\\n  Neural Networks and Learning Systems',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01182v1',\n",
       "  'published': u'2016-03-03T17:11:23Z',\n",
       "  'summary': u'  The emergence of collective dynamics in neural networks is a mechanism of the\\nanimal and human brain for information processing. In this paper, we develop a\\ncomputational technique of distributed processing elements, which are called\\nparticles. We observe the collective dynamics of particles in a complex network\\nfor transductive inference on semi-supervised learning problems. Three actions\\ngovern the particles\\' dynamics: walking, absorption, and generation. Labeled\\nvertices generate new particles that compete against rival particles for edge\\ndomination. Active particles randomly walk in the network until they are\\nabsorbed by either a rival vertex or an edge currently dominated by rival\\nparticles. The result from the model simulation consists of sets of edges\\nsorted by the label dominance. Each set tends to form a connected subnetwork to\\nrepresent a data class. Although the intrinsic dynamics of the model is a\\nstochastic one, we prove there exists a deterministic version with largely\\nreduced computational complexity; specifically, with subquadratic growth.\\nFurthermore, the edge domination process corresponds to an unfolding map.\\nIntuitively, edges \"stretch\" and \"shrink\" according to edge dynamics.\\nConsequently, such effect summarizes the relevant relationships between\\nvertices and uncovered data classes. The proposed model captures important\\ndetails of connectivity patterns over the edge dynamics evolution, which\\ncontrasts with previous approaches focused on vertex dynamics. Computer\\nsimulations reveal that our model can identify nonlinear features in both real\\nand artificial data, including boundaries between distinct classes and the\\noverlapping structure of data.\\n',\n",
       "  'title': u'Network Unfolding Map by Edge Dynamics Modeling'},\n",
       " u'1601.03483': {'arxivid': u'1601.03483',\n",
       "  'authorsaffil': [[u'Renato Cordeiro de Amorim', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Journal of Classification (to appear)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03483v1',\n",
       "  'published': u'2015-09-22T08:46:39Z',\n",
       "  'summary': u'  In a real-world data set there is always the possibility, rather high in our\\nopinion, that different features may have different degrees of relevance. Most\\nmachine learning algorithms deal with this fact by either selecting or\\ndeselecting features in the data preprocessing phase. However, we maintain that\\neven among relevant features there may be different degrees of relevance, and\\nthis should be taken into account during the clustering process. With over 50\\nyears of history, K-Means is arguably the most popular partitional clustering\\nalgorithm there is. The first K-Means based clustering algorithm to compute\\nfeature weights was designed just over 30 years ago. Various such algorithms\\nhave been designed since but there has not been, to our knowledge, a survey\\nintegrating empirical evidence of cluster recovery ability, common flaws, and\\npossible directions for future research. This paper elaborates on the concept\\nof feature weighting and addresses these issues by critically analysing some of\\nthe most popular, or innovative, feature weighting mechanisms based in K-Means.\\n',\n",
       "  'title': u'A survey on feature weighting based K-Means algorithms'},\n",
       " u'1602.05436': {'arxivid': u'1602.05436',\n",
       "  'authorsaffil': [[u'Mike Gartrell', None],\n",
       "   [u'Ulrich Paquet', None],\n",
       "   [u'Noam Koenigstein', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'10 pages, 4 figures. Submitted to KDD 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05436v1',\n",
       "  'published': u'2016-02-17T14:40:52Z',\n",
       "  'summary': u'  Determinantal point processes (DPPs) have garnered attention as an elegant\\nprobabilistic model of set diversity. They are useful for a number of subset\\nselection tasks, including product recommendation. DPPs are parametrized by a\\npositive semi-definite kernel matrix. In this work we present a new method for\\nlearning the DPP kernel from observed data using a low-rank factorization of\\nthis kernel. We show that this low-rank factorization enables a learning\\nalgorithm that is nearly an order of magnitude faster than previous approaches,\\nwhile also providing for a method for computing product recommendation\\npredictions that is far faster (up to 20x faster or more for large item\\ncatalogs) than previous techniques that involve a full-rank DPP kernel.\\nFurthermore, we show that our method provides equivalent or sometimes better\\npredictive performance than prior full-rank DPP approaches, and better\\nperformance than several other competing recommendation methods in many cases.\\nWe conduct an extensive experimental evaluation using several real-world\\ndatasets in the domain of product recommendation to demonstrate the utility of\\nour method, along with its limitations.\\n',\n",
       "  'title': u'Low-Rank Factorization of Determinantal Point Processes for\\n  Recommendation'},\n",
       " u'1602.05439': {'arxivid': u'1602.05439',\n",
       "  'authorsaffil': [[u'Arnaud Browet', None],\n",
       "   [u'Christophe De Vleeschouwer', None],\n",
       "   [u'Laurent Jacques', None],\n",
       "   [u'Navrita Mathiah', None],\n",
       "   [u'Bechara Saykali', None],\n",
       "   [u'Isabelle Migeotte', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'submitted to ICIP',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05439v1',\n",
       "  'published': u'2016-02-17T14:47:32Z',\n",
       "  'summary': u'  The progress in imaging techniques have allowed the study of various aspect\\nof cellular mechanisms. To isolate individual cells in live imaging data, we\\nintroduce an elegant image segmentation framework that effectively extracts\\ncell boundaries, even in the presence of poor edge details. Our approach works\\nin two stages. First, we estimate pixel interior/border/exterior class\\nprobabilities using random ferns. Then, we use an energy minimization framework\\nto compute boundaries whose localization is compliant with the pixel class\\nprobabilities. We validate our approach on a manually annotated dataset.\\n',\n",
       "  'title': u'Cell segmentation with random ferns and graph-cuts'},\n",
       " u'1605.04603': {'arxivid': u'1605.04603',\n",
       "  'authorsaffil': [[u'Roman Novak', None], [u'Yaroslav Nikulin', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'A short class project report (15 pages)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1605.04603v1',\n",
       "  'published': u'2016-05-15T20:11:27Z',\n",
       "  'summary': u'  In this work we investigate different avenues of improving the Neural\\nAlgorithm of Artistic Style (by Leon A. Gatys, Alexander S. Ecker and Matthias\\nBethge, arXiv:1508.06576).\\n  While showing great results when transferring homogeneous and repetitive\\npatterns, the original style representation often fails to capture more complex\\nproperties, like having separate styles of foreground and background. This\\nleads to visual artifacts and undesirable textures appearing in unexpected\\nregions when performing style transfer.\\n  We tackle this issue with a variety of approaches, mostly by modifying the\\nstyle representation in order for it to capture more information and impose a\\ntighter constraint on the style transfer result.\\n  In our experiments, we subjectively evaluate our best method as producing\\nfrom barely noticeable to significant improvements in the quality of style\\ntransfer.\\n',\n",
       "  'title': u'Improving the Neural Algorithm of Artistic Style'},\n",
       " u'1602.04124': {'arxivid': u'1602.04124',\n",
       "  'authorsaffil': [[u'Srinath Sridhar', None],\n",
       "   [u'Franziska Mueller', None],\n",
       "   [u'Antti Oulasvirta', None],\n",
       "   [u'Christian Theobalt', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages, Accepted version of paper published at CVPR 2015',\n",
       "  'doi': u'10.1109/CVPR.2015.7298941',\n",
       "  'journalref': u'Computer Vision and Pattern Recognition (CVPR), 2015 IEEE\\n  Conference on , vol., no., pp.3213-3221, 7-12 June 2015',\n",
       "  'link': u'http://arxiv.org/abs/1602.04124v1',\n",
       "  'published': u'2016-02-12T17:05:04Z',\n",
       "  'summary': u'  Markerless tracking of hands and fingers is a promising enabler for\\nhuman-computer interaction. However, adoption has been limited because of\\ntracking inaccuracies, incomplete coverage of motions, low framerate, complex\\ncamera setups, and high computational requirements. In this paper, we present a\\nfast method for accurately tracking rapid and complex articulations of the hand\\nusing a single depth camera. Our algorithm uses a novel detection-guided\\noptimization strategy that increases the robustness and speed of pose\\nestimation. In the detection step, a randomized decision forest classifies\\npixels into parts of the hand. In the optimization step, a novel objective\\nfunction combines the detected part labels and a Gaussian mixture\\nrepresentation of the depth to estimate a pose that best fits the depth. Our\\napproach needs comparably less computational resources which makes it extremely\\nfast (50 fps without GPU support). The approach also supports varying static,\\nor moving, camera-to-scene arrangements. We show the benefits of our method by\\nevaluating on public datasets and comparing against previous work.\\n',\n",
       "  'title': u'Fast and Robust Hand Tracking Using Detection-Guided Optimization'},\n",
       " u'1509.03295': {'arxivid': u'1509.03295',\n",
       "  'authorsaffil': [[u'Ramon Ferrer-i-Cancho', None],\n",
       "   [u'Carlos G\\xf3mez-Rodr\\xedguez', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.SI', u'physics.soc-ph'],\n",
       "  'comment': u'Minor corrections',\n",
       "  'doi': None,\n",
       "  'journalref': u'Liberating language research from dogmas of the 20th century.\\n  Glottometrics 33, 33-34 (2016)',\n",
       "  'link': u'http://arxiv.org/abs/1509.03295v3',\n",
       "  'published': u'2015-09-09T11:27:03Z',\n",
       "  'summary': u'  A commentary on the article \"Large-scale evidence of dependency length\\nminimization in 37 languages\" by Futrell, Mahowald & Gibson (PNAS 2015 112 (33)\\n10336-10341).\\n',\n",
       "  'title': u'Liberating language research from dogmas of the 20th century'},\n",
       " u'1602.05659': {'arxivid': u'1602.05659',\n",
       "  'authorsaffil': [[u'Fuqiang Liu', None],\n",
       "   [u'Fukun Bi', None],\n",
       "   [u'Yiding Yang', None],\n",
       "   [u'Liang Chen', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'9 pages, 9 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05659v2',\n",
       "  'published': u'2016-02-18T02:24:54Z',\n",
       "  'summary': u'  This paper proposes a universal method, Boost Picking, to train supervised\\nclassification models mainly by un-labeled data. Boost Picking only adopts two\\nweak classifiers to estimate and correct the error. It is theoretically proved\\nthat Boost Picking could train a supervised model mainly by un-labeled data as\\neffectively as the same model trained by 100% labeled data, only if recalls of\\nthe two weak classifiers are all greater than zero and the sum of precisions is\\ngreater than one. Based on Boost Picking, we present \"Test along with Training\\n(TawT)\" to improve the generalization of supervised models. Both Boost Picking\\nand TawT are successfully tested in varied little data sets.\\n',\n",
       "  'title': u'Boost Picking: A Universal Method on Converting Supervised\\n  Classification to Semi-supervised Classification'},\n",
       " u'1511.04891': {'arxivid': u'1511.04891',\n",
       "  'authorsaffil': [[u'Mohamed Elhoseiny', None],\n",
       "   [u'Scott Cohen', None],\n",
       "   [u'Walter Chang', None],\n",
       "   [u'Brian Price', None],\n",
       "   [u'Ahmed Elgammal', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'Jan 7 Update',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04891v4',\n",
       "  'published': u'2015-11-16T09:56:04Z',\n",
       "  'summary': u'  We study scalable and uniform understanding of facts in images. Existing\\nvisual recognition systems are typically modeled differently for each fact type\\nsuch as objects, actions, and interactions. We propose a setting where all\\nthese facts can be modeled simultaneously with a capacity to understand\\nunbounded number of facts in a structured way. The training data comes as\\nstructured facts in images, including (1) objects (e.g., $<$boy$>$), (2)\\nattributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and\\n(4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic\\nlanguage view (e.g., $<$ boy, playing$>$) and a visual view (an image with this\\nfact). We show that learning visual facts in a structured way enables not only\\na uniform but also generalizable visual understanding. We propose and\\ninvestigate recent and strong approaches from the multiview learning literature\\nand also introduce two learning representation models as potential baselines.\\nWe applied the investigated methods on several datasets that we augmented with\\nstructured facts and a large scale dataset of more than 202,000 facts and\\n814,000 images. Our experiments show the advantage of relating facts by the\\nstructure by the proposed models compared to the designed baselines on\\nbidirectional fact retrieval.\\n',\n",
       "  'title': u'Sherlock: Scalable Fact Learning in Images'},\n",
       " u'1601.03805': {'arxivid': u'1601.03805',\n",
       "  'authorsaffil': [[u'Junbin Gao', None],\n",
       "   [u'Yi Guo', None],\n",
       "   [u'Zhiyong Wang', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'20 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03805v1',\n",
       "  'published': u'2016-01-15T03:33:35Z',\n",
       "  'summary': u'  Traditional neural networks assume vectorial inputs as the network is\\narranged as layers of single line of computing units called neurons. This\\nspecial structure requires the non-vectorial inputs such as matrices to be\\nconverted into vectors. This process can be problematic. Firstly, the spatial\\ninformation among elements of the data may be lost during vectorisation.\\nSecondly, the solution space becomes very large which demands very special\\ntreatments to the network parameters and high computational cost. To address\\nthese issues, we propose matrix neural networks (MatNet), which takes matrices\\ndirectly as inputs. Each neuron senses summarised information through bilinear\\nmapping from lower layer units in exactly the same way as the classic feed\\nforward neural networks. Under this structure, back prorogation and gradient\\ndescent combination can be utilised to obtain network parameters efficiently.\\nFurthermore, it can be conveniently extended for multimodal inputs. We apply\\nMatNet to MNIST handwritten digits classification and image super resolution\\ntasks to show its effectiveness. Without too much tweaking MatNet achieves\\ncomparable performance as the state-of-the-art methods in both tasks with\\nconsiderably reduced complexity.\\n',\n",
       "  'title': u'Matrix Neural Networks'},\n",
       " u'1602.02358': {'arxivid': u'1602.02358',\n",
       "  'authorsaffil': [[u'Haohan Zhu', None],\n",
       "   [u'Xianrui Meng', None],\n",
       "   [u'George Kollios', None]],\n",
       "  'categoryterms': [u'cs.DB', u'cs.LG', u'cs.SI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02358v3',\n",
       "  'published': u'2016-02-07T11:00:03Z',\n",
       "  'summary': u'  Node similarity is a fundamental problem in graph analytics. However, node\\nsimilarity between nodes in different graphs (inter-graph nodes) has not\\nreceived a lot of attention yet. The inter-graph node similarity is important\\nin learning a new graph based on the knowledge of an existing graph (transfer\\nlearning on graphs) and has applications in biological, communication, and\\nsocial networks. In this paper, we propose a novel distance function for\\nmeasuring inter-graph node similarity with edit distance, called NED. In NED,\\ntwo nodes are compared according to their local neighborhood structures which\\nare represented as unordered k-adjacent trees, without relying on labels or\\nother assumptions. Since the computation problem of tree edit distance on\\nunordered trees is NP-Complete, we propose a modified tree edit distance,\\ncalled TED*, for comparing neighborhood trees. TED* is a metric distance, as\\nthe original tree edit distance, but more importantly, TED* is polynomially\\ncomputable. As a metric distance, NED admits efficient indexing, provides\\ninterpretable results, and shows to perform better than existing approaches on\\na number of data analysis tasks, including graph de-anonymization. Finally, the\\nefficiency and effectiveness of NED are empirically demonstrated using\\nreal-world graphs.\\n',\n",
       "  'title': u'NED: An Inter-Graph Node Metric Based On Edit Distance'},\n",
       " u'1601.03809': {'arxivid': u'1601.03809',\n",
       "  'authorsaffil': [[u'Mostafa Sayyed', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.CY'],\n",
       "  'comment': u'108 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03809v1',\n",
       "  'published': u'2015-11-03T07:21:44Z',\n",
       "  'summary': u'  In this research, computerized maintenance management will be investigated.\\nThe rise of maintenance cost forced the research community to look for more\\neffective ways to schedule maintenance operations. Using computerized models to\\ncome up with optimal maintenance policy has led to better equipment utilization\\nand lower costs. This research adopts Condition-Based Maintenance model where\\nthe maintenance decision is generated based on equipment conditions. Artificial\\nNeural Network technique is proposed to capture and analyze equipment condition\\nsignals which lead to higher level of knowledge gathering. This knowledge is\\nused to accurately estimate equipment failure time. Based on these estimations,\\nan optimal maintenance management policy can be achieved.\\n',\n",
       "  'title': u'Artificial neural network approach for condition-based maintenance'},\n",
       " u'1604.01272': {'arxivid': u'1604.01272',\n",
       "  'authorsaffil': [[u'Despoina Christou', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.IR', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01272v1',\n",
       "  'published': u'2016-04-05T14:32:48Z',\n",
       "  'summary': u'  Feature extraction has gained increasing attention in the field of machine\\nlearning, as in order to detect patterns, extract information, or predict\\nfuture observations from big data, the urge of informative features is crucial.\\nThe process of extracting features is highly linked to dimensionality reduction\\nas it implies the transformation of the data from a sparse high-dimensional\\nspace, to higher level meaningful abstractions. This dissertation employs\\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\\nAllocation to capture higher level features of paragraph vectors. Although\\nNeural Networks for distributed paragraph representations are considered the\\nstate of the art for extracting paragraph vectors, we show that a quick topic\\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\\nplot the projected representations using T-Distributed Stochastic Neighbor\\nEmbedding to depict the context similarities. These similarities, expressed as\\nmovie distances, can be used for movies recommendation. The recommended movies\\nof this approach are compared with the recommended movies from IMDB, which use\\na collaborative filtering recommendation approach, to show that our two models\\ncould constitute either an alternative or a supplementary recommendation\\napproach.\\n',\n",
       "  'title': u'Feature extraction using Latent Dirichlet Allocation and Neural\\n  Networks: A case study on movie synopses'},\n",
       " u'1511.00758': {'arxivid': u'1511.00758',\n",
       "  'authorsaffil': [[u'Sudeep Pillai', None],\n",
       "   [u'Srikumar Ramalingam', None],\n",
       "   [u'John J. Leonard', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.CV'],\n",
       "  'comment': u'Accepted to International Conference on Robotics and Automation\\n  (ICRA) 2016; 8 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.00758v2',\n",
       "  'published': u'2015-11-03T02:20:56Z',\n",
       "  'summary': u'  Traditional stereo algorithms have focused their efforts on reconstruction\\nquality and have largely avoided prioritizing for run time performance. Robots,\\non the other hand, require quick maneuverability and effective computation to\\nobserve its immediate environment and perform tasks within it. In this work, we\\npropose a high-performance and tunable stereo disparity estimation method, with\\na peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that can\\npotentially enable robots to quickly reconstruct their immediate surroundings\\nand maneuver at high-speeds. Our key contribution is a disparity estimation\\nalgorithm that iteratively approximates the scene depth via a piece-wise planar\\nmesh from stereo imagery, with a fast depth validation step for semi-dense\\nreconstruction. The mesh is initially seeded with sparsely matched keypoints,\\nand is recursively tessellated and refined as needed (via a resampling stage),\\nto provide the desired stereo disparity accuracy. The inherent simplicity and\\nspeed of our approach, with the ability to tune it to a desired reconstruction\\nquality and runtime performance makes it a compelling solution for applications\\nin high-speed vehicles.\\n',\n",
       "  'title': u'High-Performance and Tunable Stereo Reconstruction'},\n",
       " u'1602.02350': {'arxivid': u'1602.02350',\n",
       "  'authorsaffil': [[u'Alon Gonen', None],\n",
       "   [u'Francesco Orabona', None],\n",
       "   [u'Shai Shalev-Shwartz', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02350v2',\n",
       "  'published': u'2016-02-07T08:37:18Z',\n",
       "  'summary': u'  We develop a novel preconditioning method for ridge regression, based on\\nrecent linear sketching methods. By equipping Stochastic Variance Reduced\\nGradient (SVRG) with this preconditioning process, we obtain a significant\\nspeed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.\\n',\n",
       "  'title': u'Solving Ridge Regression using Sketched Preconditioned SVRG'},\n",
       " u'1601.00825': {'arxivid': u'1601.00825',\n",
       "  'authorsaffil': [[u'Simone Palazzo', None],\n",
       "   [u'Concetto Spampinato', None],\n",
       "   [u'Daniela Giordano', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Submitted to PAMI',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00825v1',\n",
       "  'published': u'2016-01-05T13:48:05Z',\n",
       "  'summary': u'  Video object segmentation can be considered as one of the most challenging\\ncomputer vision problems. Indeed, so far, no existing solution is able to\\neffectively deal with the peculiarities of real-world videos, especially in\\ncases of articulated motion and object occlusions; limitations that appear more\\nevident when we compare their performance with the human one. However, manually\\nsegmenting objects in videos is largely impractical as it requires a lot of\\nhuman time and concentration. To address this problem, in this paper we propose\\nan interactive video object segmentation method, which exploits, on one hand,\\nthe capability of humans to identify correctly objects in visual scenes, and on\\nthe other hand, the collective human brainpower to solve challenging tasks. In\\nparticular, our method relies on a web game to collect human inputs on object\\nlocations, followed by an accurate segmentation phase achieved by optimizing an\\nenergy function encoding spatial and temporal constraints between object\\nregions as well as human-provided input. Performance analysis carried out on\\nchallenging video datasets with some users playing the game demonstrated that\\nour method shows a better trade-off between annotation times and segmentation\\naccuracy than interactive video annotation and automated video object\\nsegmentation approaches.\\n',\n",
       "  'title': u'Gamifying Video Object Segmentation'},\n",
       " u'1507.04808': {'arxivid': u'1507.04808',\n",
       "  'authorsaffil': [[u'Iulian V. Serban', None],\n",
       "   [u'Alessandro Sordoni', None],\n",
       "   [u'Yoshua Bengio', None],\n",
       "   [u'Aaron Courville', None],\n",
       "   [u'Joelle Pineau', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.LG', u'cs.NE', u'I.5.1; I.2.7'],\n",
       "  'comment': u'8 pages with references; Published in AAAI 2016 (Special Track on\\n  Cognitive Systems)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.04808v3',\n",
       "  'published': u'2015-07-17T00:21:39Z',\n",
       "  'summary': u'  We investigate the task of building open domain, conversational dialogue\\nsystems based on large dialogue corpora using generative models. Generative\\nmodels produce system responses that are autonomously generated word-by-word,\\nopening up the possibility for realistic, flexible interactions. In support of\\nthis goal, we extend the recently proposed hierarchical recurrent\\nencoder-decoder neural network to the dialogue domain, and demonstrate that\\nthis model is competitive with state-of-the-art neural language models and\\nback-off n-gram models. We investigate the limitations of this and similar\\napproaches, and show how its performance can be improved by bootstrapping the\\nlearning from a larger question-answer pair corpus and from pretrained word\\nembeddings.\\n',\n",
       "  'title': u'Building End-To-End Dialogue Systems Using Generative Hierarchical\\n  Neural Network Models'},\n",
       " u'1502.00598': {'arxivid': u'1502.00598',\n",
       "  'authorsaffil': [[u'Maurits Kaptein', None], [u'Davide Iannuzzi', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'20 Pages, 7 Figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.00598v3',\n",
       "  'published': u'2015-02-02T20:00:13Z',\n",
       "  'summary': u'  We often encounter situations in which an experimenter wants to find, by\\nsequential experimentation, $x_{max} = \\\\arg\\\\max_{x} f(x)$, where $f(x)$ is a\\n(possibly unknown) function of a well controllable variable $x$. Taking\\ninspiration from physics and engineering, we have designed a new method to\\naddress this problem. In this paper, we first introduce the method in\\ncontinuous time, and then present two algorithms for use in sequential\\nexperiments. Through a series of simulation studies, we show that the method is\\neffective for finding maxima of unknown functions by experimentation, even when\\nthe maximum of the functions drifts or when the signal to noise ratio is low.\\n',\n",
       "  'title': u'Lock in Feedback in Sequential Experiments'},\n",
       " u'1509.06569': {'arxivid': u'1509.06569',\n",
       "  'authorsaffil': [[u'Alexander Novikov', None],\n",
       "   [u'Dmitry Podoprikhin', None],\n",
       "   [u'Anton Osokin', None],\n",
       "   [u'Dmitry Vetrov', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.06569v2',\n",
       "  'published': u'2015-09-22T12:31:03Z',\n",
       "  'summary': u'  Deep neural networks currently demonstrate state-of-the-art performance in\\nseveral domains. At the same time, models of this class are very demanding in\\nterms of computational resources. In particular, a large amount of memory is\\nrequired by commonly used fully-connected layers, making it hard to use the\\nmodels on low-end devices and stopping the further increase of the model size.\\nIn this paper we convert the dense weight matrices of the fully-connected\\nlayers to the Tensor Train format such that the number of parameters is reduced\\nby a huge factor and at the same time the expressive power of the layer is\\npreserved. In particular, for the Very Deep VGG networks we report the\\ncompression factor of the dense weight matrix of a fully-connected layer up to\\n200000 times leading to the compression factor of the whole network up to 7\\ntimes.\\n',\n",
       "  'title': u'Tensorizing Neural Networks'},\n",
       " u'1603.05629': {'arxivid': u'1603.05629',\n",
       "  'authorsaffil': [[u'Hanjun Dai', None],\n",
       "   [u'Bo Dai', None],\n",
       "   [u'Le Song', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'ICML 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05629v3',\n",
       "  'published': u'2016-03-17T19:29:46Z',\n",
       "  'summary': u'  Kernel classifiers and regressors designed for structured data, such as\\nsequences, trees and graphs, have significantly advanced a number of\\ninterdisciplinary areas such as computational biology and drug design.\\nTypically, kernels are designed beforehand for a data type which either exploit\\nstatistics of the structures or make use of probabilistic generative models,\\nand then a discriminative classifier is learned based on the kernels via convex\\noptimization. However, such an elegant two-stage approach also limited kernel\\nmethods from scaling up to millions of data points, and exploiting\\ndiscriminative information to learn feature representations.\\n  We propose, structure2vec, an effective and scalable approach for structured\\ndata representation based on the idea of embedding latent variable models into\\nfeature spaces, and learning such feature spaces using discriminative\\ninformation. Interestingly, structure2vec extracts features by performing a\\nsequence of function mappings in a way similar to graphical model inference\\nprocedures, such as mean field and belief propagation. In applications\\ninvolving millions of data points, we showed that structure2vec runs 2 times\\nfaster, produces models which are $10,000$ times smaller, while at the same\\ntime achieving the state-of-the-art predictive performance.\\n',\n",
       "  'title': u'Discriminative Embeddings of Latent Variable Models for Structured Data'},\n",
       " u'1510.02777': {'arxivid': u'1510.02777',\n",
       "  'authorsaffil': [[u'Yoshua Bengio', None], [u'Asja Fischer', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1509.05936',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.02777v2',\n",
       "  'published': u'2015-10-09T19:21:32Z',\n",
       "  'summary': u'  We show that Langevin MCMC inference in an energy-based model with latent\\nvariables has the property that the early steps of inference, starting from a\\nstationary point, correspond to propagating error gradients into internal\\nlayers, similarly to back-propagation. The error that is back-propagated is\\nwith respect to visible units that have received an outside driving force\\npushing them away from the stationary point. Back-propagated error gradients\\ncorrespond to temporal derivatives of the activation of hidden units. This\\nobservation could be an element of a theory for explaining how brains perform\\ncredit assignment in deep hierarchies as efficiently as back-propagation does.\\nIn this theory, the continuous-valued latent variables correspond to averaged\\nvoltage potential (across time, spikes, and possibly neurons in the same\\nminicolumn), and neural computation corresponds to approximate inference and\\nerror back-propagation at the same time.\\n',\n",
       "  'title': u'Early Inference in Energy-Based Models Approximates Back-Propagation'},\n",
       " u'1603.05310': {'arxivid': u'1603.05310',\n",
       "  'authorsaffil': [[u'Vinay Venkataraman', None],\n",
       "   [u'Karthikeyan Natesan Ramamurthy', None],\n",
       "   [u'Pavan Turaga', None]],\n",
       "  'categoryterms': [u'cs.CG', u'cs.CV'],\n",
       "  'comment': u'5 pages, Under review in International Conference on Image Processing',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05310v1',\n",
       "  'published': u'2016-03-16T23:15:50Z',\n",
       "  'summary': u'  In this paper, we propose a novel framework for dynamical analysis of human\\nactions from 3D motion capture data using topological data analysis. We model\\nhuman actions using the topological features of the attractor of the dynamical\\nsystem. We reconstruct the phase-space of time series corresponding to actions\\nusing time-delay embedding, and compute the persistent homology of the\\nphase-space reconstruction. In order to better represent the topological\\nproperties of the phase-space, we incorporate the temporal adjacency\\ninformation when computing the homology groups. The persistence of these\\nhomology groups encoded using persistence diagrams are used as features for the\\nactions. Our experiments with action recognition using these features\\ndemonstrate that the proposed approach outperforms other baseline methods.\\n',\n",
       "  'title': u'Persistent Homology of Attractors For Action Recognition'},\n",
       " u'1602.02196': {'arxivid': u'1602.02196',\n",
       "  'authorsaffil': [[u'Alexander Rakhlin', None], [u'Karthik Sridharan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02196v1',\n",
       "  'published': u'2016-02-06T00:34:59Z',\n",
       "  'summary': u'  We present efficient algorithms for the problem of contextual bandits with\\ni.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of\\npolicies. Our algorithm BISTRO requires d calls to the empirical risk\\nminimization (ERM) oracle per round, where d is the number of actions. The\\nmethod uses unlabeled data to make the problem computationally simple. When the\\nERM problem itself is computationally hard, we extend the approach by employing\\nmultiplicative approximation algorithms for the ERM. The integrality gap of the\\nrelaxation only enters in the regret bound rather than the benchmark. Finally,\\nwe show that the adversarial version of the contextual bandit problem is\\nlearnable (and efficient) whenever the full-information supervised online\\nlearning problem has a non-trivial regret guarantee (and efficient).\\n',\n",
       "  'title': u'BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits'},\n",
       " u'1503.03167': {'arxivid': u'1503.03167',\n",
       "  'authorsaffil': [[u'Tejas D. Kulkarni', None],\n",
       "   [u'Will Whitney', None],\n",
       "   [u'Pushmeet Kohli', None],\n",
       "   [u'Joshua B. Tenenbaum', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.GR', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'First two authors contributed equally',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.03167v4',\n",
       "  'published': u'2015-03-11T04:08:42Z',\n",
       "  'summary': u\"  This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a\\nmodel that learns an interpretable representation of images. This\\nrepresentation is disentangled with respect to transformations such as\\nout-of-plane rotations and lighting variations. The DC-IGN model is composed of\\nmultiple layers of convolution and de-convolution operators and is trained\\nusing the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a\\ntraining procedure to encourage neurons in the graphics code layer to represent\\na specific transformation (e.g. pose or light). Given a single input image, our\\nmodel can generate new images of the same object with variations in pose and\\nlighting. We present qualitative and quantitative results of the model's\\nefficacy at learning a 3D rendering engine.\\n\",\n",
       "  'title': u'Deep Convolutional Inverse Graphics Network'},\n",
       " u'1602.02191': {'arxivid': u'1602.02191',\n",
       "  'authorsaffil': [[u'Mohammad Gheshlaghi Azar', None],\n",
       "   [u'Eva Dyer', None],\n",
       "   [u'Konrad Kording', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02191v3',\n",
       "  'published': u'2016-02-05T23:23:32Z',\n",
       "  'summary': u'  Finding efficient and provable methods to solve non-convex optimization\\nproblems is an outstanding challenge in machine learning and optimization\\ntheory. A popular approach used to tackle non-convex problems is to use convex\\nrelaxation techniques to find a convex surrogate for the problem.\\nUnfortunately, convex relaxations typically must be found on a\\nproblem-by-problem basis. Thus, providing a general-purpose strategy to\\nestimate a convex relaxation would have a wide reaching impact. Here, we\\nintroduce Convex Relaxation Regression (CoRR), an approach for learning convex\\nrelaxations for a class of smooth functions. The main idea behind our approach\\nis to estimate the convex envelope of a function $f$ by evaluating $f$ at a set\\nof $T$ random points and then fitting a convex function to these function\\nevaluations. We prove that with probability greater than $1-\\\\delta$, the\\nsolution of our algorithm converges to the global optimizer of $f$ with error\\n$\\\\mathcal{O} \\\\Big( \\\\big(\\\\frac{\\\\log(1/\\\\delta) }{T} \\\\big)^{\\\\alpha} \\\\Big)$ for\\nsome $\\\\alpha> 0$. Our approach enables the use of convex optimization tools to\\nsolve a class of non-convex optimization problems.\\n',\n",
       "  'title': u'Convex Relaxation Regression: Black-Box Optimization of Smooth Functions\\n  by Learning Their Convex Envelopes'},\n",
       " u'1411.6387': {'arxivid': u'1411.6387',\n",
       "  'authorsaffil': [[u'Fayao Liu', None],\n",
       "   [u'Chunhua Shen', None],\n",
       "   [u'Guosheng Lin', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'fixed some typos. in CVPR15 proceedings',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1411.6387v2',\n",
       "  'published': u'2014-11-24T09:13:00Z',\n",
       "  'summary': u'  We consider the problem of depth estimation from a single monocular image in\\nthis work. It is a challenging task as no reliable depth cues are available,\\ne.g., stereo correspondences, motions, etc. Previous efforts have been focusing\\non exploiting geometric priors or additional sources of information, with all\\nusing hand-crafted features. Recently, there is mounting evidence that features\\nfrom deep convolutional neural networks (CNN) are setting new records for\\nvarious vision applications. On the other hand, considering the continuous\\ncharacteristic of the depth values, depth estimations can be naturally\\nformulated into a continuous conditional random field (CRF) learning problem.\\nTherefore, we in this paper present a deep convolutional neural field model for\\nestimating depths from a single image, aiming to jointly explore the capacity\\nof deep CNN and continuous CRF. Specifically, we propose a deep structured\\nlearning scheme which learns the unary and pairwise potentials of continuous\\nCRF in a unified deep CNN framework.\\n  The proposed method can be used for depth estimations of general scenes with\\nno geometric priors nor any extra information injected. In our case, the\\nintegral of the partition function can be analytically calculated, thus we can\\nexactly solve the log-likelihood optimization. Moreover, solving the MAP\\nproblem for predicting depths of a new image is highly efficient as closed-form\\nsolutions exist. We experimentally demonstrate that the proposed method\\noutperforms state-of-the-art depth estimation methods on both indoor and\\noutdoor scene datasets.\\n',\n",
       "  'title': u'Deep Convolutional Neural Fields for Depth Estimation from a Single\\n  Image'},\n",
       " u'1603.04767': {'arxivid': u'1603.04767',\n",
       "  'authorsaffil': [[u'Angel X. Chang', None],\n",
       "   [u'Valentin I. Spitkovsky', None],\n",
       "   [u'Christopher D. Manning', None],\n",
       "   [u'Eneko Agirre', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04767v1',\n",
       "  'published': u'2016-03-15T17:16:02Z',\n",
       "  'summary': u'  Named Entity Disambiguation (NED) is the task of linking a named-entity\\nmention to an instance in a knowledge-base, typically Wikipedia. This task is\\nclosely related to word-sense disambiguation (WSD), where the supervised\\nword-expert approach has prevailed. In this work we present the results of the\\nword-expert approach to NED, where one classifier is built for each target\\nentity mention string. The resources necessary to build the system, a\\ndictionary and a set of training instances, have been automatically derived\\nfrom Wikipedia. We provide empirical evidence of the value of this approach, as\\nwell as a study of the differences between WSD and NED, including ambiguity and\\nsynonymy statistics.\\n',\n",
       "  'title': u'Evaluating the word-expert approach for Named-Entity Disambiguation'},\n",
       " u'1603.06531': {'arxivid': u'1603.06531',\n",
       "  'authorsaffil': [[u'Otkrist Gupta', None],\n",
       "   [u'Dan Raviv', None],\n",
       "   [u'Ramesh Raskar', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06531v1',\n",
       "  'published': u'2016-03-21T18:33:29Z',\n",
       "  'summary': u'  In this paper we present architectures based on deep neural nets for gesture\\nrecognition in videos, which are invariant to local scaling. We amalgamate\\nautoencoder and predictor architectures using an adaptive weighting scheme\\ncoping with a reduced size labeled dataset, while enriching our models from\\nenormous unlabeled sets. We further improve robustness to lighting conditions\\nby introducing a new adaptive filer based on temporal local scale\\nnormalization. We provide superior results over known methods, including recent\\nreported approaches based on neural nets.\\n',\n",
       "  'title': u'Deep video gesture recognition using illumination invariants'},\n",
       " u'1603.07849': {'arxivid': u'1603.07849',\n",
       "  'authorsaffil': [[u'Eric Makita', None], [u'Artem Lenskiy', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.LG'],\n",
       "  'comment': u'5 pages, 4 figures, 8th International Conference on Machine Learning\\n  and Computing, Hong Kong',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07849v1',\n",
       "  'published': u'2016-03-25T08:49:39Z',\n",
       "  'summary': u'  This paper proposes a movie genre-prediction based on multinomial probability\\nmodel. To the best of our knowledge, this problem has not been addressed yet in\\nthe field of recommender system. The prediction of a movie genre has many\\npractical applications including complementing the items categories given by\\nexperts and providing a surprise effect in the recommendations given to a user.\\nWe employ mulitnomial event model to estimate a likelihood of a movie given\\ngenre and the Bayes rule to evaluate the posterior probability of a genre given\\na movie. Experiments with the MovieLens dataset validate our approach. We\\nachieved 70% prediction rate using only 15% of the whole set for training.\\n',\n",
       "  'title': u'A multinomial probabilistic model for movie genre predictions'},\n",
       " u'1601.02603': {'arxivid': u'1601.02603',\n",
       "  'authorsaffil': [[u'Marian-Andrei Rizoiu', None],\n",
       "   [u'Julien Velcin', None],\n",
       "   [u'St\\xe9phane Lallich', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DS'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1142/S0218213014600136',\n",
       "  'journalref': u'Int. J. Artif. Intell. Tools 23, 1460013 (2014) [26 pages]',\n",
       "  'link': u'http://arxiv.org/abs/1601.02603v1',\n",
       "  'published': u'2016-01-11T01:20:26Z',\n",
       "  'summary': u'  In this paper, we propose a new time-aware dissimilarity measure that takes\\ninto account the temporal dimension. Observations that are close in the\\ndescription space, but distant in time are considered as dissimilar. We also\\npropose a method to enforce the segmentation contiguity, by introducing, in the\\nobjective function, a penalty term inspired from the Normal Distribution\\nFunction. We combine the two propositions into a novel time-driven constrained\\nclustering algorithm, called TDCK-Means, which creates a partition of coherent\\nclusters, both in the multidimensional space and in the temporal space. This\\nalgorithm uses soft semi-supervised constraints, to encourage adjacent\\nobservations belonging to the same entity to be assigned to the same cluster.\\nWe apply our algorithm to a Political Studies dataset in order to detect\\ntypical evolution phases. We adapt the Shannon entropy in order to measure the\\nentity contiguity, and we show that our proposition consistently improves\\ntemporal cohesion of clusters, without any significant loss in the\\nmultidimensional variance.\\n',\n",
       "  'title': u'How to Use Temporal-Driven Constrained Clustering to Detect Typical\\n  Evolutions'},\n",
       " u'1603.07846': {'arxivid': u'1603.07846',\n",
       "  'authorsaffil': [[u'Wei Wang', None],\n",
       "   [u'Gang Chen', None],\n",
       "   [u'Haibo Chen', None],\n",
       "   [u'Tien Tuan Anh Dinh', None],\n",
       "   [u'Jinyang Gao', None],\n",
       "   [u'Beng Chin Ooi', None],\n",
       "   [u'Kian-Lee Tan', None],\n",
       "   [u'Sheng Wang', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DC'],\n",
       "  'comment': u'submitted to TOMM (under review)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07846v1',\n",
       "  'published': u'2016-03-25T08:46:02Z',\n",
       "  'summary': u'  Recently, deep learning techniques have enjoyed success in various multimedia\\napplications, such as image classification and multi-modal data analysis. Large\\ndeep learning models are developed for learning rich representations of complex\\ndata. There are two challenges to overcome before deep learning can be widely\\nadopted in multimedia and other applications. One is usability, namely the\\nimplementation of different models and training algorithms must be done by\\nnon-experts without much effort especially when the model is large and complex.\\nThe other is scalability, that is the deep learning system must be able to\\nprovision for a huge demand of computing resources for training large models\\nwith massive datasets. To address these two challenges, in this paper, we\\ndesign a distributed deep learning platform called SINGA which has an intuitive\\nprogramming model based on the common layer abstraction of deep learning\\nmodels. Good scalability is achieved through flexible distributed training\\narchitecture and specific optimization techniques. SINGA runs on GPUs as well\\nas on CPUs, and we show that it outperforms many other state-of-the-art deep\\nlearning systems. Our experience with developing and training deep learning\\nmodels for real-life multimedia applications in SINGA shows that the platform\\nis both usable and scalable.\\n',\n",
       "  'title': u'Deep Learning At Scale and At Ease'},\n",
       " u'1604.01278': {'arxivid': u'1604.01278',\n",
       "  'authorsaffil': [[u'Guntis Barzdins', None], [u'Didzis Gosko', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'NAACL HLT 2016, SemEval-2016 Task 8 submission',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01278v1',\n",
       "  'published': u'2016-04-05T14:46:18Z',\n",
       "  'summary': u'  Two extensions to the AMR smatch scoring script are presented. The first\\nextension com-bines the smatch scoring script with the C6.0 rule-based\\nclassifier to produce a human-readable report on the error patterns frequency\\nobserved in the scored AMR graphs. This first extension results in 4% gain over\\nthe state-of-art CAMR baseline parser by adding to it a manually crafted\\nwrapper fixing the identified CAMR parser errors. The second extension combines\\na per-sentence smatch with an en-semble method for selecting the best AMR graph\\namong the set of AMR graphs for the same sentence. This second modification\\nau-tomatically yields further 0.4% gain when ap-plied to outputs of two\\nnondeterministic AMR parsers: a CAMR+wrapper parser and a novel character-level\\nneural translation AMR parser. For AMR parsing task the character-level neural\\ntranslation attains surprising 7% gain over the carefully optimized word-level\\nneural translation. Overall, we achieve smatch F1=62% on the SemEval-2016\\nofficial scor-ing set and F1=67% on the LDC2015E86 test set.\\n',\n",
       "  'title': u'RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and\\n  Character-Level Neural Translation on AMR Parsing Accuracy'},\n",
       " u'1601.07255': {'arxivid': u'1601.07255',\n",
       "  'authorsaffil': [[u'Lin Wu', None],\n",
       "   [u'Chunhua Shen', None],\n",
       "   [u'Anton van den Hengel', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'7 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07255v1',\n",
       "  'published': u'2016-01-27T03:49:34Z',\n",
       "  'summary': u'  In this paper, we propose a deep end-to-end neu- ral network to\\nsimultaneously learn high-level features and a corresponding similarity metric\\nfor person re-identification. The network takes a pair of raw RGB images as\\ninput, and outputs a similarity value indicating whether the two input images\\ndepict the same person. A layer of computing neighborhood range differences\\nacross two input images is employed to capture local relationship between\\npatches. This operation is to seek a robust feature from input images. By\\nincreasing the depth to 10 weight layers and using very small (3$\\\\times$3)\\nconvolution filters, our architecture achieves a remarkable improvement on the\\nprior-art configurations. Meanwhile, an adaptive Root- Mean-Square (RMSProp)\\ngradient decent algorithm is integrated into our architecture, which is\\nbeneficial to deep nets. Our method consistently outperforms state-of-the-art\\non two large datasets (CUHK03 and Market-1501), and a medium-sized data set\\n(CUHK01).\\n',\n",
       "  'title': u'PersonNet: Person Re-identification with Deep Convolutional Neural\\n  Networks'},\n",
       " u'1602.02518': {'arxivid': u'1602.02518',\n",
       "  'authorsaffil': [[u'Sahely Bhadra', None],\n",
       "   [u'Samuel Kaski', None],\n",
       "   [u'Juho Rousu', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02518v1',\n",
       "  'published': u'2016-02-08T10:29:13Z',\n",
       "  'summary': u'  In this paper, we introduce the first method that (1) can complete kernel\\nmatrices with completely missing rows and columns as opposed to individual\\nmissing kernel values, (2) does not require any of the kernels to be complete a\\npriori, and (3) can tackle non-linear kernels. These aspects are necessary in\\npractical applications such as integrating legacy data sets, learning under\\nsensor failures and learning when measurements are costly for some of the\\nviews. The proposed approach predicts missing rows by modelling both\\nwithin-view and between-view relationships among kernel values. We show, both\\non simulated data and real world data, that the proposed method outperforms\\nexisting techniques in the restricted settings where they are available, and\\nextends applicability to new settings.\\n',\n",
       "  'title': u'Multi-view Kernel Completion'},\n",
       " u'1601.07252': {'arxivid': u'1601.07252',\n",
       "  'authorsaffil': [[u'Anshul Gupta', None],\n",
       "   [u'Ricardo Gutierrez-Osuna', None],\n",
       "   [u'Matthew Christy', None],\n",
       "   [u'Richard Furuta', None],\n",
       "   [u'Laura Mandell', None]],\n",
       "  'categoryterms': [u'cs.CV',\n",
       "   u'cs.AI',\n",
       "   u'cs.DL',\n",
       "   u'stat.AP',\n",
       "   u'stat.ML',\n",
       "   u'I.5; I.2'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07252v1',\n",
       "  'published': u'2016-01-27T03:24:05Z',\n",
       "  'summary': u'  Identifying the type of font (e.g., Roman, Blackletter) used in historical\\ndocuments can help optical character recognition (OCR) systems produce more\\naccurate text transcriptions. Towards this end, we present an active-learning\\nstrategy that can significantly reduce the number of labeled samples needed to\\ntrain a font classifier. Our approach extracts image-based features that\\nexploit geometric differences between fonts at the word level, and combines\\nthem into a bag-of-word representation for each page in a document. We evaluate\\nsix sampling strategies based on uncertainty, dissimilarity and diversity\\ncriteria, and test them on a database containing over 3,000 historical\\ndocuments with Blackletter, Roman and Mixed fonts. Our results show that a\\ncombination of uncertainty and diversity achieves the highest predictive\\naccuracy (89% of test cases correctly classified) while requiring only a small\\nfraction of the data (17%) to be labeled. We discuss the implications of this\\nresult for mass digitization projects of historical documents.\\n',\n",
       "  'title': u'Font Identification in Historical Documents Using Active Learning'},\n",
       " u'1601.04293': {'arxivid': u'1601.04293',\n",
       "  'authorsaffil': [[u'Amir Rosenfeld', None], [u'Shimon Ullman', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'our more recent work on a related topic is described in a separate\\n  paper : http://arxiv.org/abs/1511.03814',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04293v1',\n",
       "  'published': u'2016-01-17T13:24:44Z',\n",
       "  'summary': u'  Action recognition in still images has seen major improvement in recent years\\ndue to advances in human pose estimation, object recognition and stronger\\nfeature representations. However, there are still many cases in which\\nperformance remains far from that of humans. In this paper, we approach the\\nproblem by learning explicitly, and then integrating three components of\\ntransitive actions: (1) the human body part relevant to the action (2) the\\nobject being acted upon and (3) the specific form of interaction between the\\nperson and the object. The process uses class-specific features and relations\\nnot used in the past for action recognition and which use inherently two cycles\\nin the process unlike most standard approaches. We focus on face-related\\nactions (FRA), a subset of actions that includes several currently challenging\\ncategories. We present an average relative improvement of 52% over state-of-the\\nart. We also make a new benchmark publicly available.\\n',\n",
       "  'title': u'Face-space Action Recognition by Face-Object Interactions'},\n",
       " u'1512.09300': {'arxivid': u'1512.09300',\n",
       "  'authorsaffil': [[u'Anders Boesen Lindbo Larsen', None],\n",
       "   [u'S\\xf8ren Kaae S\\xf8nderby', None],\n",
       "   [u'Hugo Larochelle', None],\n",
       "   [u'Ole Winther', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.09300v2',\n",
       "  'published': u'2015-12-31T14:53:39Z',\n",
       "  'summary': u'  We present an autoencoder that leverages learned representations to better\\nmeasure similarities in data space. By combining a variational autoencoder with\\na generative adversarial network we can use learned feature representations in\\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\\nwe replace element-wise errors with feature-wise errors to better capture the\\ndata distribution while offering invariance towards e.g. translation. We apply\\nour method to images of faces and show that it outperforms VAEs with\\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\\nthat the method learns an embedding in which high-level abstract visual\\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic.\\n',\n",
       "  'title': u'Autoencoding beyond pixels using a learned similarity metric'},\n",
       " u'1601.07258': {'arxivid': u'1601.07258',\n",
       "  'authorsaffil': [[u'Kuldeep Kulkarni', None], [u'Pavan Turaga', None]],\n",
       "  'categoryterms': [u'cs.CV', u'math.OC'],\n",
       "  'comment': u'Submitted to TPAMI',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07258v1',\n",
       "  'published': u'2016-01-27T04:32:20Z',\n",
       "  'summary': u'  We propose a framework called ReFInE to directly obtain integral image\\nestimates from a very small number of spatially multiplexed measurements of the\\nscene without iterative reconstruction of any auxiliary image, and demonstrate\\ntheir practical utility in visual object tracking. Specifically, we design\\nmeasurement matrices which are tailored to facilitate extremely fast estimation\\nof the integral image, by using a single-shot linear operation on the measured\\nvector. Leveraging a prior model for the images, we formulate a nuclear norm\\nminimization problem with second order conic constraints to jointly obtain the\\nmeasurement matrix and the linear operator. Through qualitative and\\nquantitative experiments, we show that high quality integral image estimates\\ncan be obtained using our framework at very low measurement rates. Further, on\\na standard dataset of 50 videos, we present object tracking results which are\\ncomparable to the state-of-the-art methods, even at an extremely low\\nmeasurement rate of 1%.\\n',\n",
       "  'title': u'Fast Integral Image Estimation at 1% measurement rate'},\n",
       " u'1601.04296': {'arxivid': u'1601.04296',\n",
       "  'authorsaffil': [[u'Adel Ammar', None],\n",
       "   [u'Sylvie Labroue', None],\n",
       "   [u'Estelle Obligis', None],\n",
       "   [u'Michel Cr\\xe9pon', None],\n",
       "   [u'Sylvie Thiria', None]],\n",
       "  'categoryterms': [u'cs.NE', u'physics.ao-ph'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04296v1',\n",
       "  'published': u'2016-01-17T13:56:38Z',\n",
       "  'summary': u'  This article deals with an important aspect of the neural network retrieval\\nof sea surface salinity (SSS) from SMOS brightness temperatures (TBs). The\\nneural network retrieval method is an empirical approach that offers the\\npossibility of being independent from any theoretical emissivity model, during\\nthe in-flight phase. A Previous study [1] has proven that this approach is\\napplicable to all pixels on ocean, by designing a set of neural networks with\\ndifferent inputs. The present study focuses on the choice of the learning\\ndatabase and demonstrates that a judicious distribution of the geophysical\\nparameters allows to markedly reduce the systematic regional biases of the\\nretrieved SSS, which are due to the high noise on the TBs. An equalization of\\nthe distribution of the geophysical parameters, followed by a new technique for\\nboosting the learning process, makes the regional biases almost disappear for\\nlatitudes between 40{\\\\deg}S and 40{\\\\deg}N, while the global standard deviation\\nremains between 0.6 psu (at the center of the of the swath) and 1 psu (at the\\nedges).\\n',\n",
       "  'title': u'Building a Learning Database for the Neural Network Retrieval of Sea\\n  Surface Salinity from SMOS Brightness Temperatures'},\n",
       " u'1602.07107': {'arxivid': u'1602.07107',\n",
       "  'authorsaffil': [[u'Thomas Bonald', None], [u'Richard Combes', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'23 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07107v1',\n",
       "  'published': u'2016-02-23T10:21:58Z',\n",
       "  'summary': u'  We propose a streaming algorithm for the binary classification of data based\\non crowdsourcing. The algorithm learns the competence of each labeller by\\ncomparing her labels to those of other labellers on the same tasks and uses\\nthis information to minimize the prediction error rate on each task. We provide\\nperformance guarantees of our algorithm for a fixed population of independent\\nlabellers. In particular, we show that our algorithm is optimal in the sense\\nthat the cumulative regret compared to the optimal decision with known labeller\\nerror probabilities is finite, independently of the number of tasks to label.\\nThe complexity of the algorithm is linear in the number of labellers and the\\nnumber of tasks, up to some logarithmic factors. Numerical experiments\\nillustrate the performance of our algorithm compared to existing algorithms,\\nincluding simple majority voting and expectation-maximization algorithms, on\\nboth synthetic and real datasets.\\n',\n",
       "  'title': u'A Streaming Algorithm for Crowdsourced Data Classification'},\n",
       " u'1510.03421': {'arxivid': u'1510.03421',\n",
       "  'authorsaffil': [[u'Michal Jungiewicz', None],\n",
       "   [u'Micha\\u0142 \\u0141opuszy\\u0144ski', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.3233/978-1-61499-609-5-185',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.03421v2',\n",
       "  'published': u'2015-10-12T13:52:40Z',\n",
       "  'summary': u'  In this work, we analyze the utility of two dimensional document maps for\\nexploratory analysis of Polish case law. We start by comparing two methods of\\ngenerating such visualizations. First is based on linear principal component\\nanalysis (PCA). Second makes use of the modern nonlinear t-Distributed\\nStochastic Neighbor Embedding method (t-SNE). We apply both PCA and t-SNE to a\\ncorpus of judgments from different courts in Poland. It emerges that t-SNE\\nprovides better, more interpretable results than PCA. As a next test, we apply\\nt-SNE to randomly selected sample of common court judgments corresponding to\\ndifferent keywords. We show that t-SNE, in this case, reveals hidden topical\\nstructure of the documents related to keyword,,pension\". In conclusion, we find\\nthat the t-SNE method could be a promising tool to facilitate the exploitative\\nanalysis of legal texts, e.g., by complementing search or browse functionality\\nin legal databases.\\n',\n",
       "  'title': u'Towards Meaningful Maps of Polish Case Law'},\n",
       " u'1602.06458': {'arxivid': u'1602.06458',\n",
       "  'authorsaffil': [[u'Babak Salimi', None], [u'Leopoldo Bertossi', None]],\n",
       "  'categoryterms': [u'cs.DB', u'cs.AI'],\n",
       "  'comment': u'To appear in Proceedings Flairs, 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06458v1',\n",
       "  'published': u'2016-02-20T20:57:59Z',\n",
       "  'summary': u'  Causality has been recently introduced in databases, to model, characterize\\nand possibly compute causes for query results (answers). Connections between\\nqueryanswer causality, consistency-based diagnosis, database repairs (wrt.\\nintegrity constraint violations), abductive diagnosis and the view-update\\nproblem have been established. In this work we further investigate connections\\nbetween query-answer causality and abductive diagnosis and the view-update\\nproblem. In this context, we also define and investigate the notion of\\nquery-answer causality in the presence of integrity constraints.\\n',\n",
       "  'title': u'Causes for Query Answers from Databases, Datalog Abduction and\\n  View-Updates: The Presence of Integrity Constraints'},\n",
       " u'1602.03040': {'arxivid': u'1602.03040',\n",
       "  'authorsaffil': [[u'Rotem Dror', None], [u'Roi Reichart', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03040v2',\n",
       "  'published': u'2016-02-09T15:51:19Z',\n",
       "  'summary': u'  We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a\\nnew structured prediction algorithm that generalizes the Collins Structured\\nPerceptron (CSP). Unlike CSP, the update rule of SWVP explicitly exploits the\\ninternal structure of the predicted labels. We prove the convergence of SWVP\\nfor linearly separable training sets, provide mistake and generalization\\nbounds, and show that in the general case these bounds are tighter than those\\nof the CSP special case. In synthetic data experiments with data drawn from an\\nHMM, various variants of SWVP substantially outperform its CSP special case.\\nSWVP also provides encouraging initial dependency parsing results.\\n',\n",
       "  'title': u'The Structured Weighted Violations Perceptron Algorithm'},\n",
       " u'1601.02197': {'arxivid': u'1601.02197',\n",
       "  'authorsaffil': [[u'Wei-Long Zheng', None],\n",
       "   [u'Jia-Yi Zhu', None],\n",
       "   [u'Bao-Liang Lu', None]],\n",
       "  'categoryterms': [u'cs.HC', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02197v1',\n",
       "  'published': u'2016-01-10T10:43:24Z',\n",
       "  'summary': u'  In this paper, we investigate stable patterns of electroencephalogram (EEG)\\nover time for emotion recognition using a machine learning approach. Up to now,\\nvarious findings of activated patterns associated with different emotions have\\nbeen reported. However, their stability over time has not been fully\\ninvestigated yet. In this paper, we focus on identifying EEG stability in\\nemotion recognition. To validate the efficiency of the machine learning\\nalgorithms used in this study, we systematically evaluate the performance of\\nvarious popular feature extraction, feature selection, feature smoothing and\\npattern classification methods with the DEAP dataset and a newly developed\\ndataset for this study. The experimental results indicate that stable patterns\\nexhibit consistency across sessions; the lateral temporal areas activate more\\nfor positive emotion than negative one in beta and gamma bands; the neural\\npatterns of neutral emotion have higher alpha responses at parietal and\\noccipital sites; and for negative emotion, the neural patterns have significant\\nhigher delta responses at parietal and occipital sites and higher gamma\\nresponses at prefrontal sites. The performance of our emotion recognition\\nsystem shows that the neural patterns are relatively stable within and between\\nsessions.\\n',\n",
       "  'title': u'Identifying Stable Patterns over Time for Emotion Recognition from EEG'},\n",
       " u'1602.07109': {'arxivid': u'1602.07109',\n",
       "  'authorsaffil': [[u'Maximilian Soelch', None],\n",
       "   [u'Justin Bayer', None],\n",
       "   [u'Marvin Ludersdorfer', None],\n",
       "   [u'Patrick van der Smagt', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'Accepted as workshop paper at ICLR 2016; accepted as workshop paper\\n  for anomaly detection workshop at ICML 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07109v4',\n",
       "  'published': u'2016-02-23T10:31:51Z',\n",
       "  'summary': u'  Approximate variational inference has shown to be a powerful tool for\\nmodeling unknown complex probability distributions. Recent advances in the\\nfield allow us to learn probabilistic models of sequences that ac- tively\\nexploit spatial and temporal structure. We apply a Stochastic Recurrent Network\\n(STORN) to learn robot time series data. Our evaluation demonstrates that we\\ncan ro- bustly detect anomalies both off- and on-line.\\n',\n",
       "  'title': u'Variational Inference for On-line Anomaly Detection in High-Dimensional\\n  Time Series'},\n",
       " u'1605.01635': {'arxivid': u'1605.01635',\n",
       "  'authorsaffil': [[u'Seyed Omid Sadjadi', None],\n",
       "   [u'Jason Pelecanos', None],\n",
       "   [u'Sriram Ganapathy', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.SD', u'stat.ML'],\n",
       "  'comment': u'submitted to INTERSPEECH 2016. arXiv admin note: substantial text\\n  overlap with arXiv:1602.07291',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1605.01635v1',\n",
       "  'published': u'2016-05-05T15:57:21Z',\n",
       "  'summary': u'  We present the recent advances along with an error analysis of the IBM\\nspeaker recognition system for conversational speech. Some of the key\\nadvancements that contribute to our system include: a nearest-neighbor\\ndiscriminant analysis (NDA) approach (as opposed to LDA) for intersession\\nvariability compensation in the i-vector space, the application of speaker and\\nchannel-adapted features derived from an automatic speech recognition (ASR)\\nsystem for speaker recognition, and the use of a DNN acoustic model with a very\\nlarge number of output units (~10k senones) to compute the frame-level soft\\nalignments required in the i-vector estimation process. We evaluate these\\ntechniques on the NIST 2010 SRE extended core conditions (C1-C9), as well as\\nthe 10sec-10sec condition. To our knowledge, results achieved by our system\\nrepresent the best performances published to date on these conditions. For\\nexample, on the extended tel-tel condition (C5) the system achieves an EER of\\n0.59%. To garner further understanding of the remaining errors (on C5), we\\nexamine the recordings associated with the low scoring target trials, where\\nvarious issues are identified for the problematic recordings/trials.\\nInterestingly, it is observed that correcting the pathological recordings not\\nonly improves the scores for the target trials but also for the nontarget\\ntrials.\\n',\n",
       "  'title': u'The IBM Speaker Recognition System: Recent Advances and Error Analysis'},\n",
       " u'1603.05191': {'arxivid': u'1603.05191',\n",
       "  'authorsaffil': [[u'Chenxin Ma', None], [u'Martin Tak\\xe1\\u010d', None]],\n",
       "  'categoryterms': [u'cs.LG', u'math.OC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05191v1',\n",
       "  'published': u'2016-03-16T17:50:33Z',\n",
       "  'summary': u'  In this paper we study inexact dumped Newton method implemented in a\\ndistributed environment. We start with an original DiSCO algorithm\\n[Communication-Efficient Distributed Optimization of Self-Concordant Empirical\\nLoss, Yuchen Zhang and Lin Xiao, 2015]. We will show that this algorithm may\\nnot scale well and propose an algorithmic modifications which will lead to less\\ncommunications, better load-balancing and more efficient computation. We\\nperform numerical experiments with an regularized empirical loss minimization\\ninstance described by a 273GB dataset.\\n',\n",
       "  'title': u'Distributed Inexact Damped Newton Method: Data Partitioning and\\n  Load-Balancing'},\n",
       " u'1603.05835': {'arxivid': u'1603.05835',\n",
       "  'authorsaffil': [[u'Hendrik Dirks', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.CV', u'cs.MS', u'I.4, G.1.6, G.4'],\n",
       "  'comment': u'10 pages, 1 table',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05835v1',\n",
       "  'published': u'2016-03-18T11:01:23Z',\n",
       "  'summary': u'  \\\\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convex\\nvariational problems in image processing and beyond. Such problems often\\nconsist of non-differentiable parts and involve linear operators. The toolbox\\nuses a primal-dual scheme to avoid (computationally) inefficient operator\\ninversion and to get reliable error estimates. From the user-side,\\n\\\\textbf{FlexBox} expects the primal formulation of the problem, automatically\\ndecouples operators and dualizes the problem. For large-scale problems,\\n\\\\textbf{FlexBox} also comes with a \\\\cpp-module, which can be used stand-alone\\nor together with MATLAB via MEX-interfaces. Besides various pre-implemented\\ndata-fidelities and regularization-terms, \\\\textbf{FlexBox} is able to handle\\narbitrary operators while being easily extendable, due to its object-oriented\\ndesign. The toolbox is available at\\n\\\\href{http://www.flexbox.im}{http://www.flexbox.im}\\n',\n",
       "  'title': u'A Flexible Primal-Dual Toolbox'},\n",
       " u'1602.06291': {'arxivid': u'1602.06291',\n",
       "  'authorsaffil': [[u'Shalini Ghosh', None],\n",
       "   [u'Oriol Vinyals', None],\n",
       "   [u'Brian Strope', None],\n",
       "   [u'Scott Roy', None],\n",
       "   [u'Tom Dean', None],\n",
       "   [u'Larry Heck', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06291v2',\n",
       "  'published': u'2016-02-19T20:52:08Z',\n",
       "  'summary': u'  Documents exhibit sequential structure at multiple levels of abstraction\\n(e.g., sentences, paragraphs, sections). These abstractions constitute a\\nnatural hierarchy for representing the context in which to infer the meaning of\\nwords and larger fragments of text. In this paper, we present CLSTM (Contextual\\nLSTM), an extension of the recurrent neural network LSTM (Long-Short Term\\nMemory) model, where we incorporate contextual features (e.g., topics) into the\\nmodel. We evaluate CLSTM on three specific NLP tasks: word prediction, next\\nsentence selection, and sentence topic prediction. Results from experiments run\\non two corpora, English documents in Wikipedia and a subset of articles from a\\nrecent snapshot of English Google News, indicate that using both words and\\ntopics as features improves performance of the CLSTM models over baseline LSTM\\nmodels for these tasks. For example on the next sentence selection task, we get\\nrelative accuracy improvements of 21% for the Wikipedia dataset and 18% for the\\nGoogle News dataset. This clearly demonstrates the significant benefit of using\\ncontext appropriately in natural language (NL) tasks. This has implications for\\na wide variety of NL applications like question answering, sentence completion,\\nparaphrase generation, and next utterance prediction in dialog systems.\\n',\n",
       "  'title': u'Contextual LSTM (CLSTM) models for Large scale NLP tasks'},\n",
       " u'1504.04343': {'arxivid': u'1504.04343',\n",
       "  'authorsaffil': [[u'Stefan Hadjis', None],\n",
       "   [u'Firas Abuzaid', None],\n",
       "   [u'Ce Zhang', None],\n",
       "   [u'Christopher R\\xe9', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.04343v2',\n",
       "  'published': u'2015-04-16T19:11:08Z',\n",
       "  'summary': u'  We present Caffe con Troll (CcT), a fully compatible end-to-end version of\\nthe popular framework Caffe with rebuilt internals. We built CcT to examine the\\nperformance characteristics of training and deploying general-purpose\\nconvolutional neural networks across different hardware architectures. We find\\nthat, by employing standard batching optimizations for CPU training, we achieve\\na 4.5x throughput improvement over Caffe on popular networks like CaffeNet.\\nMoreover, with these improvements, the end-to-end training time for CNNs is\\ndirectly proportional to the FLOPS delivered by the CPU, which enables us to\\nefficiently train hybrid CPU-GPU systems for CNNs.\\n',\n",
       "  'title': u'Caffe con Troll: Shallow Ideas to Speed Up Deep Learning'},\n",
       " u'1410.7856': {'arxivid': u'1410.7856',\n",
       "  'authorsaffil': [[u'Hossein Azari Soufiani', None],\n",
       "   [u'David C. Parkes', None],\n",
       "   [u'Lirong Xia', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.MA'],\n",
       "  'comment': u'Full version of a NIPS-14 paper under the same title, fixed a typo in\\n  Theorem 1',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1410.7856v2',\n",
       "  'published': u'2014-10-29T01:46:50Z',\n",
       "  'summary': u'  In this paper, we take a statistical decision-theoretic viewpoint on social\\nchoice, putting a focus on the decision to be made on behalf of a system of\\nagents. In our framework, we are given a statistical ranking model, a decision\\nspace, and a loss function defined on (parameter, decision) pairs, and\\nformulate social choice mechanisms as decision rules that minimize expected\\nloss. This suggests a general framework for the design and analysis of new\\nsocial choice mechanisms. We compare Bayesian estimators, which minimize\\nBayesian expected loss, for the Mallows model and the Condorcet model\\nrespectively, and the Kemeny rule. We consider various normative properties, in\\naddition to computational complexity and asymptotic behavior. In particular, we\\nshow that the Bayesian estimator for the Condorcet model satisfies some desired\\nproperties such as anonymity, neutrality, and monotonicity, can be computed in\\npolynomial time, and is asymptotically different from the other two rules when\\nthe data are generated from the Condorcet model for some ground truth\\nparameter.\\n',\n",
       "  'title': u'A Statistical Decision-Theoretic Framework for Social Choice'},\n",
       " u'1512.02325': {'arxivid': u'1512.02325',\n",
       "  'authorsaffil': [[u'Wei Liu', None],\n",
       "   [u'Dragomir Anguelov', None],\n",
       "   [u'Dumitru Erhan', None],\n",
       "   [u'Christian Szegedy', None],\n",
       "   [u'Scott Reed', None],\n",
       "   [u'Cheng-Yang Fu', None],\n",
       "   [u'Alexander C. Berg', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.02325v2',\n",
       "  'published': u'2015-12-08T04:46:38Z',\n",
       "  'summary': u'  We present a method for detecting objects in images using a single deep\\nneural network. Our approach, named SSD, discretizes the output space of\\nbounding boxes into a set of default boxes over different aspect ratios and\\nscales per feature map location. At prediction time, the network generates\\nscores for the presence of each object category in each default box and\\nproduces adjustments to the box to better match the object shape. Additionally,\\nthe network combines predictions from multiple feature maps with different\\nresolutions to naturally handle objects of various sizes. Our SSD model is\\nsimple relative to methods that require object proposals because it completely\\neliminates proposal generation and subsequent pixel or feature resampling stage\\nand encapsulates all computation in a single network. This makes SSD easy to\\ntrain and straightforward to integrate into systems that require a detection\\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\\nconfirm that SSD has comparable accuracy to methods that utilize an additional\\nobject proposal step and is much faster, while providing a unified framework\\nfor both training and inference. Compared to other single stage methods, SSD\\nhas much better accuracy, even with a smaller input image size. For $300\\\\times\\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\\nX and for $500\\\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\\ncomparable state of the art Faster R-CNN model. Code is available at\\n\\\\url{https://github.com/weiliu89/caffe/tree/ssd} .\\n',\n",
       "  'title': u'SSD: Single Shot MultiBox Detector'},\n",
       " u'1602.02283': {'arxivid': u'1602.02283',\n",
       "  'authorsaffil': [[u'Dominik Csiba', None], [u'Peter Richt\\xe1rik', None]],\n",
       "  'categoryterms': [u'cs.LG', u'math.OC', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02283v1',\n",
       "  'published': u'2016-02-06T17:35:53Z',\n",
       "  'summary': u'  Minibatching is a very well studied and highly popular technique in\\nsupervised learning, used by practitioners due to its ability to accelerate\\ntraining through better utilization of parallel processing power and reduction\\nof stochastic variance. Another popular technique is importance sampling -- a\\nstrategy for preferential sampling of more important examples also capable of\\naccelerating the training process. However, despite considerable effort by the\\ncommunity in these areas, and due to the inherent technical difficulty of the\\nproblem, there is no existing work combining the power of importance sampling\\nwith the strength of minibatching. In this paper we propose the first {\\\\em\\nimportance sampling for minibatches} and give simple and rigorous complexity\\nanalysis of its performance. We illustrate on synthetic problems that for\\ntraining data of certain properties, our sampling can lead to several orders of\\nmagnitude improvement in training time. We then test the new sampling on\\nseveral popular datasets, and show that the improvement can reach an order of\\nmagnitude.\\n',\n",
       "  'title': u'Importance Sampling for Minibatches'},\n",
       " u'1602.00374': {'arxivid': u'1602.00374',\n",
       "  'authorsaffil': [[u'Ahmed M. Alaa', None],\n",
       "   [u'Kyeong H. Moon', None],\n",
       "   [u'William Hsu', None],\n",
       "   [u'Mihaela van der Schaar', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00374v1',\n",
       "  'published': u'2016-02-01T03:21:46Z',\n",
       "  'summary': u'  Breast cancer screening policies attempt to achieve timely diagnosis by the\\nregular screening of apparently healthy women. Various clinical decisions are\\nneeded to manage the screening process; those include: selecting the screening\\ntests for a woman to take, interpreting the test outcomes, and deciding whether\\nor not a woman should be referred to a diagnostic test. Such decisions are\\ncurrently guided by clinical practice guidelines (CPGs), which represent a\\none-size-fits-all approach that are designed to work well on average for a\\npopulation, without guaranteeing that it will work well uniformly over that\\npopulation. Since the risks and benefits of screening are functions of each\\npatients features, personalized screening policies that are tailored to the\\nfeatures of individuals are needed in order to ensure that the right tests are\\nrecommended to the right woman. In order to address this issue, we present\\nConfidentCare: a computer-aided clinical decision support system that learns a\\npersonalized screening policy from the electronic health record (EHR) data.\\nConfidentCare operates by recognizing clusters of similar patients, and\\nlearning the best screening policy to adopt for each cluster. A cluster of\\npatients is a set of patients with similar features (e.g. age, breast density,\\nfamily history, etc.), and the screening policy is a set of guidelines on what\\nactions to recommend for a woman given her features and screening test scores.\\nConfidentCare algorithm ensures that the policy adopted for every cluster of\\npatients satisfies a predefined accuracy requirement with a high level of\\nconfidence. We show that our algorithm outperforms the current CPGs in terms of\\ncost-efficiency and false positive rates.\\n',\n",
       "  'title': u'ConfidentCare: A Clinical Decision Support System for Personalized\\n  Breast Cancer Screening'},\n",
       " u'1601.06035': {'arxivid': u'1601.06035',\n",
       "  'authorsaffil': [[u'Cyril Stark', None]],\n",
       "  'categoryterms': [u'cs.LG',\n",
       "   u'cs.IT',\n",
       "   u'math.IT',\n",
       "   u'math.OC',\n",
       "   u'quant-ph',\n",
       "   u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06035v1',\n",
       "  'published': u'2016-01-22T15:09:18Z',\n",
       "  'summary': u'  Physicists use quantum models to describe the behavior of physical systems.\\nQuantum models owe their success to their interpretability, to their relation\\nto probabilistic models (quantization of classical models) and to their high\\npredictive power. Beyond physics, these properties are valuable in general data\\nscience. This motivates the use of quantum models to analyze general\\nnonphysical datasets. Here we provide both empirical and theoretical insights\\ninto the application of quantum models in data science. In the theoretical part\\nof this paper, we firstly show that quantum models can be exponentially more\\nefficient than probabilistic models because there exist datasets that admit\\nlow-dimensional quantum models and only exponentially high-dimensional\\nprobabilistic models. Secondly, we explain in what sense quantum models realize\\na useful relaxation of compressed probabilistic models. Thirdly, we show that\\nsparse datasets admit low-dimensional quantum models and finally, we introduce\\na method to compute hierarchical orderings of properties of users (e.g.,\\npersonality traits) and items (e.g., genres of movies). In the empirical part\\nof the paper, we evaluate quantum models in item recommendation and observe\\nthat the predictive power of quantum-inspired recommender systems can compete\\nwith state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,\\nwe make use of the interpretability of quantum models by computing hierarchical\\norderings of properties of users and items. This work establishes a connection\\nbetween data science (item recommendation), information theory (communication\\ncomplexity), mathematical programming (positive semidefinite factorizations)\\nand physics (quantum models).\\n',\n",
       "  'title': u'Recommender systems inspired by the structure of quantum theory'},\n",
       " u'1601.06032': {'arxivid': u'1601.06032',\n",
       "  'authorsaffil': [[u'Wangmeng Zuo', None],\n",
       "   [u'Xiaohe Wu', None],\n",
       "   [u'Liang Lin', None],\n",
       "   [u'Lei Zhang', None],\n",
       "   [u'Ming-Hsuan Yang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06032v1',\n",
       "  'published': u'2016-01-22T15:02:50Z',\n",
       "  'summary': u'  Sampling and budgeting training examples are two essential factors in\\ntracking algorithms based on support vector machines (SVMs) as a trade-off\\nbetween accuracy and efficiency. Recently, the circulant matrix formed by dense\\nsampling of translated image patches has been utilized in correlation filters\\nfor fast tracking. In this paper, we derive an equivalent formulation of a SVM\\nmodel with circulant matrix expression and present an efficient alternating\\noptimization method for visual tracking. We incorporate the discrete Fourier\\ntransform with the proposed alternating optimization process, and pose the\\ntracking problem as an iterative learning of support correlation filters (SCFs)\\nwhich find the global optimal solution with real-time performance. For a given\\ncirculant data matrix with n^2 samples of size n*n, the computational\\ncomplexity of the proposed algorithm is O(n^2*logn) whereas that of the\\nstandard SVM-based approaches is at least O(n^4). In addition, we extend the\\nSCF-based tracking algorithm with multi-channel features, kernel functions, and\\nscale-adaptive approaches to further improve the tracking performance.\\nExperimental results on a large benchmark dataset show that the proposed\\nSCF-based algorithms perform favorably against the state-of-the-art tracking\\nmethods in terms of accuracy and speed.\\n',\n",
       "  'title': u'Learning Support Correlation Filters for Visual Tracking'},\n",
       " u'1602.02285': {'arxivid': u'1602.02285',\n",
       "  'authorsaffil': [[u'Uri Shaham', None],\n",
       "   [u'Xiuyuan Cheng', None],\n",
       "   [u'Omer Dror', None],\n",
       "   [u'Ariel Jaffe', None],\n",
       "   [u'Boaz Nadler', None],\n",
       "   [u'Joseph Chang', None],\n",
       "   [u'Yuval Kluger', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02285v1',\n",
       "  'published': u'2016-02-06T17:56:59Z',\n",
       "  'summary': u'  We show how deep learning methods can be applied in the context of\\ncrowdsourcing and unsupervised ensemble learning. First, we prove that the\\npopular model of Dawid and Skene, which assumes that all classifiers are\\nconditionally independent, is {\\\\em equivalent} to a Restricted Boltzmann\\nMachine (RBM) with a single hidden node. Hence, under this model, the posterior\\nprobabilities of the true labels can be instead estimated via a trained RBM.\\nNext, to address the more general case, where classifiers may strongly violate\\nthe conditional independence assumption, we propose to apply RBM-based Deep\\nNeural Net (DNN). Experimental results on various simulated and real-world\\ndatasets demonstrate that our proposed DNN approach outperforms other\\nstate-of-the-art methods, in particular when the data violates the conditional\\nindependence assumption.\\n',\n",
       "  'title': u'A Deep Learning Approach to Unsupervised Ensemble Learning'},\n",
       " u'1601.02049': {'arxivid': u'1601.02049',\n",
       "  'authorsaffil': [[u'Rados\\u0142aw Adamczak', None]],\n",
       "  'categoryterms': [u'math.PR', u'cs.LG', u'math.ST', u'stat.TH'],\n",
       "  'comment': u'Minor typos corrected',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02049v2',\n",
       "  'published': u'2016-01-08T23:00:40Z',\n",
       "  'summary': u'  We consider the problem of recovering an invertible $n \\\\times n$ matrix $A$\\nand a sparse $n \\\\times p$ random matrix $X$ based on the observation of $Y =\\nAX$ (up to a scaling and permutation of columns of $A$ and rows of $X$). Using\\nonly elementary tools from the theory of empirical processes we show that a\\nversion of the Er-SpUD algorithm by Spielman, Wang and Wright with high\\nprobability recovers $A$ and $X$ exactly, provided that $p \\\\ge Cn\\\\log n$, which\\nis optimal up to the constant $C$.\\n',\n",
       "  'title': u'A note on the sample complexity of the Er-SpUD algorithm by Spielman,\\n  Wang and Wright for exact recovery of sparsely used dictionaries'},\n",
       " u'1502.02846': {'arxivid': u'1502.02846',\n",
       "  'authorsaffil': [[u'Maren Mahsereci', None], [u'Philipp Hennig', None]],\n",
       "  'categoryterms': [u'cs.LG', u'math.OC', u'stat.ML'],\n",
       "  'comment': u'12 pages, including supplements',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.02846v4',\n",
       "  'published': u'2015-02-10T10:36:25Z',\n",
       "  'summary': u'  In deterministic optimization, line searches are a standard tool ensuring\\nstability and efficiency. Where only stochastic gradients are available, no\\ndirect equivalent has so far been formulated, because uncertain gradients do\\nnot allow for a strict sequence of decisions collapsing the search space. We\\nconstruct a probabilistic line search by combining the structure of existing\\ndeterministic methods with notions from Bayesian optimization. Our method\\nretains a Gaussian process surrogate of the univariate optimization objective,\\nand uses a probabilistic belief over the Wolfe conditions to monitor the\\ndescent. The algorithm has very low computational cost, and no user-controlled\\nparameters. Experiments show that it effectively removes the need to define a\\nlearning rate for stochastic gradient descent.\\n',\n",
       "  'title': u'Probabilistic Line Searches for Stochastic Optimization'},\n",
       " u'1512.07928': {'arxivid': u'1512.07928',\n",
       "  'authorsaffil': [[u'Seunghoon Hong', None],\n",
       "   [u'Junhyuk Oh', None],\n",
       "   [u'Bohyung Han', None],\n",
       "   [u'Honglak Lee', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07928v1',\n",
       "  'published': u'2015-12-24T22:33:27Z',\n",
       "  'summary': u'  We propose a novel weakly-supervised semantic segmentation algorithm based on\\nDeep Convolutional Neural Network (DCNN). Contrary to existing\\nweakly-supervised approaches, our algorithm exploits auxiliary segmentation\\nannotations available for different categories to guide segmentations on images\\nwith only image-level class labels. To make the segmentation knowledge\\ntransferrable across categories, we design a decoupled encoder-decoder\\narchitecture with attention model. In this architecture, the model generates\\nspatial highlights of each category presented in an image using an attention\\nmodel, and subsequently generates foreground segmentation for each highlighted\\nregion using decoder. Combining attention model, we show that the decoder\\ntrained with segmentation annotations in different categories can boost the\\nperformance of weakly-supervised semantic segmentation. The proposed algorithm\\ndemonstrates substantially improved performance compared to the\\nstate-of-the-art weakly-supervised techniques in challenging PASCAL VOC 2012\\ndataset when our model is trained with the annotations in 60 exclusive\\ncategories in Microsoft COCO dataset.\\n',\n",
       "  'title': u'Learning Transferrable Knowledge for Semantic Segmentation with Deep\\n  Convolutional Neural Network'},\n",
       " u'1601.02513': {'arxivid': u'1601.02513',\n",
       "  'authorsaffil': [[u'Vassilis Kalofolias', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'physics.data-an'],\n",
       "  'comment': u'8 pages + supplementary material. Accepted in AISTATS 2016, Cadiz,\\n  Spain',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02513v1',\n",
       "  'published': u'2016-01-11T16:23:30Z',\n",
       "  'summary': u'  We propose a framework that learns the graph structure underlying a set of\\nsmooth signals. Given $X\\\\in\\\\mathbb{R}^{m\\\\times n}$ whose rows reside on the\\nvertices of an unknown graph, we learn the edge weights\\n$w\\\\in\\\\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that\\n$\\\\text{tr}{X^\\\\top LX}$ is small. We show that the problem is a weighted\\n$\\\\ell$-1 minimization that leads to naturally sparse solutions. We point out\\nhow known graph learning or construction techniques fall within our framework\\nand propose a new model that performs better than the state of the art in many\\nsettings. We present efficient, scalable primal-dual based algorithms for both\\nour model and the previous state of the art, and evaluate their performance on\\nartificial and real data.\\n',\n",
       "  'title': u'How to learn a graph from smooth signals'},\n",
       " u'1512.01355': {'arxivid': u'1512.01355',\n",
       "  'authorsaffil': [[u'Luca Bertinetto', None],\n",
       "   [u'Jack Valmadre', None],\n",
       "   [u'Stuart Golodetz', None],\n",
       "   [u'Ondrej Miksik', None],\n",
       "   [u'Philip Torr', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'To appear in CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.01355v2',\n",
       "  'published': u'2015-12-04T09:56:48Z',\n",
       "  'summary': u'  Correlation Filter-based trackers have recently achieved excellent\\nperformance, showing great robustness to challenging situations exhibiting\\nmotion blur and illumination changes. However, since the model that they learn\\ndepends strongly on the spatial layout of the tracked object, they are\\nnotoriously sensitive to deformation. Models based on colour statistics have\\ncomplementary traits: they cope well with variation in shape, but suffer when\\nillumination is not consistent throughout a sequence. Moreover, colour\\ndistributions alone can be insufficiently discriminative. In this paper, we\\nshow that a simple tracker combining complementary cues in a ridge regression\\nframework can operate faster than 80 FPS and outperform not only all entries in\\nthe popular VOT14 competition, but also recent and far more sophisticated\\ntrackers according to multiple benchmarks.\\n',\n",
       "  'title': u'Staple: Complementary Learners for Real-Time Tracking'},\n",
       " u'1602.08556': {'arxivid': u'1602.08556',\n",
       "  'authorsaffil': [[u'Gopalakrishnan Srinivasan', None],\n",
       "   [u'Parami Wijesinghe', None],\n",
       "   [u'Syed Shakib Sarwar', None],\n",
       "   [u'Akhilesh Jaiswal', None],\n",
       "   [u'Kaushik Roy', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'Accepted in Design, Automation and Test in Europe 2016 conference\\n  (DATE-2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08556v1',\n",
       "  'published': u'2016-02-27T05:36:42Z',\n",
       "  'summary': u'  Multilayered artificial neural networks (ANN) have found widespread utility\\nin classification and recognition applications. The scale and complexity of\\nsuch networks together with the inadequacies of general purpose computing\\nplatforms have led to a significant interest in the development of efficient\\nhardware implementations. In this work, we focus on designing energy efficient\\non-chip storage for the synaptic weights. In order to minimize the power\\nconsumption of typical digital CMOS implementations of such large-scale\\nnetworks, the digital neurons could be operated reliably at scaled voltages by\\nreducing the clock frequency. On the contrary, the on-chip synaptic storage\\ndesigned using a conventional 6T SRAM is susceptible to bitcell failures at\\nreduced voltages. However, the intrinsic error resiliency of NNs to small\\nsynaptic weight perturbations enables us to scale the operating voltage of the\\n6TSRAM. Our analysis on a widely used digit recognition dataset indicates that\\nthe voltage can be scaled by 200mV from the nominal operating voltage (950mV)\\nfor practically no loss (less than 0.5%) in accuracy (22nm predictive\\ntechnology). Scaling beyond that causes substantial performance degradation\\nowing to increased probability of failures in the MSBs of the synaptic weights.\\nWe, therefore propose a significance driven hybrid 8T-6T SRAM, wherein the\\nsensitive MSBs are stored in 8T bitcells that are robust at scaled voltages due\\nto decoupled read and write paths. In an effort to further minimize the area\\npenalty, we present a synaptic-sensitivity driven hybrid memory architecture\\nconsisting of multiple 8T-6T SRAM banks. Our circuit to system-level simulation\\nframework shows that the proposed synaptic-sensitivity driven architecture\\nprovides a 30.91% reduction in the memory access power with a 10.41% area\\noverhead, for less than 1% loss in the classification accuracy.\\n',\n",
       "  'title': u'Significance Driven Hybrid 8T-6T SRAM for Energy-Efficient Synaptic\\n  Storage in Artificial Neural Networks'},\n",
       " u'1510.03519': {'arxivid': u'1510.03519',\n",
       "  'authorsaffil': [[u'Janarthanan Rajendran', None],\n",
       "   [u'Mitesh M. Khapra', None],\n",
       "   [u'Sarath Chandar', None],\n",
       "   [u'Balaraman Ravindran', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'12 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.03519v2',\n",
       "  'published': u'2015-10-13T03:25:18Z',\n",
       "  'summary': u'  Recently there has been a lot of interest in learning common representations\\nfor multiple views of data. These views could belong to different modalities or\\nlanguages. Typically, such common representations are learned using a parallel\\ncorpus between the two views (say, 1M images and their English captions). In\\nthis work, we address a real-world scenario where no direct parallel data is\\navailable between two views of interest (say, V1 and V2) but parallel data is\\navailable between each of these views and a pivot view (V3). We propose a model\\nfor learning a common representation for V1, V2 and V3 using only the parallel\\ndata available between V1V3 and V2V3. The proposed model is generic and even\\nworks when there are n views of interest and only one pivot view which acts as\\na bridge between them. There are two specific downstream applications that we\\nfocus on (i) Transfer learning between languages L1,L2,...,Ln using a pivot\\nlanguage L and (ii) cross modal access between images and a language L1 using a\\npivot language L2. We evaluate our model using two datasets : (i) publicly\\navailable multilingual TED corpus and (ii) a new multilingual multimodal\\ndataset created and released as a part of this work. On both these datasets,\\nour model outperforms state of the art approaches.\\n',\n",
       "  'title': u'Bridge Correlational Neural Networks for Multilingual Multimodal\\n  Representation Learning'},\n",
       " u'1510.03517': {'arxivid': u'1510.03517',\n",
       "  'authorsaffil': [[u'Xiang Wang', None],\n",
       "   [u'Ronald D. Haynes', None],\n",
       "   [u'Qihong Feng', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.03517v2',\n",
       "  'published': u'2015-10-13T03:09:43Z',\n",
       "  'summary': u'  Determining optimal well placements and controls are two important tasks in\\noil field development. These problems are computationally expensive, nonconvex,\\nand contain multiple optima. The practical solution of these problems require\\nefficient and robust algorithms. In this paper, the multilevel coordinate\\nsearch (MCS) algorithm is applied for well placement and control optimization\\nproblems. MCS is a derivative-free algorithm that combines global and local\\nsearch. Both synthetic and real oil fields are considered. The performance of\\nMCS is compared to generalized pattern search (GPS), particle swarm\\noptimization (PSO), and covariance matrix adaptive evolution strategy (CMA-ES)\\nalgorithms. Results show that the MCS algorithm is strongly competitive, and\\noutperforms for the joint optimization problem and with a limited computational\\nbudget. The effect of parameter settings for MCS are compared for the test\\nexamples. For the joint optimization problem we compare the performance of the\\nsimultaneous and sequential procedures and show the utility of the latter.\\n',\n",
       "  'title': u'A Multilevel Coordinate Search Algorithm for Well Placement, Control and\\n  Joint Optimization'},\n",
       " u'1409.1556': {'arxivid': u'1409.1556',\n",
       "  'authorsaffil': [[u'Karen Simonyan', None], [u'Andrew Zisserman', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1409.1556v6',\n",
       "  'published': u'2014-09-04T19:48:04Z',\n",
       "  'summary': u'  In this work we investigate the effect of the convolutional network depth on\\nits accuracy in the large-scale image recognition setting. Our main\\ncontribution is a thorough evaluation of networks of increasing depth using an\\narchitecture with very small (3x3) convolution filters, which shows that a\\nsignificant improvement on the prior-art configurations can be achieved by\\npushing the depth to 16-19 weight layers. These findings were the basis of our\\nImageNet Challenge 2014 submission, where our team secured the first and the\\nsecond places in the localisation and classification tracks respectively. We\\nalso show that our representations generalise well to other datasets, where\\nthey achieve state-of-the-art results. We have made our two best-performing\\nConvNet models publicly available to facilitate further research on the use of\\ndeep visual representations in computer vision.\\n',\n",
       "  'title': u'Very Deep Convolutional Networks for Large-Scale Image Recognition'},\n",
       " u'1601.03117': {'arxivid': u'1601.03117',\n",
       "  'authorsaffil': [[u'Fengyuan Zhu', None],\n",
       "   [u'Guangyong Chen', None],\n",
       "   [u'Jianye Hao', None],\n",
       "   [u'Pheng-Ann Heng', None]],\n",
       "  'categoryterms': [u'cs.CV', u'stat.ML'],\n",
       "  'comment': u'25 pages, 11 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03117v1',\n",
       "  'published': u'2016-01-13T02:44:36Z',\n",
       "  'summary': u'  Most existing image denoising approaches assumed the noise to be homogeneous\\nwhite Gaussian distributed with known intensity. However, in real noisy images,\\nthe noise models are usually unknown beforehand and can be much more complex.\\nThis paper addresses this problem and proposes a novel blind image denoising\\nalgorithm to recover the clean image from noisy one with the unknown noise\\nmodel. To model the empirical noise of an image, our method introduces the\\nmixture of Gaussian distribution, which is flexible enough to approximate\\ndifferent continuous distributions. The problem of blind image denoising is\\nreformulated as a learning problem. The procedure is to first build a two-layer\\nstructural model for noisy patches and consider the clean ones as latent\\nvariable. To control the complexity of the noisy patch model, this work\\nproposes a novel Bayesian nonparametric prior called \"Dependent Dirichlet\\nProcess Tree\" to build the model. Then, this study derives a variational\\ninference algorithm to estimate model parameters and recover clean patches. We\\napply our method on synthesis and real noisy images with different noise\\nmodels. Comparing with previous approaches, ours achieves better performance.\\nThe experimental results indicate the efficiency of the proposed algorithm to\\ncope with practical image denoising tasks.\\n',\n",
       "  'title': u'Blind Image Denoising via Dependent Dirichlet Process Tree'},\n",
       " u'1603.09405': {'arxivid': u'1603.09405',\n",
       "  'authorsaffil': [[u'Peng Li', None], [u'Heng Huang', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09405v1',\n",
       "  'published': u'2016-03-30T22:39:59Z',\n",
       "  'summary': u'  Neural network based approaches for sentence relation modeling automatically\\ngenerate hidden matching features from raw sentence pairs. However, the quality\\nof matching feature representation may not be satisfied due to complex semantic\\nrelations such as entailment or contradiction. To address this challenge, we\\npropose a new deep neural network architecture that jointly leverage\\npre-trained word embedding and auxiliary character embedding to learn sentence\\nmeanings. The two kinds of word sequence representations as inputs into\\nmulti-layer bidirectional LSTM to learn enhanced sentence representation. After\\nthat, we construct matching features followed by another temporal CNN to learn\\nhigh-level hidden matching feature representations. Experimental results\\ndemonstrate that our approach consistently outperforms the existing methods on\\nstandard evaluation datasets.\\n',\n",
       "  'title': u'Enhancing Sentence Relation Modeling with Auxiliary Character-level\\n  Embedding'},\n",
       " u'1507.00677': {'arxivid': u'1507.00677',\n",
       "  'authorsaffil': [[u'Takeru Miyato', None],\n",
       "   [u'Shin-ichi Maeda', None],\n",
       "   [u'Masanori Koyama', None],\n",
       "   [u'Ken Nakae', None],\n",
       "   [u'Shin Ishii', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'Under review as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.00677v9',\n",
       "  'published': u'2015-07-02T18:01:23Z',\n",
       "  'summary': u\"  We propose local distributional smoothness (LDS), a new notion of smoothness\\nfor statistical model that can be used as a regularization term to promote the\\nsmoothness of the model distribution. We named the LDS based regularization as\\nvirtual adversarial training (VAT). The LDS of a model at an input datapoint is\\ndefined as the KL-divergence based robustness of the model distribution against\\nlocal perturbation around the datapoint. VAT resembles adversarial training,\\nbut distinguishes itself in that it determines the adversarial direction from\\nthe model distribution alone without using the label information, making it\\napplicable to semi-supervised learning. The computational cost for VAT is\\nrelatively low. For neural network, the approximated gradient of the LDS can be\\ncomputed with no more than three pairs of forward and back propagations. When\\nwe applied our technique to supervised and semi-supervised learning for the\\nMNIST dataset, it outperformed all the training methods other than the current\\nstate of the art method, which is based on a highly advanced generative model.\\nWe also applied our method to SVHN and NORB, and confirmed our method's\\nsuperior performance over the current state of the art semi-supervised method\\napplied to these datasets.\\n\",\n",
       "  'title': u'Distributional Smoothing with Virtual Adversarial Training'},\n",
       " u'1604.00266': {'arxivid': u'1604.00266',\n",
       "  'authorsaffil': [[u'Elnaserledinellah Mahmood Abdelwahab', None],\n",
       "   [u'Karim Daghbouche', None],\n",
       "   [u'Nadra Ahmad Shannan', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LO', u'D.1.6; F.2; F.3; F.4; I.2; F.1.3'],\n",
       "  'comment': u'36 pages, 6 Figures. J.Acad.(N.Y.)4,2:52-87, published May 16 2014',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00266v1',\n",
       "  'published': u'2016-03-10T20:56:15Z',\n",
       "  'summary': u'  The historic background of algorithmic processing with regard to etymology\\nand methodology is translated into terms of mathematical logic and Computer\\nScience. A formal logic structure is introduced by exemplaryquestions posed to\\nFiqh-chapters to define alogic query language. As a foundation, ageneric\\nalgorithm for deciding Fiqh-rulings is designed to enable and further leverage\\nrule of law (vs. rule by law) with full transparency and complete algorithmic\\ncoverage of Islamic law eventually providing legal security, legal equality,\\nand full legal accountability.This is implemented by disentangling and\\nreinstating classic Fiqh-methodology (usul al-Fiqh) with the expressive power\\nof subsets of First Order Logic (FOL)sustainably substituting ad hoc reasoning\\nwith falsifiable rational argumentation. The results are discussed in formal\\nterms of completeness, decidability and complexity of formal Fiqh-systems.\\nAnEntscheidungsproblem for formal Fiqh-Systems is formulated and validated.\\n',\n",
       "  'title': u'The Algorithm of Islamic Jurisprudence (Fiqh) with Validation of an\\n  Entscheidungsproblem'},\n",
       " u'1603.03235': {'arxivid': u'1603.03235',\n",
       "  'authorsaffil': [[u'Amir Soleimani', None],\n",
       "   [u'Kazim Fouladi', None],\n",
       "   [u'Babak N. Araabi', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'15 pages, 6 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03235v3',\n",
       "  'published': u'2016-03-10T12:23:03Z',\n",
       "  'summary': u'  The crucial role of datasets in signature verification systems has motivated\\nresearchers to collect signature samples. However, with regard to the distinct\\ncharacteristics of Persian signature, existing offline signature datasets\\ncannot be used in Persian systems. This paper presents a new and public Persian\\noffline signature dataset, UTSig, which consists of 8280 images from 115\\nclasses that each class has 27 genuine, 3 opposite-hand signatures of the\\ngenuine signer, and 42 skilled forgeries made by 6 forgers from 230 people.\\nCompared to the other public datasets, UTSig has larger number of samples,\\nclasses, and forgers. Meanwhile its samples were collected by considering\\nvariables such as signing period, writing instrument, signature box size, and\\nnumber of observable samples for forgers. Reviewing the main characteristics of\\noffline signature datasets, we statistically show that Persian signatures has\\nfewer number of branch points and end points. We propose and test four\\ndifferent training and testing setups for UTSig. Results of our experiments\\nshow that training genuine samples along with opposite-hand signed samples and\\nrandom forgeries can improve the performance in terms of equal error rate and\\nminimum cost of log likelihood ratio which is an information theoretic\\ncriterion.\\n',\n",
       "  'title': u'UTSig: A Persian Offline Signature Dataset'},\n",
       " u'1512.04652': {'arxivid': u'1512.04652',\n",
       "  'authorsaffil': [[u'Mitra Montazeri', None],\n",
       "   [u'Mahdieh Soleymani Baghshah', None],\n",
       "   [u'Ahmad Enhesari', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'Published in the Journal of Basic and Applied Scientific Research,\\n  2013',\n",
       "  'doi': None,\n",
       "  'journalref': u'J. Basic Appl. Sci. Res, 2013. 3(10): p. 134-140',\n",
       "  'link': u'http://arxiv.org/abs/1512.04652v2',\n",
       "  'published': u'2015-12-15T05:15:07Z',\n",
       "  'summary': u'  Background: Lung cancer was known as primary cancers and the survival rate of\\ncancer is about 15%. Early detection of lung cancer is the leading factor in\\nsurvival rate. All symptoms (features) of lung cancer do not appear until the\\ncancer spreads to other areas. It needs an accurate early detection of lung\\ncancer, for increasing the survival rate. For accurate detection, it need\\ncharacterizes efficient features and delete redundancy features among all\\nfeatures. Feature selection is the problem of selecting informative features\\namong all features. Materials and Methods: Lung cancer database consist of 32\\npatient records with 57 features. This database collected by Hong and Youngand\\nindexed in the University of California Irvine repository. Experimental\\ncontents include the extracted from the clinical data and X-ray data, etc. The\\ndata described 3 types of pathological lung cancers and all features are taking\\nan integer value 0-3. In our study, new method is proposed for identify\\nefficient features of lung cancer. It is based on Hyper-Heuristic. Results: We\\nobtained an accuracy of 80.63% using reduced 11 feature set. The proposed\\nmethod compare to the accuracy of 5 machine learning feature selections. The\\naccuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75.\\nConclusions: The proposed method has better performance with the highest level\\nof accuracy. Therefore, the proposed model is recommended for identifying an\\nefficient symptom of Disease. These finding are very important in health\\nresearch, particularly in allocation of medical resources for patients who\\npredicted as high-risks\\n',\n",
       "  'title': u'Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of\\n  Lung Cancer Disease'},\n",
       " u'1506.08438': {'arxivid': u'1506.08438',\n",
       "  'authorsaffil': [[u'Ozan Sener', None],\n",
       "   [u'Amir Zamir', None],\n",
       "   [u'Silvio Savarese', None],\n",
       "   [u'Ashutosh Saxena', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.08438v4',\n",
       "  'published': u'2015-06-28T19:16:38Z',\n",
       "  'summary': u'  Human communication typically has an underlying structure. This is reflected\\nin the fact that in many user generated videos, a starting point, ending, and\\ncertain objective steps between these two can be identified. In this paper, we\\npropose a method for parsing a video into such semantic steps in an\\nunsupervised way. The proposed method is capable of providing a semantic\\n\"storyline\" of the video composed of its objective steps. We accomplish this\\nusing both visual and language cues in a joint generative model. The proposed\\nmethod can also provide a textual description for each of the identified\\nsemantic steps and video segments. We evaluate this method on a large number of\\ncomplex YouTube videos and show results of unprecedented quality for this\\nintricate and impactful problem.\\n',\n",
       "  'title': u'Unsupervised Semantic Parsing of Video Collections'},\n",
       " u'1602.03001': {'arxivid': u'1602.03001',\n",
       "  'authorsaffil': [[u'Miltiadis Allamanis', None],\n",
       "   [u'Hao Peng', None],\n",
       "   [u'Charles Sutton', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL', u'cs.SE'],\n",
       "  'comment': u'Code, data and visualization at\\n  http://groups.inf.ed.ac.uk/cup/codeattention/',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03001v2',\n",
       "  'published': u'2016-02-09T14:36:49Z',\n",
       "  'summary': u\"  Attention mechanisms in neural networks have proved useful for problems in\\nwhich the input and output do not have fixed dimension. Often there exist\\nfeatures that are locally translation invariant and would be valuable for\\ndirecting the model's attention, but previous attentional architectures are not\\nconstructed to learn such features specifically. We introduce an attentional\\nneural network that employs convolution on the input tokens to detect local\\ntime-invariant and long-range topical attention features in a context-dependent\\nway. We apply this architecture to the problem of extreme summarization of\\nsource code snippets into short, descriptive function name-like summaries.\\nUsing those features, the model sequentially generates a summary by\\nmarginalizing over two attention mechanisms: one that predicts the next summary\\ntoken based on the attention weights of the input tokens and another that is\\nable to copy a code token as-is directly into the summary. We demonstrate our\\nconvolutional attention neural network's performance on 10 popular Java\\nprojects showing that it achieves better performance compared to previous\\nattentional mechanisms.\\n\",\n",
       "  'title': u'A Convolutional Attention Network for Extreme Summarization of Source\\n  Code'},\n",
       " u'1511.07763': {'arxivid': u'1511.07763',\n",
       "  'authorsaffil': [[u'Spyros Gidaris', None], [u'Nikos Komodakis', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'Extended technical report -- short version to appear as oral paper on\\n  CVPR 2016. Code: https://github.com/gidariss/LocNet/',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07763v2',\n",
       "  'published': u'2015-11-24T15:42:01Z',\n",
       "  'summary': u'  We propose a novel object localization methodology with the purpose of\\nboosting the localization accuracy of state-of-the-art object detection\\nsystems. Our model, given a search region, aims at returning the bounding box\\nof an object of interest inside this region. To accomplish its goal, it relies\\non assigning conditional probabilities to each row and column of this region,\\nwhere these probabilities provide useful information regarding the location of\\nthe boundaries of the object inside the search region and allow the accurate\\ninference of the object bounding box under a simple probabilistic framework.\\n  For implementing our localization model, we make use of a convolutional\\nneural network architecture that is properly adapted for this task, called\\nLocNet. We show experimentally that LocNet achieves a very significant\\nimprovement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and\\nthat it can be very easily coupled with recent state-of-the-art object\\ndetection systems, helping them to boost their performance. Finally, we\\ndemonstrate that our detection approach can achieve high detection accuracy\\neven when it is given as input a set of sliding windows, thus proving that it\\nis independent of box proposal methods.\\n',\n",
       "  'title': u'LocNet: Improving Localization Accuracy for Object Detection'},\n",
       " u'1511.04143': {'arxivid': u'1511.04143',\n",
       "  'authorsaffil': [[u'Matthew Hausknecht', None], [u'Peter Stone', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'cs.MA', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04143v4',\n",
       "  'published': u'2015-11-13T02:34:33Z',\n",
       "  'summary': u'  Recent work has shown that deep neural networks are capable of approximating\\nboth value functions and policies in reinforcement learning domains featuring\\ncontinuous state and action spaces. However, to the best of our knowledge no\\nprevious work has succeeded at using deep neural networks in structured\\n(parameterized) continuous action spaces. To fill this gap, this paper focuses\\non learning within the domain of simulated RoboCup soccer, which features a\\nsmall set of discrete action types, each of which is parameterized with\\ncontinuous variables. The best learned agent can score goals more reliably than\\nthe 2012 RoboCup champion agent. As such, this paper represents a successful\\nextension of deep reinforcement learning to the class of parameterized action\\nspace MDPs.\\n',\n",
       "  'title': u'Deep Reinforcement Learning in Parameterized Action Space'},\n",
       " u'1603.02638': {'arxivid': u'1603.02638',\n",
       "  'authorsaffil': [[u'Hossein Mohammadi', None],\n",
       "   [u'Rodolphe Le Riche', None],\n",
       "   [u'Eric Touboul', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02638v1',\n",
       "  'published': u'2016-03-08T19:42:14Z',\n",
       "  'summary': u'  The Efficient Global Optimization (EGO) algorithm uses a conditional\\nGaus-sian Process (GP) to approximate an objective function known at a finite\\nnumber of observation points and sequentially adds new points which maximize\\nthe Expected Improvement criterion according to the GP. The important factor\\nthat controls the efficiency of EGO is the GP covariance function (or kernel)\\nwhich should be chosen according to the objective function. Traditionally, a\\npa-rameterized family of covariance functions is considered whose parameters\\nare learned through statistical procedures such as maximum likelihood or\\ncross-validation. However, it may be questioned whether statistical procedures\\nfor learning covariance functions are the most efficient for optimization as\\nthey target a global agreement between the GP and the observations which is not\\nthe ultimate goal of optimization. Furthermore, statistical learning procedures\\nare computationally expensive. The main alternative to the statistical learning\\nof the GP is self-adaptation, where the algorithm tunes the kernel parameters\\nbased on their contribution to objective function improvement. After\\nquestioning the possibility of self-adaptation for kriging based optimizers,\\nthis paper proposes a novel approach for tuning the length-scale of the GP in\\nEGO: At each iteration, a small ensemble of kriging models structured by their\\nlength-scales is created. All of the models contribute to an iterate in an\\nEGO-like fashion. Then, the set of models is densified around the model whose\\nlength-scale yielded the best iterate and further points are produced.\\nNumerical experiments are provided which motivate the use of many\\nlength-scales. The tested implementation does not perform better than the\\nclassical EGO algorithm in a sequential context but show the potential of the\\napproach for parallel implementations.\\n',\n",
       "  'title': u'Small ensembles of kriging models for optimization'},\n",
       " u'1602.03551': {'arxivid': u'1602.03551',\n",
       "  'authorsaffil': [[u'Stephanie L. Hyland', None],\n",
       "   [u'Theofanis Karaletsos', None],\n",
       "   [u'Gunnar R\\xe4tsch', None]],\n",
       "  'categoryterms': [u'cs.CL', u'stat.AP'],\n",
       "  'comment': u'6 pages, 2 figures, to appear at SDM-DMMH 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03551v1',\n",
       "  'published': u'2016-02-10T22:02:29Z',\n",
       "  'summary': u'  Identifying relationships between concepts is a key aspect of scientific\\nknowledge synthesis. Finding these links often requires a researcher to\\nlaboriously search through scien- tific papers and databases, as the size of\\nthese resources grows ever larger. In this paper we describe how distributional\\nsemantics can be used to unify structured knowledge graphs with unstructured\\ntext to predict new relationships between medical concepts, using a\\nprobabilistic generative model. Our approach is also designed to ameliorate\\ndata sparsity and scarcity issues in the medical domain, which make language\\nmodelling more challenging. Specifically, we integrate the medical relational\\ndatabase (SemMedDB) with text from electronic health records (EHRs) to perform\\nknowledge graph completion. We further demonstrate the ability of our model to\\npredict relationships between tokens not appearing in the relational database.\\n',\n",
       "  'title': u'Knowledge Transfer with Medical Language Embeddings'},\n",
       " u'1602.03552': {'arxivid': u'1602.03552',\n",
       "  'authorsaffil': [[u'Jihun Hamm', None],\n",
       "   [u'Paul Cao', None],\n",
       "   [u'Mikhail Belkin', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CR'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03552v1',\n",
       "  'published': u'2016-02-10T22:02:43Z',\n",
       "  'summary': u\"  Learning a classifier from private data collected by multiple parties is an\\nimportant problem that has many potential applications. How can we build an\\naccurate and differentially private global classifier by combining\\nlocally-trained classifiers from different parties, without access to any\\nparty's private data? We propose to transfer the `knowledge' of the local\\nclassifier ensemble by first creating labeled data from auxiliary unlabeled\\ndata, and then train a global $\\\\epsilon$-differentially private classifier. We\\nshow that majority voting is too sensitive and therefore propose a new risk\\nweighted by class probabilities estimated from the ensemble. Relative to a\\nnon-private solution, our private solution has a generalization error bounded\\nby $O(\\\\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows\\nstrong privacy without performance loss when $M$ is large, such as in\\ncrowdsensing applications. We demonstrate the performance of our method with\\nrealistic tasks of activity recognition, network intrusion detection, and\\nmalicious URL detection.\\n\",\n",
       "  'title': u'Learning Privately from Multiparty Data'},\n",
       " u'1511.05946': {'arxivid': u'1511.05946',\n",
       "  'authorsaffil': [[u'Marcin Moczulski', None],\n",
       "   [u'Misha Denil', None],\n",
       "   [u'Jeremy Appleyard', None],\n",
       "   [u'Nando de Freitas', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05946v5',\n",
       "  'published': u'2015-11-18T20:52:17Z',\n",
       "  'summary': u'  The linear layer is one of the most pervasive modules in deep learning\\nrepresentations. However, it requires $O(N^2)$ parameters and $O(N^2)$\\noperations. These costs can be prohibitive in mobile applications or prevent\\nscaling in many domains. Here, we introduce a deep, differentiable,\\nfully-connected neural network module composed of diagonal matrices of\\nparameters, $\\\\mathbf{A}$ and $\\\\mathbf{D}$, and the discrete cosine transform\\n$\\\\mathbf{C}$. The core module, structured as $\\\\mathbf{ACDC^{-1}}$, has $O(N)$\\nparameters and incurs $O(N log N )$ operations. We present theoretical results\\nshowing how deep cascades of ACDC layers approximate linear layers. ACDC is,\\nhowever, a stand-alone module and can be used in combination with any other\\ntypes of module. In our experiments, we show that it can indeed be successfully\\ninterleaved with ReLU modules in convolutional neural networks for image\\nrecognition. Our experiments also study critical factors in the training of\\nthese structured modules, including initialization and depth. Finally, this\\npaper also provides a connection between structured linear transforms used in\\ndeep learning and the field of Fourier optics, illustrating how ACDC could in\\nprinciple be implemented with lenses and diffractive elements.\\n',\n",
       "  'title': u'ACDC: A Structured Efficient Linear Layer'},\n",
       " u'1511.05942': {'arxivid': u'1511.05942',\n",
       "  'authorsaffil': [[u'Edward Choi', None],\n",
       "   [u'Mohammad Taha Bahadori', None],\n",
       "   [u'Andy Schuetz', None],\n",
       "   [u'Walter F. Stewart', None],\n",
       "   [u'Jimeng Sun', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Updating',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05942v8',\n",
       "  'published': u'2015-11-18T20:47:44Z',\n",
       "  'summary': u\"  Large amount of Electronic Health Record (EHR) data have been collected over\\nmillions of patients over multiple years. The rich longitudinal EHR data\\ndocumented the collective experiences of physicians including diagnosis,\\nmedication prescription and procedures. We argue it is possible now to leverage\\nthe EHR data to model how physicians behave, and we call our model Doctor AI.\\nTowards this direction of modeling clinical behavior of physicians, we develop\\na successful application of Recurrent Neural Networks (RNN) to jointly forecast\\nthe future disease diagnosis and medication prescription along with their\\ntiming. Unlike traditional classification models where a single target is of\\ninterest, our model can assess the entire history of patients and make\\ncontinuous and multilabel predictions based on patients' historical data. We\\nevaluate the performance of the proposed method on a large real-world EHR data\\nover 260K patients over 8 years. We observed Doctor AI can perform differential\\ndiagnosis with similar accuracy to physicians. In particular, Doctor AI\\nachieves up to 79% recall@30, significantly higher than several baselines.\\nMoreover, we demonstrate great generalizability of Doctor AI by applying the\\nresulting models on data from a completely different medication institution\\nachieving comparable performance.\\n\",\n",
       "  'title': u'Doctor AI: Predicting Clinical Events via Recurrent Neural Networks'},\n",
       " u'1604.00036': {'arxivid': u'1604.00036',\n",
       "  'authorsaffil': [[u'Jose Oramas', None], [u'Tinne Tuytelaars', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'29 pages, 19 Figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00036v1',\n",
       "  'published': u'2016-03-31T20:18:16Z',\n",
       "  'summary': u'  In this paper we present a hierarchical method to discover mid-level elements\\nwith the objective of modeling visual compatibility between objects. At the\\nbase-level, our method identifies patterns of CNN activations with the aim of\\nmodeling different variations/styles in which objects of the classes of\\ninterest may occur. At the top-level, the proposed method discovers patterns of\\nco-occurring activations of base-level elements that define visual\\ncompatibility between pairs of object classes. Experiments on the massive\\nAmazon dataset show the strength of our method at describing object classes and\\nthe characteristics that drive the compatibility between them.\\n',\n",
       "  'title': u'Modeling Visual Compatibility through Hierarchical Mid-level Elements'},\n",
       " u'1602.03860': {'arxivid': u'1602.03860',\n",
       "  'authorsaffil': [[u'Srinath Sridhar', None],\n",
       "   [u'Helge Rhodin', None],\n",
       "   [u'Hans-Peter Seidel', None],\n",
       "   [u'Antti Oulasvirta', None],\n",
       "   [u'Christian Theobalt', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'8 pages, Accepted version of paper published at 3DV 2014',\n",
       "  'doi': u'10.1109/3DV.2014.37',\n",
       "  'journalref': u'2nd International Conference on , vol.1, no., pp.319-326, 8-11\\n  Dec. 2014',\n",
       "  'link': u'http://arxiv.org/abs/1602.03860v1',\n",
       "  'published': u'2016-02-11T20:03:53Z',\n",
       "  'summary': u'  Real-time marker-less hand tracking is of increasing importance in\\nhuman-computer interaction. Robust and accurate tracking of arbitrary hand\\nmotion is a challenging problem due to the many degrees of freedom, frequent\\nself-occlusions, fast motions, and uniform skin color. In this paper, we\\npropose a new approach that tracks the full skeleton motion of the hand from\\nmultiple RGB cameras in real-time. The main contributions include a new\\ngenerative tracking method which employs an implicit hand shape representation\\nbased on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that is\\nsmooth and analytically differentiable making fast gradient based pose\\noptimization possible. This shape representation, together with a full\\nperspective projection model, enables more accurate hand modeling than a\\nrelated baseline method from literature. Our method achieves better accuracy\\nthan previous methods and runs at 25 fps. We show these improvements both\\nqualitatively and quantitatively on publicly available datasets.\\n',\n",
       "  'title': u'Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model'},\n",
       " u'1505.01980': {'arxivid': u'1505.01980',\n",
       "  'authorsaffil': [[u'Larry Bull', None]],\n",
       "  'categoryterms': [u'cs.NE', u'q-bio.MN', u'q-bio.PE'],\n",
       "  'comment': u'arXiv admin note: substantial text overlap with arXiv:1306.4793,\\n  arXiv:1303.7220',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.01980v1',\n",
       "  'published': u'2015-05-08T10:20:06Z',\n",
       "  'summary': u'  The editing of transcribed RNA by other molecules such that the form of the\\nfinal product differs from that specified in the corresponding DNA sequence is\\nubiquitous. This paper uses an abstract, tunable Boolean genetic regulatory\\nnetwork model to explore aspects of RNA editing. In particular, it is shown how\\ndynamically altering expressed sequences via a guide RNA-inspired mechanism can\\nbe selected for by simulated evolution under various single and multicellular\\nscenarios.\\n',\n",
       "  'title': u'Evolving Boolean Networks with RNA Editing'},\n",
       " u'1602.03483': {'arxivid': u'1602.03483',\n",
       "  'authorsaffil': [[u'Felix Hill', None],\n",
       "   [u'Kyunghyun Cho', None],\n",
       "   [u'Anna Korhonen', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03483v1',\n",
       "  'published': u'2016-02-10T18:49:58Z',\n",
       "  'summary': u\"  Unsupervised methods for learning distributed representations of words are\\nubiquitous in today's NLP research, but far less is known about the best ways\\nto learn distributed phrase or sentence representations from unlabelled data.\\nThis paper is a systematic comparison of models that learn such\\nrepresentations. We find that the optimal approach depends critically on the\\nintended application. Deeper, more complex models are preferable for\\nrepresentations to be used in supervised systems, but shallow log-linear models\\nwork best for building representation spaces that can be decoded with simple\\nspatial distance metrics. We also propose two new unsupervised\\nrepresentation-learning objectives designed to optimise the trade-off between\\ntraining time, domain portability and performance.\\n\",\n",
       "  'title': u'Learning Distributed Representations of Sentences from Unlabelled Data'},\n",
       " u'1603.00260': {'arxivid': u'1603.00260',\n",
       "  'authorsaffil': [[u'Dhruv Gupta', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.CL'],\n",
       "  'comment': u'Extended research report of an extended abstract published at WSDM\\n  2016 Doctoral Consortium. in WSDM 2016 Proceedings of the Ninth ACM\\n  International Conference on Web Search and Data Mining',\n",
       "  'doi': u'10.1145/2835776.2855083',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00260v1',\n",
       "  'published': u'2016-03-01T13:14:33Z',\n",
       "  'summary': u'  In this article, I present the questions that I seek to answer in my PhD\\nresearch. I posit to analyze natural language text with the help of semantic\\nannotations and mine important events for navigating large text corpora.\\nSemantic annotations such as named entities, geographic locations, and temporal\\nexpressions can help us mine events from the given corpora. These events thus\\nprovide us with useful means to discover the locked knowledge in them. I pose\\nthree problems that can help unlock this knowledge vault in semantically\\nannotated text corpora: i. identifying important events; ii. semantic search;\\nand iii. event analytics.\\n',\n",
       "  'title': u'Event Search and Analytics: Detecting Events in Semantically Annotated\\n  Corpora for Search and Analytics'},\n",
       " u'1602.03481': {'arxivid': u'1602.03481',\n",
       "  'authorsaffil': [[u'Ashish Khetan', None], [u'Sewoong Oh', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.HC', u'cs.SI', u'stat.ML'],\n",
       "  'comment': u'15 pages, 4 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03481v1',\n",
       "  'published': u'2016-02-10T18:46:30Z',\n",
       "  'summary': u'  Crowdsourcing systems provide scalable and cost-effective human-powered\\nsolutions at marginal cost, for classification tasks where humans are\\nsignificantly better than the machines. Although traditional approaches in\\naggregating crowdsourced labels have relied on the Dawid-Skene model, this\\nfails to capture how some tasks are inherently more difficult than the others.\\nSeveral generalizations have been proposed, but inference becomes intractable\\nand typical solutions resort to heuristics. To bridge this gap, we study a\\nrecently proposed generalize Dawid-Skene model, and propose a linear-time\\nalgorithm based on spectral methods. We show near-optimality of the proposed\\napproach, by providing an upper bound on the error and comparing it to a\\nfundamental limit. We provide numerical experiments on synthetic data matching\\nour analyses, and also on real datasets demonstrating that the spectral method\\nsignificantly improves over simple majority voting and is comparable to other\\nmethods.\\n',\n",
       "  'title': u'Reliable Crowdsourcing under the Generalized Dawid-Skene Model'},\n",
       " u'1603.08042': {'arxivid': u'1603.08042',\n",
       "  'authorsaffil': [[u'Rohit Prabhavalkar', None],\n",
       "   [u'Ouais Alsharif', None],\n",
       "   [u'Antoine Bruguier', None],\n",
       "   [u'Ian McGraw', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'Accepted in ICASSP 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08042v2',\n",
       "  'published': u'2016-03-25T21:43:28Z',\n",
       "  'summary': u'  We study the problem of compressing recurrent neural networks (RNNs). In\\nparticular, we focus on the compression of RNN acoustic models, which are\\nmotivated by the goal of building compact and accurate speech recognition\\nsystems which can be run efficiently on mobile devices. In this work, we\\npresent a technique for general recurrent model compression that jointly\\ncompresses both recurrent and non-recurrent inter-layer weight matrices. We\\nfind that the proposed technique allows us to reduce the size of our Long\\nShort-Term Memory (LSTM) acoustic model to a third of its original size with\\nnegligible loss in accuracy.\\n',\n",
       "  'title': u'On the Compression of Recurrent Neural Networks with an Application to\\n  LVCSR acoustic modeling for Embedded Speech Recognition'},\n",
       " u'1511.06362': {'arxivid': u'1511.06362',\n",
       "  'authorsaffil': [[u'Jonathan Huang', None], [u'Kevin Murphy', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': u'10 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06362v2',\n",
       "  'published': u'2015-11-19T20:56:27Z',\n",
       "  'summary': u'  We present a generative model of images based on layering, in which image\\nlayers are individually generated, then composited from front to back. We are\\nthus able to factor the appearance of an image into the appearance of\\nindividual objects within the image --- and additionally for each individual\\nobject, we can factor content from pose. Unlike prior work on layered models,\\nwe learn a shape prior for each object/layer, allowing the model to tease out\\nwhich object is in front by looking for a consistent shape, without needing\\naccess to motion cues or any labeled data. We show that ordinary stochastic\\ngradient variational bayes (SGVB), which optimizes our fully differentiable\\nlower-bound on the log-likelihood, is sufficient to learn an interpretable\\nrepresentation of images. Finally we present experiments demonstrating the\\neffectiveness of the model for inferring foreground and background objects in\\nimages.\\n',\n",
       "  'title': u'Efficient inference in occlusion-aware generative models of images'},\n",
       " u'1511.06361': {'arxivid': u'1511.06361',\n",
       "  'authorsaffil': [[u'Ivan Vendrov', None],\n",
       "   [u'Ryan Kiros', None],\n",
       "   [u'Sanja Fidler', None],\n",
       "   [u'Raquel Urtasun', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL', u'cs.CV'],\n",
       "  'comment': u'ICLR camera-ready version',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06361v6',\n",
       "  'published': u'2015-11-19T20:56:14Z',\n",
       "  'summary': u'  Hypernymy, textual entailment, and image captioning can be seen as special\\ncases of a single visual-semantic hierarchy over words, sentences, and images.\\nIn this paper we advocate for explicitly modeling the partial order structure\\nof this hierarchy. Towards this goal, we introduce a general method for\\nlearning ordered representations, and show how it can be applied to a variety\\nof tasks involving images and language. We show that the resulting\\nrepresentations improve performance over current approaches for hypernym\\nprediction and image-caption retrieval.\\n',\n",
       "  'title': u'Order-Embeddings of Images and Language'},\n",
       " u'1512.05875': {'arxivid': u'1512.05875',\n",
       "  'authorsaffil': [[u'Alessandro Fontana', None]],\n",
       "  'categoryterms': [u'q-bio.NC', u'cs.AI'],\n",
       "  'comment': u'11 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05875v2',\n",
       "  'published': u'2015-12-18T09:08:49Z',\n",
       "  'summary': u'  The borderline and narcissistic personality disorders are important\\nnosographic entities and have been subject of intensive investigation. The\\ncurrently prevailing psychodynamic theory for mental disorders is based on the\\nrepertoire of defense mechanisms employed. Another fruitful line of research is\\nconcerned with the study of psychological traumas and with dissociation as a\\ndefensive response. Both theories can be used to shed light on some aspects of\\npathological mental functioning, and have many points of contact. This work\\nstarts from a model of emotional behaviour, inspired by an approach derived\\nfrom the field of artificial intelligence, and based on neural networks. The\\nmodel is then used to overcome the dichotomy between the two aforementioned\\npsychological theories, and conceive a common framework for the description of\\npersonality disorders.\\n',\n",
       "  'title': u'The Quadripolar Relational Model: an Artificial Intelligence framework\\n  for the description of personality disorders'},\n",
       " u'1603.08048': {'arxivid': u'1603.08048',\n",
       "  'authorsaffil': [[u'Michael Ruster', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.SI', u'stat.ML'],\n",
       "  'comment': u\"Master's Thesis, Koblenz 2016\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08048v1',\n",
       "  'published': u'2016-03-25T22:36:40Z',\n",
       "  'summary': u'  This thesis focuses on gaining linguistic insights into textual discussions\\non a word level. It was of special interest to distinguish messages that\\nconstructively contribute to a discussion from those that are detrimental to\\nthem. Thereby, we wanted to determine whether \"I\"- and \"You\"-messages are\\nindicators for either of the two discussion styles. These messages are nowadays\\noften used in guidelines for successful communication. Although their effects\\nhave been successfully evaluated multiple times, a large-scale analysis has\\nnever been conducted.\\n  Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions\\ntogether with the records of blocked users and developed a fully automated\\ncreation of an annotated data set. In this data set, messages were labelled\\neither constructive or disruptive. We applied binary classifiers to the data to\\ndetermine characteristic words for both discussion styles. Thereby, we also\\ninvestigated whether function words like pronouns and conjunctions play an\\nimportant role in distinguishing the two.\\n  We found that \"You\"-messages were a strong indicator for disruptive messages\\nwhich matches their attributed effects on communication. However, we found\\n\"I\"-messages to be indicative for disruptive messages as well which is contrary\\nto their attributed effects. The importance of function words could neither be\\nconfirmed nor refuted. Other characteristic words for either communication\\nstyle were not found. Yet, the results suggest that a different model might\\nrepresent disruptive and constructive messages in textual discussions better.\\n',\n",
       "  'title': u'\"Did I Say Something Wrong?\" A Word-Level Analysis of Wikipedia Articles\\n  for Deletion Discussions'},\n",
       " u'1503.03832': {'arxivid': u'1503.03832',\n",
       "  'authorsaffil': [[u'Florian Schroff', None],\n",
       "   [u'Dmitry Kalenichenko', None],\n",
       "   [u'James Philbin', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Also published, in Proceedings of the IEEE Computer Society\\n  Conference on Computer Vision and Pattern Recognition 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.03832v3',\n",
       "  'published': u'2015-03-12T18:10:53Z',\n",
       "  'summary': u'  Despite significant recent advances in the field of face recognition,\\nimplementing face verification and recognition efficiently at scale presents\\nserious challenges to current approaches. In this paper we present a system,\\ncalled FaceNet, that directly learns a mapping from face images to a compact\\nEuclidean space where distances directly correspond to a measure of face\\nsimilarity. Once this space has been produced, tasks such as face recognition,\\nverification and clustering can be easily implemented using standard techniques\\nwith FaceNet embeddings as feature vectors.\\n  Our method uses a deep convolutional network trained to directly optimize the\\nembedding itself, rather than an intermediate bottleneck layer as in previous\\ndeep learning approaches. To train, we use triplets of roughly aligned matching\\n/ non-matching face patches generated using a novel online triplet mining\\nmethod. The benefit of our approach is much greater representational\\nefficiency: we achieve state-of-the-art face recognition performance using only\\n128-bytes per face.\\n  On the widely used Labeled Faces in the Wild (LFW) dataset, our system\\nachieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves\\n95.12%. Our system cuts the error rate in comparison to the best published\\nresult by 30% on both datasets.\\n  We also introduce the concept of harmonic embeddings, and a harmonic triplet\\nloss, which describe different versions of face embeddings (produced by\\ndifferent networks) that are compatible to each other and allow for direct\\ncomparison between each other.\\n',\n",
       "  'title': u'FaceNet: A Unified Embedding for Face Recognition and Clustering'},\n",
       " u'1508.03326': {'arxivid': u'1508.03326',\n",
       "  'authorsaffil': [[u'Li Zhou', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.03326v2',\n",
       "  'published': u'2015-08-13T19:49:08Z',\n",
       "  'summary': u\"  In this survey we cover a few stochastic and adversarial contextual bandit\\nalgorithms. We analyze each algorithm's assumption and regret bound.\\n\",\n",
       "  'title': u'A Survey on Contextual Multi-armed Bandits'},\n",
       " u'1601.02865': {'arxivid': u'1601.02865',\n",
       "  'authorsaffil': [[u'Peter Nightingale', None], [u'Andrea Rendl', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02865v1',\n",
       "  'published': u'2016-01-12T14:05:35Z',\n",
       "  'summary': u\"  A description of the Essence' language as used by the tool Savile Row.\\n\",\n",
       "  'title': u\"Essence' Description\"},\n",
       " u'1603.07063': {'arxivid': u'1603.07063',\n",
       "  'authorsaffil': [[u'Xiaodan Liang', None],\n",
       "   [u'Xiaohui Shen', None],\n",
       "   [u'Jiashi Feng', None],\n",
       "   [u'Liang Lin', None],\n",
       "   [u'Shuicheng Yan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'18 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07063v1',\n",
       "  'published': u'2016-03-23T03:31:02Z',\n",
       "  'summary': u'  By taking the semantic object parsing task as an exemplar application\\nscenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network,\\nwhich is the generalization of LSTM from sequential data or multi-dimensional\\ndata to general graph-structured data. Particularly, instead of evenly and\\nfixedly dividing an image to pixels or patches in existing multi-dimensional\\nLSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each\\narbitrary-shaped superpixel as a semantically consistent node, and adaptively\\nconstruct an undirected graph for each image, where the spatial relations of\\nthe superpixels are naturally used as edges. Constructed on such an adaptive\\ngraph topology, the Graph LSTM is more naturally aligned with the visual\\npatterns in the image (e.g., object boundaries or appearance similarities) and\\nprovides a more economical information propagation route. Furthermore, for each\\noptimization step over Graph LSTM, we propose to use a confidence-driven scheme\\nto update the hidden and memory states of nodes progressively till all nodes\\nare updated. In addition, for each node, the forgets gates are adaptively\\nlearned to capture different degrees of semantic correlation with neighboring\\nnodes. Comprehensive evaluations on four diverse semantic object parsing\\ndatasets well demonstrate the significant superiority of our Graph LSTM over\\nother state-of-the-art solutions.\\n',\n",
       "  'title': u'Semantic Object Parsing with Graph LSTM'},\n",
       " u'1603.02041': {'arxivid': u'1603.02041',\n",
       "  'authorsaffil': [[u'Diana Borsa', None],\n",
       "   [u'Thore Graepel', None],\n",
       "   [u'John Shawe-Taylor', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02041v1',\n",
       "  'published': u'2016-03-07T13:03:30Z',\n",
       "  'summary': u'  We investigate a paradigm in multi-task reinforcement learning (MT-RL) in\\nwhich an agent is placed in an environment and needs to learn to perform a\\nseries of tasks, within this space. Since the environment does not change,\\nthere is potentially a lot of common ground amongst tasks and learning to solve\\nthem individually seems extremely wasteful. In this paper, we explicitly model\\nand learn this shared structure as it arises in the state-action value space.\\nWe will show how one can jointly learn optimal value-functions by modifying the\\npopular Value-Iteration and Policy-Iteration procedures to accommodate this\\nshared representation assumption and leverage the power of multi-task\\nsupervised learning. Finally, we demonstrate that the proposed model and\\ntraining procedures, are able to infer good value functions, even under low\\nsamples regimes. In addition to data efficiency, we will show in our analysis,\\nthat learning abstractions of the state space jointly across tasks leads to\\nmore robust, transferable representations with the potential for better\\ngeneralization. this shared representation assumption and leverage the power of\\nmulti-task supervised learning. Finally, we demonstrate that the proposed model\\nand training procedures, are able to infer good value functions, even under low\\nsamples regimes. In addition to data efficiency, we will show in our analysis,\\nthat learning abstractions of the state space jointly across tasks leads to\\nmore robust, transferable representations with the potential for better\\ngeneralization.\\n',\n",
       "  'title': u'Learning Shared Representations in Multi-task Reinforcement Learning'},\n",
       " u'1511.06566': {'arxivid': u'1511.06566',\n",
       "  'authorsaffil': [[u'Tuomo Valkonen', None], [u'Thomas Pock', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.CV', u'90C25, 49M29, 94A08'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06566v2',\n",
       "  'published': u'2015-11-20T11:59:11Z',\n",
       "  'summary': u'  We propose several variants of the primal-dual method due to Chambolle and\\nPock. Without requiring full strong convexity of the objective functions, our\\nmethods are accelerated on subspaces with strong convexity. This yields mixed\\nrates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect to\\nthe dual sequence, and the residual part of the primal sequence. We demonstrate\\nthe efficacy of the proposed methods on image processing problems lacking\\nstrong convexity, such as total generalised variation denoising and total\\nvariation deblurring.\\n',\n",
       "  'title': u'Acceleration of the PDHGM on strongly convex subspaces'},\n",
       " u'1402.3337': {'arxivid': u'1402.3337',\n",
       "  'authorsaffil': [[u'Kishore Konda', None],\n",
       "   [u'Roland Memisevic', None],\n",
       "   [u'David Krueger', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1402.3337v5',\n",
       "  'published': u'2014-02-13T23:37:39Z',\n",
       "  'summary': u'  Regularized training of an autoencoder typically results in hidden unit\\nbiases that take on large negative values. We show that negative biases are a\\nnatural result of using a hidden layer whose responsibility is to both\\nrepresent the input data and act as a selection mechanism that ensures sparsity\\nof the representation. We then show that negative biases impede the learning of\\ndata distributions whose intrinsic dimensionality is high. We also propose a\\nnew activation function that decouples the two roles of the hidden layer and\\nthat allows us to learn representations on data with very high intrinsic\\ndimensionality, where standard autoencoders typically fail. Since the decoupled\\nactivation function acts like an implicit regularizer, the model can be trained\\nby minimizing the reconstruction error of training data, without requiring any\\nadditional regularization.\\n',\n",
       "  'title': u'Zero-bias autoencoders and the benefits of co-adapting features'},\n",
       " u'1602.03903': {'arxivid': u'1602.03903',\n",
       "  'authorsaffil': [[u'Siwei Feng', None],\n",
       "   [u'Yuki Itoh', None],\n",
       "   [u'Mario Parente', None],\n",
       "   [u'Marco F. Duarte', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'21 pages, 8 figures, 4 tables, preprint, revised April 8 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03903v2',\n",
       "  'published': u'2016-02-11T21:25:36Z',\n",
       "  'summary': u\"  Hyperspectral signature classification is a quantitative analysis approach\\nfor hyperspectral imagery which performs detection and classification of the\\nconstituent materials at the pixel level in the scene. The classification\\nprocedure can be operated directly on hyperspectral data or performed by using\\nsome features extracted from the corresponding hyperspectral signatures\\ncontaining information like the signature's energy or shape. In this paper, we\\ndescribe a technique that applies non-homogeneous hidden Markov chain (NHMC)\\nmodels to hyperspectral signature classification. The basic idea is to use\\nstatistical models (such as NHMC) to characterize wavelet coefficients which\\ncapture the spectrum semantics (i.e., structural information) at multiple\\nlevels. Experimental results show that the approach based on NHMC models can\\noutperform existing approaches relevant in classification tasks.\\n\",\n",
       "  'title': u'Wavelet-Based Semantic Features for Hyperspectral Signature\\n  Discrimination'},\n",
       " u'1604.01729': {'arxivid': u'1604.01729',\n",
       "  'authorsaffil': [[u'Subhashini Venugopalan', None],\n",
       "   [u'Lisa Anne Hendricks', None],\n",
       "   [u'Raymond Mooney', None],\n",
       "   [u'Kate Saenko', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01729v1',\n",
       "  'published': u'2016-04-06T19:01:28Z',\n",
       "  'summary': u'  This paper investigates how linguistic knowledge mined from large text\\ncorpora can aid the generation of natural language descriptions of videos.\\nSpecifically, we integrate both a neural language model and distributional\\nsemantics trained on large text corpora into a recent LSTM-based architecture\\nfor video description. We evaluate our approach on a collection of Youtube\\nvideos as well as two large movie description datasets showing significant\\nimprovements in grammaticality while maintaining or modestly improving\\ndescriptive quality. Further, we show that such techniques can be beneficial\\nfor describing unseen object classes with no paired training data (zeroshot\\ncaptioning).\\n',\n",
       "  'title': u'Improving LSTM-based Video Description with Linguistic Knowledge Mined\\n  from Text'},\n",
       " u'1506.08230': {'arxivid': u'1506.08230',\n",
       "  'authorsaffil': [[u'Mark Tygert', None],\n",
       "   [u'Arthur Szlam', None],\n",
       "   [u'Soumith Chintala', None],\n",
       "   [u\"Marc'Aurelio Ranzato\", None],\n",
       "   [u'Yuandong Tian', None],\n",
       "   [u'Wojciech Zaremba', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'12 pages, 6 figures, 4 tables',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.08230v4',\n",
       "  'published': u'2015-06-26T22:22:21Z',\n",
       "  'summary': u'  The conventional classification schemes -- notably multinomial logistic\\nregression -- used in conjunction with convolutional networks (convnets) are\\nclassical in statistics, designed without consideration for the usual coupling\\nwith convnets, stochastic gradient descent, and backpropagation. In the\\nspecific application to supervised learning for convnets, a simple\\nscale-invariant classification stage turns out to be more robust than\\nmultinomial logistic regression, appears to result in slightly lower errors on\\nseveral standard test sets, has similar computational costs, and features\\nprecise control over the actual rate of learning. \"Scale-invariant\" means that\\nmultiplying the input values by any nonzero scalar leaves the output unchanged.\\n',\n",
       "  'title': u'Convolutional networks and learning invariant to homogeneous\\n  multiplicative scalings'},\n",
       " u'1511.04707': {'arxivid': u'1511.04707',\n",
       "  'authorsaffil': [[u'Matthias Dorfer', None],\n",
       "   [u'Rainer Kelz', None],\n",
       "   [u'Gerhard Widmer', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Published as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04707v5',\n",
       "  'published': u'2015-11-15T14:33:26Z',\n",
       "  'summary': u'  We introduce Deep Linear Discriminant Analysis (DeepLDA) which learns\\nlinearly separable latent representations in an end-to-end fashion. Classic LDA\\nextracts features which preserve class separability and is used for\\ndimensionality reduction for many classification problems. The central idea of\\nthis paper is to put LDA on top of a deep neural network. This can be seen as a\\nnon-linear extension of classic LDA. Instead of maximizing the likelihood of\\ntarget labels for individual samples, we propose an objective function that\\npushes the network to produce feature distributions which: (a) have low\\nvariance within the same class and (b) high variance between different classes.\\nOur objective is derived from the general LDA eigenvalue problem and still\\nallows to train with stochastic gradient descent and back-propagation. For\\nevaluation we test our approach on three different benchmark datasets (MNIST,\\nCIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST and\\nCIFAR-10 and outperforms a network trained with categorical cross entropy (same\\narchitecture) on a supervised setting of STL-10.\\n',\n",
       "  'title': u'Deep Linear Discriminant Analysis'},\n",
       " u'1603.01840': {'arxivid': u'1603.01840',\n",
       "  'authorsaffil': [[u'Gal Dalal', None],\n",
       "   [u'Elad Gilboa', None],\n",
       "   [u'Shie Mannor', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'stat.AP'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01840v1',\n",
       "  'published': u'2016-03-06T16:30:34Z',\n",
       "  'summary': u'  The power grid is a complex and vital system that necessitates careful\\nreliability management. Managing the grid is a difficult problem with multiple\\ntime scales of decision making and stochastic behavior due to renewable energy\\ngenerations, variable demand and unplanned outages. Solving this problem in the\\nface of uncertainty requires a new methodology with tractable algorithms. In\\nthis work, we introduce a new model for hierarchical decision making in complex\\nsystems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a\\nlevel of abstraction, for real-time power grid reliability. We devise an\\nalgorithm that alternates between slow time-scale policy improvement, and fast\\ntime-scale value function approximation. We compare our results to prevailing\\nheuristics, and show the strength of our method.\\n',\n",
       "  'title': u'Hierarchical Decision Making In Electricity Grid Management'},\n",
       " u'1603.01842': {'arxivid': u'1603.01842',\n",
       "  'authorsaffil': [[u'Enoch A-iyeh', None], [u'James F. Peters', None]],\n",
       "  'categoryterms': [u'cs.CV', u'54E40'],\n",
       "  'comment': u'9 pages, 6 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01842v1',\n",
       "  'published': u'2016-03-06T16:39:34Z',\n",
       "  'summary': u'  The focus of this article is on the detection and classification of patterns\\nbased on groupoids. The approach hinges on descriptive proximity of points in a\\nset based on the neighborliness property. This approach lends support to image\\nanalysis and understanding and in studying nearness of image segments. A\\npractical application of the approach is in terms of the analysis of natural\\nimages for pattern identification and classification.\\n',\n",
       "  'title': u'Proximal groupoid patterns In digital images'},\n",
       " u'1512.07748': {'arxivid': u'1512.07748',\n",
       "  'authorsaffil': [[u'Tomohiko Nakamura', None],\n",
       "   [u'Eita Nakamura', None],\n",
       "   [u'Shigeki Sagayama', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.LG', u'cs.MM'],\n",
       "  'comment': u'12 pages, 8 figures, version accepted in IEEE/ACM Transactions on\\n  Audio, Speech, and Language Processing',\n",
       "  'doi': u'10.1109/TASLP.2015.2507862',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07748v1',\n",
       "  'published': u'2015-12-24T08:21:48Z',\n",
       "  'summary': u'  This paper discusses real-time alignment of audio signals of music\\nperformance to the corresponding score (a.k.a. score following) which can\\nhandle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)\\nin performances. This type of score following is particularly useful in\\nautomatic accompaniment for practices and rehearsals, where errors and\\nrepeats/skips are often made. Simple extensions of the algorithms previously\\nproposed in the literature are not applicable in these situations for scores of\\npractical length due to the problem of large computational complexity. To cope\\nwith this problem, we present two hidden Markov models of monophonic\\nperformance with errors and arbitrary repeats/skips, and derive efficient\\nscore-following algorithms with an assumption that the prior probability\\ndistributions of score positions before and after repeats/skips are independent\\nfrom each other. We confirmed real-time operation of the algorithms with music\\nscores of practical length (around 10000 notes) on a modern laptop and their\\ntracking ability to the input performance within 0.7 s on average after\\nrepeats/skips in clarinet performance data. Further improvements and extension\\nfor polyphonic signals are also discussed.\\n',\n",
       "  'title': u'Real-Time Audio-to-Score Alignment of Music Performances Containing\\n  Errors and Arbitrary Repeats and Skips'},\n",
       " u'1511.06964': {'arxivid': u'1511.06964',\n",
       "  'authorsaffil': [[u'Alexander G. Ororbia II', None],\n",
       "   [u'C. Lee Giles', None],\n",
       "   [u'David Reitter', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06964v7',\n",
       "  'published': u'2015-11-22T04:53:43Z',\n",
       "  'summary': u'  Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine and\\nthe Deep Hybrid Denoising Auto-encoder, are proposed for handling\\nsemi-supervised learning problems. The models combine experts that model\\nrelevant distributions at different levels of abstraction to improve overall\\npredictive performance on discriminative tasks. Theoretical motivations and\\nalgorithms for joint learning for each are presented. We apply the new models\\nto the domain of data-streams in work towards life-long learning. The proposed\\narchitectures show improved performance compared to a pseudo-labeled, drop-out\\nrectifier network.\\n',\n",
       "  'title': u'Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and\\n  Denoising Autoencoders'},\n",
       " u'1601.06615': {'arxivid': u'1601.06615',\n",
       "  'authorsaffil': [[u'Suraj Srinivas', None],\n",
       "   [u'Ravi Kiran Sarvadevabhatla', None],\n",
       "   [u'Konda Reddy Mopuri', None],\n",
       "   [u'Nikita Prabhu', None],\n",
       "   [u'Srinivas S S Kruthiventi', None],\n",
       "   [u'R. Venkatesh Babu', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.MM'],\n",
       "  'comment': u'Published in Frontiers in Robotics and AI (http://goo.gl/6691Bm)',\n",
       "  'doi': u'10.3389/frobt.2015.00036',\n",
       "  'journalref': u'Frontiers in Robotics and AI 2(36), January 2016',\n",
       "  'link': u'http://arxiv.org/abs/1601.06615v1',\n",
       "  'published': u'2016-01-25T14:25:07Z',\n",
       "  'summary': u'  Traditional architectures for solving computer vision problems and the degree\\nof success they enjoyed have been heavily reliant on hand-crafted features.\\nHowever, of late, deep learning techniques have offered a compelling\\nalternative -- that of automatically learning problem-specific features. With\\nthis new paradigm, every problem in computer vision is now being re-examined\\nfrom a deep learning perspective. Therefore, it has become important to\\nunderstand what kind of deep networks are suitable for a given problem.\\nAlthough general surveys of this fast-moving paradigm (i.e. deep-networks)\\nexist, a survey specific to computer vision is missing. We specifically\\nconsider one form of deep networks widely used in computer vision -\\nconvolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN\\nand then examine the broad variations proposed over time to suit different\\napplications. We hope that our recipe-style survey will serve as a guide,\\nparticularly for novice practitioners intending to use deep-learning techniques\\nfor computer vision.\\n',\n",
       "  'title': u'A Taxonomy of Deep Convolutional Neural Nets for Computer Vision'},\n",
       " u'1602.08332': {'arxivid': u'1602.08332',\n",
       "  'authorsaffil': [[u'Felix Leibfried', None],\n",
       "   [u'Daniel Alexander Braun', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'Proceedings of the 32nd Conference on Uncertainty in Artificial\\n  Intelligence (UAI), New York City, NY, USA, 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08332v2',\n",
       "  'published': u'2016-02-26T14:15:03Z',\n",
       "  'summary': u'  Bounded rational decision-makers transform sensory input into motor output\\nunder limited computational resources. Mathematically, such decision-makers can\\nbe modeled as information-theoretic channels with limited transmission rate.\\nHere, we apply this formalism for the first time to multilayer feedforward\\nneural networks. We derive synaptic weight update rules for two scenarios,\\nwhere either each neuron is considered as a bounded rational decision-maker or\\nthe network as a whole. In the update rules, bounded rationality translates\\ninto information-theoretically motivated types of regularization in weight\\nspace. In experiments on the MNIST benchmark classification task for\\nhandwritten digits, we show that such information-theoretic regularization\\nsuccessfully prevents overfitting across different architectures and attains\\nresults that are competitive with other recent techniques like dropout,\\ndropconnect and Bayes by backprop, for both ordinary and convolutional neural\\nnetworks.\\n',\n",
       "  'title': u'Bounded Rational Decision-Making in Feedforward Neural Networks'},\n",
       " u'1604.00100': {'arxivid': u'1604.00100',\n",
       "  'authorsaffil': [[u'Kushal Arora', None], [u'Anand Rangarajan', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'submitted to ACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00100v1',\n",
       "  'published': u'2016-04-01T01:51:34Z',\n",
       "  'summary': u'  Traditional language models treat language as a finite state automaton on a\\nprobability space over words. This is a very strong assumption when modeling\\nsomething inherently complex such as language. In this paper, we challenge this\\nby showing how the linear chain assumption inherent in previous work can be\\ntranslated into a sequential composition tree. We then propose a new model that\\nmarginalizes over all possible composition trees thereby removing any\\nunderlying structural assumptions. As the partition function of this new model\\nis intractable, we use a recently proposed sentence level evaluation metric\\nContrastive Entropy to evaluate our model. Given this new evaluation metric, we\\nreport more than 100% improvement across distortion levels over current state\\nof the art recurrent neural network based language models.\\n',\n",
       "  'title': u'A Compositional Approach to Language Modeling'},\n",
       " u'1603.03610': {'arxivid': u'1603.03610',\n",
       "  'authorsaffil': [[u'Mark-Jan Nederhof', None]],\n",
       "  'categoryterms': [u'cs.FL', u'cs.CL', u'68T50', u'I.2.7; F.4.2'],\n",
       "  'comment': u'9 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03610v1',\n",
       "  'published': u'2016-03-11T12:32:29Z',\n",
       "  'summary': u'  We present a new proof that $O_2$ is a multiple context-free language. It\\ncontrasts with a recent proof by Salvati (2015) in its avoidance of concepts\\nthat seem specific to two-dimensional geometry, such as the complex exponential\\nfunction. Our simple proof creates realistic prospects of widening the results\\nto higher dimensions. This finding is of central importance to the relation\\nbetween extreme free word order and classes of grammars used to describe the\\nsyntax of natural language.\\n',\n",
       "  'title': u'A short proof that $O_2$ is an MCFL'},\n",
       " u'1603.08594': {'arxivid': u'1603.08594',\n",
       "  'authorsaffil': [[u'Geetanjali Rakshit', None],\n",
       "   [u'Sagar Sontakke', None],\n",
       "   [u'Pushpak Bhattacharyya', None],\n",
       "   [u'Gholamreza Haffari', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08594v1',\n",
       "  'published': u'2016-03-29T00:06:11Z',\n",
       "  'summary': u'  In this paper, we attempt to solve the problem of Prepositional Phrase (PP)\\nattachments in English. The motivation for the work comes from NLP applications\\nlike Machine Translation, for which, getting the correct attachment of\\nprepositions is very crucial. The idea is to correct the PP-attachments for a\\nsentence with the help of alignments from parallel data in another language.\\nThe novelty of our work lies in the formulation of the problem into a dual\\ndecomposition based algorithm that enforces agreement between the parse trees\\nfrom two languages as a constraint. Experiments were performed on the\\nEnglish-Hindi language pair and the performance improved by 10% over the\\nbaseline, where the baseline is the attachment predicted by the MSTParser model\\ntrained for English.\\n',\n",
       "  'title': u'Prepositional Attachment Disambiguation Using Bilingual Parsing and\\n  Alignments'},\n",
       " u'1406.1089': {'arxivid': u'1406.1089',\n",
       "  'authorsaffil': [[u'Aleksandr Aravkin', None],\n",
       "   [u'Stephen Becker', None],\n",
       "   [u'Volkan Cevher', None],\n",
       "   [u'Peder Olsen', None]],\n",
       "  'categoryterms': [u'math.OC',\n",
       "   u'stat.ML',\n",
       "   u'90C06, 81P50, 65K10, 62F35, 47N30'],\n",
       "  'comment': u'10 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1406.1089v1',\n",
       "  'published': u'2014-06-04T16:10:49Z',\n",
       "  'summary': u'  We introduce a new convex formulation for stable principal component pursuit\\n(SPCP) to decompose noisy signals into low-rank and sparse representations. For\\nnumerical solutions of our SPCP formulation, we first develop a convex\\nvariational framework and then accelerate it with quasi-Newton methods. We\\nshow, via synthetic and real data experiments, that our approach offers\\nadvantages over the classical SPCP formulations in scalability and practical\\nparameter selection.\\n',\n",
       "  'title': u'A variational approach to stable principal component pursuit'},\n",
       " u'1603.08597': {'arxivid': u'1603.08597',\n",
       "  'authorsaffil': [[u'Chen-Hsuan Lin', None],\n",
       "   [u'Rui Zhu', None],\n",
       "   [u'Simon Lucey', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'17 pages, 11 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08597v1',\n",
       "  'published': u'2016-03-29T00:34:07Z',\n",
       "  'summary': u'  The Lucas & Kanade (LK) algorithm is the method of choice for efficient dense\\nimage and object alignment. The approach is efficient as it attempts to model\\nthe connection between appearance and geometric displacement through a linear\\nrelationship that assumes independence across pixel coordinates. A drawback of\\nthe approach, however, is its generative nature. Specifically, its performance\\nis tightly coupled with how well the linear model can synthesize appearance\\nfrom geometric displacement, even though the alignment task itself is\\nassociated with the inverse problem. In this paper, we present a new approach,\\nreferred to as the Conditional LK algorithm, which: (i) directly learns linear\\nmodels that predict geometric displacement as a function of appearance, and\\n(ii) employs a novel strategy for ensuring that the generative pixel\\nindependence assumption can still be taken advantage of. We demonstrate that\\nour approach exhibits superior performance to classical generative forms of the\\nLK algorithm. Furthermore, we demonstrate its comparable performance to\\nstate-of-the-art methods such as the Supervised Descent Method with\\nsubstantially less training examples, as well as the unique ability to \"swap\"\\ngeometric warp functions without having to retrain from scratch. Finally, from\\na theoretical perspective, our approach hints at possible redundancies that\\nexist in current state-of-the-art methods for alignment that could be leveraged\\nin vision systems of the future.\\n',\n",
       "  'title': u'The Conditional Lucas & Kanade Algorithm'},\n",
       " u'1603.08262': {'arxivid': u'1603.08262',\n",
       "  'authorsaffil': [[u'Kamil Rocki', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'10 pages, submitted to AGI-16. arXiv admin note: substantial text\\n  overlap with arXiv:1512.01926',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08262v1',\n",
       "  'published': u'2016-03-27T22:01:59Z',\n",
       "  'summary': u'  There exists a theory of a single general-purpose learning algorithm which\\ncould explain the principles of its operation. This theory assumes that the\\nbrain has some initial rough architecture, a small library of simple innate\\ncircuits which are prewired at birth and proposes that all significant mental\\nalgorithms can be learned. Given current understanding and observations, this\\npaper reviews and lists the ingredients of such an algorithm from both\\narchitectural and functional perspectives.\\n',\n",
       "  'title': u'Towards Machine Intelligence'},\n",
       " u'1603.08592': {'arxivid': u'1603.08592',\n",
       "  'authorsaffil': [[u'Bor-Jeng Chen', None], [u'Gerard Medioni', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08592v1',\n",
       "  'published': u'2016-03-28T23:47:25Z',\n",
       "  'summary': u'  Tracking many vehicles in wide coverage aerial imagery is crucial for\\nunderstanding events in a large field of view. Most approaches aim to associate\\ndetections from frame differencing into tracks. However, slow or stopped\\nvehicles result in long-term missing detections and further cause tracking\\ndiscontinuities. Relying merely on appearance clue to recover missing\\ndetections is difficult as targets are extremely small and in grayscale. In\\nthis paper, we address the limitations of detection association methods by\\ncoupling it with a local context tracker (LCT), which does not rely on motion\\ndetections. On one hand, our LCT learns neighboring spatial relation and tracks\\neach target in consecutive frames using graph optimization. It takes the\\nadvantage of context constraints to avoid drifting to nearby targets. We\\ngenerate hypotheses from sparse and dense flow efficiently to keep solutions\\ntractable. On the other hand, we use detection association strategy to extract\\nshort tracks in batch processing. We explicitly handle merged detections by\\ngenerating additional hypotheses from them. Our evaluation on wide area aerial\\nimagery sequences shows significant improvement over state-of-the-art methods.\\n',\n",
       "  'title': u'Exploring Local Context for Multi-target Tracking in Wide Area Aerial\\n  Surveillance'},\n",
       " u'1601.01343': {'arxivid': u'1601.01343',\n",
       "  'authorsaffil': [[u'Ikuya Yamada', None],\n",
       "   [u'Hiroyuki Shindo', None],\n",
       "   [u'Hideaki Takeda', None],\n",
       "   [u'Yoshiyasu Takefuji', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Accepted at CoNLL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01343v4',\n",
       "  'published': u'2016-01-06T22:19:20Z',\n",
       "  'summary': u'  Named Entity Disambiguation (NED) refers to the task of resolving multiple\\nnamed entity mentions in a document to their correct references in a knowledge\\nbase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method\\nspecifically designed for NED. The proposed method jointly maps words and\\nentities into the same continuous vector space. We extend the skip-gram model\\nby using two models. The KB graph model learns the relatedness of entities\\nusing the link structure of the KB, whereas the anchor context model aims to\\nalign vectors such that similar words and entities occur close to one another\\nin the vector space by leveraging KB anchors and their context words. By\\ncombining contexts based on the proposed embedding with standard NED features,\\nwe achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset\\nand 85.2% on the TAC 2010 dataset.\\n',\n",
       "  'title': u'Joint Learning of the Embedding of Words and Entities for Named Entity\\n  Disambiguation'},\n",
       " u'1603.07442': {'arxivid': u'1603.07442',\n",
       "  'authorsaffil': [[u'Donggeun Yoo', None],\n",
       "   [u'Namil Kim', None],\n",
       "   [u'Sunggyun Park', None],\n",
       "   [u'Anthony S. Paek', None],\n",
       "   [u'In So Kweon', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI'],\n",
       "  'comment': u'The code and the dataset will be available soon',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07442v1',\n",
       "  'published': u'2016-03-24T05:20:59Z',\n",
       "  'summary': u'  We present an image-conditional image generation model. The model transfers\\nan input domain to a target domain in semantic level, and generates the target\\nimage in pixel level. To generate realistic target images, we employ the\\nreal/fake-discriminator in Generative Adversarial Nets, but also introduce a\\nnovel domain-discriminator to make the generated image relevant to the input\\nimage. We verify our model through a challenging task of generating a piece of\\nclothing from an input image of a dressed person. We present a high quality\\nclothing dataset containing the two domains, and succeed in demonstrating\\ndecent results.\\n',\n",
       "  'title': u'Pixel-Level Domain Transfer'},\n",
       " u'1511.07386': {'arxivid': u'1511.07386',\n",
       "  'authorsaffil': [[u'Iasonas Kokkinos', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'The previous version reported large improvements w.r.t. the LPO\\n  region proposal baseline, which turned out to be due to a wrong computation\\n  for the baseline. The improvements are currently less important, and are\\n  omitted. We are sorry if the reported results caused any confusion. We have\\n  also integrated reviewer feedback regarding human performance on the BSD\\n  benchmark',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07386v2',\n",
       "  'published': u'2015-11-23T19:54:09Z',\n",
       "  'summary': u'  In this work we show that adapting Deep Convolutional Neural Network training\\nto the task of boundary detection can result in substantial improvements over\\nthe current state-of-the-art in boundary detection.\\n  Our contributions consist firstly in combining a careful design of the loss\\nfor boundary detection training, a multi-resolution architecture and training\\nwith external data to improve the detection accuracy of the current state of\\nthe art. When measured on the standard Berkeley Segmentation Dataset, we\\nimprove theoptimal dataset scale F-measure from 0.780 to 0.808 - while human\\nperformance is at 0.803. We further improve performance to 0.813 by combining\\ndeep learning with grouping, integrating the Normalized Cuts technique within a\\ndeep network.\\n  We also examine the potential of our boundary detector in conjunction with\\nthe task of semantic segmentation and demonstrate clear improvements over\\nstate-of-the-art systems. Our detector is fully integrated in the popular Caffe\\nframework and processes a 320x420 image in less than a second.\\n',\n",
       "  'title': u'Pushing the Boundaries of Boundary Detection using Deep Learning'},\n",
       " u'1603.09620': {'arxivid': u'1603.09620',\n",
       "  'authorsaffil': [[u'Laurens Bliek', None],\n",
       "   [u'Hans R. G. W. Verstraete', None],\n",
       "   [u'Michel Verhaegen', None],\n",
       "   [u'Sander Wahls', None]],\n",
       "  'categoryterms': [u'cs.LG', u'math.OC', u'stat.ML'],\n",
       "  'comment': u'15 pages, 4 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09620v1',\n",
       "  'published': u'2016-03-31T15:00:06Z',\n",
       "  'summary': u'  This paper analyzes DONE, an online optimization algorithm that iteratively\\nminimizes an unknown function with costly and noisy measurements. The algorithm\\nmaintains a surrogate of the unknown function in the form of a random Fourier\\nexpansion (RFE). The surrogate is updated whenever a new measurement is\\navailable, and then used to determine the next measurement point. The algorithm\\nis comparable to Bayesian optimization algorithms, but its computational\\ncomplexity per iteration does not depend on the number of measurements. We\\nderive several theoretical results that provide insight on how the\\nhyperparameters of the algorithm should be chosen. The algorithm is compared to\\na Bayesian optimization algorithm for a benchmark problem and two optics\\napplications, namely, optical coherence tomography and optical beam-forming\\nnetwork tuning. It is found that the DONE algorithm is significantly faster\\nthan Bayesian optimization in all three discussed problems, while keeping a\\nsimilar or better performance.\\n',\n",
       "  'title': u'Online Optimization with Costly and Noisy Measurements using Random\\n  Fourier Expansions'},\n",
       " u'1511.06709': {'arxivid': u'1511.06709',\n",
       "  'authorsaffil': [[u'Rico Sennrich', None],\n",
       "   [u'Barry Haddow', None],\n",
       "   [u'Alexandra Birch', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'accepted to ACL 2016; new section on effect of back-translation\\n  quality',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06709v4',\n",
       "  'published': u'2015-11-20T17:58:37Z',\n",
       "  'summary': u'  Neural Machine Translation (NMT) has obtained state-of-the art performance\\nfor several language pairs, while only using parallel data for training.\\nTarget-side monolingual data plays an important role in boosting fluency for\\nphrase-based statistical machine translation, and we investigate the use of\\nmonolingual data for NMT. In contrast to previous work, which combines NMT\\nmodels with separately trained language models, we note that encoder-decoder\\nNMT architectures already have the capacity to learn the same information as a\\nlanguage model, and we explore strategies to train with monolingual data\\nwithout changing the neural network architecture. By pairing monolingual\\ntraining data with an automatic back-translation, we can treat it as additional\\nparallel training data, and we obtain substantial improvements on the WMT 15\\ntask English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task\\nTurkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We\\nalso show that fine-tuning on in-domain monolingual and parallel data gives\\nsubstantial improvements for the IWSLT 15 task English->German.\\n',\n",
       "  'title': u'Improving Neural Machine Translation Models with Monolingual Data'},\n",
       " u'1601.05675': {'arxivid': u'1601.05675',\n",
       "  'authorsaffil': [[u'Daniele Calandriello', None],\n",
       "   [u'Alessandro Lazaric', None],\n",
       "   [u'Michal Valko', None],\n",
       "   [u'Ioannis Koutis', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05675v1',\n",
       "  'published': u'2016-01-21T15:31:35Z',\n",
       "  'summary': u'  While the harmonic function solution performs well in many semi-supervised\\nlearning (SSL) tasks, it is known to scale poorly with the number of samples.\\nRecent successful and scalable methods, such as the eigenfunction method focus\\non efficiently approximating the whole spectrum of the graph Laplacian\\nconstructed from the data. This is in contrast to various subsampling and\\nquantization methods proposed in the past, which may fail in preserving the\\ngraph spectra. However, the impact of the approximation of the spectrum on the\\nfinal generalization error is either unknown, or requires strong assumptions on\\nthe data. In this paper, we introduce Sparse-HFS, an efficient\\nedge-sparsification algorithm for SSL. By constructing an edge-sparse and\\nspectrally similar graph, we are able to leverage the approximation guarantees\\nof spectral sparsification methods to bound the generalization error of\\nSparse-HFS. As a result, we obtain a theoretically-grounded approximation\\nscheme for graph-based SSL that also empirically matches the performance of\\nknown large-scale methods.\\n',\n",
       "  'title': u'Incremental Spectral Sparsification for Large-Scale Graph-Based\\n  Semi-Supervised Learning'},\n",
       " u'1507.00814': {'arxivid': u'1507.00814',\n",
       "  'authorsaffil': [[u'Bradly C. Stadie', None],\n",
       "   [u'Sergey Levine', None],\n",
       "   [u'Pieter Abbeel', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.00814v3',\n",
       "  'published': u'2015-07-03T04:11:15Z',\n",
       "  'summary': u'  Achieving efficient and scalable exploration in complex domains poses a major\\nchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches to\\nthe exploration problem offer strong formal guarantees, they are often\\nimpractical in higher dimensions due to their reliance on enumerating the\\nstate-action space. Hence, exploration in complex domains is often performed\\nwith simple epsilon-greedy methods. In this paper, we consider the challenging\\nAtari games domain, which requires processing raw pixel inputs and delayed\\nrewards. We evaluate several more sophisticated exploration strategies,\\nincluding Thompson sampling and Boltzman exploration, and propose a new\\nexploration method based on assigning exploration bonuses from a concurrently\\nlearned model of the system dynamics. By parameterizing our learned model with\\na neural network, we are able to develop a scalable and efficient approach to\\nexploration bonuses that can be applied to tasks with complex, high-dimensional\\nstate spaces. In the Atari domain, our method provides the most consistent\\nimprovement across a range of games that pose a major challenge for prior\\nmethods. In addition to raw game-scores, we also develop an AUC-100 metric for\\nthe Atari Learning domain to evaluate the impact of exploration on this\\nbenchmark.\\n',\n",
       "  'title': u'Incentivizing Exploration In Reinforcement Learning With Deep Predictive\\n  Models'},\n",
       " u'1602.06709': {'arxivid': u'1602.06709',\n",
       "  'authorsaffil': [[u'Dipankar Das', None],\n",
       "   [u'Sasikanth Avancha', None],\n",
       "   [u'Dheevatsa Mudigere', None],\n",
       "   [u'Karthikeyan Vaidynathan', None],\n",
       "   [u'Srinivas Sridharan', None],\n",
       "   [u'Dhiraj Kalamkar', None],\n",
       "   [u'Bharat Kaul', None],\n",
       "   [u'Pradeep Dubey', None]],\n",
       "  'categoryterms': [u'cs.DC', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06709v1',\n",
       "  'published': u'2016-02-22T10:31:24Z',\n",
       "  'summary': u'  We design and implement a distributed multinode synchronous SGD algorithm,\\nwithout altering hyper parameters, or compressing data, or altering algorithmic\\nbehavior. We perform a detailed analysis of scaling, and identify optimal\\ndesign points for different networks. We demonstrate scaling of CNNs on 100s of\\nnodes, and present what we believe to be record training throughputs. A 512\\nminibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatch\\nVGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64\\nnode cluster. We also demonstrate the generality of our approach via\\nbest-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attempt\\nto democratize deep-learning by training on an Ethernet based AWS cluster and\\nshow ~14X scaling on 16 nodes.\\n',\n",
       "  'title': u'Distributed Deep Learning Using Synchronous Stochastic Gradient Descent'},\n",
       " u'1603.04150': {'arxivid': u'1603.04150',\n",
       "  'authorsaffil': [[u'Sheng Huang', None],\n",
       "   [u'Dan Yang', None],\n",
       "   [u'Bo Liu', None],\n",
       "   [u'Xiaohong Zhang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'11pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04150v1',\n",
       "  'published': u'2016-03-14T06:54:39Z',\n",
       "  'summary': u'  Inspired by the recently remarkable successes of Sparse Representation (SR),\\nCollaborative Representation (CR) and sparse graph, we present a novel\\nhypergraph model named Regression-based Hypergraph (RH) which utilizes the\\nregression models to construct the high quality hypergraphs. Moreover, we plug\\nRH into two conventional hypergraph learning frameworks, namely hypergraph\\nspectral clustering and hypergraph transduction, to present Regression-based\\nHypergraph Spectral Clustering (RHSC) and Regression-based Hypergraph\\nTransduction (RHT) models for addressing the image clustering and\\nclassification issues. Sparse Representation and Collaborative Representation\\nare employed to instantiate two RH instances and their RHSC and RHT algorithms.\\nThe experimental results on six popular image databases demonstrate that the\\nproposed RH learning algorithms achieve promising image clustering and\\nclassification performances, and also validate that RH can inherit the\\ndesirable properties from both hypergraph models and regression models.\\n',\n",
       "  'title': u'Regression-based Hypergraph Learning for Image Clustering and\\n  Classification'},\n",
       " u'1603.04153': {'arxivid': u'1603.04153',\n",
       "  'authorsaffil': [[u'Minje Jang', None],\n",
       "   [u'Sunghyun Kim', None],\n",
       "   [u'Changho Suh', None],\n",
       "   [u'Sewoong Oh', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IT', u'cs.SI', u'math.IT', u'stat.ML'],\n",
       "  'comment': u'23 pages, 3 figures, submitted to the Journals of Machine Learning\\n  Research',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04153v1',\n",
       "  'published': u'2016-03-14T07:01:28Z',\n",
       "  'summary': u'  We explore the top-$K$ rank aggregation problem. Suppose a collection of\\nitems is compared in pairs repeatedly, and we aim to recover a consistent\\nordering that focuses on the top-$K$ ranked items based on partially revealed\\npreference information. We investigate the Bradley-Terry-Luce model in which\\none ranks items according to their perceived utilities modeled as noisy\\nobservations of their underlying true utilities. Our main contributions are\\ntwo-fold. First, in a general comparison model where item pairs to compare are\\ngiven a priori, we attain an upper and lower bound on the sample size for\\nreliable recovery of the top-$K$ ranked items. Second, more importantly,\\nextending the result to a random comparison model where item pairs to compare\\nare chosen independently with some probability, we show that in slightly\\nrestricted regimes, the gap between the derived bounds reduces to a constant\\nfactor, hence reveals that a spectral method can achieve the minimax optimality\\non the (order-wise) sample size required for top-$K$ ranking. That is to say,\\nwe demonstrate a spectral method alone to be sufficient to achieve the\\noptimality and advantageous in terms of computational complexity, as it does\\nnot require an additional stage of maximum likelihood estimation that a\\nstate-of-the-art scheme employs to achieve the optimality. We corroborate our\\nmain results by numerical experiments.\\n',\n",
       "  'title': u'Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is\\n  Optimal'},\n",
       " u'1511.06499': {'arxivid': u'1511.06499',\n",
       "  'authorsaffil': [[u'Dustin Tran', None],\n",
       "   [u'Rajesh Ranganath', None],\n",
       "   [u'David M. Blei', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.NE', u'stat.CO'],\n",
       "  'comment': u'Appears in International Conference on Learning Representations, 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06499v4',\n",
       "  'published': u'2015-11-20T06:01:23Z',\n",
       "  'summary': u'  Variational inference is a powerful tool for approximate inference, and it\\nhas been recently applied for representation learning with deep generative\\nmodels. We develop the variational Gaussian process (VGP), a Bayesian\\nnonparametric variational family, which adapts its shape to match complex\\nposterior distributions. The VGP generates approximate posterior samples by\\ngenerating latent inputs and warping them through random non-linear mappings;\\nthe distribution over random mappings is learned during inference, enabling the\\ntransformed outputs to adapt to varying complexity. We prove a universal\\napproximation theorem for the VGP, demonstrating its representative power for\\nlearning any model. For inference we present a variational objective inspired\\nby auto-encoders and perform black box inference over a wide class of models.\\nThe VGP achieves new state-of-the-art results for unsupervised learning,\\ninferring models such as the deep latent Gaussian model and the recently\\nproposed DRAW.\\n',\n",
       "  'title': u'The Variational Gaussian Process'},\n",
       " u'1604.01348': {'arxivid': u'1604.01348',\n",
       "  'authorsaffil': [[u'Kenji Kawaguchi', None],\n",
       "   [u'Leslie Pack Kaelbling', None],\n",
       "   [u'Tom\\xe1s Lozano-P\\xe9rez', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'In NIPS 2015 (Advances in Neural Information Processing Systems 2015)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01348v1',\n",
       "  'published': u'2016-04-05T17:53:59Z',\n",
       "  'summary': u'  This paper presents a Bayesian optimization method with exponential\\nconvergence without the need of auxiliary optimization and without the\\ndelta-cover sampling. Most Bayesian optimization methods require auxiliary\\noptimization: an additional non-convex global optimization problem, which can\\nbe time-consuming and hard to implement in practice. Also, the existing\\nBayesian optimization method with exponential convergence requires access to\\nthe delta-cover sampling, which was considered to be impractical. Our approach\\neliminates both requirements and achieves an exponential convergence rate.\\n',\n",
       "  'title': u'Bayesian Optimization with Exponential Convergence'},\n",
       " u'1603.08868': {'arxivid': u'1603.08868',\n",
       "  'authorsaffil': [[u'Ildik\\xf3 Pil\\xe1n', None],\n",
       "   [u'Sowmya Vajjala', None],\n",
       "   [u'Elena Volodina', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Presented at CICLING 2015 and won the best poster award (16th\\n  International Conference on Intelligent Text Processing and Computational\\n  Linguistics). To appear in International Journal of Computational Linguistics\\n  and Applications (IJLCA), 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08868v1',\n",
       "  'published': u'2016-03-29T18:12:28Z',\n",
       "  'summary': u'  Corpora and web texts can become a rich language learning resource if we have\\na means of assessing whether they are linguistically appropriate for learners\\nat a given proficiency level. In this paper, we aim at addressing this issue by\\npresenting the first approach for predicting linguistic complexity for Swedish\\nsecond language learning material on a 5-point scale. After showing that the\\ntraditional Swedish readability measure, L\\\\\"asbarhetsindex (LIX), is not\\nsuitable for this task, we propose a supervised machine learning model, based\\non a range of linguistic features, that can reliably classify texts according\\nto their difficulty level. Our model obtained an accuracy of 81.3% and an\\nF-score of 0.8, which is comparable to the state of the art in English and is\\nconsiderably higher than previously reported results for other languages. We\\nfurther studied the utility of our features with single sentences instead of\\nfull texts since sentences are a common linguistic unit in language learning\\nexercises. We trained a separate model on sentence-level data with five\\nclasses, which yielded 63.4% accuracy. Although this is lower than the document\\nlevel performance, we achieved an adjacent accuracy of 92%. Furthermore, we\\nfound that using a combination of different features, compared to using lexical\\nfeatures alone, resulted in 7% improvement in classification accuracy at the\\nsentence level, whereas at the document level, lexical features were more\\ndominant. Our models are intended for use in a freely accessible web-based\\nlanguage learning platform for the automatic generation of exercises.\\n',\n",
       "  'title': u'A Readable Read: Automatic Assessment of Language Learning Materials\\n  based on Linguistic Complexity'},\n",
       " u'1603.08869': {'arxivid': u'1603.08869',\n",
       "  'authorsaffil': [[u'Tiancheng Zhao', None], [u'Mohammad Gowayyed', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08869v1',\n",
       "  'published': u'2016-03-29T18:17:17Z',\n",
       "  'summary': u'  Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to\\nsolve large Markov Decision Processes (MDP) and provide transferable subtask\\npolicies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical\\nQ-value Iteration (HQI). We show that it is possible to effectively learn\\nrecursive optimal policies for any valid hierarchical decomposition of the\\noriginal MDP, given a fixed dataset collected from a flat stochastic behavioral\\npolicy. We first formally prove the convergence of the algorithm for tabular\\nMDP. Then our experiments on the Taxi domain show that HQI converges faster\\nthan a flat Q-value Iteration and enjoys easy state abstraction. Also, we\\ndemonstrate that our algorithm is able to learn optimal policies for different\\nhierarchical structures from the same fixed dataset, which enables model\\ncomparison without recollecting data.\\n',\n",
       "  'title': u'Algorithms for Batch Hierarchical Reinforcement Learning'},\n",
       " u'1603.04392': {'arxivid': u'1603.04392',\n",
       "  'authorsaffil': [[u'Joseph Paul Cohen', None],\n",
       "   [u'Wei Ding', None],\n",
       "   [u'Caitlin Kuhlman', None],\n",
       "   [u'Aijun Chen', None],\n",
       "   [u'Liping Di', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted to be published in Applied Intelligence 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04392v1',\n",
       "  'published': u'2016-03-14T19:03:53Z',\n",
       "  'summary': u'  This work describes algorithms for performing discrete object detection,\\nspecifically in the case of buildings, where usually only low quality RGB-only\\ngeospatial reflective imagery is available. We utilize new candidate search and\\nfeature extraction techniques to reduce the problem to a machine learning (ML)\\nclassification task. Here we can harness the complex patterns of contrast\\nfeatures contained in training data to establish a model of buildings. We avoid\\ncostly sliding windows to generate candidates; instead we innovatively stitch\\ntogether well known image processing techniques to produce candidates for\\nbuilding detection that cover 80-85% of buildings. Reducing the number of\\npossible candidates is important due to the scale of the problem. Each\\ncandidate is subjected to classification which, although linear, costs time and\\nprohibits large scale evaluation. We propose a candidate alignment algorithm to\\nboost classification performance to 80-90% precision with a linear time\\nalgorithm and show it has negligible cost. Also, we propose a new concept\\ncalled a Permutable Haar Mesh (PHM) which we use to form and traverse a search\\nspace to recover candidate buildings which were lost in the initial\\npreprocessing phase.\\n',\n",
       "  'title': u'Rapid building detection using machine learning'},\n",
       " u'1603.08865': {'arxivid': u'1603.08865',\n",
       "  'authorsaffil': [[u'Emil Axelsson', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08865v2',\n",
       "  'published': u'2016-03-29T17:58:46Z',\n",
       "  'summary': u'  This article is about an implementation and compilation technique that is\\nused in RAW-Feldspar which is a complete rewrite of the Feldspar embedded\\ndomain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-level\\nfunctional language that generates efficient C code to run on embedded targets.\\nThe gist of the technique presented in this post is the following: rather\\nwriting a back end that converts pure Feldspar expressions directly to C, we\\ntranslate them to a low-level monadic EDSL. From the low-level EDSL, C code is\\nthen generated. This approach has several advantages:\\n  1. The translation is simpler to write than a complete C back end.\\n  2. The translation is between two typed EDSLs, which rules out many potential\\nerrors.\\n  3. The low-level EDSL is reusable and can be shared between several\\nhigh-level EDSLs.\\n  Although the article contains a lot of code, most of it is in fact reusable.\\nAs mentioned in Discussion, we can write the same implementation in less than\\n50 lines of code using generic libraries that we have developed to support\\nFeldspar.\\n',\n",
       "  'title': u'Compilation as a Typed EDSL-to-EDSL Transformation'},\n",
       " u'1604.01345': {'arxivid': u'1604.01345',\n",
       "  'authorsaffil': [[u'Gabriel Schwartz', None], [u'Ko Nishino', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01345v1',\n",
       "  'published': u'2016-04-05T17:40:57Z',\n",
       "  'summary': u'  Perceptual material attributes, intrinsic visual properties of materials, are\\nbeing studied in parallel in computer vision and human vision. In neuroscience,\\nperceptual attributes are shown to be an integral part of the human neural\\nresponse during material recognition. In computer vision, however, they are\\nmerely an intermediate representation of materials and not integrated into the\\nrecognition process. In this paper, we show that perceptual material attributes\\ncan indeed be found inside a framework for local, patch-based,\\nobject-independent material recognition. We introduce a new CNN architecture,\\nthe material attribute-category CNN (MAC-CNN), that uses deep weak supervision\\nto simultaneously classify materials and discover per-pixel perceptual\\nattributes. We show that these attributes conform with past semantic material\\nattributes and enhance recognition of novel materials. We also introduce an\\nextensive new database for local material recognition. Our results show that\\nthe internal representation of the MAC-CNN generalizes well and agrees with\\nhuman perception, which has potential implications for our understanding of\\nhuman material perception as well as applications in object recognition.\\n',\n",
       "  'title': u'Discovering Perceptual Attributes in a Deep Local Material Recognition\\n  Network'},\n",
       " u'1603.08861': {'arxivid': u'1603.08861',\n",
       "  'authorsaffil': [[u'Zhilin Yang', None],\n",
       "   [u'William W. Cohen', None],\n",
       "   [u'Ruslan Salakhutdinov', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'ICML 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08861v2',\n",
       "  'published': u'2016-03-29T17:46:16Z',\n",
       "  'summary': u'  We present a semi-supervised learning framework based on graph embeddings.\\nGiven a graph between instances, we train an embedding for each instance to\\njointly predict the class label and the neighborhood context in the graph. We\\ndevelop both transductive and inductive variants of our method. In the\\ntransductive variant of our method, the class labels are determined by both the\\nlearned embeddings and input feature vectors, while in the inductive variant,\\nthe embeddings are defined as a parametric function of the feature vectors, so\\npredictions can be made on instances not seen during training. On a large and\\ndiverse set of benchmark tasks, including text classification, distantly\\nsupervised entity extraction, and entity classification, we show improved\\nperformance over many of the existing models.\\n',\n",
       "  'title': u'Revisiting Semi-Supervised Learning with Graph Embeddings'},\n",
       " u'1604.01347': {'arxivid': u'1604.01347',\n",
       "  'authorsaffil': [[u'Aayush Bansal', None],\n",
       "   [u'Bryan Russell', None],\n",
       "   [u'Abhinav Gupta', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01347v1',\n",
       "  'published': u'2016-04-05T17:51:39Z',\n",
       "  'summary': u'  We introduce an approach that leverages surface normal predictions, along\\nwith appearance cues, to retrieve 3D models for objects depicted in 2D still\\nimages from a large CAD object library. Critical to the success of our approach\\nis the ability to recover accurate surface normals for objects in the depicted\\nscene. We introduce a skip-network model built on the pre-trained Oxford VGG\\nconvolutional neural network (CNN) for surface normal prediction. Our model\\nachieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface\\nnormal prediction, and recovers fine object detail compared to previous\\nmethods. Furthermore, we develop a two-stream network over the input image and\\npredicted surface normals that jointly learns pose and style for CAD model\\nretrieval. When using the predicted surface normals, our two-stream network\\nmatches prior work using surface normals computed from RGB-D images on the task\\nof pose prediction, and achieves state of the art when using RGB-D input.\\nFinally, our two-stream network allows us to retrieve CAD models that better\\nmatch the style and pose of a depicted object compared with baseline\\napproaches.\\n',\n",
       "  'title': u'Marr Revisited: 2D-3D Alignment via Surface Normal Prediction'},\n",
       " u'1603.02740': {'arxivid': u'1603.02740',\n",
       "  'authorsaffil': [[u'Stephen Ragain', None], [u'Johan Ugander', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02740v2',\n",
       "  'published': u'2016-03-08T23:47:03Z',\n",
       "  'summary': u\"  As datasets capturing human choices grow in richness and scale---particularly\\nin online domains---there is an increasing need for choice models that escape\\ntraditional choice-theoretic axioms such as regularity, stochastic\\ntransitivity, and Luce's choice axiom. In this work we introduce the Pairwise\\nChoice Markov Chain (PCMC) model of discrete choice, an inferentially tractable\\nmodel that does not assume any of the above axioms while still satisfying the\\nfoundational axiom of uniform expansion, a considerably weaker assumption than\\nLuce's choice axiom. We show that the PCMC model significantly outperforms the\\nMultinomial Logit (MNL) model in prediction tasks on both synthetic and\\nempirical datasets known to exhibit violations of Luce's axiom. Our analysis\\nalso synthesizes several recent observations connecting the Multinomial Logit\\nmodel and Markov chains; the PCMC model retains the Multinomial Logit model as\\na special case.\\n\",\n",
       "  'title': u'Pairwise Choice Markov Chains'},\n",
       " u'1504.02763': {'arxivid': u'1504.02763',\n",
       "  'authorsaffil': [[u'Filipe Condessa', None],\n",
       "   [u'Jelena Kovacevic', None],\n",
       "   [u'Jose Bioucas-Dias', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'68-04'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.02763v2',\n",
       "  'published': u'2015-04-10T19:15:39Z',\n",
       "  'summary': u'  Classifiers with rejection are essential in real-world applications where\\nmisclassifications and their effects are critical. However, if no problem\\nspecific cost function is defined, there are no established measures to assess\\nthe performance of such classifiers. We introduce a set of desired properties\\nfor performance measures for classifiers with rejection, based on which we\\npropose a set of three performance measures for the evaluation of the\\nperformance of classifiers with rejection that satisfy the desired properties.\\nThe nonrejected accuracy measures the ability of the classifier to accurately\\nclassify nonrejected samples; the classification quality measures the correct\\ndecision making of the classifier with rejector; and the rejection quality\\nmeasures the ability to concentrate all misclassified samples onto the set of\\nrejected samples. From the measures, we derive the concept of relative\\noptimality that allows us to connect the measures to a family of cost functions\\nthat take into account the trade-off between rejection and misclassification.\\nWe illustrate the use of the proposed performance measures on classifiers with\\nrejection applied to synthetic and real-world data.\\n',\n",
       "  'title': u'Performance measures for classification systems with rejection'},\n",
       " u'1509.06825': {'arxivid': u'1509.06825',\n",
       "  'authorsaffil': [[u'Lerrel Pinto', None], [u'Abhinav Gupta', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'cs.RO'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.06825v1',\n",
       "  'published': u'2015-09-23T02:08:02Z',\n",
       "  'summary': u'  Current learning-based robot grasping approaches exploit human-labeled\\ndatasets for training the models. However, there are two problems with such a\\nmethodology: (a) since each object can be grasped in multiple ways, manually\\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\\nsemantics. While there have been attempts to train robots using trial-and-error\\nexperiments, the amount of data used in such experiments remains substantially\\nlow and hence makes the learner prone to over-fitting. In this paper, we take\\nthe leap of increasing the available training data to 40 times more than prior\\nwork, leading to a dataset size of 50K data points collected over 700 hours of\\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\\n(CNN) for the task of predicting grasp locations without severe overfitting. In\\nour formulation, we recast the regression problem to an 18-way binary\\nclassification over image patches. We also present a multi-stage learning\\napproach where a CNN trained in one stage is used to collect hard negatives in\\nsubsequent stages. Our experiments clearly show the benefit of using\\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\\nalso compare to several baselines and show state-of-the-art performance on\\ngeneralization to unseen objects for grasping.\\n',\n",
       "  'title': u'Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\\n  Robot Hours'},\n",
       " u'1509.06824': {'arxivid': u'1509.06824',\n",
       "  'authorsaffil': [[u'Christopher Xie', None],\n",
       "   [u'Sachin Patil', None],\n",
       "   [u'Teodor Moldovan', None],\n",
       "   [u'Sergey Levine', None],\n",
       "   [u'Pieter Abbeel', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.RO'],\n",
       "  'comment': u'8 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.06824v2',\n",
       "  'published': u'2015-09-23T02:04:18Z',\n",
       "  'summary': u\"  In this paper, we present a robotic model-based reinforcement learning method\\nthat combines ideas from model identification and model predictive control. We\\nuse a feature-based representation of the dynamics that allows the dynamics\\nmodel to be fitted with a simple least squares procedure, and the features are\\nidentified from a high-level specification of the robot's morphology,\\nconsisting of the number and connectivity structure of its links. Model\\npredictive control is then used to choose the actions under an optimistic model\\nof the dynamics, which produces an efficient and goal-directed exploration\\nstrategy. We present real time experimental results on standard benchmark\\nproblems involving the pendulum, cartpole, and double pendulum systems.\\nExperiments indicate that our method is able to learn a range of benchmark\\ntasks substantially faster than the previous best methods. To evaluate our\\napproach on a realistic robotic control task, we also demonstrate real time\\ncontrol of a simulated 7 degree of freedom arm.\\n\",\n",
       "  'title': u'Model-based Reinforcement Learning with Parametrized Physical Models and\\n  Optimism-Driven Exploration'},\n",
       " u'1602.00709': {'arxivid': u'1602.00709',\n",
       "  'authorsaffil': [[u'Adenilton J. da Silva', None],\n",
       "   [u'Teresa B. Ludermir', None],\n",
       "   [u'Wilson R. de Oliveira', None]],\n",
       "  'categoryterms': [u'quant-ph', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1016/j.neunet.2016.01.002',\n",
       "  'journalref': u'Neural Networks, Volume 76, April 2016, Pages 55-64',\n",
       "  'link': u'http://arxiv.org/abs/1602.00709v1',\n",
       "  'published': u'2016-01-29T13:53:42Z',\n",
       "  'summary': u'  In this work, we propose a quantum neural network named quantum perceptron\\nover a field (QPF). Quantum computers are not yet a reality and the models and\\nalgorithms proposed in this work cannot be simulated in actual (or classical)\\ncomputers. QPF is a direct generalization of a classical perceptron and solves\\nsome drawbacks found in previous models of quantum perceptrons. We also present\\na learning algorithm named Superposition based Architecture Learning algorithm\\n(SAL) that optimizes the neural network weights and architectures. SAL searches\\nfor the best architecture in a finite set of neural network architectures with\\nlinear time over the number of patterns in the training set. SAL is the first\\nlearning algorithm to determine neural network architectures in polynomial\\ntime. This speedup is obtained by the use of quantum parallelism and a\\nnon-linear quantum operator.\\n',\n",
       "  'title': u'Quantum perceptron over a field and neural network architecture\\n  selection in a quantum computer'},\n",
       " u'1601.07925': {'arxivid': u'1601.07925',\n",
       "  'authorsaffil': [[u'Randal S. Olson', None],\n",
       "   [u'Ryan J. Urbanowicz', None],\n",
       "   [u'Peter C. Andrews', None],\n",
       "   [u'Nicole A. Lavender', None],\n",
       "   [u'La Creis Kidd', None],\n",
       "   [u'Jason H. Moore', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'16 pages, 5 figures, to appear in EvoBIO 2016 proceedings',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07925v1',\n",
       "  'published': u'2016-01-28T21:45:55Z',\n",
       "  'summary': u'  Over the past decade, data science and machine learning has grown from a\\nmysterious art form to a staple tool across a variety of fields in academia,\\nbusiness, and government. In this paper, we introduce the concept of tree-based\\npipeline optimization for automating one of the most tedious parts of machine\\nlearning---pipeline design. We implement a Tree-based Pipeline Optimization\\nTool (TPOT) and demonstrate its effectiveness on a series of simulated and\\nreal-world genetic data sets. In particular, we show that TPOT can build\\nmachine learning pipelines that achieve competitive classification accuracy and\\ndiscover novel pipeline operators---such as synthetic feature\\nconstructors---that significantly improve classification accuracy on these data\\nsets. We also highlight the current challenges to pipeline optimization, such\\nas the tendency to produce pipelines that overfit the data, and suggest future\\nresearch paths to overcome these challenges. As such, this work represents an\\nearly step toward fully automating machine learning pipeline design.\\n',\n",
       "  'title': u'Automating biomedical data science through tree-based pipeline\\n  optimization'},\n",
       " u'1601.07929': {'arxivid': u'1601.07929',\n",
       "  'authorsaffil': [[u'Martin Plajner', None], [u'Ji\\u0159\\xed Vomlel', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'9 pages, v2: language corrections',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07929v2',\n",
       "  'published': u'2016-01-28T22:03:32Z',\n",
       "  'summary': u'  This paper follows previous research we have already performed in the area of\\nBayesian networks models for CAT. We present models using Item Response Theory\\n(IRT - standard CAT method), Bayesian networks, and neural networks. We\\nconducted simulated CAT tests on empirical data. Results of these tests are\\npresented for each model separately and compared.\\n',\n",
       "  'title': u'Probabilistic Models for Computerized Adaptive Testing: Experiments'},\n",
       " u'1603.06881': {'arxivid': u'1603.06881',\n",
       "  'authorsaffil': [[u'Nihar B. Shah', None],\n",
       "   [u'Sivaraman Balakrishnan', None],\n",
       "   [u'Martin J. Wainwright', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.IT', u'math.IT', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06881v1',\n",
       "  'published': u'2016-03-22T17:28:08Z',\n",
       "  'summary': u'  We study methods for aggregating pairwise comparison data in order to\\nestimate outcome probabilities for future comparisons among a collection of n\\nitems. Working within a flexible framework that imposes only a form of strong\\nstochastic transitivity (SST), we introduce an adaptivity index defined by the\\nindifference sets of the pairwise comparison probabilities. In addition to\\nmeasuring the usual worst-case risk of an estimator, this adaptivity index also\\ncaptures the extent to which the estimator adapts to instance-specific\\ndifficulty relative to an oracle estimator. We prove three main results that\\ninvolve this adaptivity index and different algorithms. First, we propose a\\nthree-step estimator termed Count-Randomize-Least squares (CRL), and show that\\nit has adaptivity index upper bounded as $\\\\sqrt{n}$ up to logarithmic factors.\\nWe then show that that conditional on the hardness of planted clique, no\\ncomputationally efficient estimator can achieve an adaptivity index smaller\\nthan $\\\\sqrt{n}$. Second, we show that a regularized least squares estimator can\\nachieve a poly-logarithmic adaptivity index, thereby demonstrating a\\n$\\\\sqrt{n}$-gap between optimal and computationally achievable adaptivity.\\nFinally, we prove that the standard least squares estimator, which is known to\\nbe optimally adaptive in several closely related problems, fails to adapt in\\nthe context of estimating pairwise probabilities.\\n',\n",
       "  'title': u'Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of\\n  Pairwise Comparisons'},\n",
       " u'1604.02677': {'arxivid': u'1604.02677',\n",
       "  'authorsaffil': [[u'Hao Chen', None],\n",
       "   [u'Xiaojuan Qi', None],\n",
       "   [u'Lequan Yu', None],\n",
       "   [u'Pheng-Ann Heng', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02677v1',\n",
       "  'published': u'2016-04-10T12:12:24Z',\n",
       "  'summary': u'  The morphology of glands has been used routinely by pathologists to assess\\nthe malignancy degree of adenocarcinomas. Accurate segmentation of glands from\\nhistology images is a crucial step to obtain reliable morphological statistics\\nfor quantitative diagnosis. In this paper, we proposed an efficient deep\\ncontour-aware network (DCAN) to solve this challenging problem under a unified\\nmulti-task learning framework. In the proposed network, multi-level contextual\\nfeatures from the hierarchical architecture are explored with auxiliary\\nsupervision for accurate gland segmentation. When incorporated with multi-task\\nregularization during the training, the discriminative capability of\\nintermediate features can be further improved. Moreover, our network can not\\nonly output accurate probability maps of glands, but also depict clear contours\\nsimultaneously for separating clustered objects, which further boosts the gland\\nsegmentation performance. This unified framework can be efficient when applied\\nto large-scale histopathological data without resorting to additional steps to\\ngenerate contours based on low-level cues for post-separating. Our method won\\nthe 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams,\\nsurpassing all the other methods by a significant margin.\\n',\n",
       "  'title': u'DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation'},\n",
       " u'1505.05192': {'arxivid': u'1505.05192',\n",
       "  'authorsaffil': [[u'Carl Doersch', None],\n",
       "   [u'Abhinav Gupta', None],\n",
       "   [u'Alexei A. Efros', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Oral paper at ICCV 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.05192v3',\n",
       "  'published': u'2015-05-19T21:18:17Z',\n",
       "  'summary': u'  This work explores the use of spatial context as a source of free and\\nplentiful supervisory signal for training a rich visual representation. Given\\nonly a large, unlabeled image collection, we extract random pairs of patches\\nfrom each image and train a convolutional neural net to predict the position of\\nthe second patch relative to the first. We argue that doing well on this task\\nrequires the model to learn to recognize objects and their parts. We\\ndemonstrate that the feature representation learned using this within-image\\ncontext indeed captures visual similarity across images. For example, this\\nrepresentation allows us to perform unsupervised visual discovery of objects\\nlike cats, people, and even birds from the Pascal VOC 2011 detection dataset.\\nFurthermore, we show that the learned ConvNet can be used in the R-CNN\\nframework and provides a significant boost over a randomly-initialized ConvNet,\\nresulting in state-of-the-art performance among algorithms which use only\\nPascal-provided training set annotations.\\n',\n",
       "  'title': u'Unsupervised Visual Representation Learning by Context Prediction'},\n",
       " u'1408.5882': {'arxivid': u'1408.5882',\n",
       "  'authorsaffil': [[u'Yoon Kim', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.NE'],\n",
       "  'comment': u'To appear in EMNLP 2014',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1408.5882v2',\n",
       "  'published': u'2014-08-25T19:48:04Z',\n",
       "  'summary': u'  We report on a series of experiments with convolutional neural networks (CNN)\\ntrained on top of pre-trained word vectors for sentence-level classification\\ntasks. We show that a simple CNN with little hyperparameter tuning and static\\nvectors achieves excellent results on multiple benchmarks. Learning\\ntask-specific vectors through fine-tuning offers further gains in performance.\\nWe additionally propose a simple modification to the architecture to allow for\\nthe use of both task-specific and static vectors. The CNN models discussed\\nherein improve upon the state of the art on 4 out of 7 tasks, which include\\nsentiment analysis and question classification.\\n',\n",
       "  'title': u'Convolutional Neural Networks for Sentence Classification'},\n",
       " u'1511.06381': {'arxivid': u'1511.06381',\n",
       "  'authorsaffil': [[u'Taehoon Lee', None],\n",
       "   [u'Minsuk Choi', None],\n",
       "   [u'Sungroh Yoon', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': u'Figure 2, 5, 7, and several descriptions revised',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06381v2',\n",
       "  'published': u'2015-11-19T21:10:29Z',\n",
       "  'summary': u'  Learning meaningful representations using deep neural networks involves\\ndesigning efficient training schemes and well-structured networks. Currently,\\nthe method of stochastic gradient descent that has a momentum with dropout is\\none of the most popular training protocols. Based on that, more advanced\\nmethods (i.e., Maxout and Batch Normalization) have been proposed in recent\\nyears, but most still suffer from performance degradation caused by small\\nperturbations, also known as adversarial examples. To address this issue, we\\npropose manifold regularized networks (MRnet) that utilize a novel training\\nobjective function that minimizes the difference between multi-layer embedding\\nresults of samples and those adversarial. Our experimental results demonstrated\\nthat MRnet is more resilient to adversarial examples and helps us to generalize\\nrepresentations on manifolds. Furthermore, combining MRnet and dropout allowed\\nus to achieve competitive classification performances for three well-known\\nbenchmarks: MNIST, CIFAR-10, and SVHN.\\n',\n",
       "  'title': u'Manifold Regularized Deep Neural Networks using Adversarial Examples'},\n",
       " u'1511.06380': {'arxivid': u'1511.06380',\n",
       "  'authorsaffil': [[u'William Lotter', None],\n",
       "   [u'Gabriel Kreiman', None],\n",
       "   [u'David Cox', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.CV', u'q-bio.NC'],\n",
       "  'comment': u'under review as conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06380v2',\n",
       "  'published': u'2015-11-19T21:10:17Z',\n",
       "  'summary': u\"  The ability to predict future states of the environment is a central pillar\\nof intelligence. At its core, effective prediction requires an internal model\\nof the world and an understanding of the rules by which the world changes.\\nHere, we explore the internal models developed by deep neural networks trained\\nusing a loss based on predicting future frames in synthetic video sequences,\\nusing a CNN-LSTM-deCNN framework. We first show that this architecture can\\nachieve excellent performance in visual sequence prediction tasks, including\\nstate-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever\\net al., 2009). Using a weighted mean-squared error and adversarial loss\\n(Goodfellow et al., 2014), the same architecture successfully extrapolates\\nout-of-the-plane rotations of computer-generated faces. Furthermore, despite\\nbeing trained end-to-end to predict only pixel-level information, our\\nPredictive Generative Networks learn a representation of the latent structure\\nof the underlying three-dimensional objects themselves. Importantly, we find\\nthat this representation is naturally tolerant to object transformations, and\\ngeneralizes well to new tasks, such as classification of static images. Similar\\nmodels trained solely with a reconstruction loss fail to generalize as\\neffectively. We argue that prediction can serve as a powerful unsupervised loss\\nfor learning rich internal representations of high-level object features.\\n\",\n",
       "  'title': u'Unsupervised Learning of Visual Structure using Predictive Generative\\n  Networks'},\n",
       " u'1511.06382': {'arxivid': u'1511.06382',\n",
       "  'authorsaffil': [[u'R Devon Hjelm', None],\n",
       "   [u'Kyunghyun Cho', None],\n",
       "   [u'Junyoung Chung', None],\n",
       "   [u'Russ Salakhutdinov', None],\n",
       "   [u'Vince Calhoun', None],\n",
       "   [u'Nebojsa Jojic', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06382v4',\n",
       "  'published': u'2015-11-19T21:11:12Z',\n",
       "  'summary': u'  Recent advances in variational inference that make use of an inference or\\nrecognition network for training and evaluating deep directed graphical models\\nhave advanced well beyond traditional variational inference and Markov chain\\nMonte Carlo methods. These techniques offer higher flexibility with simpler and\\nfaster inference; yet training and evaluation still remains a challenge. We\\npropose a method for improving the per-example approximate posterior by\\niterative refinement, which can provide notable gains in maximizing the\\nvariational lower bound of the log likelihood and works with both continuous\\nand discrete latent variables. We evaluate our approach as a method of training\\nand evaluating directed graphical models. We show that, when used for training,\\niterative refinement improves the variational lower bound and can also improve\\nthe log-likelihood over related methods. We also show that iterative refinement\\ncan be used to get a better estimate of the log-likelihood in any directed\\nmodel trained with mean-field inference.\\n',\n",
       "  'title': u'Iterative Refinement of Approximate Posterior for Training Directed\\n  Belief Networks'},\n",
       " u'1511.08228': {'arxivid': u'1511.08228',\n",
       "  'authorsaffil': [[u'\\u0141ukasz Kaiser', None], [u'Ilya Sutskever', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.08228v3',\n",
       "  'published': u'2015-11-25T21:17:43Z',\n",
       "  'summary': u'  Learning an algorithm from examples is a fundamental problem that has been\\nwidely studied. Recently it has been addressed using neural networks, in\\nparticular by Neural Turing Machines (NTMs). These are fully differentiable\\ncomputers that use backpropagation to learn their own programming. Despite\\ntheir appeal NTMs have a weakness that is caused by their sequential nature:\\nthey are not parallel and are are hard to train due to their large depth when\\nunfolded.\\n  We present a neural network architecture to address this problem: the Neural\\nGPU. It is based on a type of convolutional gated recurrent unit and, like the\\nNTM, is computationally universal. Unlike the NTM, the Neural GPU is highly\\nparallel which makes it easier to train and efficient to run.\\n  An essential property of algorithms is their ability to handle inputs of\\narbitrary size. We show that the Neural GPU can be trained on short instances\\nof an algorithmic task and successfully generalize to long instances. We\\nverified it on a number of tasks including long addition and long\\nmultiplication of numbers represented in binary. We train the Neural GPU on\\nnumbers with upto 20 bits and observe no errors whatsoever while testing it,\\neven on much longer numbers.\\n  To achieve these results we introduce a technique for training deep recurrent\\nnetworks: parameter sharing relaxation. We also found a small amount of dropout\\nand gradient noise to have a large positive effect on learning and\\ngeneralization.\\n',\n",
       "  'title': u'Neural GPUs Learn Algorithms'},\n",
       " u'1604.02801': {'arxivid': u'1604.02801',\n",
       "  'authorsaffil': [[u'Ruizhe Wang', None],\n",
       "   [u'Lingyu Wei', None],\n",
       "   [u'Etienne Vouga', None],\n",
       "   [u'Qixing Huang', None],\n",
       "   [u'Duygu Ceylan', None],\n",
       "   [u'Gerard Medioni', None],\n",
       "   [u'Hao Li', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'22 pages, 12 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02801v1',\n",
       "  'published': u'2016-04-11T06:03:09Z',\n",
       "  'summary': u'  We present an end-to-end system for reconstructing complete watertight and\\ntextured models of moving subjects such as clothed humans and animals, using\\nonly three or four handheld sensors. The heart of our framework is a new\\npairwise registration algorithm that minimizes, using a particle swarm\\nstrategy, an alignment error metric based on mutual visibility and occlusion.\\nWe show that this algorithm reliably registers partial scans with as little as\\n15% overlap without requiring any initial correspondences, and outperforms\\nalternative global registration algorithms. This registration algorithm allows\\nus to reconstruct moving subjects from free-viewpoint video produced by\\nconsumer-grade sensors, without extensive sensor calibration, constrained\\ncapture volume, expensive arrays of cameras, or templates of the subject\\ngeometry.\\n',\n",
       "  'title': u'Capturing Dynamic Textured Surfaces of Moving Targets'},\n",
       " u'1603.01354': {'arxivid': u'1603.01354',\n",
       "  'authorsaffil': [[u'Xuezhe Ma', None], [u'Eduard Hovy', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL', u'stat.ML'],\n",
       "  'comment': u'10 pages, 3 figures. To appear on ACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01354v5',\n",
       "  'published': u'2016-03-04T05:55:02Z',\n",
       "  'summary': u'  State-of-the-art sequence labeling systems traditionally require large\\namounts of task-specific knowledge in the form of hand-crafted features and\\ndata pre-processing. In this paper, we introduce a novel neutral network\\narchitecture that benefits from both word- and character-level representations\\nautomatically, by using combination of bidirectional LSTM, CNN and CRF. Our\\nsystem is truly end-to-end, requiring no feature engineering or data\\npre-processing, thus making it applicable to a wide range of sequence labeling\\ntasks. We evaluate our system on two data sets for two sequence labeling tasks\\n--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003\\ncorpus for named entity recognition (NER). We obtain state-of-the-art\\nperformance on both the two data --- 97.55\\\\% accuracy for POS tagging and\\n91.21\\\\% F1 for NER.\\n',\n",
       "  'title': u'End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF'},\n",
       " u'1603.04139': {'arxivid': u'1603.04139',\n",
       "  'authorsaffil': [[u'Junlin Yao', None], [u'Frank Nielsen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'17 pages, 7 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04139v1',\n",
       "  'published': u'2016-03-14T05:36:40Z',\n",
       "  'summary': u'  Video co-segmentation refers to the task of jointly segmenting common objects\\nappearing in a given group of videos. In practice, high-dimensional data such\\nas videos can be conceptually thought as being drawn from a union of subspaces\\ncorresponding to categories rather than from a smooth manifold. Therefore,\\nsegmenting data into respective subspaces --- subspace clustering --- finds\\nwidespread applications in computer vision, including co-segmentation.\\nState-of-the-art methods via subspace clustering seek to solve the problem in\\ntwo steps:\\n  First, an affinity matrix is built from data, with appearance features or\\nmotion patterns. Second, the data are segmented by applying spectral clustering\\nto the affinity matrix. However, this process is insufficient to obtain an\\noptimal solution since it does not take into account the {\\\\em interdependence}\\nof the affinity matrix with the segmentation. In this work, we present a novel\\nunified video co-segmentation framework inspired by the recent Structured\\nSparse Subspace Clustering ($\\\\mathrm{S^{3}C}$) based on the {\\\\em\\nself-expressiveness} model. Our method yields more consistent segmentation\\nresults. In order to improve the detectability of motion features with missing\\ntrajectories due to occlusion or tracked points moving out of frames, we add an\\nextra-dimensional signature to the motion trajectories. Moreover, we\\nreformulate the $\\\\mathrm{S^{3}C}$ algorithm by adding the affine subspace\\nconstraint in order to make it more suitable to segment rigid motions lying in\\naffine subspaces of dimension at most $3$. Experiments on MOViCS dataset\\ndemonstrate the effectiveness of our approaches and its robustness to heavy\\nnoise.\\n',\n",
       "  'title': u'SSSC-AM: A Unified Framework for Video Co-Segmentation by Structured\\n  Sparse Subspace Clustering with Appearance and Motion Features'},\n",
       " u'1603.04026': {'arxivid': u'1603.04026',\n",
       "  'authorsaffil': [[u'Huamin Ren', None],\n",
       "   [u'Hong Pan', None],\n",
       "   [u'S\\xf8ren Ingvor Olsen', None],\n",
       "   [u'Thomas B. Moeslund', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'7 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04026v1',\n",
       "  'published': u'2016-03-13T13:13:10Z',\n",
       "  'summary': u'  Sparse representation has been applied successfully in abnormal event\\ndetection, in which the baseline is to learn a dictionary accompanied by sparse\\ncodes. While much emphasis is put on discriminative dictionary construction,\\nthere are no comparative studies of sparse codes regarding abnormality\\ndetection. We comprehensively study two types of sparse codes solutions -\\ngreedy algorithms and convex L1-norm solutions - and their impact on\\nabnormality detection performance. We also propose our framework of combining\\nsparse codes with different detection methods. Our comparative experiments are\\ncarried out from various angles to better understand the applicability of\\nsparse codes, including computation time, reconstruction error, sparsity,\\ndetection accuracy, and their performance combining various detection methods.\\nExperiments show that combining OMP codes with maximum coordinate detection\\ncould achieve state-of-the-art performance on the UCSD dataset [14].\\n',\n",
       "  'title': u'A comprehensive study of sparse codes on abnormality detection'},\n",
       " u'1603.03925': {'arxivid': u'1603.03925',\n",
       "  'authorsaffil': [[u'Quanzeng You', None],\n",
       "   [u'Hailin Jin', None],\n",
       "   [u'Zhaowen Wang', None],\n",
       "   [u'Chen Fang', None],\n",
       "   [u'Jiebo Luo', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'10 pages, 5 figures, CVPR16',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03925v1',\n",
       "  'published': u'2016-03-12T15:11:43Z',\n",
       "  'summary': u'  Automatically generating a natural language description of an image has\\nattracted interests recently both because of its importance in practical\\napplications and because it connects two major artificial intelligence fields:\\ncomputer vision and natural language processing. Existing approaches are either\\ntop-down, which start from a gist of an image and convert it into words, or\\nbottom-up, which come up with words describing various aspects of an image and\\nthen combine them. In this paper, we propose a new algorithm that combines both\\napproaches through a model of semantic attention. Our algorithm learns to\\nselectively attend to semantic concept proposals and fuse them into hidden\\nstates and outputs of recurrent neural networks. The selection and fusion form\\na feedback connecting the top-down and bottom-up computation. We evaluate our\\nalgorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental\\nresults show that our algorithm significantly outperforms the state-of-the-art\\napproaches consistently across different evaluation metrics.\\n',\n",
       "  'title': u'Image Captioning with Semantic Attention'},\n",
       " u'1509.05936': {'arxivid': u'1509.05936',\n",
       "  'authorsaffil': [[u'Yoshua Bengio', None],\n",
       "   [u'Thomas Mesnard', None],\n",
       "   [u'Asja Fischer', None],\n",
       "   [u'Saizheng Zhang', None],\n",
       "   [u'Yuhuai Wu', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'q-bio.NC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.05936v2',\n",
       "  'published': u'2015-09-19T21:05:18Z',\n",
       "  'summary': u'  We introduce a weight update formula that is expressed only in terms of\\nfiring rates and their derivatives and that results in changes consistent with\\nthose associated with spike-timing dependent plasticity (STDP) rules and\\nbiological observations, even though the explicit timing of spikes is not\\nneeded. The new rule changes a synaptic weight in proportion to the product of\\nthe presynaptic firing rate and the temporal rate of change of activity on the\\npostsynaptic side. These quantities are interesting for studying theoretical\\nexplanation for synaptic changes from a machine learning perspective. In\\nparticular, if neural dynamics moved neural activity towards reducing some\\nobjective function, then this STDP rule would correspond to stochastic gradient\\ndescent on that objective function.\\n',\n",
       "  'title': u'STDP as presynaptic activity times rate of change of postsynaptic\\n  activity'},\n",
       " u'1506.06112': {'arxivid': u'1506.06112',\n",
       "  'authorsaffil': [[u'Ethan M. Rudd', None],\n",
       "   [u'Lalit P. Jain', None],\n",
       "   [u'Walter J. Scheirer', None],\n",
       "   [u'Terrance E. Boult', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Submitted as an IEEE Pattern Analysis and Machine Intelligence (PAMI)\\n  short article',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.06112v3',\n",
       "  'published': u'2015-06-19T19:04:54Z',\n",
       "  'summary': u'  It is often desirable to be able to recognize when inputs to a recognition\\nfunction correspond to classes unseen at training time. With this ability,\\nthese inputs could be re-labeled by a human, and later incorporated into the\\nrecognition function -- ideally under an efficient incremental update\\nmechanism. While good models that assume inputs from a fixed set of classes\\nexist, e.g., artificial neural networks and kernel machines, it is not\\nimmediately obvious how to extend them to perform incremental learning in the\\npresence of unknown query classes. Models that do so take little other\\ndistributional information into account when constructing recognition functions\\nand lack strong theoretical foundations. We take steps to address this gap by\\nformulating a novel, theoretically grounded classifier -- the Extreme Value\\nMachine (EVM) -- which is capable of performing open world recognition. The EVM\\nhas a well-grounded interpretation derived from statistical extreme value\\ntheory (EVT), and is the first classifier of its kind to be able to perform\\nnonlinear, kernel-free, variable bandwidth, incremental learning. We\\ndemonstrate experimentally that, compared to other classifiers in the same deep\\nnetwork derived feature space, the EVM is accurate and efficient on an\\nestablished benchmark partition of the ImageNet dataset.\\n',\n",
       "  'title': u'The Extreme Value Machine'},\n",
       " u'1511.03260': {'arxivid': u'1511.03260',\n",
       "  'authorsaffil': [[u'Paul Mineiro', None], [u'Nikos Karampatziakis', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'Reference implementation available at\\n  https://github.com/pmineiro/xlst',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03260v4',\n",
       "  'published': u'2015-11-10T20:52:52Z',\n",
       "  'summary': u'  Extreme classification problems are multiclass and multilabel classification\\nproblems where the number of outputs is so large that straightforward\\nstrategies are neither statistically nor computationally viable. One strategy\\nfor dealing with the computational burden is via a tree decomposition of the\\noutput space. While this typically leads to training and inference that scales\\nsublinearly with the number of outputs, it also results in reduced statistical\\nperformance. In this work, we identify two shortcomings of tree decomposition\\nmethods, and describe two heuristic mitigations. We compose these with an\\neigenvalue technique for constructing the tree. The end result is a\\ncomputationally efficient algorithm that provides good statistical performance\\non several extreme data sets.\\n',\n",
       "  'title': u'A Hierarchical Spectral Method for Extreme Classification'},\n",
       " u'1604.00923': {'arxivid': u'1604.00923',\n",
       "  'authorsaffil': [[u'Philip S. Thomas', None], [u'Emma Brunskill', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00923v1',\n",
       "  'published': u'2016-04-04T15:56:52Z',\n",
       "  'summary': u'  In this paper we present a new way of predicting the performance of a\\nreinforcement learning policy given historical data that may have been\\ngenerated by a different policy. The ability to evaluate a policy from\\nhistorical data is important for applications where the deployment of a bad\\npolicy can be dangerous or costly. We show empirically that our algorithm\\nproduces estimates that often have orders of magnitude lower mean squared error\\nthan existing methods---it makes more efficient use of the available data. Our\\nnew estimator is based on two advances: an extension of the doubly robust\\nestimator (Jiang and Li, 2015), and a new way to mix between model based\\nestimates and importance sampling based estimates.\\n',\n",
       "  'title': u'Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning'},\n",
       " u'1601.05409': {'arxivid': u'1601.05409',\n",
       "  'authorsaffil': [[u'Mitra Montazeri', None],\n",
       "   [u'Mahdieh Soleymani Baghshah', None],\n",
       "   [u'Aliakbar Niknafs', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.NE'],\n",
       "  'comment': u'The Fifth Iran Data Mining Conference (IDMC 2011), Amirkabir\\n  University of Technology, Tehran, Iran',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05409v1',\n",
       "  'published': u'2016-01-20T20:59:55Z',\n",
       "  'summary': u'  By Emerging huge databases and the need to efficient learning algorithms on\\nthese datasets, new problems have appeared and some methods have been proposed\\nto solve these problems by selecting efficient features. Feature selection is a\\nproblem of finding efficient features among all features in which the final\\nfeature set can improve accuracy and reduce complexity. One way to solve this\\nproblem is to evaluate all possible feature subsets. However, evaluating all\\npossible feature subsets is an exhaustive search and thus it has high\\ncomputational complexity. Until now many heuristic algorithms have been studied\\nfor solving this problem. Hyper-heuristic is a new heuristic approach which can\\nsearch the solution space effectively by applying local searches appropriately.\\nEach local search is a neighborhood searching algorithm. Since each region of\\nthe solution space can have its own characteristics, it should be chosen an\\nappropriate local search and apply it to current solution. This task is tackled\\nto a supervisor. The supervisor chooses a local search based on the functional\\nhistory of local searches. By doing this task, it can trade of between\\nexploitation and exploration. Since the existing heuristic cannot trade of\\nbetween exploration and exploitation appropriately, the solution space has not\\nbeen searched appropriately in these methods and thus they have low convergence\\nrate. For the first time, in this paper use a hyper-heuristic approach to find\\nan efficient feature subset. In the proposed method, genetic algorithm is used\\nas a supervisor and 16 heuristic algorithms are used as local searches.\\nEmpirical study of the proposed method on several commonly used data sets from\\nUCI data sets indicates that it outperforms recent existing methods in the\\nliterature for feature selection.\\n',\n",
       "  'title': u'Selecting Efficient Features via a Hyper-Heuristic Approach'},\n",
       " u'1604.00921': {'arxivid': u'1604.00921',\n",
       "  'authorsaffil': [[u'Hussein A. Abbass', None],\n",
       "   [u'George Leu', None],\n",
       "   [u'Kathryn Merrick', None]],\n",
       "  'categoryterms': [u'cs.CY', u'cs.AI', u'cs.HC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00921v1',\n",
       "  'published': u'2016-03-16T11:00:23Z',\n",
       "  'summary': u\"  Despite the advances made in artificial intelligence, software agents, and\\nrobotics, there is little we see today that we can truly call a fully\\nautonomous system. We conjecture that the main inhibitor for advancing autonomy\\nis lack of trust. Trusted autonomy is the scientific and engineering field to\\nestablish the foundations and ground work for developing trusted autonomous\\nsystems (robotics and software agents) that can be used in our daily life, and\\ncan be integrated with humans seamlessly, naturally and efficiently.\\n  In this paper, we review this literature to reveal opportunities for\\nresearchers and practitioners to work on topics that can create a leap forward\\nin advancing the field of trusted autonomy. We focus the paper on the `trust'\\ncomponent as the uniting technology between humans and machines. Our inquiry\\ninto this topic revolves around three sub-topics: (1) reviewing and positioning\\nthe trust modelling literature for the purpose of trusted autonomy; (2)\\nreviewing a critical subset of sensor technologies that allow a machine to\\nsense human states; and (3) distilling some critical questions for advancing\\nthe field of trusted autonomy. The inquiry is augmented with conceptual models\\nthat we propose along the way by recompiling and reshaping the literature into\\nforms that enables trusted autonomous systems to become a reality. The paper\\noffers a vision for a Trusted Cyborg Swarm, an extension of our previous\\nCognitive Cyber Symbiosis concept, whereby humans and machines meld together in\\na harmonious, seamless, and coordinated manner.\\n\",\n",
       "  'title': u'A Review of Theoretical and Practical Challenges of Trusted Autonomy in\\n  Big Data'},\n",
       " u'1601.05403': {'arxivid': u'1601.05403',\n",
       "  'authorsaffil': [[u'Jo\\xe3o Sedoc', None],\n",
       "   [u'Jean Gallier', None],\n",
       "   [u'Lyle Ungar', None],\n",
       "   [u'Dean Foster', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05403v1',\n",
       "  'published': u'2016-01-20T20:37:47Z',\n",
       "  'summary': u'  Vector space representations of words capture many aspects of word\\nsimilarity, but such methods tend to make vector spaces in which antonyms (as\\nwell as synonyms) are close to each other. We present a new signed spectral\\nnormalized graph cut algorithm, signed clustering, that overlays existing\\nthesauri upon distributionally derived vector representations of words, so that\\nantonym relationships between word pairs are represented by negative weights.\\nOur signed clustering algorithm produces clusters of words which simultaneously\\ncapture distributional and synonym relations. We evaluate these clusters\\nagainst the SimLex-999 dataset (Hill et al.,2014) of human judgments of word\\npair similarities, and also show the benefit of using our clusters to predict\\nthe sentiment of a given text.\\n',\n",
       "  'title': u'Semantic Word Clusters Using Signed Normalized Graph Cuts'},\n",
       " u'1602.08132': {'arxivid': u'1602.08132',\n",
       "  'authorsaffil': [[u'Zhenhao Ge', None],\n",
       "   [u'Sudhendu R. Sharma', None],\n",
       "   [u'Mark J. T. Smith', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.CV'],\n",
       "  'comment': u'4th International Congress on Image and Signal Processing (CISP) 2011',\n",
       "  'doi': u'10.1109/CISP.2011.6100685',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08132v1',\n",
       "  'published': u'2016-02-25T22:17:31Z',\n",
       "  'summary': u'  Systems based on automatic speech recognition (ASR) technology can provide\\nimportant functionality in computer assisted language learning applications.\\nThis is a young but growing area of research motivated by the large number of\\nstudents studying foreign languages. Here we propose a Hidden Markov Model\\n(HMM)-based method to detect mispronunciations. Exploiting the specific dialog\\nscripting employed in language learning software, HMMs are trained for\\ndifferent pronunciations. New adaptive features have been developed and\\nobtained through an adaptive warping of the frequency scale prior to computing\\nthe cepstral coefficients. The optimization criterion used for the warping\\nfunction is to maximize separation of two major groups of pronunciations\\n(native and non-native) in terms of classification rate. Experimental results\\nshow that the adaptive frequency scale yields a better coefficient\\nrepresentation leading to higher classification rates in comparison with\\nconventional HMMs using Mel-frequency cepstral coefficients.\\n',\n",
       "  'title': u'Adaptive Frequency Cepstral Coefficients for Word Mispronunciation\\n  Detection'},\n",
       " u'1602.08771': {'arxivid': u'1602.08771',\n",
       "  'authorsaffil': [[u'Adam White', None], [u'Martha White', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'stat.ML'],\n",
       "  'comment': u'Autonomous Agents and Multi-agent Systems, 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08771v2',\n",
       "  'published': u'2016-02-28T21:23:54Z',\n",
       "  'summary': u'  Off-policy reinforcement learning has many applications including: learning\\nfrom demonstration, learning multiple goal seeking policies in parallel, and\\nrepresenting predictive knowledge. Recently there has been an proliferation of\\nnew policy-evaluation algorithms that fill a longstanding algorithmic void in\\nreinforcement learning: combining robustness to off-policy sampling, function\\napproximation, linear complexity, and temporal difference (TD) updates. This\\npaper contains two main contributions. First, we derive two new hybrid TD\\npolicy-evaluation algorithms, which fill a gap in this collection of\\nalgorithms. Second, we perform an empirical comparison to elicit which of these\\nnew linear TD methods should be preferred in different situations, and make\\nconcrete suggestions about practical use.\\n',\n",
       "  'title': u'Investigating practical linear temporal difference learning'},\n",
       " u'1603.03724': {'arxivid': u'1603.03724',\n",
       "  'authorsaffil': [[u'Niharika Gauraha', None], [u'Swapan K. Parui', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03724v1',\n",
       "  'published': u'2016-03-11T19:06:33Z',\n",
       "  'summary': u'  In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable\\nselection in high dimensional sparse regression models with strongly correlated\\nvariables. To handle correlated variables, the concept of clustering or\\ngrouping variables and then pursuing model fitting is widely accepted. When the\\ndimension is very high, finding an appropriate group structure is as difficult\\nas the original problem. The ACL is a three-stage procedure where, at the first\\nstage, we use the Lasso(or its adaptive or thresholded version) to do initial\\nselection, then we also include those variables which are not selected by the\\nLasso but are strongly correlated with the variables selected by the Lasso. At\\nthe second stage we cluster the variables based on the reduced set of\\npredictors and in the third stage we perform sparse estimation such as Lasso on\\ncluster representatives or the group Lasso based on the structures generated by\\nclustering procedure. We show that our procedure is consistent and efficient in\\nfinding true underlying population group structure(under assumption of\\nirrepresentable and beta-min conditions). We also study the group selection\\nconsistency of our method and we support the theory using simulated and\\npseudo-real dataset examples.\\n',\n",
       "  'title': u'Efficient Clustering of Correlated Variables and Variable Selection in\\n  High-Dimensional Linear Models'},\n",
       " u'1511.06030': {'arxivid': u'1511.06030',\n",
       "  'authorsaffil': [[u'Bryan Hooi', None],\n",
       "   [u'Neil Shah', None],\n",
       "   [u'Alex Beutel', None],\n",
       "   [u'Stephan Gunnemann', None],\n",
       "   [u'Leman Akoglu', None],\n",
       "   [u'Mohit Kumar', None],\n",
       "   [u'Disha Makhija', None],\n",
       "   [u'Christos Faloutsos', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'9 pages; v2: minor typos corrected',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06030v2',\n",
       "  'published': u'2015-11-19T00:16:17Z',\n",
       "  'summary': u'  Review fraud is a pervasive problem in online commerce, in which fraudulent\\nsellers write or purchase fake reviews to manipulate perception of their\\nproducts and services. Fake reviews are often detected based on several signs,\\nincluding 1) they occur in short bursts of time; 2) fraudulent user accounts\\nhave skewed rating distributions. However, these may both be true in any given\\ndataset. Hence, in this paper, we propose an approach for detecting fraudulent\\nreviews which combines these 2 approaches in a principled manner, allowing\\nsuccessful detection even when one of these signs is not present. To combine\\nthese 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD)\\nmodel, a flexible Bayesian model of user rating behavior. Based on our model we\\nformulate a likelihood-based suspiciousness metric, Normalized Expected\\nSurprise Total (NEST). We propose a linear-time algorithm for performing\\nBayesian inference using our model and computing the metric. Experiments on\\nreal data show that BIRDNEST successfully spots review fraud in large,\\nreal-world graphs: the 50 most suspicious users of the Flipkart platform\\nflagged by our algorithm were investigated and all identified as fraudulent by\\ndomain experts at Flipkart.\\n',\n",
       "  'title': u'BIRDNEST: Bayesian Inference for Ratings-Fraud Detection'},\n",
       " u'1304.0806': {'arxivid': u'1304.0806',\n",
       "  'authorsaffil': [[u'Faruk Karaaslan', None],\n",
       "   [u'Naim Cagman', None],\n",
       "   [u'Saban Yilmaz', None]],\n",
       "  'categoryterms': [u'cs.AI', u'62C86 (Decision theory and fuzziness)'],\n",
       "  'comment': u'This paper has been withdrawn by the author due to a crucial errors\\n  in the notation and some problems in the algorithm',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1304.0806v3',\n",
       "  'published': u'2013-04-02T22:10:00Z',\n",
       "  'summary': u'  In this work, we present definition of intuitionistic fuzzy parameterized\\n(IFP) intuitionistic fuzzy soft set and its operations. Then we define\\nIFP-aggregation operator to form IFP-intuitionistic fuzzy soft-decision-making\\nmethod which allows constructing more efficient decision processes.\\n',\n",
       "  'title': u'IFP-Intuitionistic fuzzy soft set theory and its applications'},\n",
       " u'1511.06038': {'arxivid': u'1511.06038',\n",
       "  'authorsaffil': [[u'Yishu Miao', None],\n",
       "   [u'Lei Yu', None],\n",
       "   [u'Phil Blunsom', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'ICML 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06038v4',\n",
       "  'published': u'2015-11-19T01:23:28Z',\n",
       "  'summary': u'  Recent advances in neural variational inference have spawned a renaissance in\\ndeep latent variable models. In this paper we introduce a generic variational\\ninference framework for generative and conditional models of text. While\\ntraditional variational methods derive an analytic approximation for the\\nintractable distributions over latent variables, here we construct an inference\\nnetwork conditioned on the discrete text input to provide the variational\\ndistribution. We validate this framework on two very different text modelling\\napplications, generative document modelling and supervised question answering.\\nOur neural variational document model combines a continuous stochastic document\\nrepresentation with a bag-of-words generative model and achieves the lowest\\nreported perplexities on two standard test corpora. The neural answer selection\\nmodel employs a stochastic representation layer within an attention mechanism\\nto extract the semantics between a question and answer pair. On two question\\nanswering benchmarks this model exceeds all previous published benchmarks.\\n',\n",
       "  'title': u'Neural Variational Inference for Text Processing'},\n",
       " u'1601.04105': {'arxivid': u'1601.04105',\n",
       "  'authorsaffil': [[u'Mohsen Taheriyan', None],\n",
       "   [u'Craig A. Knoblock', None],\n",
       "   [u'Pedro Szekely', None],\n",
       "   [u'Jose Luis Ambite', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'Web Semantics: Science, Services and Agents on the World Wide Web,\\n  2016',\n",
       "  'doi': u'10.1016/j.websem.2015.12.003',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04105v1',\n",
       "  'published': u'2016-01-16T00:55:25Z',\n",
       "  'summary': u'  Information sources such as relational databases, spreadsheets, XML, JSON,\\nand Web APIs contain a tremendous amount of structured data that can be\\nleveraged to build and augment knowledge graphs. However, they rarely provide a\\nsemantic model to describe their contents. Semantic models of data sources\\nrepresent the implicit meaning of the data by specifying the concepts and the\\nrelationships within the data. Such models are the key ingredients to\\nautomatically publish the data into knowledge graphs. Manually modeling the\\nsemantics of data sources requires significant effort and expertise, and\\nalthough desirable, building these models automatically is a challenging\\nproblem. Most of the related work focuses on semantic annotation of the data\\nfields (source attributes). However, constructing a semantic model that\\nexplicitly describes the relationships between the attributes in addition to\\ntheir semantic types is critical.\\n  We present a novel approach that exploits the knowledge from a domain\\nontology and the semantic models of previously modeled sources to automatically\\nlearn a rich semantic model for a new source. This model represents the\\nsemantics of the new source in terms of the concepts and relationships defined\\nby the domain ontology. Given some sample data from the new source, we leverage\\nthe knowledge in the domain ontology and the known semantic models to construct\\na weighted graph that represents the space of plausible semantic models for the\\nnew source. Then, we compute the top k candidate semantic models and suggest to\\nthe user a ranked list of the semantic models for the new source. The approach\\ntakes into account user corrections to learn more accurate semantic models on\\nfuture data sources. Our evaluation shows that our method generates expressive\\nsemantic models for data sources and services with minimal user input. ...\\n',\n",
       "  'title': u'Learning the Semantics of Structured Data Sources'},\n",
       " u'1410.3916': {'arxivid': u'1410.3916',\n",
       "  'authorsaffil': [[u'Jason Weston', None],\n",
       "   [u'Sumit Chopra', None],\n",
       "   [u'Antoine Bordes', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CL', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1410.3916v11',\n",
       "  'published': u'2014-10-15T03:13:18Z',\n",
       "  'summary': u'  We describe a new class of learning models called memory networks. Memory\\nnetworks reason with inference components combined with a long-term memory\\ncomponent; they learn how to use these jointly. The long-term memory can be\\nread and written to, with the goal of using it for prediction. We investigate\\nthese models in the context of question answering (QA) where the long-term\\nmemory effectively acts as a (dynamic) knowledge base, and the output is a\\ntextual response. We evaluate them on a large-scale QA task, and a smaller, but\\nmore complex, toy task generated from a simulated world. In the latter, we show\\nthe reasoning power of such models by chaining multiple supporting sentences to\\nanswer questions that require understanding the intension of verbs.\\n',\n",
       "  'title': u'Memory Networks'},\n",
       " u'1603.03729': {'arxivid': u'1603.03729',\n",
       "  'authorsaffil': [[u'Vasile Patrascu', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.13140/RG.2.1.2667.1762',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03729v1',\n",
       "  'published': u'2016-03-10T04:18:38Z',\n",
       "  'summary': u'  Starting from the primary representation of neutrosophic information, namely\\nthe degree of truth, degree of indeterminacy and degree of falsity, we define a\\nnuanced representation in a penta valued fuzzy space, described by the index of\\ntruth, index of falsity, index of ignorance, index of contradiction and index\\nof hesitation. Also, it was constructed an associated penta valued logic and\\nthen using this logic, it was defined for the proposed penta valued structure\\nthe following operators: union, intersection, negation, complement and dual.\\nThen, the penta valued representation is extended to a hexa valued one, adding\\nthe sixth component, namely the index of ambiguity.\\n',\n",
       "  'title': u'Penta and Hexa Valued Representation of Neutrosophic Information'},\n",
       " u'1604.03498': {'arxivid': u'1604.03498',\n",
       "  'authorsaffil': [[u'Wenying Ma', None],\n",
       "   [u'Liangliang Cao', None],\n",
       "   [u'Lei Yu', None],\n",
       "   [u'Guoping Long', None],\n",
       "   [u'Yucheng Li', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'accepted by ICMR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03498v1',\n",
       "  'published': u'2016-04-12T18:22:08Z',\n",
       "  'summary': u'  Fisher vector has been widely used in many multimedia retrieval and visual\\nrecognition applications with good performance. However, the computation\\ncomplexity prevents its usage in real-time video monitoring. In this work, we\\nproposed and implemented GPU-FV, a fast Fisher vector extraction method with\\nthe help of modern GPUs. The challenge of implementing Fisher vector on GPUs\\nlies in the data dependency in feature extraction and expensive memory access\\nin Fisher vector computing. To handle these challenges, we carefully designed\\nGPU-FV in a way that utilizes the computing power of GPU as much as possible,\\nand applied optimizations such as loop tiling to boost the performance. GPU-FV\\nis about 12 times faster than the CPU version, and 50\\\\% faster than a\\nnon-optimized GPU implementation. For standard video input (320*240), GPU-FV\\ncan process each frame within 34ms on a model GPU. Our experiments show that\\nGPU-FV obtains a similar recognition accuracy as traditional FV on VOC 2007 and\\nCaltech 256 image sets. We also applied GPU-FV for realtime video monitoring\\ntasks and found that GPU-FV outperforms a number of previous works. Especially,\\nwhen the number of training examples are small, GPU-FV outperforms the recent\\npopular deep CNN features borrowed from ImageNet. The code can be downloaded\\nfrom the following link https://bitbucket.org/mawenjing/gpu-fv.\\n',\n",
       "  'title': u'GPU-FV: Realtime Fisher Vector and Its Applications in Video Monitoring'},\n",
       " u'1602.02481': {'arxivid': u'1602.02481',\n",
       "  'authorsaffil': [[u'Sungjoon Choi', None],\n",
       "   [u'Qian-Yi Zhou', None],\n",
       "   [u'Stephen Miller', None],\n",
       "   [u'Vladlen Koltun', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.GR'],\n",
       "  'comment': u'Technical report',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02481v3',\n",
       "  'published': u'2016-02-08T07:20:52Z',\n",
       "  'summary': u'  We have created a dataset of more than ten thousand 3D scans of real objects.\\nTo create the dataset, we recruited 70 operators, equipped them with\\nconsumer-grade mobile 3D scanning setups, and paid them to scan objects in\\ntheir environments. The operators scanned objects of their choosing, outside\\nthe laboratory and without direct supervision by computer vision professionals.\\nThe result is a large and diverse collection of object scans: from shoes, mugs,\\nand toys to grand pianos, construction vehicles, and large outdoor sculptures.\\nWe worked with an attorney to ensure that data acquisition did not violate\\nprivacy constraints. The acquired data was irrevocably placed in the public\\ndomain and is available freely at http://redwood-data.org/3dscan .\\n',\n",
       "  'title': u'A Large Dataset of Object Scans'},\n",
       " u'1509.02301': {'arxivid': u'1509.02301',\n",
       "  'authorsaffil': [[u'Octavian-Eugen Ganea', None],\n",
       "   [u'Marina Ganea', None],\n",
       "   [u'Aurelien Lucchi', None],\n",
       "   [u'Carsten Eickhoff', None],\n",
       "   [u'Thomas Hofmann', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.02301v3',\n",
       "  'published': u'2015-09-08T09:43:13Z',\n",
       "  'summary': u'  Many fundamental problems in natural language processing rely on determining\\nwhat entities appear in a given text. Commonly referenced as entity linking,\\nthis step is a fundamental component of many NLP tasks such as text\\nunderstanding, automatic summarization, semantic search or machine translation.\\nName ambiguity, word polysemy, context dependencies and a heavy-tailed\\ndistribution of entities contribute to the complexity of this problem.\\n  We here propose a probabilistic approach that makes use of an effective\\ngraphical model to perform collective entity disambiguation. Input mentions\\n(i.e.,~linkable token spans) are disambiguated jointly across an entire\\ndocument by combining a document-level prior of entity co-occurrences with\\nlocal information captured from mentions and their surrounding context. The\\nmodel is based on simple sufficient statistics extracted from data, thus\\nrelying on few parameters to be learned.\\n  Our method does not require extensive feature engineering, nor an expensive\\ntraining procedure. We use loopy belief propagation to perform approximate\\ninference. The low complexity of our model makes this step sufficiently fast\\nfor real-time usage. We demonstrate the accuracy of our approach on a wide\\nrange of benchmark datasets, showing that it matches, and in many cases\\noutperforms, existing state-of-the-art methods.\\n',\n",
       "  'title': u'Probabilistic Bag-Of-Hyperlinks Model for Entity Linking'},\n",
       " u'1509.03185': {'arxivid': u'1509.03185',\n",
       "  'authorsaffil': [[u'Andrew J. R. Simpson', None]],\n",
       "  'categoryterms': [u'cs.LG', u'68Txx'],\n",
       "  'comment': u'arXiv admin note: substantial text overlap with arXiv:1509.00913',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.03185v1',\n",
       "  'published': u'2015-09-10T15:12:00Z',\n",
       "  'summary': u\"  In a recent article we described a new type of deep neural network - a\\nPerpetual Learning Machine (PLM) - which is capable of learning 'on the fly'\\nlike a brain by existing in a state of Perpetual Stochastic Gradient Descent\\n(PSGD). Here, by simulating the process of practice, we demonstrate both\\nselective memory and selective forgetting when we introduce statistical recall\\nbiases during PSGD. Frequently recalled memories are remembered, whilst\\nmemories recalled rarely are forgotten. This results in a 'use it or lose it'\\nstimulus driven memory process that is similar to human memory.\\n\",\n",
       "  'title': u'Use it or Lose it: Selective Memory and Forgetting in a Perpetual\\n  Learning Machine'},\n",
       " u'1602.04915': {'arxivid': u'1602.04915',\n",
       "  'authorsaffil': [[u'Jason D. Lee', None],\n",
       "   [u'Max Simchowitz', None],\n",
       "   [u'Michael I. Jordan', None],\n",
       "   [u'Benjamin Recht', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'math.OC'],\n",
       "  'comment': u'Submitted to COLT 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04915v2',\n",
       "  'published': u'2016-02-16T05:43:31Z',\n",
       "  'summary': u'  We show that gradient descent converges to a local minimizer, almost surely\\nwith random initialization. This is proved by applying the Stable Manifold\\nTheorem from dynamical systems theory.\\n',\n",
       "  'title': u'Gradient Descent Converges to Minimizers'},\n",
       " u'1509.03789': {'arxivid': u'1509.03789',\n",
       "  'authorsaffil': [[u'Bardia Yousefi', None], [u'Chu Kiong Loo', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CV'],\n",
       "  'comment': u\"author's version, SWJ 2014\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.03789v2',\n",
       "  'published': u'2015-09-13T00:34:18Z',\n",
       "  'summary': u'  Studies on computational neuroscience through functional magnetic resonance\\nimaging (fMRI) and following biological inspired system stated that human\\naction recognition in the brain of mammalian leads two distinct pathways in the\\nmodel, which are specialized for analysis of motion (optic flow) and form\\ninformation. Principally, we have defined a novel and robust form features\\napplying active basis model as form extractor in form pathway in the biological\\ninspired model. An unbalanced synergetic neural net-work classifies shapes and\\nstructures of human objects along with tuning its attention parameter by\\nquantum particle swarm optimization (QPSO) via initiation of Centroidal Voronoi\\nTessellations. These tools utilized and justified as strong tools for following\\nbiological system model in form pathway. But the final decision has done by\\ncombination of ultimate outcomes of both pathways via fuzzy inference which\\nincreases novality of proposed model. Combination of these two brain pathways\\nis done by considering each feature sets in Gaussian membership functions with\\nfuzzy product inference method. Two configurations have been proposed for form\\npathway: applying multi-prototype human action templates using two time\\nsynergetic neural network for obtaining uniform template regarding each\\nactions, and second scenario that it uses abstracting human action in four\\nkey-frames. Experimental results showed promising accuracy performance on\\ndifferent datasets (KTH and Weizmann).\\n',\n",
       "  'title': u'Bio-Inspired Human Action Recognition using Hybrid Max-Product\\n  Neuro-Fuzzy Classifier and Quantum-Behaved PSO'},\n",
       " u'1603.02412': {'arxivid': u'1603.02412',\n",
       "  'authorsaffil': [[u'Tomoya Murata', None], [u'Taiji Suzuki', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'30 pages, 12 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02412v1',\n",
       "  'published': u'2016-03-08T08:26:28Z',\n",
       "  'summary': u'  We consider a composite convex minimization problem associated with\\nregularized empirical risk minimization, which often arises in machine\\nlearning. We propose two new stochastic gradient methods that are based on\\nstochastic dual averaging method with variance reduction. Our methods generate\\na sparser solution than the existing methods because we do not need to take the\\naverage of the history of the solutions. This is favorable in terms of both\\ninterpretability and generalization. Moreover, our methods have theoretical\\nsupport for both a strongly and a non-strongly convex regularizer and achieve\\nthe best known convergence rates among existing nonaccelerated stochastic\\ngradient methods.\\n',\n",
       "  'title': u'Stochastic dual averaging methods using variance reduction techniques\\n  for regularized empirical risk minimization problems'},\n",
       " u'1507.02879': {'arxivid': u'1507.02879',\n",
       "  'authorsaffil': [[u'M. Saquib Sarfraz', None],\n",
       "   [u'Rainer Stiefelhagen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'BMVC 2015 (oral)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.02879v1',\n",
       "  'published': u'2015-07-10T12:55:34Z',\n",
       "  'summary': u'  Cross modal face matching between the thermal and visible spectrum is a much\\nde- sired capability for night-time surveillance and security applications. Due\\nto a very large modality gap, thermal-to-visible face recognition is one of the\\nmost challenging face matching problem. In this paper, we present an approach\\nto bridge this modality gap by a significant margin. Our approach captures the\\nhighly non-linear relationship be- tween the two modalities by using a deep\\nneural network. Our model attempts to learn a non-linear mapping from visible\\nto thermal spectrum while preserving the identity in- formation. We show\\nsubstantive performance improvement on a difficult thermal-visible face\\ndataset. The presented approach improves the state-of-the-art by more than 10%\\nin terms of Rank-1 identification and bridge the drop in performance due to the\\nmodality gap by more than 40%.\\n',\n",
       "  'title': u'Deep Perceptual Mapping for Thermal to Visible Face Recognition'},\n",
       " u'1603.08067': {'arxivid': u'1603.08067',\n",
       "  'authorsaffil': [[u'Bo Li', None],\n",
       "   [u'Tianfu Wu', None],\n",
       "   [u'Caiming Xiong', None],\n",
       "   [u'Song-Chun Zhu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted by CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08067v1',\n",
       "  'published': u'2016-03-26T03:45:00Z',\n",
       "  'summary': u'  Physical fluents, a term originally used by Newton [40], refers to\\ntime-varying object states in dynamic scenes. In this paper, we are interested\\nin inferring the fluents of vehicles from video. For example, a door (hood,\\ntrunk) is open or closed through various actions, light is blinking to turn.\\nRecognizing these fluents has broad applications, yet have received scant\\nattention in the computer vision literature. Car fluent recognition entails a\\nunified framework for car detection, car part localization and part status\\nrecognition, which is made difficult by large structural and appearance\\nvariations, low resolutions and occlusions. This paper learns a\\nspatial-temporal And-Or hierarchical model to represent car fluents. The\\nlearning of this model is formulated under the latent structural SVM framework.\\nSince there are no publicly related dataset, we collect and annotate a car\\nfluent dataset consisting of car videos with diverse fluents. In experiments,\\nthe proposed method outperforms several highly related baseline methods in\\nterms of car fluent recognition and car part localization.\\n',\n",
       "  'title': u'Recognizing Car Fluents from Video'},\n",
       " u'1104.2373': {'arxivid': u'1104.2373',\n",
       "  'authorsaffil': [[u'Michael P. Friedlander', None], [u'Mark Schmidt', None]],\n",
       "  'categoryterms': [u'cs.NA', u'cs.SY', u'math.OC', u'stat.ML'],\n",
       "  'comment': u'26 pages. Revised proofs of Theorems 2.6 and 3.1, results unchanged',\n",
       "  'doi': u'10.1137/110830629',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1104.2373v4',\n",
       "  'published': u'2011-04-13T04:20:07Z',\n",
       "  'summary': u'  Many structured data-fitting applications require the solution of an\\noptimization problem involving a sum over a potentially large number of\\nmeasurements. Incremental gradient algorithms offer inexpensive iterations by\\nsampling a subset of the terms in the sum. These methods can make great\\nprogress initially, but often slow as they approach a solution. In contrast,\\nfull-gradient methods achieve steady convergence at the expense of evaluating\\nthe full objective and gradient on each iteration. We explore hybrid methods\\nthat exhibit the benefits of both approaches. Rate-of-convergence analysis\\nshows that by controlling the sample size in an incremental gradient algorithm,\\nit is possible to maintain the steady convergence rates of full-gradient\\nmethods. We detail a practical quasi-Newton implementation based on this\\napproach. Numerical experiments illustrate its potential benefits.\\n',\n",
       "  'title': u'Hybrid Deterministic-Stochastic Methods for Data Fitting'},\n",
       " u'1604.03247': {'arxivid': u'1604.03247',\n",
       "  'authorsaffil': [[u'Dinesh Govindaraj', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03247v1',\n",
       "  'published': u'2016-04-12T04:56:24Z',\n",
       "  'summary': u'  Object Categorization is a challenging problem, especially when the images\\nhave clutter background, occlusions or different lighting conditions. In the\\npast, many descriptors have been proposed which aid object categorization even\\nin such adverse conditions. Each descriptor has its own merits and de-merits.\\nSome descriptors are invariant to transformations while the others are more\\ndiscriminative. Past research has shown that, employing multiple descriptors\\nrather than any single descriptor leads to better recognition. The problem of\\nlearning the optimal combination of the available descriptors for a particular\\nclassification task is studied. Multiple Kernel Learning (MKL) framework has\\nbeen developed for learning an optimal combination of descriptors for object\\ncategorization. Existing MKL formulations often employ block l-1 norm\\nregularization which is equivalent to selecting a single kernel from a library\\nof kernels. Since essentially a single descriptor is selected, the existing\\nformulations maybe sub- optimal for object categorization. A MKL formulation\\nbased on block l-infinity norm regularization has been developed, which chooses\\nan optimal combination of kernels as opposed to selecting a single kernel. A\\nComposite Multiple Kernel Learning(CKL) formulation based on mixed l-infinity\\nand l-1 norm regularization has been developed. These formulations end in\\nSecond Order Cone Programs(SOCP). Other efficient alter- native algorithms for\\nthese formulation have been implemented. Empirical results on benchmark\\ndatasets show significant improvement using these new MKL formulations.\\n',\n",
       "  'title': u'Thesis: Multiple Kernel Learning for Object Categorization'},\n",
       " u'1503.06666': {'arxivid': u'1503.06666',\n",
       "  'authorsaffil': [[u'Francisco Raposo', None],\n",
       "   [u'Ricardo Ribeiro', None],\n",
       "   [u'David Martins de Matos', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.LG', u'cs.SD', u'H.5.5'],\n",
       "  'comment': u'24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio,\\n  Speech and Language Processing',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.06666v3',\n",
       "  'published': u'2015-03-23T14:48:24Z',\n",
       "  'summary': u'  In order to satisfy processing time constraints, many MIR tasks process only\\na segment of the whole music signal. This practice may lead to decreasing\\nperformance, since the most important information for the tasks may not be in\\nthose processed segments. In this paper, we leverage generic summarization\\nalgorithms, previously applied to text and speech summarization, to summarize\\nitems in music datasets. These algorithms build summaries, that are both\\nconcise and diverse, by selecting appropriate segments from the input signal\\nwhich makes them good candidates to summarize music as well. We evaluate the\\nsummarization process on binary and multiclass music genre classification\\ntasks, by comparing the performance obtained using summarized datasets against\\nthe performances obtained using continuous segments (which is the traditional\\nmethod used for addressing the previously mentioned time constraints) and full\\nsongs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA,\\nMMR, and a Support Sets-based Centrality model improve classification\\nperformance when compared to selected 30-second baselines. We also show that\\nsummarized datasets lead to a classification performance whose difference is\\nnot statistically significant from using full songs. Furthermore, we make an\\nargument stating the advantages of sharing summarized datasets for future MIR\\nresearch.\\n',\n",
       "  'title': u'Using Generic Summarization to Improve Music Information Retrieval Tasks'},\n",
       " u'1510.00452': {'arxivid': u'1510.00452',\n",
       "  'authorsaffil': [[u'Akshay Balsubramani', None], [u'Yoav Freund', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'NIPS 2015, \"Learning Faster from Easy Data II\" Workshop',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.00452v4',\n",
       "  'published': u'2015-10-01T23:58:46Z',\n",
       "  'summary': u'  We address the problem of aggregating an ensemble of binary classifiers in a\\nsemi-supervised setting. Recently, this problem was solved optimally using a\\ngame-theoretic approach, but that analysis was specific to the 0-1 loss. In\\nthis paper, we generalize the minimax optimal algorithm of the previous work to\\na very general, novel class of loss functions, including but not limited to all\\nconvex surrogates, while extending its performance and efficiency guarantees.\\n  The result is a family of parameter-free ensemble aggregation algorithms\\nwhich use labeled and unla- beled data; these are as efficient as linear\\nlearning and prediction for convex risk minimization, but work without any\\nrelaxations on many non-convex loss functions. The prediction algorithms take a\\nform familiar in decision theory, applying sigmoid functions to a generalized\\nnotion of ensemble margin, but without the assumptions typically made in\\nmargin-based learning.\\n',\n",
       "  'title': u'Optimal Binary Classifier Aggregation for General Losses'},\n",
       " u'1604.01354': {'arxivid': u'1604.01354',\n",
       "  'authorsaffil': [[u'Stephen Lombardi', None], [u'Ko Nishino', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'16 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01354v1',\n",
       "  'published': u'2016-04-05T18:18:41Z',\n",
       "  'summary': u'  Recovering the radiometric properties of a scene (i.e., the reflectance,\\nillumination, and geometry) is a long-sought ability of computer vision that\\ncan provide invaluable information for a wide range of applications.\\nDeciphering the radiometric ingredients from the appearance of a real-world\\nscene, as opposed to a single isolated object, is particularly challenging as\\nit generally consists of various objects with different material compositions\\nexhibiting complex reflectance and light interactions that are also part of the\\nillumination. We introduce the first method for radiometric scene decomposition\\nthat handles those intricacies. We use RGB-D images to bootstrap geometry\\nrecovery and simultaneously recover the complex reflectance and natural\\nillumination while refining the noisy initial geometry and segmenting the scene\\ninto different material regions. Most important, we handle real-world scenes\\nconsisting of multiple objects of unknown materials, which necessitates the\\nmodeling of spatially-varying complex reflectance, natural illumination,\\ntexture, interreflection and shadows. We systematically evaluate the\\neffectiveness of our method on synthetic scenes and demonstrate its application\\nto real-world scenes. The results show that rich radiometric information can be\\nrecovered from RGB-D images and demonstrate a new role RGB-D sensors can play\\nfor general scene understanding tasks.\\n',\n",
       "  'title': u'Radiometric Scene Decomposition: Scene Reflectance, Illumination, and\\n  Geometry from RGB-D Images'},\n",
       " u'1602.05531': {'arxivid': u'1602.05531',\n",
       "  'authorsaffil': [[u'Simone Bianco', None],\n",
       "   [u'Luigi Celona', None],\n",
       "   [u'Paolo Napoletano', None],\n",
       "   [u'Raimondo Schettini', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05531v3',\n",
       "  'published': u'2016-02-17T19:12:50Z',\n",
       "  'summary': u'  In this work we investigate the use of deep learning for distortion-generic\\nblind image quality assessment. We report on different design choices, ranging\\nfrom the use of features extracted from pre-trained Convolutional Neural\\nNetworks (CNNs) as a generic image description, to the use of features\\nextracted from a CNN fine-tuned for the image quality task. Our best proposal,\\nnamed DeepBIQ, estimates the image quality by average pooling the scores\\npredicted on multiple sub-regions of the original image. The score of each\\nsub-region is computed using a Support Vector Regression (SVR) machine taking\\nas input features extracted using a CNN fine-tuned for category-based image\\nquality assessment. Experimental results on the LIVE In the Wild Image Quality\\nChallenge Database and on the LIVE Image Quality Assessment Database show that\\nDeepBIQ outperforms the state-of-the-art methods compared, having a Linear\\nCorrelation Coefficient (LCC) with human subjective scores of almost 0.91 and\\n0.98 respectively. Furthermore, in most of the cases, the quality score\\npredictions of DeepBIQ are closer to the average observer than those of a\\ngeneric human observer.\\n',\n",
       "  'title': u'On the Use of Deep Learning for Blind Image Quality Assessment'},\n",
       " u'1603.06159': {'arxivid': u'1603.06159',\n",
       "  'authorsaffil': [[u'Sashank J. Reddi', None],\n",
       "   [u'Suvrit Sra', None],\n",
       "   [u'Barnabas Poczos', None],\n",
       "   [u'Alex Smola', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06159v1',\n",
       "  'published': u'2016-03-19T23:28:44Z',\n",
       "  'summary': u\"  We analyze a fast incremental aggregated gradient method for optimizing\\nnonconvex problems of the form $\\\\min_x \\\\sum_i f_i(x)$. Specifically, we analyze\\nthe SAGA algorithm within an Incremental First-order Oracle framework, and show\\nthat it converges to a stationary point provably faster than both gradient\\ndescent and stochastic gradient descent. We also discuss a Polyak's special\\nclass of nonconvex problems for which SAGA converges at a linear rate to the\\nglobal optimum. Finally, we analyze the practically valuable regularized and\\nminibatch variants of SAGA. To our knowledge, this paper presents the first\\nanalysis of fast convergence for an incremental aggregated gradient method for\\nnonconvex problems.\\n\",\n",
       "  'title': u'Fast Incremental Method for Nonconvex Optimization'},\n",
       " u'1602.09013': {'arxivid': u'1602.09013',\n",
       "  'authorsaffil': [[u'Anastasia Podosinnikova', None],\n",
       "   [u'Francis Bach', None],\n",
       "   [u'Simon Lacoste-Julien', None]],\n",
       "  'categoryterms': [u'stat.ML',\n",
       "   u'cs.LG',\n",
       "   u'62H12 62H20 62H25 62F10 68T05 68T50 90C26 90C90',\n",
       "   u'I.2.6; I.2.7; G.1.3; G.1.6; G.3'],\n",
       "  'comment': u'Appears in: Proceedings of the 33rd International Conference on\\n  Machine Learning (ICML 2016). 22 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.09013v2',\n",
       "  'published': u'2016-02-29T15:51:50Z',\n",
       "  'summary': u'  We introduce three novel semi-parametric extensions of probabilistic\\ncanonical correlation analysis with identifiability guarantees. We consider\\nmoment matching techniques for estimation in these models. For that, by drawing\\nexplicit links between the new models and a discrete version of independent\\ncomponent analysis (DICA), we first extend the DICA cumulant tensors to the new\\ndiscrete version of CCA. By further using a close connection with independent\\ncomponent analysis, we introduce generalized covariance matrices, which can\\nreplace the cumulant tensors in the moment matching framework, and, therefore,\\nimprove sample complexity and simplify derivations and algorithms\\nsignificantly. As the tensor power method or orthogonal joint diagonalization\\nare not applicable in the new setting, we use non-orthogonal joint\\ndiagonalization techniques for matching the cumulants. We demonstrate\\nperformance of the proposed models and estimation techniques on experiments\\nwith both synthetic and real datasets.\\n',\n",
       "  'title': u'Beyond CCA: Moment Matching for Multi-View Models'},\n",
       " u'1602.03943': {'arxivid': u'1602.03943',\n",
       "  'authorsaffil': [[u'Naman Agarwal', None],\n",
       "   [u'Brian Bullins', None],\n",
       "   [u'Elad Hazan', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03943v3',\n",
       "  'published': u'2016-02-12T01:38:05Z',\n",
       "  'summary': u'  Stochastic optimization and, in particular, first-order stochastic methods\\nare a cornerstone of modern machine learning due to their extremely efficient\\nper-iteration computational cost. Second-order methods, while able to provide\\nfaster per-iteration convergence, have been much less explored due to the high\\ncost of computing the second-order information. In this paper we develop a\\nsecond-order stochastic method for optimization problems arising in machine\\nlearning based on novel matrix randomization techniques that match the\\nper-iteration cost of gradient descent, yet enjoy the linear-convergence\\nproperties of second-order optimization. We also consider the special case of\\nself-concordant functions where we show that a first order method can achieve\\nlinear convergence with guarantees independent of the condition number. We\\ndemonstrate significant speedups for training linear classifiers over several\\nconvex benchmarks.\\n',\n",
       "  'title': u'Second Order Stochastic Optimization in Linear Time'},\n",
       " u'1603.06155': {'arxivid': u'1603.06155',\n",
       "  'authorsaffil': [[u'Jiwei Li', None],\n",
       "   [u'Michel Galley', None],\n",
       "   [u'Chris Brockett', None],\n",
       "   [u'Georgios P. Spithourakis', None],\n",
       "   [u'Jianfeng Gao', None],\n",
       "   [u'Bill Dolan', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Accepted for publication at ACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06155v2',\n",
       "  'published': u'2016-03-19T23:15:18Z',\n",
       "  'summary': u'  We present persona-based models for handling the issue of speaker consistency\\nin neural response generation. A speaker model encodes personas in distributed\\nembeddings that capture individual characteristics such as background\\ninformation and speaking style. A dyadic speaker-addressee model captures\\nproperties of interactions between two interlocutors. Our models yield\\nqualitative performance improvements in both perplexity and BLEU scores over\\nbaseline sequence-to-sequence models, with similar gains in speaker consistency\\nas measured by human judges.\\n',\n",
       "  'title': u'A Persona-Based Neural Conversation Model'},\n",
       " u'1511.03466': {'arxivid': u'1511.03466',\n",
       "  'authorsaffil': [[u'Ksenia Konyushkova', None],\n",
       "   [u'Nikolaos Arvanitopoulos', None],\n",
       "   [u'Zhargalma Dandarova Robert', None],\n",
       "   [u'Pierre-Yves Brandt', None],\n",
       "   [u'Sabine S\\xfcsstrunk', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03466v2',\n",
       "  'published': u'2015-11-11T11:47:58Z',\n",
       "  'summary': u\"  This paper introduces a novel approach to data analysis designed for the\\nneeds of specialists in psychology of religion. We detect developmental and\\ncross-cultural patterns in children's drawings of God(s) and other supernatural\\nagents. We develop methods to objectively evaluate our empirical observations\\nof the drawings with respect to: (1) the gravity center, (2) the average\\nintensities of the colors \\\\emph{green} and \\\\emph{yellow}, (3) the use of\\ndifferent colors (palette) and (4) the visual complexity of the drawings. We\\nfind statistically significant differences across ages and countries in the\\ngravity centers and in the average intensities of colors. These findings\\nsupport the hypotheses of the experts and raise new questions for further\\ninvestigation.\\n\",\n",
       "  'title': u'God(s) Know(s): Developmental and Cross-Cultural Patterns in Children\\n  Drawings'},\n",
       " u'1602.08045': {'arxivid': u'1602.08045',\n",
       "  'authorsaffil': [[u'Zhenhao Ge', None],\n",
       "   [u'Sudhendu R. Sharma', None],\n",
       "   [u'Mark J. T. Smith', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.LG'],\n",
       "  'comment': u'Society of Photo-Optical Instrumentation Engineers (SPIE) Conference\\n  Series',\n",
       "  'doi': u'10.1117/12.919235',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08045v1',\n",
       "  'published': u'2016-02-25T19:18:06Z',\n",
       "  'summary': u'  Various algorithms for text-independent speaker recognition have been\\ndeveloped through the decades, aiming to improve both accuracy and e?ciency.\\nThis paper presents a novel PCA/LDA-based approach that is faster than\\ntraditional statistical model-based methods and achieves competitive results.\\nFirst, the performance based on only PCA and only LDA is measured; then a mixed\\nmodel, taking advantages of both methods, is introduced. A subset of the TIMIT\\ncorpus composed of 200 male speakers, is used for enrollment, validation and\\ntesting. The best results achieve 100%; 96% and 95% classi?cation rate at\\npopulation level 50; 100 and 200, using 39-dimensional MFCC features with delta\\nand double delta. These results are based on 12-second text-independent speech\\nfor training and 4-second data for test. These are comparable to the\\nconventional MFCC-GMM methods, but require signi?cantly less time to train and\\noperate.\\n',\n",
       "  'title': u'PCA/LDA Approach for Text-Independent Speaker Recognition'},\n",
       " u'1602.04621': {'arxivid': u'1602.04621',\n",
       "  'authorsaffil': [[u'Ian Osband', None],\n",
       "   [u'Charles Blundell', None],\n",
       "   [u'Alexander Pritzel', None],\n",
       "   [u'Benjamin Van Roy', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.SY', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04621v1',\n",
       "  'published': u'2016-02-15T10:54:20Z',\n",
       "  'summary': u'  Efficient exploration in complex environments remains a major challenge for\\nreinforcement learning. We propose bootstrapped DQN, a simple algorithm that\\nexplores in a computationally and statistically efficient manner through use of\\nrandomized value functions. Unlike dithering strategies such as epsilon-greedy\\nexploration, bootstrapped DQN carries out temporally-extended (or deep)\\nexploration; this can lead to exponentially faster learning. We demonstrate\\nthese benefits in complex stochastic MDPs and in the large-scale Arcade\\nLearning Environment. Bootstrapped DQN substantially improves learning times\\nand performance across most Atari games.\\n',\n",
       "  'title': u'Deep Exploration via Bootstrapped DQN'},\n",
       " u'1602.08952': {'arxivid': u'1602.08952',\n",
       "  'authorsaffil': [[u'\\xc1kos K\\xe1d\\xe1r', None],\n",
       "   [u'Grzegorz Chrupa\\u0142a', None],\n",
       "   [u'Afra Alishahi', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08952v2',\n",
       "  'published': u'2016-02-29T13:31:17Z',\n",
       "  'summary': u'  We present novel methods for analyzing the activation patterns of RNNs from a\\nlinguistic point of view and explore the types of linguistic structure they\\nlearn. As a case study, we use a multi-task gated recurrent network\\narchitecture consisting of two parallel pathways with shared word embeddings\\ntrained on predicting the representations of the visual scene corresponding to\\nan input sentence, and predicting the next word in the same sentence. Based on\\nour proposed method to estimate the amount of contribution of individual tokens\\nin the input to the final prediction of the networks we show that the image\\nprediction pathway: a) is sensitive to the information structure of the\\nsentence b) pays selective attention to lexical categories and grammatical\\nfunctions that carry semantic information c) learns to treat the same input\\ntoken differently depending on its grammatical functions in the sentence. In\\ncontrast the language model is comparatively more sensitive to words with a\\nsyntactic function. Furthermore, we propose methods to ex- plore the function\\nof individual hidden units in RNNs and show that the two pathways of the\\narchitecture in our case study contain specialized units tuned to patterns\\ninformative for the task, some of which can carry activations to later time\\nsteps to encode long-term dependencies.\\n',\n",
       "  'title': u'Representation of linguistic form and function in recurrent neural\\n  networks'},\n",
       " u'1512.08178': {'arxivid': u'1512.08178',\n",
       "  'authorsaffil': [[u'Jean-Baptiste Fiot', None],\n",
       "   [u'Francesco Dinuzzo', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.08178v1',\n",
       "  'published': u'2015-12-27T07:18:03Z',\n",
       "  'summary': u'  We explore the application of kernel-based multi-task learning techniques to\\nforecast the demand of electricity in multiple nodes of a distribution network.\\nWe show that recently developed output kernel learning techniques are\\nparticularly well suited to solve this problem, as they allow to flexibly model\\nthe complex seasonal effects that characterize electricity demand data, while\\nlearning and exploiting correlations between multiple demand profiles. We also\\ndemonstrate that kernels with a multiplicative structure yield superior\\npredictive performance with respect to the widely adopted (generalized)\\nadditive models. Our study is based on residential and industrial smart meter\\ndata provided by the Irish Commission for Energy Regulation (CER).\\n',\n",
       "  'title': u'Electricity Demand Forecasting by Multi-Task Learning'},\n",
       " u'1509.08333': {'arxivid': u'1509.08333',\n",
       "  'authorsaffil': [[u'Hsiang-Fu Yu', None],\n",
       "   [u'Nikhil Rao', None],\n",
       "   [u'Inderjit S. Dhillon', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.08333v3',\n",
       "  'published': u'2015-09-28T14:37:14Z',\n",
       "  'summary': u'  High-dimensional time series prediction is needed in applications as diverse\\nas demand forecasting and climatology. Often, such applications require methods\\nthat are both highly scalable, and deal with noisy data in terms of corruptions\\nor missing values. Classical time series methods usually fall short of handling\\nboth these issues. In this paper, we propose to adapt matrix matrix completion\\napproaches that have previously been successfully applied to large scale noisy\\ndata, but which fail to adequately model high-dimensional time series due to\\ntemporal dependencies. We present a novel temporal regularized matrix\\nfactorization (TRMF) framework which supports data-driven temporal dependency\\nlearning and enables forecasting ability to our new matrix factorization\\napproach. TRMF is highly general, and subsumes many existing matrix\\nfactorization approaches for time series data. We make interesting connections\\nto graph regularized matrix factorization methods in the context of learning\\nthe dependencies. Experiments on both real and synthetic data show that TRMF\\noutperforms several existing approaches for common time series tasks.\\n',\n",
       "  'title': u'High-dimensional Time Series Prediction with Missing Values'},\n",
       " u'1412.1897': {'arxivid': u'1412.1897',\n",
       "  'authorsaffil': [[u'Anh Nguyen', None],\n",
       "   [u'Jason Yosinski', None],\n",
       "   [u'Jeff Clune', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.NE'],\n",
       "  'comment': u'To appear at CVPR 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1412.1897v4',\n",
       "  'published': u'2014-12-05T05:29:43Z',\n",
       "  'summary': u'  Deep neural networks (DNNs) have recently been achieving state-of-the-art\\nperformance on a variety of pattern-recognition tasks, most notably visual\\nclassification problems. Given that DNNs are now able to classify objects in\\nimages with near-human-level performance, questions naturally arise as to what\\ndifferences remain between computer and human vision. A recent study revealed\\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\\nlion a library). Here we show a related result: it is easy to produce images\\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\\ncertainty that white noise static is a lion). Specifically, we take\\nconvolutional neural networks trained to perform well on either the ImageNet or\\nMNIST datasets and then find images with evolutionary algorithms or gradient\\nascent that DNNs label with high confidence as belonging to each dataset class.\\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\\nbelieve with near certainty are familiar objects, which we call \"fooling\\nimages\" (more generally, fooling examples). Our results shed light on\\ninteresting differences between human vision and current DNNs, and raise\\nquestions about the generality of DNN computer vision.\\n',\n",
       "  'title': u'Deep Neural Networks are Easily Fooled: High Confidence Predictions for\\n  Unrecognizable Images'},\n",
       " u'1602.03570': {'arxivid': u'1602.03570',\n",
       "  'authorsaffil': [[u'Azadeh Alavi', None],\n",
       "   [u'Vishal M Patel', None],\n",
       "   [u'Rama Chellappa', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'14 pages, 6 figures, conference',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03570v3',\n",
       "  'published': u'2016-02-10T23:14:17Z',\n",
       "  'summary': u'  It is proven that encoding images and videos through Symmetric Positive\\nDefinite (SPD) matrices, and considering the Riemannian geometry of the\\nresulting space, can lead to increased classification performance. Taking into\\naccount manifold geometry is typically done via embedding the manifolds in\\ntangent spaces, or Reproducing Kernel Hilbert Spaces (RKHS). Recently, it was\\nshown that embedding such manifolds into a Random Projection Spaces (RPS),\\nrather than RKHS or tangent space, leads to higher classification and\\nclustering performance. However, based on structure and dimensionality of the\\nrandomly generated hyperplanes, the classification performance over RPS may\\nvary significantly. In addition, fine-tuning RPS is data expensive (as it\\nrequires validation-data), time consuming, and resource demanding. In this\\npaper, we introduce an approach to learn an optimized kernel-based projection\\n(with fixed dimensionality), by employing the concept of subspace clustering.\\nAs such, we encode the association of data points to the underlying subspace of\\neach point, to generate meaningful hyperplanes. Further, we adopt the concept\\nof dictionary learning and sparse coding, and discriminative analysis, for the\\noptimized kernel-based projection space (OPS) on SPD manifolds. We validate our\\nalgorithm on several classification tasks. The experiment results also\\ndemonstrate that the proposed method outperforms state-of-the-art methods on\\nsuch manifolds.\\n',\n",
       "  'title': u'Optimized Kernel-based Projection Space of Riemannian Manifolds'},\n",
       " u'1602.01729': {'arxivid': u'1602.01729',\n",
       "  'authorsaffil': [[u'Fei Zhu', None],\n",
       "   [u'Abderrahim Halimi', None],\n",
       "   [u'Paul Honeine', None],\n",
       "   [u'Badong Chen', None],\n",
       "   [u'Nanning Zheng', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV', u'cs.NE'],\n",
       "  'comment': u'23 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01729v1',\n",
       "  'published': u'2016-02-04T16:21:09Z',\n",
       "  'summary': u'  In hyperspectral images, some spectral bands suffer from low signal-to-noise\\nratio due to noisy acquisition and atmospheric effects, thus requiring robust\\ntechniques for the unmixing problem. This paper presents a robust supervised\\nspectral unmixing approach for hyperspectral images. The robustness is achieved\\nby writing the unmixing problem as the maximization of the correntropy\\ncriterion subject to the most commonly used constraints. Two unmixing problems\\nare derived: the first problem considers the fully-constrained unmixing, with\\nboth the non-negativity and sum-to-one constraints, while the second one deals\\nwith the non-negativity and the sparsity-promoting of the abundances. The\\ncorresponding optimization problems are solved efficiently using an alternating\\ndirection method of multipliers (ADMM) approach. Experiments on synthetic and\\nreal hyperspectral images validate the performance of the proposed algorithms\\nfor different scenarios, demonstrating that the correntropy-based unmixing is\\nrobust to outlier bands.\\n',\n",
       "  'title': u'Correntropy Maximization via ADMM - Application to Robust Hyperspectral\\n  Unmixing'},\n",
       " u'1602.01728': {'arxivid': u'1602.01728',\n",
       "  'authorsaffil': [[u'M. J. Shafiee', None],\n",
       "   [u'P. Siva', None],\n",
       "   [u'C. Scharfenberger', None],\n",
       "   [u'P. Fieguth', None],\n",
       "   [u'A. Wong', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'5 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01728v1',\n",
       "  'published': u'2016-02-04T16:20:26Z',\n",
       "  'summary': u'  In this paper, a novel approach to visual salience detection via Neural\\nResponse Divergence (NeRD) is proposed, where synaptic portions of deep neural\\nnetworks, previously trained for complex object recognition, are leveraged to\\ncompute low level cues that can be used to compute image region\\ndistinctiveness. Based on this concept , an efficient visual salience detection\\nframework is proposed using deep convolutional StochasticNets. Experimental\\nresults using CSSD and MSRA10k natural image datasets show that the proposed\\nNeRD approach can achieve improved performance when compared to\\nstate-of-the-art image saliency approaches, while the attaining low\\ncomputational complexity necessary for near-real-time computer vision\\napplications.\\n',\n",
       "  'title': u'NeRD: a Neural Response Divergence Approach to Visual Salience Detection'},\n",
       " u'1512.05193': {'arxivid': u'1512.05193',\n",
       "  'authorsaffil': [[u'Wenpeng Yin', None],\n",
       "   [u'Hinrich Sch\\xfctze', None],\n",
       "   [u'Bing Xiang', None],\n",
       "   [u'Bowen Zhou', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Accepted by TACL, to appear',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05193v3',\n",
       "  'published': u'2015-12-16T14:55:17Z',\n",
       "  'summary': u\"  How to model a pair of sentences is a critical issue in many NLP tasks such\\nas answer selection (AS), paraphrase identification (PI) and textual entailment\\n(TE). Most prior work (i) deals with one individual task by fine-tuning a\\nspecific system; (ii) models each sentence's representation separately, rarely\\nconsidering the impact of the other sentence; or (iii) relies fully on manually\\ndesigned, task-specific linguistic features. This work presents a general\\nAttention Based Convolutional Neural Network (ABCNN) for modeling a pair of\\nsentences. We make three contributions. (i) ABCNN can be applied to a wide\\nvariety of tasks that require modeling of sentence pairs. (ii) We propose three\\nattention schemes that integrate mutual influence between sentences into CNN;\\nthus, the representation of each sentence takes into consideration its\\ncounterpart. These interdependent sentence pair representations are more\\npowerful than isolated sentence representations. (iii) ABCNN achieves\\nstate-of-the-art performance on AS, PI and TE tasks.\\n\",\n",
       "  'title': u'ABCNN: Attention-Based Convolutional Neural Network for Modeling\\n  Sentence Pairs'},\n",
       " u'1604.03628': {'arxivid': u'1604.03628',\n",
       "  'authorsaffil': [[u'Jianwei Yang', None],\n",
       "   [u'Devi Parikh', None],\n",
       "   [u'Dhruv Batra', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'18 pages, 10 figures, 12 tables, 2016 IEEE Conference on Computer\\n  Vision and Pattern Recognition (CVPR)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03628v2',\n",
       "  'published': u'2016-04-13T01:24:59Z',\n",
       "  'summary': u'  In this paper, we propose a recurrent framework for joint unsupervised\\nlearning of deep representations and image clusters. In our framework,\\nsuccessive operations in a clustering algorithm are expressed as steps in a\\nrecurrent process, stacked on top of representations output by a Convolutional\\nNeural Network (CNN). During training, image clusters and representations are\\nupdated jointly: image clustering is conducted in the forward pass, while\\nrepresentation learning in the backward pass. Our key idea behind this\\nframework is that good representations are beneficial to image clustering and\\nclustering results provide supervisory signals to representation learning. By\\nintegrating two processes into a single model with a unified weighted triplet\\nloss and optimizing it end-to-end, we can obtain not only more powerful\\nrepresentations, but also more precise image clusters. Extensive experiments\\nshow that our method outperforms the state-of-the-art on image clustering\\nacross a variety of image datasets. Moreover, the learned representations\\ngeneralize well when transferred to other tasks.\\n',\n",
       "  'title': u'Joint Unsupervised Learning of Deep Representations and Image Clusters'},\n",
       " u'1603.02250': {'arxivid': u'1603.02250',\n",
       "  'authorsaffil': [[u'Dean Foster', None],\n",
       "   [u'Satyen Kale', None],\n",
       "   [u'Howard Karloff', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02250v1',\n",
       "  'published': u'2016-03-07T20:49:52Z',\n",
       "  'summary': u'  We consider the online sparse linear regression problem, which is the problem\\nof sequentially making predictions observing only a limited number of features\\nin each round, to minimize regret with respect to the best sparse linear\\nregressor, where prediction accuracy is measured by square loss. We give an\\ninefficient algorithm that obtains regret bounded by $\\\\tilde{O}(\\\\sqrt{T})$\\nafter $T$ prediction rounds. We complement this result by showing that no\\nalgorithm running in polynomial time per iteration can achieve regret bounded\\nby $O(T^{1-\\\\delta})$ for any constant $\\\\delta > 0$ unless $\\\\text{NP} \\\\subseteq\\n\\\\text{BPP}$. This computational hardness result resolves an open problem\\npresented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013).\\nThis hardness result holds even if the algorithm is allowed to access more\\nfeatures than the best sparse linear regressor up to a logarithmic factor in\\nthe dimension.\\n',\n",
       "  'title': u'Online Sparse Linear Regression'},\n",
       " u'1601.03128': {'arxivid': u'1601.03128',\n",
       "  'authorsaffil': [[u'Anand Mishra', None],\n",
       "   [u'Karteek Alahari', None],\n",
       "   [u'C. V. Jawahar', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1016/j.cviu.2016.01.002',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03128v1',\n",
       "  'published': u'2016-01-13T04:47:28Z',\n",
       "  'summary': u'  Recognizing scene text is a challenging problem, even more so than the\\nrecognition of scanned documents. This problem has gained significant attention\\nfrom the computer vision community in recent years, and several methods based\\non energy minimization frameworks and deep learning approaches have been\\nproposed. In this work, we focus on the energy minimization framework and\\npropose a model that exploits both bottom-up and top-down cues for recognizing\\ncropped words extracted from street images. The bottom-up cues are derived from\\nindividual character detections from an image. We build a conditional random\\nfield model on these detections to jointly model the strength of the detections\\nand the interactions between them. These interactions are top-down cues\\nobtained from a lexicon-based prior, i.e., language statistics. The optimal\\nword represented by the text image is obtained by minimizing the energy\\nfunction corresponding to the random field model. We evaluate our proposed\\nalgorithm extensively on a number of cropped scene text benchmark datasets,\\nnamely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word,\\nand show better performance than comparable methods. We perform a rigorous\\nanalysis of all the steps in our approach and analyze the results. We also show\\nthat state-of-the-art convolutional neural network features can be integrated\\nin our framework to further improve the recognition performance.\\n',\n",
       "  'title': u'Enhancing Energy Minimization Framework for Scene Text Recognition with\\n  Top-Down Cues'},\n",
       " u'1603.02252': {'arxivid': u'1603.02252',\n",
       "  'authorsaffil': [[u'Wenbin Li', None],\n",
       "   [u'Darren Cosker', None],\n",
       "   [u'Matthew Brown', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Preprint version of our paper accepted by Journal of Intelligent and\\n  Fuzzy Systems',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02252v1',\n",
       "  'published': u'2016-03-07T20:51:15Z',\n",
       "  'summary': u'  It is hard to densely track a nonrigid object in long term, which is a\\nfundamental research issue in the computer vision community. This task often\\nrelies on estimating pairwise correspondences between images over time where\\nthe error is accumulated and leads to a drift issue. In this paper, we\\nintroduce a novel optimization framework with an Anchor Patch constraint. It is\\nsupposed to significantly reduce overall errors given long sequences containing\\nnon-rigidly deformable objects. Our framework can be applied to any dense\\ntracking algorithm, e.g. optical flow. We demonstrate the success of our\\napproach by showing significant error reduction on 6 popular optical flow\\nalgorithms applied to a range of real-world nonrigid benchmarks. We also\\nprovide quantitative analysis of our approach given synthetic occlusions and\\nimage noise.\\n',\n",
       "  'title': u'Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences'},\n",
       " u'1603.09302': {'arxivid': u'1603.09302',\n",
       "  'authorsaffil': [[u'Valsamis Ntouskos', None], [u'Fiora Pirri', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09302v2',\n",
       "  'published': u'2016-03-30T18:27:22Z',\n",
       "  'summary': u'  We introduce a novel model for spatially varying variational data fusion,\\ndriven by point-wise confidence values. The proposed model allows for the joint\\nestimation of the data and the confidence values based on the spatial coherence\\nof the data. We discuss the main properties of the introduced model as well as\\nsuitable algorithms for estimating the solution of the corresponding biconvex\\nminimization problem and their convergence. The performance of the proposed\\nmodel is evaluated considering the problem of depth image fusion by using both\\nsynthetic and real data from publicly available datasets.\\n',\n",
       "  'title': u'Confidence driven TGV fusion'},\n",
       " u'1601.03124': {'arxivid': u'1601.03124',\n",
       "  'authorsaffil': [[u'Guangyong Chen', None],\n",
       "   [u'Fengyuan Zhu', None],\n",
       "   [u'Pheng Ann Heng', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'26 pages, 10 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03124v1',\n",
       "  'published': u'2016-01-13T04:20:09Z',\n",
       "  'summary': u'  Dyadic Data Prediction (DDP) is an important problem in many research areas.\\nThis paper develops a novel fully Bayesian nonparametric framework which\\nintegrates two popular and complementary approaches, discrete mixed membership\\nmodeling and continuous latent factor modeling into a unified Heterogeneous\\nMatrix Factorization~(HeMF) model, which can predict the unobserved dyadics\\naccurately. The HeMF can determine the number of communities automatically and\\nexploit the latent linear structure for each bicluster efficiently. We propose\\na Variational Bayesian method to estimate the parameters and missing data. We\\nfurther develop a novel online learning approach for Variational inference and\\nuse it for the online learning of HeMF, which can efficiently cope with the\\nimportant large-scale DDP problem. We evaluate the performance of our method on\\nthe EachMoive, MovieLens and Netflix Prize collaborative filtering datasets.\\nThe experiment shows that, our model outperforms state-of-the-art methods on\\nall benchmarks. Compared with Stochastic Gradient Method (SGD), our online\\nlearning approach achieves significant improvement on the estimation accuracy\\nand robustness.\\n',\n",
       "  'title': u'Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization'},\n",
       " u'1602.05404': {'arxivid': u'1602.05404',\n",
       "  'authorsaffil': [[u'Jos W. H. M. Uiterwijk', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05404v1',\n",
       "  'published': u'2016-02-17T13:34:04Z',\n",
       "  'summary': u'  We have developed a program called MUDoS (Maastricht University Domineering\\nSolver) that solves Domineering positions in a very efficient way. This enables\\nthe solution of known positions so far (up to the 10 x 10 board) much quicker\\n(measured in number of investigated nodes).\\n  More importantly, it enables the solution of the 11 x 11 Domineering board, a\\nboard up till now far out of reach of previous Domineering solvers. The\\nsolution needed the investigation of 259,689,994,008 nodes, using almost half a\\nyear of computation time on a single simple desktop computer. The results show\\nthat under optimal play the first player wins the 11 x 11 Domineering game,\\nirrespective if Vertical or Horizontal starts the game.\\n  In addition, several other boards hitherto unsolved were solved. Using the\\nconvention that Vertical starts, the 8 x 15, 11 x 9, 12 x 8, 12 x 15, 14 x 8,\\nand 17 x 6 boards are all won by Vertical, whereas the 6 x 17, 8 x 12, 9 x 11,\\nand 11 x 10 boards are all won by Horizontal.\\n',\n",
       "  'title': u'11 x 11 Domineering is Solved: The first player wins'},\n",
       " u'1601.00893': {'arxivid': u'1601.00893',\n",
       "  'authorsaffil': [[u'Oren Melamud', None],\n",
       "   [u'David McClosky', None],\n",
       "   [u'Siddharth Patwardhan', None],\n",
       "   [u'Mohit Bansal', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00893v1',\n",
       "  'published': u'2016-01-05T16:28:42Z',\n",
       "  'summary': u'  We provide the first extensive evaluation of how using different types of\\ncontext to learn skip-gram word embeddings affects performance on a wide range\\nof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic\\ntasks tend to exhibit a clear preference to particular types of contexts and\\nhigher dimensionality, more careful tuning is required for finding the optimal\\nsettings for most of the extrinsic tasks that we considered. Furthermore, for\\nthese extrinsic tasks, we find that once the benefit from increasing the\\nembedding dimensionality is mostly exhausted, simple concatenation of word\\nembeddings, learned with different context types, can yield further performance\\ngains. As an additional contribution, we propose a new variant of the skip-gram\\nmodel that learns word embeddings from weighted contexts of substitute words.\\n',\n",
       "  'title': u'The Role of Context Types and Dimensionality in Learning Word Embeddings'},\n",
       " u'1511.05042': {'arxivid': u'1511.05042',\n",
       "  'authorsaffil': [[u'Alexandre de Br\\xe9bisson', None],\n",
       "   [u'Pascal Vincent', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Published at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05042v3',\n",
       "  'published': u'2015-11-16T17:15:51Z',\n",
       "  'summary': u'  In a multi-class classification problem, it is standard to model the output\\nof a neural network as a categorical distribution conditioned on the inputs.\\nThe output must therefore be positive and sum to one, which is traditionally\\nenforced by a softmax. This probabilistic mapping allows to use the maximum\\nlikelihood principle, which leads to the well-known log-softmax loss. However\\nthe choice of the softmax function seems somehow arbitrary as there are many\\nother possible normalizing functions. It is thus unclear why the log-softmax\\nloss would perform better than other loss alternatives. In particular Vincent\\net al. (2015) recently introduced a class of loss functions, called the\\nspherical family, for which there exists an efficient algorithm to compute the\\nupdates of the output weights irrespective of the output size. In this paper,\\nwe explore several loss functions from this family as possible alternatives to\\nthe traditional log-softmax. In particular, we focus our investigation on\\nspherical bounds of the log-softmax loss and on two spherical log-likelihood\\nlosses, namely the log-Spherical Softmax suggested by Vincent et al. (2015) and\\nthe log-Taylor Softmax that we introduce. Although these alternatives do not\\nyield as good results as the log-softmax loss on two language modeling tasks,\\nthey surprisingly outperform it in our experiments on MNIST and CIFAR-10,\\nsuggesting that they might be relevant in a broad range of applications.\\n',\n",
       "  'title': u'An Exploration of Softmax Alternatives Belonging to the Spherical Loss\\n  Family'},\n",
       " u'1506.02327': {'arxivid': u'1506.02327',\n",
       "  'authorsaffil': [[u'Cheng-Tao Chung', None],\n",
       "   [u'Cheng-Yu Tsai', None],\n",
       "   [u'Hsiang-Hung Lu', None],\n",
       "   [u'Yuan-ming Liou', None],\n",
       "   [u'Yen-Chen Wu', None],\n",
       "   [u'Yen-Ju Lu', None],\n",
       "   [u'Hung-yi Lee', None],\n",
       "   [u'Lin-shan Lee', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'submitted to Interspeech 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.02327v1',\n",
       "  'published': u'2015-06-07T23:52:54Z',\n",
       "  'summary': u'  This paper summarizes the work done by the authors for the Zero Resource\\nSpeech Challenge organized in the technical program of Interspeech 2015. The\\ngoal of the challenge is to discover linguistic units directly from unlabeled\\nspeech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work\\nautomatically discovers multiple sets of acoustic tokens from the given corpus.\\nEach acoustic token set is specified by a set of hyperparameters that describe\\nthe model configuration. These sets of acoustic tokens carry different\\ncharacteristics of the given corpus and the language behind thus can be\\nmutually reinforced. The multiple sets of token labels are then used as the\\ntargets of a Multi-target DNN (MDNN) trained on low-level acoustic features.\\nBottleneck features extracted from the MDNN are used as feedback for the MAT\\nand the MDNN itself. We call this iterative system the Multi-layered Acoustic\\nTokenizing Deep Neural Network (MAT-DNN) which generates high quality features\\nfor track 1 of the challenge and acoustic tokens for track 2 of the challenge.\\n',\n",
       "  'title': u'A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for\\n  Unsupervised Discovery of Linguistic Units and Generation of High Quality\\n  Features'},\n",
       " u'1604.02885': {'arxivid': u'1604.02885',\n",
       "  'authorsaffil': [[u'Nikolay Savinov', None],\n",
       "   [u'Christian Haene', None],\n",
       "   [u'Lubor Ladicky', None],\n",
       "   [u'Marc Pollefeys', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted as a spotlight oral paper by CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02885v2',\n",
       "  'published': u'2016-04-11T11:12:24Z',\n",
       "  'summary': u'  We propose an approach for dense semantic 3D reconstruction which uses a data\\nterm that is defined as potentials over viewing rays, combined with continuous\\nsurface area penalization. Our formulation is a convex relaxation which we\\naugment with a crucial non-convex constraint that ensures exact handling of\\nvisibility. To tackle the non-convex minimization problem, we propose a\\nmajorize-minimize type strategy which converges to a critical point. We\\ndemonstrate the benefits of using the non-convex constraint experimentally. For\\nthe geometry-only case, we set a new state of the art on two datasets of the\\ncommonly used Middlebury multi-view stereo benchmark. Moreover, our\\ngeneral-purpose formulation directly reconstructs thin objects, which are\\nusually treated with specialized algorithms. A qualitative evaluation on the\\ndense semantic 3D reconstruction task shows that we improve significantly over\\nprevious methods.\\n',\n",
       "  'title': u'Semantic 3D Reconstruction with Continuous Regularization and Ray\\n  Potentials Using a Visibility Consistency Constraint'},\n",
       " u'1603.06987': {'arxivid': u'1603.06987',\n",
       "  'authorsaffil': [[u'Lamberto Ballan', None],\n",
       "   [u'Francesco Castaldo', None],\n",
       "   [u'Alexandre Alahi', None],\n",
       "   [u'Francesco Palmieri', None],\n",
       "   [u'Silvio Savarese', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06987v1',\n",
       "  'published': u'2016-03-22T21:19:42Z',\n",
       "  'summary': u'  When given a single frame of the video, humans can not only interpret the\\ncontent of the scene, but also they are able to forecast the near future. This\\nability is mostly driven by their rich prior knowledge about the visual world,\\nboth in terms of (\\\\emph{i}) the dynamics of moving agents, as well as\\n(\\\\emph{ii}) the semantic of the scene. In this work we exploit the interplay\\nbetween these two key elements to predict scene-specific motion patterns.\\nFirst, we extract patch descriptors encoding the probability of moving to the\\nadjacent patches, and the probability of being in that particular patch or\\nchanging behavior. Then, we introduce a Dynamic Bayesian Network which exploits\\nthis scene specific knowledge for trajectory prediction. Experimental results\\ndemonstrate that our method is able to accurately predict trajectories and\\ntransfer predictions to a novel scene characterized by similar elements.\\n',\n",
       "  'title': u'Knowledge Transfer for Scene-specific Motion Prediction'},\n",
       " u'1602.04133': {'arxivid': u'1602.04133',\n",
       "  'authorsaffil': [[u'Thang D. Bui', None],\n",
       "   [u'Daniel Hern\\xe1ndez-Lobato', None],\n",
       "   [u'Yingzhen Li', None],\n",
       "   [u'Jos\\xe9 Miguel Hern\\xe1ndez-Lobato', None],\n",
       "   [u'Richard E. Turner', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04133v1',\n",
       "  'published': u'2016-02-12T17:32:39Z',\n",
       "  'summary': u'  Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations\\nof Gaussian processes (GPs) and are formally equivalent to neural networks with\\nmultiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic\\nmodels and as such are arguably more flexible, have a greater capacity to\\ngeneralise, and provide better calibrated uncertainty estimates than\\nalternative deep models. This paper develops a new approximate Bayesian\\nlearning scheme that enables DGPs to be applied to a range of medium to large\\nscale regression problems for the first time. The new method uses an\\napproximate Expectation Propagation procedure and a novel and efficient\\nextension of the probabilistic backpropagation algorithm for learning. We\\nevaluate the new method for non-linear regression on eleven real-world\\ndatasets, showing that it always outperforms GP regression and is almost always\\nbetter than state-of-the-art deterministic and sampling-based approximate\\ninference methods for Bayesian neural networks. As a by-product, this work\\nprovides a comprehensive analysis of six approximate Bayesian methods for\\ntraining neural networks.\\n',\n",
       "  'title': u'Deep Gaussian Processes for Regression using Approximate Expectation\\n  Propagation'},\n",
       " u'1511.02025': {'arxivid': u'1511.02025',\n",
       "  'authorsaffil': [[u'Patrick J. Miller', None],\n",
       "   [u'Gitta H. Lubke', None],\n",
       "   [u'Daniel B. McArtor', None],\n",
       "   [u'C. S. Bergeman', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.02025v2',\n",
       "  'published': u'2015-11-06T10:30:22Z',\n",
       "  'summary': u\"  Technology and collaboration enable dramatic increases in the size of\\npsychological and psychiatric data collections, but finding structure in these\\nlarge data sets with many collected variables is challenging. Decision tree\\nensembles like random forests (Strobl, Malley, and Tutz, 2009) are a useful\\ntool for finding structure, but are difficult to interpret with multiple\\noutcome variables which are often of interest in psychology. To find and\\ninterpret structure in data sets with multiple outcomes and many predictors\\n(possibly exceeding the sample size), we introduce a multivariate extension to\\na decision tree ensemble method called Gradient Boosted Regression Trees\\n(Friedman, 2001). Our method, multivariate tree boosting, can be used for\\nidentifying important predictors, detecting predictors with non-linear effects\\nand interactions without specification of such effects, and for identifying\\npredictors that cause two or more outcome variables to covary without\\nparametric assumptions. We provide the R package 'mvtboost' to estimate, tune,\\nand interpret the resulting model, which extends the implementation of\\nunivariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous,\\nmultivariate outcomes. To illustrate the approach, we analyze predictors of\\npsychological well-being (Ryff and Keyes, 1995). Simulations verify that our\\napproach identifies predictors with non-linear effects and achieves high\\nprediction accuracy, exceeding or matching the performance of (penalized)\\nmultivariate multiple regression and multivariate decision trees over a wide\\nrange of conditions.\\n\",\n",
       "  'title': u'Finding structure in data using multivariate tree boosting'},\n",
       " u'1601.04920': {'arxivid': u'1601.04920',\n",
       "  'authorsaffil': [[u'St\\xe9phane Mallat', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'17 pages, 4 Figures',\n",
       "  'doi': u'10.1098/rsta.2015.0203',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04920v1',\n",
       "  'published': u'2016-01-19T13:40:47Z',\n",
       "  'summary': u'  Deep convolutional networks provide state of the art classifications and\\nregressions results over many high-dimensional problems. We review their\\narchitecture, which scatters data with a cascade of linear filter weights and\\nnon-linearities. A mathematical framework is introduced to analyze their\\nproperties. Computations of invariants involve multiscale contractions, the\\nlinearization of hierarchical symmetries, and sparse separations. Applications\\nare discussed.\\n',\n",
       "  'title': u'Understanding Deep Convolutional Networks'},\n",
       " u'1603.01056': {'arxivid': u'1603.01056',\n",
       "  'authorsaffil': [[u'Chao Wang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'11 pages, 6 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01056v1',\n",
       "  'published': u'2016-03-03T11:20:23Z',\n",
       "  'summary': u'  Pectoral muscle identification is often required for breast cancer risk\\nanalysis, such as estimating breast density. Traditional methods are\\noverwhelmingly based on manual visual assessment or straight line fitting for\\nthe pectoral muscle boundary, which are inefficient and inaccurate since\\npectoral muscle in mammograms can have curved boundaries.\\n  This paper proposes a novel and automatic pectoral muscle identification\\nalgorithm for MLO view mammograms. It is suitable for both scanned film and\\nfull field digital mammograms. This algorithm is demonstrated using a public\\ndomain software ImageJ. A validation of this algorithm has been performed using\\nreal-world data and it shows promising result.\\n',\n",
       "  'title': u'A novel and automatic pectoral muscle identification algorithm for\\n  mediolateral oblique (MLO) view mammograms using ImageJ'},\n",
       " u'1603.02644': {'arxivid': u'1603.02644',\n",
       "  'authorsaffil': [[u'Christophe Dupuy', u'SIERRA'],\n",
       "   [u'Francis Bach', u'SIERRA, LIENS']],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Under submission in ICML 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02644v2',\n",
       "  'published': u'2016-03-08T19:57:47Z',\n",
       "  'summary': u'  We study parameter inference in large-scale latent variable models. We first\\npropose an unified treatment of online inference for latent variable models\\nfrom a non-canonical exponential family, and draw explicit links between\\nseveral previously proposed frequentist or Bayesian methods. We then propose a\\nnovel inference method for the frequentist estimation of parameters, that\\nadapts MCMC methods to online inference of latent variable models with the\\nproper use of local Gibbs sampling. Then, for latent Dirich-let allocation,we\\nprovide an extensive set of experiments and comparisons with existing work,\\nwhere our new approach outperforms all previously proposed methods. In\\nparticular, using Gibbs sampling for latent variable inference is superior to\\nvariational inference in terms of test log-likelihoods. Moreover, Bayesian\\ninference through variational methods perform poorly, sometimes leading to\\nworse fits with latent variables of higher dimensionality.\\n',\n",
       "  'title': u'Online but Accurate Inference for Latent Variable Models with Local\\n  Gibbs Sampling'},\n",
       " u'1602.03960': {'arxivid': u'1602.03960',\n",
       "  'authorsaffil': [[u'Sujay Kumar Jauhar', None],\n",
       "   [u'Peter Turney', None],\n",
       "   [u'Eduard Hovy', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Keywords: Data, General Knowledge, Tables, Question Answering, MCQ,\\n  Crowd-sourcing, Mechanical Turk',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03960v1',\n",
       "  'published': u'2016-02-12T03:54:43Z',\n",
       "  'summary': u'  We describe two new related resources that facilitate modelling of general\\nknowledge reasoning in 4th grade science exams. The first is a collection of\\ncurated facts in the form of tables, and the second is a large set of\\ncrowd-sourced multiple-choice questions covering the facts in the tables.\\nThrough the setup of the crowd-sourced annotation task we obtain implicit\\nalignment information between questions and tables. We envisage that the\\nresources will be useful not only to researchers working on question answering,\\nbut also to people investigating a diverse range of other applications such as\\ninformation extraction, question parsing, answer type identification, and\\nlexical semantic modelling.\\n',\n",
       "  'title': u'TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice\\n  Questions'},\n",
       " u'1511.05552': {'arxivid': u'1511.05552',\n",
       "  'authorsaffil': [[u'Andre Xian Ming Chang', None],\n",
       "   [u'Berin Martini', None],\n",
       "   [u'Eugenio Culurciello', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'7 pages, 8 figures, changed format, added figures, added references,\\n  modified introduction',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05552v4',\n",
       "  'published': u'2015-11-17T02:20:37Z',\n",
       "  'summary': u'  Recurrent Neural Networks (RNNs) have the ability to retain memory and learn\\ndata sequences. Due to the recurrent nature of RNNs, it is sometimes hard to\\nparallelize all its computations on conventional hardware. CPUs do not\\ncurrently offer large parallelism, while GPUs offer limited parallelism due to\\nsequential components of RNN models. In this paper we present a hardware\\nimplementation of Long-Short Term Memory (LSTM) recurrent network on the\\nprogrammable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with $2$\\nlayers and $128$ hidden units in hardware and it has been tested using a\\ncharacter level language model. The implementation is more than $21\\\\times$\\nfaster than the ARM CPU embedded on the Zynq 7020 FPGA. This work can\\npotentially evolve to a RNN co-processor for future mobile devices.\\n',\n",
       "  'title': u'Recurrent Neural Networks Hardware Implementation on FPGA'},\n",
       " u'1602.05660': {'arxivid': u'1602.05660',\n",
       "  'authorsaffil': [[u'Fuqiang Liu', None],\n",
       "   [u'Fukun Bi', None],\n",
       "   [u'Liang Chen', None],\n",
       "   [u'Hao Shi', None],\n",
       "   [u'Wei Liu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'5 pages, 5 figures',\n",
       "  'doi': u'10.1109/LGRS.2015.2507982',\n",
       "  'journalref': u'IEEE Geoscience and Remote Sensing Letter, Year: 2016, Volume: 13,\\n  Pages: 242 - 246',\n",
       "  'link': u'http://arxiv.org/abs/1602.05660v1',\n",
       "  'published': u'2016-02-18T02:25:16Z',\n",
       "  'summary': u'  This letter proposes a synthetic aperture radar (SAR) image registration\\nmethod named Feature-Area Optimization (FAO). First, the traditional area-based\\noptimization model is reconstructed and decomposed into three key but uncertain\\nfactors: initialization, slice set and regularization. Next, structural\\nfeatures are extracted by scale invariant feature transform (SIFT) in\\ndual-resolution space (SIFT-DRS), a novel SIFT-Like method dedicated to FAO.\\nThen, the three key factors are determined based on these features. Finally,\\nsolving the factor-determined optimization model can get the registration\\nresult. A series of experiments demonstrate that the proposed method can\\nregister multi-temporal SAR images accurately and efficiently.\\n',\n",
       "  'title': u'Feature-Area Optimization: A Novel SAR Image Registration Method'},\n",
       " u'1512.01815': {'arxivid': u'1512.01815',\n",
       "  'authorsaffil': [[u'David Gadot', None], [u'Lior Wolf', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.01815v2',\n",
       "  'published': u'2015-12-06T18:30:28Z',\n",
       "  'summary': u'  We propose a new pipeline for optical flow computation, based on Deep\\nLearning techniques. We suggest using a Siamese CNN to independently, and in\\nparallel, compute the descriptors of both images. The learned descriptors are\\nthen compared efficiently using the L2 norm and do not require network\\nprocessing of patch pairs. The success of the method is based on an innovative\\nloss function that computes higher moments of the loss distributions for each\\ntraining batch. Combined with an Approximate Nearest Neighbor patch matching\\nmethod and a flow interpolation technique, state of the art performance is\\nobtained on the most challenging and competitive optical flow benchmarks.\\n',\n",
       "  'title': u'PatchBatch: a Batch Augmented Loss for Optical Flow'},\n",
       " u'1602.01895': {'arxivid': u'1602.01895',\n",
       "  'authorsaffil': [[u'Shijian Tang', None], [u'Song Han', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.CL', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01895v1',\n",
       "  'published': u'2016-02-05T00:17:18Z',\n",
       "  'summary': u'  Generating natural language descriptions for images is a challenging task.\\nThe traditional way is to use the convolutional neural network (CNN) to extract\\nimage features, followed by recurrent neural network (RNN) to generate\\nsentences. In this paper, we present a new model that added memory cells to\\ngate the feeding of image features to the deep neural network. The intuition is\\nenabling our model to memorize how much information from images should be fed\\nat each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed\\nthat our model outperforms other state-of-the-art models with higher BLEU\\nscores.\\n',\n",
       "  'title': u'Generate Image Descriptions based on Deep RNN and Memory Cells for\\n  Images Features'},\n",
       " u'1602.01541': {'arxivid': u'1602.01541',\n",
       "  'authorsaffil': [[u'Cecilia Aguerrebere', None],\n",
       "   [u'Mauricio Delbracio', None],\n",
       "   [u'Alberto Bartesaghi', None],\n",
       "   [u'Guillermo Sapiro', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01541v1',\n",
       "  'published': u'2016-02-04T02:25:52Z',\n",
       "  'summary': u\"  The performance of multi-image alignment, bringing different images into one\\ncoordinate system, is critical in many applications with varied signal-to-noise\\nratio (SNR) conditions. A great amount of effort is being invested into\\ndeveloping methods to solve this problem. Several important questions thus\\narise, including: Which are the fundamental limits in multi-image alignment\\nperformance? Does having access to more images improve the alignment?\\nTheoretical bounds provide a fundamental benchmark to compare methods and can\\nhelp establish whether improvements can be made. In this work, we tackle the\\nproblem of finding the performance limits in image registration when multiple\\nshifted and noisy observations are available. We derive and analyze the\\nCram\\\\'er-Rao and Ziv-Zakai lower bounds under different statistical models for\\nthe underlying image. The accuracy of the derived bounds is experimentally\\nassessed through a comparison to the maximum likelihood estimator. We show the\\nexistence of different behavior zones depending on the difficulty level of the\\nproblem, given by the SNR conditions of the input images. We find that\\nincreasing the number of images is only useful below a certain SNR threshold,\\nabove which the pairwise MLE estimation proves to be optimal. The analysis we\\npresent here brings further insight into the fundamental limitations of the\\nmulti-image alignment problem.\\n\",\n",
       "  'title': u'Fundamental Limits in Multi-image Alignment'},\n",
       " u'1508.00964': {'arxivid': u'1508.00964',\n",
       "  'authorsaffil': [[u'Namyoon Lee', None]],\n",
       "  'categoryterms': [u'cs.IT', u'cs.LG', u'math.IT'],\n",
       "  'comment': u'12 pages, Submitted to IEEE TSP',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.00964v2',\n",
       "  'published': u'2015-08-05T03:23:34Z',\n",
       "  'summary': u'  A reliable support detection is essential for a greedy algorithm to\\nreconstruct a sparse signal accurately from compressed and noisy measurements.\\nThis paper proposes a novel support detection method for greedy algorithms,\\nwhich is referred to as \"\\\\textit{maximum a posteriori (MAP) support\\ndetection}\". Unlike existing support detection methods that identify support\\nindices with the largest correlation value in magnitude per iteration, the\\nproposed method selects them with the largest likelihood ratios computed under\\nthe true and null support hypotheses by simultaneously exploiting the\\ndistributions of sensing matrix, sparse signal, and noise. Leveraging this\\ntechnique, MAP-Matching Pursuit (MAP-MP) is first presented to show the\\nadvantages of exploiting the proposed support detection method, and a\\nsufficient condition for perfect signal recovery is derived for the case when\\nthe sparse signal is binary. Subsequently, a set of iterative greedy\\nalgorithms, called MAP-generalized Orthogonal Matching Pursuit (MAP-gOMP),\\nMAP-Compressive Sampling Matching Pursuit (MAP-CoSaMP), and MAP-Subspace\\nPursuit (MAP-SP) are presented to demonstrate the applicability of the proposed\\nsupport detection method to existing greedy algorithms. From empirical results,\\nit is shown that the proposed greedy algorithms with highly reliable support\\ndetection can be better, faster, and easier to implement than basis pursuit via\\nlinear programming.\\n',\n",
       "  'title': u'MAP Support Detection for Greedy Sparse Signal Recovery Algorithms in\\n  Compressive Sensing'},\n",
       " u'1602.02164': {'arxivid': u'1602.02164',\n",
       "  'authorsaffil': [[u'David Gamarnik', None], [u'Sidhant Misra', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.NA'],\n",
       "  'comment': u'8 pages, 2 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02164v1',\n",
       "  'published': u'2016-02-05T21:07:16Z',\n",
       "  'summary': u'  We consider the problem of reconstructing a low rank matrix from a subset of\\nits entries and analyze two variants of the so-called Alternating Minimization\\nalgorithm, which has been proposed in the past. We establish that when the\\nunderlying matrix has rank $r=1$, has positive bounded entries, and the graph\\n$\\\\mathcal{G}$ underlying the revealed entries has bounded degree and diameter\\nwhich is at most logarithmic in the size of the matrix, both algorithms succeed\\nin reconstructing the matrix approximately in polynomial time starting from an\\narbitrary initialization. We further provide simulation results which suggest\\nthat the second algorithm which is based on the message passing type updates,\\nperforms significantly better.\\n',\n",
       "  'title': u'A Note on Alternating Minimization Algorithm for the Matrix Completion\\n  Problem'},\n",
       " u'1604.00938': {'arxivid': u'1604.00938',\n",
       "  'authorsaffil': [[u'Tomasz Jurczyk', None], [u'Jinho D. Choi', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00938v1',\n",
       "  'published': u'2016-04-04T16:33:15Z',\n",
       "  'summary': u'  This paper presents a precursory yet novel approach to the question answering\\ntask using structural decomposition. Our system first generates linguistic\\nstructures such as syntactic and semantic trees from text, decomposes them into\\nmultiple fields, then indexes the terms in each field. For each question, it\\ndecomposes the question into multiple fields, measures the relevance score of\\neach field to the indexed ones, then ranks all documents by their relevance\\nscores and weights associated with the fields, where the weights are learned\\nthrough statistical modeling. Our final model gives an absolute improvement of\\nover 40% to the baseline approach using simple search for detecting documents\\ncontaining answers.\\n',\n",
       "  'title': u'Multi-Field Structural Decomposition for Question Answering'},\n",
       " u'1602.05394': {'arxivid': u'1602.05394',\n",
       "  'authorsaffil': [[u'Rodolphe Jenatton', None],\n",
       "   [u'Jim Huang', None],\n",
       "   [u'Dominik Csiba', None],\n",
       "   [u'Cedric Archambeau', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'math.OC', u'math.ST', u'stat.TH'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05394v2',\n",
       "  'published': u'2016-02-17T12:57:08Z',\n",
       "  'summary': u'  We consider online optimization in the 1-lookahead setting, where the\\nobjective does not decompose additively over the rounds of the online game. The\\nresulting formulation enables us to deal with non-stationary and/or long-term\\nconstraints , which arise, for example, in online display advertising problems.\\nWe propose an on-line primal-dual algorithm for which we obtain dynamic\\ncumulative regret guarantees. They depend on the convexity and the smoothness\\nof the non-additive penalty, as well as terms capturing the smoothness with\\nwhich the residuals of the non-stationary and long-term constraints vary over\\nthe rounds. We conduct experiments on synthetic data to illustrate the benefits\\nof the non-additive penalty and show vanishing regret convergence on live\\ntraffic data collected by a display advertising platform in production.\\n',\n",
       "  'title': u'Online optimization and regret guarantees for non-additive long-term\\n  constraints'},\n",
       " u'1602.02169': {'arxivid': u'1602.02169',\n",
       "  'authorsaffil': [[u'Mauricio Toro', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'70 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02169v1',\n",
       "  'published': u'2016-02-05T21:26:53Z',\n",
       "  'summary': u'  We can program a Real-Time (RT) music improvisation system in C++ without a\\nformal semantic or we can model it with process calculi such as the\\nNon-deterministic Timed Concurrent Constraint (ntcc) calculus. \"A Concurrent\\nConstraints Factor Oracle (FO) model for Music Improvisation\" (Ccfomi) is an\\nimprovisation model specified on ntcc. Since Ccfomi improvises\\nnon-deterministically, there is no control on choices and therefore little\\ncontrol over the sequence variation during the improvisation. To avoid this, we\\nextended Ccfomi using the Probabilistic Non-deterministic Timed Concurrent\\nConstraint calculus. Our extension to Ccfomi does not change the time and space\\ncomplexity of building the FO, thus making our extension compatible with RT.\\nHowever, there was not a ntcc interpreter capable of RT to execute Ccfomi. We\\ndeveloped Ntccrt --a RT capable interpreter for ntcc-- and we executed Ccfomi\\non Ntccrt. In the future, we plan to extend Ntccrt to execute our extension to\\nCcfomi.\\n',\n",
       "  'title': u'Probabilistic Extension to the Concurrent Constraint Factor Oracle Model\\n  for Music Improvisation'},\n",
       " u'1511.05622': {'arxivid': u'1511.05622',\n",
       "  'authorsaffil': [[u'Yann N. Dauphin', None], [u'David Grangier', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05622v4',\n",
       "  'published': u'2015-11-17T23:50:35Z',\n",
       "  'summary': u'  Conditional belief networks introduce stochastic binary variables in neural\\nnetworks. Contrary to a classical neural network, a belief network can predict\\nmore than the expected value of the output $Y$ given the input $X$. It can\\npredict a distribution of outputs $Y$ which is useful when an input can admit\\nmultiple outputs whose average is not necessarily a valid answer. Such networks\\nare particularly relevant to inverse problems such as image prediction for\\ndenoising, or text to speech. However, traditional sigmoid belief networks are\\nhard to train and are not suited to continuous problems. This work introduces a\\nnew family of networks called linearizing belief nets or LBNs. A LBN decomposes\\ninto a deep linear network where each linear unit can be turned on or off by\\nnon-deterministic binary latent units. It is a universal approximator of\\nreal-valued conditional distributions and can be trained using gradient\\ndescent. Moreover, the linear pathways efficiently propagate continuous\\ninformation and they act as multiplicative skip-connections that help\\noptimization by removing gradient diffusion. This yields a model which trains\\nefficiently and improves the state-of-the-art on image denoising and facial\\nexpression generation with the Toronto faces dataset.\\n',\n",
       "  'title': u'Predicting distributions with Linearizing Belief Networks'},\n",
       " u'1603.04713': {'arxivid': u'1603.04713',\n",
       "  'authorsaffil': [[u'Wenjie Pei', None],\n",
       "   [u'David M. J. Tax', None],\n",
       "   [u'Laurens van der Maaten', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'11 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04713v1',\n",
       "  'published': u'2016-03-15T14:57:44Z',\n",
       "  'summary': u'  Traditional techniques for measuring similarities between time series are\\nbased on handcrafted similarity measures, whereas more recent learning-based\\napproaches cannot exploit external supervision. We combine ideas from\\ntime-series modeling and metric learning, and study siamese recurrent networks\\n(SRNs) that minimize a classification loss to learn a good similarity measure\\nbetween time series. Specifically, our approach learns a vectorial\\nrepresentation for each time series in such a way that similar time series are\\nmodeled by similar representations, and dissimilar time series by dissimilar\\nrepresentations. Because it is a similarity prediction models, SRNs are\\nparticularly well-suited to challenging scenarios such as signature\\nrecognition, in which each person is a separate class and very few examples per\\nclass are available. We demonstrate the potential merits of SRNs in\\nwithin-domain and out-of-domain classification experiments and in one-shot\\nlearning experiments on tasks such as signature, voice, and sign language\\nrecognition.\\n',\n",
       "  'title': u'Modeling Time Series Similarity with Siamese Recurrent Networks'},\n",
       " u'1601.07243': {'arxivid': u'1601.07243',\n",
       "  'authorsaffil': [[u'Jean Honorio', None]],\n",
       "  'categoryterms': [u'cs.GT', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07243v2',\n",
       "  'published': u'2016-01-27T01:49:50Z',\n",
       "  'summary': u\"  We analyze the sample complexity of learning sparse graphical games from\\npurely behavioral data. That is, we assume that we can only observe the\\nplayers' joint actions and not their payoffs. We analyze the sufficient and\\nnecessary number of samples for the correct recovery of the set of\\npure-strategy Nash equilibria (PSNE) of the true game. Our analysis focuses on\\nsparse directed graphs with $n$ nodes and at most $k$ parents per node. By\\nusing VC dimension arguments, we show that if the number of samples is greater\\nthan ${O(k n \\\\log^2{n})}$, then maximum likelihood estimation correctly\\nrecovers the PSNE with high probability. By using information-theoretic\\narguments, we show that if the number of samples is less than ${\\\\Omega(k n\\n\\\\log^2{n})}$, then any conceivable method fails to recover the PSNE with\\narbitrary probability.\\n\",\n",
       "  'title': u'On the Sample Complexity of Learning Sparse Graphical Games'},\n",
       " u'1603.00802': {'arxivid': u'1603.00802',\n",
       "  'authorsaffil': [[u'Ali Tehrani-Saleh', None], [u'Christoph Adami', None]],\n",
       "  'categoryterms': [u'q-bio.PE', u'cs.CV', u'nlin.AO', u'q-bio.NC'],\n",
       "  'comment': u'8 pages, 10 figures, submitted to 15th Artificial Life conference\\n  (ALife 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00802v1',\n",
       "  'published': u'2016-03-02T17:36:31Z',\n",
       "  'summary': u'  Flies that walk in a covered planar arena on straight paths avoid colliding\\nwith each other, but which of the two flies stops is not random.\\nHigh-throughput video observations, coupled with dedicated experiments with\\ncontrolled robot flies have revealed that flies utilize the type of optic flow\\non their retina as a determinant of who should stop, a strategy also used by\\nship captains to determine which of two ships on a collision course should\\nthrow engines in reverse. We use digital evolution to test whether this\\nstrategy evolves when collision avoidance is the sole penalty. We find that the\\nstrategy does indeed evolve in a narrow range of cost/benefit ratios, for\\nexperiments in which the \"regressive motion\" cue is error free. We speculate\\nthat these stringent conditions may not be sufficient to evolve the strategy in\\nreal flies, pointing perhaps to auxiliary costs and benefits not modeled in our\\nstudy\\n',\n",
       "  'title': u'Flies as Ship Captains? Digital Evolution Unravels Selective Pressures\\n  to Avoid Collision in Drosophila'},\n",
       " u'1602.05719': {'arxivid': u'1602.05719',\n",
       "  'authorsaffil': [[u'Jaros\\u0142aw B\\u0142asiok', None],\n",
       "   [u'Jelani Nelson', None]],\n",
       "  'categoryterms': [u'cs.LG',\n",
       "   u'cs.DS',\n",
       "   u'cs.IT',\n",
       "   u'math.IT',\n",
       "   u'math.PR',\n",
       "   u'I.2.6; F.2.0'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05719v1',\n",
       "  'published': u'2016-02-18T08:51:08Z',\n",
       "  'summary': u'  In \"dictionary learning\" we observe $Y = AX + E$ for some\\n$Y\\\\in\\\\mathbb{R}^{n\\\\times p}$, $A \\\\in\\\\mathbb{R}^{m\\\\times n}$, and\\n$X\\\\in\\\\mathbb{R}^{m\\\\times p}$. The matrix $Y$ is observed, and $A, X, E$ are\\nunknown. Here $E$ is \"noise\" of small norm, and $X$ is column-wise sparse. The\\nmatrix $A$ is referred to as a {\\\\em dictionary}, and its columns as {\\\\em\\natoms}. Then, given some small number $p$ of samples, i.e.\\\\ columns of $Y$, the\\ngoal is to learn the dictionary $A$ up to small error, as well as $X$. The\\nmotivation is that in many applications data is expected to sparse when\\nrepresented by atoms in the \"right\" dictionary $A$ (e.g.\\\\ images in the Haar\\nwavelet basis), and the goal is to learn $A$ from the data to then use it for\\nother applications.\\n  Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD with\\nprovable guarantees when $E = 0$ and $m = n$. They showed if $X$ has\\nindependent entries with an expected $s$ non-zeroes per column for $1 \\\\lesssim\\ns \\\\lesssim \\\\sqrt{n}$, and with non-zero entries being subgaussian, then for\\n$p\\\\gtrsim n^2\\\\log^2 n$ with high probability ER-SpUD outputs matrices $A\\', X\\'$\\nwhich equal $A, X$ up to permuting and scaling columns (resp.\\\\ rows) of $A$\\n(resp.\\\\ $X$). They conjectured $p\\\\gtrsim n\\\\log n$ suffices, which they showed\\nwas information theoretically necessary for {\\\\em any} algorithm to succeed when\\n$s \\\\simeq 1$. Significant progress was later obtained in [LV15].\\n  We show that for a slight variant of ER-SpUD, $p\\\\gtrsim n\\\\log(n/\\\\delta)$\\nsamples suffice for successful recovery with probability $1-\\\\delta$. We also\\nshow that for the unmodified ER-SpUD, $p\\\\gtrsim n^{1.99}$ samples are required\\neven to learn $A, X$ with polynomially small success probability. This resolves\\nthe main conjecture of [SWW12], and contradicts the main result of [LV15],\\nwhich claimed that $p\\\\gtrsim n\\\\log^4 n$ guarantees success whp.\\n',\n",
       "  'title': u'An improved analysis of the ER-SpUD dictionary learning algorithm'},\n",
       " u'1506.05254': {'arxivid': u'1506.05254',\n",
       "  'authorsaffil': [[u'John Schulman', None],\n",
       "   [u'Nicolas Heess', None],\n",
       "   [u'Theophane Weber', None],\n",
       "   [u'Pieter Abbeel', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Advances in Neural Information Processing Systems 28 (NIPS 2015)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.05254v3',\n",
       "  'published': u'2015-06-17T09:32:31Z',\n",
       "  'summary': u\"  In a variety of problems originating in supervised, unsupervised, and\\nreinforcement learning, the loss function is defined by an expectation over a\\ncollection of random variables, which might be part of a probabilistic model or\\nthe external world. Estimating the gradient of this loss function, using\\nsamples, lies at the core of gradient-based learning algorithms for these\\nproblems. We introduce the formalism of stochastic computation\\ngraphs---directed acyclic graphs that include both deterministic functions and\\nconditional probability distributions---and describe how to easily and\\nautomatically derive an unbiased estimator of the loss function's gradient. The\\nresulting algorithm for computing the gradient estimator is a simple\\nmodification of the standard backpropagation algorithm. The generic scheme we\\npropose unifies estimators derived in variety of prior work, along with\\nvariance-reduction techniques therein. It could assist researchers in\\ndeveloping intricate models involving a combination of stochastic and\\ndeterministic operations, enabling, for example, attention, memory, and control\\nactions.\\n\",\n",
       "  'title': u'Gradient Estimation Using Stochastic Computation Graphs'},\n",
       " u'1601.03239': {'arxivid': u'1601.03239',\n",
       "  'authorsaffil': [[u'Victor Schetinger', None],\n",
       "   [u'Massimo Iuliani', None],\n",
       "   [u'Alessandro Piva', None],\n",
       "   [u'Manuel M. Oliveira', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.MM'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03239v1',\n",
       "  'published': u'2016-01-13T13:38:36Z',\n",
       "  'summary': u'  The field of image composition is constantly trying to improve the ways in\\nwhich an image can be altered and enhanced. While this is usually done in the\\nname of aesthetics and practicality, it also provides tools that can be used to\\nmaliciously alter images. In this sense, the field of digital image forensics\\nhas to be prepared to deal with the influx of new technology, in a constant\\narms-race. In this paper, the current state of this arms-race is analyzed,\\nsurveying the state-of-the-art and providing means to compare both sides. A\\nnovel scale to classify image forensics assessments is proposed, and\\nexperiments are performed to test composition techniques in regards to\\ndifferent forensics traces. We show that even though research in forensics\\nseems unaware of the advanced forms of image composition, it possesses the\\nbasic tools to detect it.\\n',\n",
       "  'title': u'Digital Image Forensics vs. Image Composition: An Indirect Arms Race'},\n",
       " u'1602.05242': {'arxivid': u'1602.05242',\n",
       "  'authorsaffil': [[u'Nima Anari', None],\n",
       "   [u'Shayan Oveis Gharan', None],\n",
       "   [u'Alireza Rezaei', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DS', u'math.PR'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05242v3',\n",
       "  'published': u'2016-02-16T23:32:53Z',\n",
       "  'summary': u'  Strongly Rayleigh distributions are natural generalizations of product and\\ndeterminantal probability distributions and satisfy strongest form of negative\\ndependence properties. We show that the \"natural\" Monte Carlo Markov Chain\\n(MCMC) is rapidly mixing in the support of a {\\\\em homogeneous} strongly\\nRayleigh distribution. As a byproduct, our proof implies Markov chains can be\\nused to efficiently generate approximate samples of a $k$-determinantal point\\nprocess. This answers an open question raised by Deshpande and Rademacher.\\n',\n",
       "  'title': u'Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh\\n  Distributions and Determinantal Point Processes'},\n",
       " u'1603.00961': {'arxivid': u'1603.00961',\n",
       "  'authorsaffil': [[u'Tobias L\\xfcddemann', None], [u'Jan Egger', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.GR'],\n",
       "  'comment': u'6 pages, 4 figures, 1 table, 43 references',\n",
       "  'doi': None,\n",
       "  'journalref': u'SPIE Medical Imaging Conference 2016, Paper 9784-113',\n",
       "  'link': u'http://arxiv.org/abs/1603.00961v1',\n",
       "  'published': u'2016-03-03T03:39:59Z',\n",
       "  'summary': u'  Among all types of cancer, gynecological malignancies belong to the 4th most\\nfrequent type of cancer among women. Besides chemotherapy and external beam\\nradiation, brachytherapy is the standard procedure for the treatment of these\\nmalignancies. In the progress of treatment planning, localization of the tumor\\nas the target volume and adjacent organs of risks by segmentation is crucial to\\naccomplish an optimal radiation distribution to the tumor while simultaneously\\npreserving healthy tissue. Segmentation is performed manually and represents a\\ntime-consuming task in clinical daily routine. This study focuses on the\\nsegmentation of the rectum/sigmoid colon as an Organ-At-Risk in gynecological\\nbrachytherapy. The proposed segmentation method uses an interactive,\\ngraph-based segmentation scheme with a user-defined template. The scheme\\ncreates a directed two dimensional graph, followed by the minimal cost closed\\nset computation on the graph, resulting in an outlining of the rectum. The\\ngraphs outline is dynamically adapted to the last calculated cut. Evaluation\\nwas performed by comparing manual segmentations of the rectum/sigmoid colon to\\nresults achieved with the proposed method. The comparison of the algorithmic to\\nmanual results yielded to a Dice Similarity Coefficient value of 83.85+/-4.08%,\\nin comparison to 83.97+/-8.08% for the comparison of two manual segmentations\\nof the same physician. Utilizing the proposed methodology resulted in a median\\ntime of 128 seconds per dataset, compared to 300 seconds needed for pure manual\\nsegmentation.\\n',\n",
       "  'title': u'Interactive and Scale Invariant Segmentation of the Rectum/Sigmoid via\\n  User-Defined Templates'},\n",
       " u'1511.07972': {'arxivid': u'1511.07972',\n",
       "  'authorsaffil': [[u'Volker Tresp', None],\n",
       "   [u'Crist\\xf3bal Esteban', None],\n",
       "   [u'Yinchong Yang', None],\n",
       "   [u'Stephan Baier', None],\n",
       "   [u'Denis Krompa\\xdf', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale\\n  Representation Learning',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07972v9',\n",
       "  'published': u'2015-11-25T07:06:09Z',\n",
       "  'summary': u'  Embedding learning, a.k.a. representation learning, has been shown to be able\\nto model large-scale semantic knowledge graphs. A key concept is a mapping of\\nthe knowledge graph to a tensor representation whose entries are predicted by\\nmodels using latent representations of generalized entities. Latent variable\\nmodels are well suited to deal with the high dimensionality and sparsity of\\ntypical knowledge graphs. In recent publications the embedding models were\\nextended to also consider time evolutions, time patterns and subsymbolic\\nrepresentations. In this paper we map embedding models, which were developed\\npurely as solutions to technical problems for modelling temporal knowledge\\ngraphs, to various cognitive memory functions, in particular to semantic and\\nconcept memory, episodic memory, sensory memory, short-term memory, and working\\nmemory. We discuss learning, query answering, the path from sensory input to\\nsemantic decoding, and the relationship between episodic memory and semantic\\nmemory. We introduce a number of hypotheses on human memory that can be derived\\nfrom the developed mathematical models.\\n',\n",
       "  'title': u'Learning with Memory Embeddings'},\n",
       " u'1510.02125': {'arxivid': u'1510.02125',\n",
       "  'authorsaffil': [[u'David Schlangen', None],\n",
       "   [u'Sina Zarriess', None],\n",
       "   [u'Casey Kennington', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'11 pages; as in Proceedings of ACL 2016, Berlin, 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.02125v3',\n",
       "  'published': u'2015-10-07T20:52:22Z',\n",
       "  'summary': u'  A common use of language is to refer to visually present objects. Modelling\\nit in computers requires modelling the link between language and perception.\\nThe \"words as classifiers\" model of grounded semantics views words as\\nclassifiers of perceptual contexts, and composes the meaning of a phrase\\nthrough composition of the denotations of its component words. It was recently\\nshown to perform well in a game-playing scenario with a small number of object\\ntypes. We apply it to two large sets of real-world photographs that contain a\\nmuch larger variety of types and for which referring expressions are available.\\nUsing a pre-trained convolutional neural network to extract image features, and\\naugmenting these with in-picture positional information, we show that the model\\nachieves performance competitive with the state of the art in a reference\\nresolution task (given expression, find bounding box of its referent), while,\\nas we argue, being conceptually simpler and more flexible.\\n',\n",
       "  'title': u'Resolving References to Objects in Photographs using the\\n  Words-As-Classifiers Model'},\n",
       " u'1506.08536': {'arxivid': u'1506.08536',\n",
       "  'authorsaffil': [[u'Luca Citi', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'11 pages, no figures, updated version of technical report',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.08536v2',\n",
       "  'published': u'2015-06-29T08:11:52Z',\n",
       "  'summary': u'  This report presents an algorithm for the solution of multiple kernel\\nlearning (MKL) problems with elastic-net constraints on the kernel weights.\\n',\n",
       "  'title': u'A simple yet efficient algorithm for multiple kernel learning under\\n  elastic-net constraints'},\n",
       " u'1602.03808': {'arxivid': u'1602.03808',\n",
       "  'authorsaffil': [[u'Kwang In Kim', None],\n",
       "   [u'James Tompkin', None],\n",
       "   [u'Hanspeter Pfister', None],\n",
       "   [u'Christian Theobalt', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'Accepted version of paper published at CVPR 2015,\\n  http://dx.doi.org/10.1109/CVPR.2015.7298831',\n",
       "  'doi': u'10.1109/CVPR.2015.7298831',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03808v1',\n",
       "  'published': u'2016-02-11T18:08:58Z',\n",
       "  'summary': u'  In many learning tasks, the structure of the target space of a function holds\\nrich information about the relationships between evaluations of functions on\\ndifferent data points. Existing approaches attempt to exploit this relationship\\ninformation implicitly by enforcing smoothness on function evaluations only.\\nHowever, what happens if we explicitly regularize the relationships between\\nfunction evaluations? Inspired by homophily, we regularize based on a smooth\\nrelationship function, either defined from the data or with labels. In\\nexperiments, we demonstrate that this significantly improves the performance of\\nstate-of-the-art algorithms in semi-supervised classification and in spectral\\ndata embedding for constrained clustering and dimensionality reduction.\\n',\n",
       "  'title': u'Semi-supervised Learning with Explicit Relationship Regularization'},\n",
       " u'1602.02706': {'arxivid': u'1602.02706',\n",
       "  'authorsaffil': [[u'Julien Audiffren', u'CMLA'],\n",
       "   [u'Ralaivola Liva', u'LIF']],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02706v2',\n",
       "  'published': u'2016-02-08T19:32:18Z',\n",
       "  'summary': u'  We adress the problem of dueling bandits defined on partially ordered sets,\\nor posets. In this setting, arms may not be comparable, and there may be\\nseveral (incomparable) optimal arms. We propose an algorithm, UnchainedBandits,\\nthat efficiently finds the set of optimal arms of any poset even when pairs of\\ncomparable arms cannot be distinguished from pairs of incomparable arms, with a\\nset of minimal assumptions. This algorithm relies on the concept of decoys,\\nwhich stems from social psychology. For the easier case where the\\nincomparability information may be accessible, we propose a second algorithm,\\nSlicingBandits, which takes advantage of this information and achieves a very\\nsignificant gain of performance compared to UnchainedBandits. We provide\\ntheoretical guarantees and experimental evaluation for both algorithms.\\n',\n",
       "  'title': u'Decoy Bandits Dueling on a Poset'},\n",
       " u'1604.02619': {'arxivid': u'1604.02619',\n",
       "  'authorsaffil': [[u'Lluis Gomez-Bigorda', None],\n",
       "   [u'Dimosthenis Karatzas', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02619v2',\n",
       "  'published': u'2016-04-10T00:03:16Z',\n",
       "  'summary': u'  Motivated by the success of powerful while expensive techniques to recognize\\nwords in a holistic way, object proposals techniques emerge as an alternative\\nto the traditional text detectors. In this paper we introduce a novel object\\nproposals method that is specifically designed for text. We rely on a\\nsimilarity based region grouping algorithm that generates a hierarchy of word\\nhypotheses. Over the nodes of this hierarchy it is possible to apply a holistic\\nword recognition method in an efficient way.\\n  Our experiments demonstrate that the presented method is superior in its\\nability of producing good quality word proposals when compared with\\nclass-independent algorithms. We show impressive recall rates with a few\\nthousand proposals in different standard benchmarks, including focused or\\nincidental text datasets, and multi-language scenarios. Moreover, the\\ncombination of our object proposals with existing whole-word recognizers shows\\ncompetitive performance in end-to-end word spotting, and, in some benchmarks,\\noutperforms previously published results. Concretely, in the challenging\\nICDAR2015 Incidental Text dataset, we overcome in more than 10 percent f-score\\nthe best-performing method in the last ICDAR Robust Reading Competition. Source\\ncode of the complete end-to-end system is available at\\nhttps://github.com/lluisgomez/TextProposals\\n',\n",
       "  'title': u'TextProposals: a Text-specific Selective Search Algorithm for Word\\n  Spotting in the Wild'},\n",
       " u'1602.06289': {'arxivid': u'1602.06289',\n",
       "  'authorsaffil': [[u'Stanis\\u0142aw Jastrz\\u0119bski', None],\n",
       "   [u'Damian Le\\u015bniak', None],\n",
       "   [u'Wojciech Marian Czarnecki', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06289v1',\n",
       "  'published': u'2016-02-19T20:48:19Z',\n",
       "  'summary': u'  This paper shows how one can directly apply natural language processing (NLP)\\nmethods to classification problems in cheminformatics. Connection between these\\nseemingly separate fields is shown by considering standard textual\\nrepresentation of compound, SMILES. The problem of activity prediction against\\na target protein is considered, which is a crucial part of computer aided drug\\ndesign process. Conducted experiments show that this way one can not only\\noutrank state of the art results of hand crafted representations but also gets\\ndirect structural insights into the way decisions are made.\\n',\n",
       "  'title': u'Learning to SMILE(S)'},\n",
       " u'1512.02337': {'arxivid': u'1512.02337',\n",
       "  'authorsaffil': [[u'Samuel B. Hopkins', None],\n",
       "   [u'Tselil Schramm', None],\n",
       "   [u'Jonathan Shi', None],\n",
       "   [u'David Steurer', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.CC', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'62 pages, title changed, to appear at STOC 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.02337v2',\n",
       "  'published': u'2015-12-08T05:49:07Z',\n",
       "  'summary': u'  We consider two problems that arise in machine learning applications: the\\nproblem of recovering a planted sparse vector in a random linear subspace and\\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\\nproblems, the best known guarantees are based on the sum-of-squares method. We\\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\\nproblems but the running time is significantly faster.\\n  For the planted sparse vector problem, we give an algorithm with running time\\nnearly linear in the input size that approximately recovers a planted sparse\\nvector with up to constant relative sparsity in a random subspace of $\\\\mathbb\\nR^n$ of dimension up to $\\\\tilde \\\\Omega(\\\\sqrt n)$. These recovery guarantees\\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\\nlogarithmic factors.\\n  For tensor decomposition, we give an algorithm with running time close to\\nlinear in the input size (with exponent $\\\\approx 1.086$) that approximately\\nrecovers a component of a random 3-tensor over $\\\\mathbb R^n$ of rank up to\\n$\\\\tilde \\\\Omega(n^{4/3})$. The best previous algorithm for this problem due to\\nGe and Ma (RANDOM 2015) works up to rank $\\\\tilde \\\\Omega(n^{3/2})$ but requires\\nquasipolynomial time.\\n',\n",
       "  'title': u'Fast spectral algorithms from sum-of-squares proofs: tensor\\n  decomposition and planted sparse vectors'},\n",
       " u'1511.04397': {'arxivid': u'1511.04397',\n",
       "  'authorsaffil': [[u'Ehsan Hosseini-Asl', None], [u'Angshuman Guha', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'Under review as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04397v3',\n",
       "  'published': u'2015-11-13T18:46:01Z',\n",
       "  'summary': u'  In this paper, we propose a new text recognition model based on measuring the\\nvisual similarity of text and predicting the content of unlabeled texts. First\\na Siamese convolutional network is trained with deep supervision on a labeled\\ntraining dataset. This network projects texts into a similarity manifold. The\\nDeeply Supervised Siamese network learns visual similarity of texts. Then a\\nK-nearest neighbor classifier is used to predict unlabeled text based on\\nsimilarity distance to labeled texts. The performance of the model is evaluated\\non three datasets of machine-print and hand-written text combined. We\\ndemonstrate that the model reduces the cost of human estimation by $50\\\\%-85\\\\%$.\\nThe error of the system is less than $0.5\\\\%$. The proposed model outperform\\nconventional Siamese network by finding visually-similar barely-readable and\\nreadable text, e.g. machine-printed, handwritten, due to deep supervision. The\\nresults also demonstrate that the predicted labels are sometimes better than\\nhuman labels e.g. spelling correction.\\n',\n",
       "  'title': u'Similarity-based Text Recognition by Deeply Supervised Siamese Network'},\n",
       " u'1603.05763': {'arxivid': u'1603.05763',\n",
       "  'authorsaffil': [[u'Boshra Rajaei', None],\n",
       "   [u'Rafael Grompone von Gioi', None],\n",
       "   [u'Jean-Michel Morel', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05763v1',\n",
       "  'published': u'2016-03-18T04:05:35Z',\n",
       "  'summary': u'  In this paper, we reconsider the early computer vision bottom-up program,\\naccording to which higher level features (geometric structures) in an image\\ncould be built up recursively from elementary features by simple grouping\\nprinciples coming from Gestalt theory. Taking advantage of the (recent)\\nadvances in reliable line segment detectors, we propose three feature detectors\\nthat constitute one step up in this bottom up pyramid. For any digital image,\\nour unsupervised algorithm computes three classic Gestalts from the set of\\npredetected line segments: good continuations, nonlocal alignments, and bars.\\nThe methodology is based on a common stochastic {\\\\it a contrario model}\\nyielding three simple detection formulas, characterized by their number of\\nfalse alarms. This detection algorithm is illustrated on several digital\\nimages.\\n',\n",
       "  'title': u'From line segments to more organized Gestalts'},\n",
       " u'1509.00685': {'arxivid': u'1509.00685',\n",
       "  'authorsaffil': [[u'Alexander M. Rush', None],\n",
       "   [u'Sumit Chopra', None],\n",
       "   [u'Jason Weston', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI'],\n",
       "  'comment': u'Proceedings of EMNLP 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.00685v2',\n",
       "  'published': u'2015-09-02T13:20:40Z',\n",
       "  'summary': u'  Summarization based on text extraction is inherently limited, but\\ngeneration-style abstractive methods have proven challenging to build. In this\\nwork, we propose a fully data-driven approach to abstractive sentence\\nsummarization. Our method utilizes a local attention-based model that generates\\neach word of the summary conditioned on the input sentence. While the model is\\nstructurally simple, it can easily be trained end-to-end and scales to a large\\namount of training data. The model shows significant performance gains on the\\nDUC-2004 shared task compared with several strong baselines.\\n',\n",
       "  'title': u'A Neural Attention Model for Abstractive Sentence Summarization'},\n",
       " u'1603.03144': {'arxivid': u'1603.03144',\n",
       "  'authorsaffil': [[u'Yi Yang', None], [u'Jacob Eisenstein', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.DL'],\n",
       "  'comment': u'Accepted to NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03144v2',\n",
       "  'published': u'2016-03-10T04:27:15Z',\n",
       "  'summary': u'  As more historical texts are digitized, there is interest in applying natural\\nlanguage processing tools to these archives. However, the performance of these\\ntools is often unsatisfactory, due to language change and genre differences.\\nSpelling normalization heuristics are the dominant solution for dealing with\\nhistorical texts, but this approach fails to account for changes in usage and\\nvocabulary. In this empirical paper, we assess the capability of domain\\nadaptation techniques to cope with historical texts, focusing on the classic\\nbenchmark task of part-of-speech tagging. We evaluate several domain adaptation\\nmethods on the task of tagging Early Modern English and Modern British English\\ntexts in the Penn Corpora of Historical English. We demonstrate that the\\nFeature Embedding method for unsupervised domain adaptation outperforms word\\nembeddings and Brown clusters, showing the importance of embedding the entire\\nfeature space, rather than just individual words. Feature Embeddings also give\\nbetter performance than spelling normalization, but the combination of the two\\nmethods is better still, yielding a 5% raw improvement in tagging accuracy on\\nEarly Modern English texts.\\n',\n",
       "  'title': u'Part-of-Speech Tagging for Historical English'},\n",
       " u'1601.02502': {'arxivid': u'1601.02502',\n",
       "  'authorsaffil': [[u'Jocelyn Coulmance', None],\n",
       "   [u'Jean-Marc Marty', None],\n",
       "   [u'Guillaume Wenzek', None],\n",
       "   [u'Amine Benhalloum', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'EMNLP 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02502v1',\n",
       "  'published': u'2016-01-11T16:12:32Z',\n",
       "  'summary': u'  We introduce Trans-gram, a simple and computationally-efficient method to\\nsimultaneously learn and align wordembeddings for a variety of languages, using\\nonly monolingual data and a smaller set of sentence-aligned data. We use our\\nnew method to compute aligned wordembeddings for twenty-one languages using\\nEnglish as a pivot language. We show that some linguistic features are aligned\\nacross languages for which we do not have aligned data, even though those\\nproperties do not exist in the pivot language. We also achieve state of the art\\nresults on standard cross-lingual text classification and word translation\\ntasks.\\n',\n",
       "  'title': u'Trans-gram, Fast Cross-lingual Word-embeddings'},\n",
       " u'1601.07021': {'arxivid': u'1601.07021',\n",
       "  'authorsaffil': [[u'Qingxiang Feng', None],\n",
       "   [u'Jeng-Shyang Pan', None],\n",
       "   [u'Jar-Ferr Yang', None],\n",
       "   [u'Yang-Ting Chou', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07021v1',\n",
       "  'published': u'2016-01-26T13:41:48Z',\n",
       "  'summary': u'  In this paper, a novel method, called polyhedron volume ratio classification\\n(PVRC) is proposed for image recognition\\n',\n",
       "  'title': u'Polyhedron Volume-Ratio-based Classification for Image Recognition'},\n",
       " u'1604.03912': {'arxivid': u'1604.03912',\n",
       "  'authorsaffil': [[u'Michael Herman', None],\n",
       "   [u'Tobias Gindele', None],\n",
       "   [u'J\\xf6rg Wagner', None],\n",
       "   [u'Felix Schmitt', None],\n",
       "   [u'Wolfram Burgard', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'cs.SY', u'stat.ML'],\n",
       "  'comment': u'accepted to appear in AISTATS 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03912v1',\n",
       "  'published': u'2016-04-13T19:06:41Z',\n",
       "  'summary': u\"  Inverse Reinforcement Learning (IRL) describes the problem of learning an\\nunknown reward function of a Markov Decision Process (MDP) from observed\\nbehavior of an agent. Since the agent's behavior originates in its policy and\\nMDP policies depend on both the stochastic system dynamics as well as the\\nreward function, the solution of the inverse problem is significantly\\ninfluenced by both. Current IRL approaches assume that if the transition model\\nis unknown, additional samples from the system's dynamics are accessible, or\\nthe observed behavior provides enough samples of the system's dynamics to solve\\nthe inverse problem accurately. These assumptions are often not satisfied. To\\novercome this, we present a gradient-based IRL approach that simultaneously\\nestimates the system's dynamics. By solving the combined optimization problem,\\nour approach takes into account the bias of the demonstrations, which stems\\nfrom the generating policy. The evaluation on a synthetic MDP and a transfer\\nlearning task shows improvements regarding the sample efficiency as well as the\\naccuracy of the estimated reward functions and transition models.\\n\",\n",
       "  'title': u'Inverse Reinforcement Learning with Simultaneous Estimation of Rewards\\n  and Dynamics'},\n",
       " u'1504.08083': {'arxivid': u'1504.08083',\n",
       "  'authorsaffil': [[u'Ross Girshick', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'To appear in ICCV 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.08083v2',\n",
       "  'published': u'2015-04-30T05:13:08Z',\n",
       "  'summary': u'  This paper proposes a Fast Region-based Convolutional Network method (Fast\\nR-CNN) for object detection. Fast R-CNN builds on previous work to efficiently\\nclassify object proposals using deep convolutional networks. Compared to\\nprevious work, Fast R-CNN employs several innovations to improve training and\\ntesting speed while also increasing detection accuracy. Fast R-CNN trains the\\nvery deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and\\nachieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains\\nVGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is\\nimplemented in Python and C++ (using Caffe) and is available under the\\nopen-source MIT License at https://github.com/rbgirshick/fast-rcnn.\\n',\n",
       "  'title': u'Fast R-CNN'},\n",
       " u'1603.01292': {'arxivid': u'1603.01292',\n",
       "  'authorsaffil': [[u'Abhineet Singh', None],\n",
       "   [u'Ankush Roy', None],\n",
       "   [u'Xi Zhang', None],\n",
       "   [u'Martin Jagersand', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01292v2',\n",
       "  'published': u'2016-03-03T21:50:09Z',\n",
       "  'summary': u'  This paper presents a new way to study registration based trackers by\\ndecomposing them into three constituent sub modules: appearance model, state\\nspace model and search method. It is often the case that when a new tracker is\\nintroduced in literature, it only contributes to one or two of these sub\\nmodules while using existing methods for the rest. Since these are often\\nselected arbitrarily by the authors, they may not be optimal for the new\\nmethod. In such cases, our breakdown can help to experimentally find the best\\ncombination of methods for these sub modules while also providing a framework\\nwithin which the contributions of the new tracker can be clearly demarcated and\\nthus studied better. We show how existing trackers can be broken down using the\\nsuggested methodology and compare the performance of the default configuration\\nchosen by the authors against other possible combinations to demonstrate the\\nnew insights that can be gained by such an approach. We also present an open\\nsource system that provides a convenient interface to plug in a new method for\\nany sub module and test it against all possible combinations of methods for the\\nother two sub modules while also serving as a fast and efficient solution for\\npractical tracking requirements.\\n',\n",
       "  'title': u'Modular Decomposition and Analysis of Registration based Trackers'},\n",
       " u'1504.08081': {'arxivid': u'1504.08081',\n",
       "  'authorsaffil': [[u'Wenlian Lu', None],\n",
       "   [u'Ren Zheng', None],\n",
       "   [u'Xinlei Yi', None],\n",
       "   [u'Tianping Chen', None]],\n",
       "  'categoryterms': [u'nlin.AO', u'math.OC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.08081v1',\n",
       "  'published': u'2015-04-30T04:55:53Z',\n",
       "  'summary': u'  In this paper, we investigate convergence of a class of analytic neural\\nnetworks with event-triggered rule. This model is general and include Hopfield\\nneural network as a special case. The event-trigger rule efficiently reduces\\nthe frequency of information transmission between synapses of the neurons. The\\nsynaptic feedback of each neuron keeps a constant value based on the outputs of\\nits neighbours at its latest triggering time but changes until the next\\ntriggering time of this neuron that is determined by certain criterion via its\\nneighborhood information. It is proved that the analytic neural network is\\ncompletely stable under this event-triggered rule. The main technique of proof\\nis the ${\\\\L}$ojasiewicz inequality to prove the finiteness of trajectory\\nlength. The realization of this event-triggered rule is verified by the\\nexclusion of Zeno behaviors. Numerical examples are provided to illustrate the\\ntheoretical results and present the optimisation capability of the network\\ndynamics.\\n',\n",
       "  'title': u'Global Convergence of Analytic Neural Networks with Event-triggered\\n  Synaptic Feedbacks'},\n",
       " u'1509.00519': {'arxivid': u'1509.00519',\n",
       "  'authorsaffil': [[u'Yuri Burda', None],\n",
       "   [u'Roger Grosse', None],\n",
       "   [u'Ruslan Salakhutdinov', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Submitted to ICLR 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.00519v3',\n",
       "  'published': u'2015-09-01T22:33:13Z',\n",
       "  'summary': u\"  The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\\nproposed generative model pairing a top-down generative network with a\\nbottom-up recognition network which approximates posterior inference. It\\ntypically makes strong assumptions about posterior inference, for instance that\\nthe posterior distribution is approximately factorial, and that its parameters\\ncan be approximated with nonlinear regression from the observations. As we show\\nempirically, the VAE objective can lead to overly simplified representations\\nwhich fail to use the network's entire modeling capacity. We present the\\nimportance weighted autoencoder (IWAE), a generative model with the same\\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\\nbound derived from importance weighting. In the IWAE, the recognition network\\nuses multiple samples to approximate the posterior, giving it increased\\nflexibility to model complex posteriors which do not fit the VAE modeling\\nassumptions. We show empirically that IWAEs learn richer latent space\\nrepresentations than VAEs, leading to improved test log-likelihood on density\\nestimation benchmarks.\\n\",\n",
       "  'title': u'Importance Weighted Autoencoders'},\n",
       " u'1512.09194': {'arxivid': u'1512.09194',\n",
       "  'authorsaffil': [[u'Shuchang Zhou', None],\n",
       "   [u'Jia-Nan Wu', None],\n",
       "   [u'Yuxin Wu', None],\n",
       "   [u'Xinyu Zhou', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.09194v2',\n",
       "  'published': u'2015-12-31T01:32:16Z',\n",
       "  'summary': u'  In this paper, we propose and study a technique to reduce the number of\\nparameters and computation time in convolutional neural networks. We use\\nKronecker product to exploit the local structures within convolution and\\nfully-connected layers, by replacing the large weight matrices by combinations\\nof multiple Kronecker products of smaller matrices. Just as the Kronecker\\nproduct is a generalization of the outer product from vectors to matrices, our\\nmethod is a generalization of the low rank approximation method for convolution\\nneural networks. We also introduce combinations of different shapes of\\nKronecker product to increase modeling capacity. Experiments on SVHN, scene\\ntext recognition and ImageNet dataset demonstrate that we can achieve $3.3\\n\\\\times$ speedup or $3.6 \\\\times$ parameter reduction with less than 1\\\\% drop in\\naccuracy, showing the effectiveness and efficiency of our method. Moreover, the\\ncomputation efficiency of Kronecker layer makes using larger feature map\\npossible, which in turn enables us to outperform the previous state-of-the-art\\non both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese character\\nrecognition) datasets.\\n',\n",
       "  'title': u'Exploiting Local Structures with the Kronecker Layer in Convolutional\\n  Networks'},\n",
       " u'1603.00968': {'arxivid': u'1603.00968',\n",
       "  'authorsaffil': [[u'Ye Zhang', None],\n",
       "   [u'Stephen Roller', None],\n",
       "   [u'Byron Wallace', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'This paper got accepted by NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00968v2',\n",
       "  'published': u'2016-03-03T04:12:02Z',\n",
       "  'summary': u'  We introduce a novel, simple convolution neural network (CNN) architecture -\\nmulti-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of\\nword embeddings for sentence classification. MGNC-CNN extracts features from\\ninput embedding sets independently and then joins these at the penultimate\\nlayer in the network to form a final feature vector. We then adopt a group\\nregularization strategy that differentially penalizes weights associated with\\nthe subcomponents generated from the respective embedding sets. This model is\\nmuch simpler than comparable alternative architectures and requires\\nsubstantially less training time. Furthermore, it is flexible in that it does\\nnot require input word embeddings to be of the same dimensionality. We show\\nthat MGNC-CNN consistently outperforms baseline models.\\n',\n",
       "  'title': u'MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for\\n  Sentence Classification'},\n",
       " u'1601.03650': {'arxivid': u'1601.03650',\n",
       "  'authorsaffil': [[u'Vuong Van Bui', None], [u'Cuong Anh Le', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03650v4',\n",
       "  'published': u'2016-01-14T16:30:09Z',\n",
       "  'summary': u'  IBM models are very important word alignment models in Machine Translation.\\nFollowing the Maximum Likelihood Estimation principle to estimate their\\nparameters, the models will easily overfit the training data when the data are\\nsparse. While smoothing is a very popular solution in Language Model, there\\nstill lacks studies on smoothing for word alignment. In this paper, we propose\\na framework which generalizes the notable work Moore [2004] of applying\\nadditive smoothing to word alignment models. The framework allows developers to\\ncustomize the smoothing amount for each pair of word. The added amount will be\\nscaled appropriately by a common factor which reflects how much the framework\\ntrusts the adding strategy according to the performance on data. We also\\ncarefully examine various performance criteria and propose a smoothened version\\nof the error count, which generally gives the best result.\\n',\n",
       "  'title': u'Smoothing parameter estimation framework for IBM word alignment models'},\n",
       " u'1603.09439': {'arxivid': u'1603.09439',\n",
       "  'authorsaffil': [[u'Phuc Xuan Nguyen', None],\n",
       "   [u'Gregory Rogez', None],\n",
       "   [u'Charless Fowlkes', None],\n",
       "   [u'Deva Ramanan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09439v2',\n",
       "  'published': u'2016-03-31T02:19:53Z',\n",
       "  'summary': u'  Micro-videos are six-second videos popular on social media networks with\\nseveral unique properties. Firstly, because of the authoring process, they\\ncontain significantly more diversity and narrative structure than existing\\ncollections of video \"snippets\". Secondly, because they are often captured by\\nhand-held mobile cameras, they contain specialized viewpoints including\\nthird-person, egocentric, and self-facing views seldom seen in traditional\\nproduced video. Thirdly, due to to their continuous production and publication\\non social networks, aggregate micro-video content contains interesting\\nopen-world dynamics that reflects the temporal evolution of tag topics. These\\naspects make micro-videos an appealing well of visual data for developing\\nlarge-scale models for video understanding. We analyze a novel dataset of\\nmicro-videos labeled with 58 thousand tags. To analyze this data, we introduce\\nviewpoint-specific and temporally-evolving models for video understanding,\\ndefined over state-of-the-art motion and deep visual features. We conclude that\\nour dataset opens up new research opportunities for large-scale video analysis,\\nnovel viewpoints, and open-world dynamics.\\n',\n",
       "  'title': u'The Open World of Micro-Videos'},\n",
       " u'1502.05888': {'arxivid': u'1502.05888',\n",
       "  'authorsaffil': [[u'Jer\\xf4me Lang', None],\n",
       "   [u'Gabriella Pigozzi', None],\n",
       "   [u'Marija Slavkovik', None],\n",
       "   [u'Leendert van der Torre', None],\n",
       "   [u'Srdjan Vesic', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.05888v2',\n",
       "  'published': u'2015-02-20T14:50:53Z',\n",
       "  'summary': u'  The literature on judgment aggregation is moving from studying impossibility\\nresults regarding aggregation rules towards studying specific judgment\\naggregation rules. Here we give a structured list of most rules that have been\\nproposed and studied recently in the literature, together with various\\nproperties of such rules. We fi?rst focus on the majority-preservation\\nproperty, which generalizes Condorcet-consistency, and identify which of the\\nrules satisfy it. We study the inclusion relationships that hold between the\\nrules. Finally, we consider two forms of unanimity, monotonicity, homogeneity,\\nand reinforcement, and we identify which of the rules satisfy these properties.\\n',\n",
       "  'title': u'A partial taxonomy of judgment aggregation rules, and their properties'},\n",
       " u'1602.02018': {'arxivid': u'1602.02018',\n",
       "  'authorsaffil': [[u'Nicolas Tremblay', None],\n",
       "   [u'Gilles Puy', None],\n",
       "   [u'Remi Gribonval', None],\n",
       "   [u'Pierre Vandergheynst', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'12 pages, 2 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02018v2',\n",
       "  'published': u'2016-02-05T13:42:27Z',\n",
       "  'summary': u'  Spectral clustering has become a popular technique due to its high\\nperformance in many contexts. It comprises three main steps: create a\\nsimilarity graph between N objects to cluster, compute the first k eigenvectors\\nof its Laplacian matrix to define a feature vector for each object, and run\\nk-means on these features to separate objects into k classes. Each of these\\nthree steps becomes computationally intensive for large N and/or k. We propose\\nto speed up the last two steps based on recent results in the emerging field of\\ngraph signal processing: graph filtering of random signals, and random sampling\\nof bandlimited graph signals. We prove that our method, with a gain in\\ncomputation time that can reach several orders of magnitude, is in fact an\\napproximation of spectral clustering, for which we are able to control the\\nerror. We test the performance of our method on artificial and real-world\\nnetwork data.\\n',\n",
       "  'title': u'Compressive Spectral Clustering'},\n",
       " u'1601.01974': {'arxivid': u'1601.01974',\n",
       "  'authorsaffil': [[u'Francesco Orabona', None], [u'D\\xe1vid P\\xe1l', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1502.05744',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01974v1',\n",
       "  'published': u'2016-01-08T18:47:18Z',\n",
       "  'summary': u'  We design algorithms for online linear optimization that have optimal regret\\nand at the same time do not need to know any upper or lower bounds on the norm\\nof the loss vectors. We achieve adaptiveness to the norms of the loss vectors\\nby scale invariance, i.e., our algorithms make exactly the same decisions if\\nthe sequence of loss vectors is multiplied by any positive constant. One of our\\nalgorithms works for any decision set, bounded or unbounded. For unbounded\\ndecisions sets, this is the first adaptive algorithm for online linear\\noptimization with a non-vacuous regret bound.\\n  We also study a popular scale-free variant of online mirror descent\\nalgorithm, and we show that in two natural settings it has linear or worse\\nregret.\\n',\n",
       "  'title': u'Scale-Free Online Learning'},\n",
       " u'1602.03014': {'arxivid': u'1602.03014',\n",
       "  'authorsaffil': [[u'Yutian Chen', None], [u'Max Welling', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03014v2',\n",
       "  'published': u'2016-02-09T14:59:45Z',\n",
       "  'summary': u'  Herding defines a deterministic dynamical system at the edge of chaos. It\\ngenerates a sequence of model states and parameters by alternating parameter\\nperturbations with state maximizations, where the sequence of states can be\\ninterpreted as \"samples\" from an associated MRF model. Herding differs from\\nmaximum likelihood estimation in that the sequence of parameters does not\\nconverge to a fixed point and differs from an MCMC posterior sampling approach\\nin that the sequence of states is generated deterministically. Herding may be\\ninterpreted as a\"perturb and map\" method where the parameter perturbations are\\ngenerated using a deterministic nonlinear dynamical system rather than randomly\\nfrom a Gumbel distribution. This chapter studies the distinct statistical\\ncharacteristics of the herding algorithm and shows that the fast convergence\\nrate of the controlled moments may be attributed to edge of chaos dynamics. The\\nherding algorithm can also be generalized to models with latent variables and\\nto a discriminative learning setting. The perceptron cycling theorem ensures\\nthat the fast moment matching property is preserved in the more general\\nframework.\\n',\n",
       "  'title': u'Herding as a Learning System with Edge-of-Chaos Dynamics'},\n",
       " u'1602.06064': {'arxivid': u'1602.06064',\n",
       "  'authorsaffil': [[u'Tianxing He', None],\n",
       "   [u'Yu Zhang', None],\n",
       "   [u'Jasha Droppo', None],\n",
       "   [u'Kai Yu', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06064v3',\n",
       "  'published': u'2016-02-19T07:27:49Z',\n",
       "  'summary': u'  We propose to train bi-directional neural network language model(NNLM) with\\nnoise contrastive estimation(NCE). Experiments are conducted on a rescore task\\non the PTB data set. It is shown that NCE-trained bi-directional NNLM\\noutperformed the one trained by conventional maximum likelihood training. But\\nstill(regretfully), it did not out-perform the baseline uni-directional NNLM.\\n',\n",
       "  'title': u'On Training Bi-directional Neural Network Language Model with Noise\\n  Contrastive Estimation'},\n",
       " u'1604.01170': {'arxivid': u'1604.01170',\n",
       "  'authorsaffil': [[u'Antonia Godoy-Lorite', None],\n",
       "   [u'Roger Guimera', None],\n",
       "   [u'Cristopher Moore', None],\n",
       "   [u'Marta Sales-Pardo', None]],\n",
       "  'categoryterms': [u'cs.SI', u'cs.IR', u'cs.LG', u'physics.soc-ph'],\n",
       "  'comment': u'9 pages, 4 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01170v2',\n",
       "  'published': u'2016-04-05T08:28:08Z',\n",
       "  'summary': u\"  With ever-increasing amounts of online information available, modeling and\\npredicting individual preferences-for books or articles, for example-is\\nbecoming more and more important. Good predictions enable us to improve advice\\nto users, and obtain a better understanding of the socio-psychological\\nprocesses that determine those preferences. We have developed a collaborative\\nfiltering model, with an associated scalable algorithm, that makes accurate\\npredictions of individuals' preferences. Our approach is based on the explicit\\nassumption that there are groups of individuals and of items, and that the\\npreferences of an individual for an item are determined only by their group\\nmemberships. Importantly, we allow each individual and each item to belong\\nsimultaneously to mixtures of different groups and, unlike many popular\\napproaches, such as matrix factorization, we do not assume implicitly or\\nexplicitly that individuals in each group prefer items in a single group of\\nitems. The resulting overlapping groups and the predicted preferences can be\\ninferred with a expectation-maximization algorithm whose running time scales\\nlinearly (per iteration). Our approach enables us to predict individual\\npreferences in large datasets, and is considerably more accurate than the\\ncurrent algorithms for such large datasets.\\n\",\n",
       "  'title': u'Accurate and scalable social recommendation using mixed-membership\\n  stochastic block models'},\n",
       " u'1507.04717': {'arxivid': u'1507.04717',\n",
       "  'authorsaffil': [[u'Alessandro Rudi', None],\n",
       "   [u'Raffaello Camoriano', None],\n",
       "   [u'Lorenzo Rosasco', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'updated version of NIPS 2015 (oral)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.04717v6',\n",
       "  'published': u'2015-07-16T19:26:27Z',\n",
       "  'summary': u'  We study Nystr\\\\\"om type subsampling approaches to large scale kernel methods,\\nand prove learning bounds in the statistical learning setting, where random\\nsampling and high probability estimates are considered. In particular, we prove\\nthat these approaches can achieve optimal learning bounds, provided the\\nsubsampling level is suitably chosen. These results suggest a simple\\nincremental variant of Nystr\\\\\"om Kernel Regularized Least Squares, where the\\nsubsampling level implements a form of computational regularization, in the\\nsense that it controls at the same time regularization and computations.\\nExtensive experimental analysis shows that the considered approach achieves\\nstate of the art performances on benchmark large scale datasets.\\n',\n",
       "  'title': u'Less is More: Nystr\\xf6m Computational Regularization'},\n",
       " u'1603.05623': {'arxivid': u'1603.05623',\n",
       "  'authorsaffil': [[u'Dimitri Van De Ville', None]],\n",
       "  'categoryterms': [u'cs.SI', u'cs.CE', u'cs.DM'],\n",
       "  'comment': u'15 pages, 7 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05623v1',\n",
       "  'published': u'2016-03-17T19:11:46Z',\n",
       "  'summary': u'  Network science plays an increasingly important role to model complex data in\\nmany scientific disciplines. One notable feature of network organization is\\ncommunity structure, which refers to clusters of tightly interconnected nodes.\\nA prominent problem is how to investigate the relationship between macro-scale\\nmodules that are retrieved by optimizing global network measures, and\\nmicro-scale structure that are defined by specific queries of the analysis\\n(e.g., nodal features). By generalizing fundamental concepts of joint\\nspace-frequency localization to network theory, here we propose a flexible\\nframework to study interactions between micro- and macro-structure. Similar to\\npointing and focusing a magnifying glass, the analysis can be directed to\\nspecific micro-scale structure, while the degree of interaction with the\\nmacro-scale community structure can be seamlessly controlled. In addition, the\\nmethod is computationally efficient as a result of the underlying\\nlow-dimensional optimization problem.\\n',\n",
       "  'title': u'Steering Macro-Scale Network Community Structure by Micro-Scale Features'},\n",
       " u'1603.05285': {'arxivid': u'1603.05285',\n",
       "  'authorsaffil': [[u'Freddie \\xc5str\\xf6m', None],\n",
       "   [u'Stefania Petra', None],\n",
       "   [u'Bernhard Schmitzer', None],\n",
       "   [u'Christoph Schn\\xf6rr', None]],\n",
       "  'categoryterms': [u'cs.CV', u'math.OC', u'62H35, 65K05, 68U10, 62M40'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05285v1',\n",
       "  'published': u'2016-03-16T21:15:49Z',\n",
       "  'summary': u'  We introduce a novel geometric approach to the image labeling problem.\\nAbstracting from specific labeling applications, a general objective function\\nis defined on a manifold of stochastic matrices, whose elements assign prior\\ndata that are given in any metric space, to observed image measurements. The\\ncorresponding Riemannian gradient flow entails a set of replicator equations,\\none for each data point, that are spatially coupled by geometric averaging on\\nthe manifold. Starting from uniform assignments at the barycenter as natural\\ninitialization, the flow terminates at some global maximum, each of which\\ncorresponds to an image labeling that uniquely assigns the prior data. Our\\ngeometric variational approach constitutes a smooth non-convex inner\\napproximation of the general image labeling problem, implemented with sparse\\ninterior-point numerics in terms of parallel multiplicative updates that\\nconverge efficiently.\\n',\n",
       "  'title': u'Image Labeling by Assignment'},\n",
       " u'1602.02389': {'arxivid': u'1602.02389',\n",
       "  'authorsaffil': [[u'Jiashi Feng', None],\n",
       "   [u'Tom Zahavy', None],\n",
       "   [u'Bingyi Kang', None],\n",
       "   [u'Huan Xu', None],\n",
       "   [u'Shie Mannor', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02389v2',\n",
       "  'published': u'2016-02-07T16:50:14Z',\n",
       "  'summary': u'  The question why deep learning algorithms perform so well in practice has\\npuzzled machine learning theoreticians and practitioners alike. However, most\\nof well-established approaches, such as hypothesis capacity, robustness or\\nsparseness, have not provided complete explanations, due to the high complexity\\nof the deep learning algorithms and their inherent randomness. In this work, we\\nintroduce a new approach -- ensemble robustness -- towards characterizing the\\ngeneralization performance of generic deep learning algorithms. Ensemble\\nrobustness concerns robustness of the population of the hypotheses that may be\\noutput by a learning algorithm. Through the lens of ensemble robustness, we\\nreveal that a stochastic learning algorithm can generalize well as long as its\\nsensitiveness to adversarial perturbation is bounded in average, or\\nequivalently, the performance variance of the algorithm is small. Quantifying\\nthe ensemble robustness of various deep learning algorithms may be difficult\\nanalytically. However, extensive simulations for seven common deep learning\\nalgorithms for different network architectures provide supporting evidence for\\nour claims. In addition, as an example for utilizing ensemble robustness, we\\npropose a novel semi-supervised learning method that outperforms the\\nstate-of-the-art. Furthermore, our work explains the good performance of\\nseveral published deep learning algorithms.\\n',\n",
       "  'title': u'Ensemble Robustness of Deep Learning Algorithms'},\n",
       " u'1511.05950': {'arxivid': u'1511.05950',\n",
       "  'authorsaffil': [[u'Wei Zhang', None],\n",
       "   [u'Suyog Gupta', None],\n",
       "   [u'Xiangru Lian', None],\n",
       "   [u'Ji Liu', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Accepted by IJCAI 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05950v5',\n",
       "  'published': u'2015-11-18T20:53:33Z',\n",
       "  'summary': u'  Deep neural networks have been shown to achieve state-of-the-art performance\\nin several machine learning tasks. Stochastic Gradient Descent (SGD) is the\\npreferred optimization algorithm for training these networks and asynchronous\\nSGD (ASGD) has been widely adopted for accelerating the training of large-scale\\ndeep networks in a distributed computing environment. However, in practice it\\nis quite challenging to tune the training hyperparameters (such as learning\\nrate) when using ASGD so as achieve convergence and linear speedup, since the\\nstability of the optimization algorithm is strongly influenced by the\\nasynchronous nature of parameter updates. In this paper, we propose a variant\\nof the ASGD algorithm in which the learning rate is modulated according to the\\ngradient staleness and provide theoretical guarantees for convergence of this\\nalgorithm. Experimental verification is performed on commonly-used image\\nclassification benchmarks: CIFAR10 and Imagenet to demonstrate the superior\\neffectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and\\nthe conventional ASGD algorithm.\\n',\n",
       "  'title': u'Staleness-aware Async-SGD for Distributed Deep Learning'},\n",
       " u'1511.05952': {'arxivid': u'1511.05952',\n",
       "  'authorsaffil': [[u'Tom Schaul', None],\n",
       "   [u'John Quan', None],\n",
       "   [u'Ioannis Antonoglou', None],\n",
       "   [u'David Silver', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Published at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05952v4',\n",
       "  'published': u'2015-11-18T20:54:44Z',\n",
       "  'summary': u'  Experience replay lets online reinforcement learning agents remember and\\nreuse experiences from the past. In prior work, experience transitions were\\nuniformly sampled from a replay memory. However, this approach simply replays\\ntransitions at the same frequency that they were originally experienced,\\nregardless of their significance. In this paper we develop a framework for\\nprioritizing experience, so as to replay important transitions more frequently,\\nand therefore learn more efficiently. We use prioritized experience replay in\\nDeep Q-Networks (DQN), a reinforcement learning algorithm that achieved\\nhuman-level performance across many Atari games. DQN with prioritized\\nexperience replay achieves a new state-of-the-art, outperforming DQN with\\nuniform replay on 41 out of 49 games.\\n',\n",
       "  'title': u'Prioritized Experience Replay'},\n",
       " u'1602.02383': {'arxivid': u'1602.02383',\n",
       "  'authorsaffil': [[u'William Whitney', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u\"MIT Master's of Engineering thesis\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02383v1',\n",
       "  'published': u'2016-02-07T15:32:30Z',\n",
       "  'summary': u'  Representation learning is the foundation for the recent success of neural\\nnetwork models. However, the distributed representations generated by neural\\nnetworks are far from ideal. Due to their highly entangled nature, they are di\\ncult to reuse and interpret, and they do a poor job of capturing the sparsity\\nwhich is present in real- world transformations. In this paper, I describe\\nmethods for learning disentangled representations in the two domains of\\ngraphics and computation. These methods allow neural methods to learn\\nrepresentations which are easy to interpret and reuse, yet they incur little or\\nno penalty to performance. In the Graphics section, I demonstrate the ability\\nof these methods to infer the generating parameters of images and rerender\\nthose images under novel conditions. In the Computation section, I describe a\\nmodel which is able to factorize a multitask learning problem into subtasks and\\nwhich experiences no catastrophic forgetting. Together these techniques provide\\nthe tools to design a wide range of models that learn disentangled\\nrepresentations and better model the factors of variation in the real world.\\n',\n",
       "  'title': u'Disentangled Representations in Neural Models'},\n",
       " u'1602.02386': {'arxivid': u'1602.02386',\n",
       "  'authorsaffil': [[u'Qingming Tang', None],\n",
       "   [u'Lifu Tu', None],\n",
       "   [u'Weiran Wang', None],\n",
       "   [u'Jinbo Xu', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02386v1',\n",
       "  'published': u'2016-02-07T16:11:18Z',\n",
       "  'summary': u'  We propose a novel method for network inference from partially observed edges\\nusing a node-specific degree prior. The degree prior is derived from observed\\nedges in the network to be inferred, and its hyper-parameters are determined by\\ncross validation. Then we formulate network inference as a matrix completion\\nproblem regularized by our degree prior. Our theoretical analysis indicates\\nthat this prior favors a network following the learned degree distribution, and\\nmay lead to improved network recovery error bound than previous work.\\nExperimental results on both simulated and real biological networks demonstrate\\nthe superior performance of our method in various settings.\\n',\n",
       "  'title': u'Network Inference by Learned Node-Specific Degree Prior'},\n",
       " u'1601.06750': {'arxivid': u'1601.06750',\n",
       "  'authorsaffil': [[u'Divya Padmanabhan', None],\n",
       "   [u'Satyanath Bhat', None],\n",
       "   [u'Dinesh Garg', None],\n",
       "   [u'Shirish Shevade', None],\n",
       "   [u'Y. Narahari', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06750v2',\n",
       "  'published': u'2016-01-25T20:14:22Z',\n",
       "  'summary': u'  We study the problem of training an accurate linear regression model by\\nprocuring labels from multiple noisy crowd annotators, under a budget\\nconstraint. We propose a Bayesian model for linear regression in crowdsourcing\\nand use variational inference for parameter estimation. To minimize the number\\nof labels crowdsourced from the annotators, we adopt an active learning\\napproach. In this specific context, we prove the equivalence of well-studied\\ncriteria of active learning like entropy minimization and expected error\\nreduction. Interestingly, we observe that we can decouple the problems of\\nidentifying an optimal unlabeled instance and identifying an annotator to label\\nit. We observe a useful connection between the multi-armed bandit framework and\\nthe annotator selection in active learning. Due to the nature of the\\ndistribution of the rewards on the arms, we use the Robust Upper Confidence\\nBound (UCB) scheme with truncated empirical mean estimator to solve the\\nannotator selection problem. This yields provable guarantees on the regret. We\\nfurther apply our model to the scenario where annotators are strategic and\\ndesign suitable incentives to induce them to put in their best efforts.\\n',\n",
       "  'title': u'A Robust UCB Scheme for Active Learning in Regression from Strategic\\n  Crowds'},\n",
       " u'1507.06970': {'arxivid': u'1507.06970',\n",
       "  'authorsaffil': [[u'Horia Mania', None],\n",
       "   [u'Xinghao Pan', None],\n",
       "   [u'Dimitris Papailiopoulos', None],\n",
       "   [u'Benjamin Recht', None],\n",
       "   [u'Kannan Ramchandran', None],\n",
       "   [u'Michael I. Jordan', None]],\n",
       "  'categoryterms': [u'stat.ML',\n",
       "   u'cs.DC',\n",
       "   u'cs.DS',\n",
       "   u'cs.LG',\n",
       "   u'math.OC',\n",
       "   u'65K10, 65Y05, 68W10, 68W20'],\n",
       "  'comment': u'30 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.06970v2',\n",
       "  'published': u'2015-07-24T19:36:13Z',\n",
       "  'summary': u'  We introduce and analyze stochastic optimization methods where the input to\\neach gradient update is perturbed by bounded noise. We show that this framework\\nforms the basis of a unified approach to analyze asynchronous implementations\\nof stochastic optimization algorithms.In this framework, asynchronous\\nstochastic optimization algorithms can be thought of as serial methods\\noperating on noisy inputs. Using our perturbed iterate framework, we provide\\nnew analyses of the Hogwild! algorithm and asynchronous stochastic coordinate\\ndescent, that are simpler than earlier analyses, remove many assumptions of\\nprevious models, and in some cases yield improved upper bounds on the\\nconvergence rates. We proceed to apply our framework to develop and analyze\\nKroMagnon: a novel, parallel, sparse stochastic variance-reduced gradient\\n(SVRG) algorithm. We demonstrate experimentally on a 16-core machine that the\\nsparse and parallel version of SVRG is in some cases more than four orders of\\nmagnitude faster than the standard SVRG algorithm.\\n',\n",
       "  'title': u'Perturbed Iterate Analysis for Asynchronous Stochastic Optimization'},\n",
       " u'1603.09320': {'arxivid': u'1603.09320',\n",
       "  'authorsaffil': [[u'Yu. A. Malkov', None], [u'D. A. Yashunin', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.CV', u'cs.IR', u'cs.SI'],\n",
       "  'comment': u'21 pages, 13 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09320v2',\n",
       "  'published': u'2016-03-30T19:29:44Z',\n",
       "  'summary': u'  We present a new algorithm for the approximate K-nearest neighbor search\\nbased on navigable small world graphs with controllable hierarchy (Hierarchical\\nNSW). The proposed approach is fully graph-based, without any need for\\nadditional search structures which are typically used at coarse search stage of\\nthe most proximity graph techniques. Hierarchical NSW incrementally builds\\nmulti-layer structure consisting from hierarchical set of proximity graphs\\n(layers) for nested subsets of the stored elements. The maximum layer in which\\nan element is present is selected randomly with exponentially decaying\\nprobability distribution. This allows producing graphs similar to the\\npreviously studied Navigable Small World (NSW) structures while additionally\\nhaving the links separated by their characteristic distance scales. Starting\\nsearch from the upper layer together with utilizing the scale separation boosts\\nthe performance compared to the NSW and allows a logarithmic complexity\\nscaling. Additional employment of a heuristic for selecting proximity graph\\nneighbors significantly increases performance at high recall and in case of\\nhighly clustered data. Performance evaluation has demonstrated that the\\nproposed general metric space method is able to strongly outperform many\\nprevious state-of-art vector-only approaches. Similarity of the algorithm to\\nthe skip list structure allows straightforward balanced distributed\\nimplementation.\\n',\n",
       "  'title': u'Efficient and robust approximate nearest neighbor search using\\n  Hierarchical Navigable Small World graphs'},\n",
       " u'1602.02899': {'arxivid': u'1602.02899',\n",
       "  'authorsaffil': [[u'Ferhat \\xd6zg\\xfcr \\xc7atak', None]],\n",
       "  'categoryterms': [u'cs.CR', u'cs.LG'],\n",
       "  'comment': u'22nd International Conference, ICONIP 2015',\n",
       "  'doi': u'10.1007/978-3-319-26535-3_39',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02899v1',\n",
       "  'published': u'2016-02-09T08:37:26Z',\n",
       "  'summary': u'  Especially in the Big Data era, the usage of different classification methods\\nis increasing day by day. The success of these classification methods depends\\non the effectiveness of learning methods. Extreme learning machine (ELM)\\nclassification algorithm is a relatively new learning method built on\\nfeed-forward neural-network. ELM classification algorithm is a simple and fast\\nmethod that can create a model from high-dimensional data sets. Traditional ELM\\nlearning algorithm implicitly assumes complete access to whole data set. This\\nis a major privacy concern in most of cases. Sharing of private data (i.e.\\nmedical records) is prevented because of security concerns. In this research,\\nwe propose an efficient and secure privacy-preserving learning algorithm for\\nELM classification over data that is vertically partitioned among several\\nparties. The new learning method preserves the privacy on numerical attributes,\\nbuilds a classification model without sharing private data without disclosing\\nthe data of each party to others.\\n',\n",
       "  'title': u'Secure Multi-Party Computation Based Privacy Preserving Extreme Learning\\n  Machine Algorithm Over Vertically Distributed Data'},\n",
       " u'1603.00275': {'arxivid': u'1603.00275',\n",
       "  'authorsaffil': [[u'Korsuk Sirinukunwattana', None],\n",
       "   [u'Josien P. W. Pluim', None],\n",
       "   [u'Hao Chen', None],\n",
       "   [u'Xiaojuan Qi', None],\n",
       "   [u'Pheng-Ann Heng', None],\n",
       "   [u'Yun Bo Guo', None],\n",
       "   [u'Li Yang Wang', None],\n",
       "   [u'Bogdan J. Matuszewski', None],\n",
       "   [u'Elia Bruni', None],\n",
       "   [u'Urko Sanchez', None],\n",
       "   [u'Anton B\\xf6hm', None],\n",
       "   [u'Olaf Ronneberger', None],\n",
       "   [u'Bassem Ben Cheikh', None],\n",
       "   [u'Daniel Racoceanu', None],\n",
       "   [u'Philipp Kainz', None],\n",
       "   [u'Michael Pfeiffer', None],\n",
       "   [u'Martin Urschler', None],\n",
       "   [u'David R. J. Snead', None],\n",
       "   [u'Nasir M. Rajpoot', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00275v1',\n",
       "  'published': u'2016-03-01T13:53:48Z',\n",
       "  'summary': u\"  Colorectal adenocarcinoma originating in intestinal glandular structures is\\nthe most common form of colon cancer. In clinical practice, the morphology of\\nintestinal glands, including architectural appearance and glandular formation,\\nis used by pathologists to inform prognosis and plan the treatment of\\nindividual patients. However, achieving good inter-observer as well as\\nintra-observer reproducibility of cancer grading is still a major challenge in\\nmodern pathology. An automated approach which quantifies the morphology of\\nglands is a solution to the problem. This paper provides an overview to the\\nGland Segmentation in Colon Histology Images Challenge Contest (GlaS) held at\\nMICCAI'2015. Details of the challenge, including organization, dataset and\\nevaluation criteria, are presented, along with the method descriptions and\\nevaluation results from the top performing methods.\\n\",\n",
       "  'title': u'Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest'},\n",
       " u'1603.06708': {'arxivid': u'1603.06708',\n",
       "  'authorsaffil': [[u'Changsheng Li', None],\n",
       "   [u'Fan Wei', None],\n",
       "   [u'Junchi Yan', None],\n",
       "   [u'Weishan Dong', None],\n",
       "   [u'Qingshan Liu', None],\n",
       "   [u'Xiaoyu Zhang', None],\n",
       "   [u'Hongyuan Zha', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06708v2',\n",
       "  'published': u'2016-03-22T09:03:40Z',\n",
       "  'summary': u'  In this paper, we propose a novel multi-label learning framework, called\\nMulti-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the\\nself-paced learning strategy into multi-label learning regime. In light of the\\nbenefits of adopting the easy-to-hard strategy proposed by self-paced learning,\\nthe devised MLSPL aims to learn multiple labels jointly by gradually including\\nlabel learning tasks and instances into model training from the easy to the\\nhard. We first introduce a self-paced function as a regularizer in the\\nmulti-label learning formulation, so as to simultaneously rank priorities of\\nthe label learning tasks and the instances in each learning iteration.\\nConsidering that different multi-label learning scenarios often need different\\nself-paced schemes during optimization, we thus propose a general way to find\\nthe desired self-paced functions. Experimental results on three benchmark\\ndatasets suggest the state-of-the-art performance of our approach.\\n',\n",
       "  'title': u'A Self-Paced Regularization Framework for Multi-Label Learning'},\n",
       " u'1512.05840': {'arxivid': u'1512.05840',\n",
       "  'authorsaffil': [[u'Pau Perng-Hwa Kung', None]],\n",
       "  'categoryterms': [u'cs.CY', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05840v1',\n",
       "  'published': u'2015-12-18T01:25:47Z',\n",
       "  'summary': u'  Newsroom in online ecosystem is difficult to untangle. With prevalence of\\nsocial media, interactions between journalists and individuals become visible,\\nbut lack of understanding to inner processing of information feedback loop in\\npublic sphere leave most journalists baffled. Can we provide an organized view\\nto characterize journalist behaviors on individual level to know better of the\\necosystem? To this end, I propose Poisson Factorization Machine (PFM), a\\nBayesian analogue to matrix factorization that assumes Poisson distribution for\\ngenerative process. The model generalizes recent studies on Poisson Matrix\\nFactorization to account temporal interaction which involves tensor-like\\nstructure, and label information. Two inference procedures are designed, one\\nbased on batch variational EM and another stochastic variational inference\\nscheme that efficiently scales with data size. An important novelty in this\\nnote is that I show how to stack layers of PFM to introduce a deep\\narchitecture. This work discusses some potential results applying the model and\\nexplains how such latent factors may be useful for analyzing latent behaviors\\nfor data exploration.\\n',\n",
       "  'title': u'Deep Poisson Factorization Machines: factor analysis for mapping\\n  behaviors in journalist ecosystem'},\n",
       " u'1603.07051': {'arxivid': u'1603.07051',\n",
       "  'authorsaffil': [[u'Mohamed El Yafrani', None], [u'Bela\\xefd Ahiod', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.DS', u'cs.NE'],\n",
       "  'comment': u'12th ACS/IEEE International Conference on Computer Systems and\\n  Applications (AICCSA) 2015. November 17-20, 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07051v1',\n",
       "  'published': u'2016-03-23T02:30:35Z',\n",
       "  'summary': u'  Real-world problems are very difficult to optimize. However, many researchers\\nhave been solving benchmark problems that have been extensively investigated\\nfor the last decades even if they have very few direct applications. The\\nTraveling Thief Problem (TTP) is a NP-hard optimization problem that aims to\\nprovide a more realistic model. TTP targets particularly routing problem under\\npacking/loading constraints which can be found in supply chain management and\\ntransportation. In this paper, TTP is presented and formulated mathematically.\\nA combined local search algorithm is proposed and compared with Random Local\\nSearch (RLS) and Evolutionary Algorithm (EA). The obtained results are quite\\npromising since new better solutions were found.\\n',\n",
       "  'title': u'Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief\\n  Problem'},\n",
       " u'1604.02085': {'arxivid': u'1604.02085',\n",
       "  'authorsaffil': [[u'Daniel Heger', None], [u'Katharina Krischer', None]],\n",
       "  'categoryterms': [u'nlin.AO', u'cs.CV', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02085v1',\n",
       "  'published': u'2016-04-07T17:31:13Z',\n",
       "  'summary': u'  Uncertain recognition success, unfavorable scaling of connection complexity\\nor dependence on complex external input impair the usefulness of current\\noscillatory neural networks for pattern recognition or restrict technical\\nrealizations to small networks. We propose a new network architecture of\\ncoupled oscillators for pattern recognition which shows none of the mentioned\\naws. Furthermore we illustrate the recognition process with simulation results\\nand analyze the new dynamics analytically: Possible output patterns are\\nisolated attractors of the system. Additionally, simple criteria for\\nrecognition success are derived from a lower bound on the basins of attraction.\\n',\n",
       "  'title': u'A robust autoassociative memory with coupled networks of Kuramoto-type\\n  oscillators'},\n",
       " u'1512.05844': {'arxivid': u'1512.05844',\n",
       "  'authorsaffil': [[u'Mohammad Javad Shafiee', None],\n",
       "   [u'Parthipan Siva', None],\n",
       "   [u'Paul Fieguth', None],\n",
       "   [u'Alexander Wong', None]],\n",
       "  'categoryterms': [u'cs.CV', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.15353/vsnl.v1i1.44',\n",
       "  'journalref': u'Vision Letters, Vol. 1, No. 1, pp. VL115, 2015',\n",
       "  'link': u'http://arxiv.org/abs/1512.05844v1',\n",
       "  'published': u'2015-12-18T03:09:29Z',\n",
       "  'summary': u'  Transfer learning is a recent field of machine learning research that aims to\\nresolve the challenge of dealing with insufficient training data in the domain\\nof interest. This is a particular issue with traditional deep neural networks\\nwhere a large amount of training data is needed. Recently, StochasticNets was\\nproposed to take advantage of sparse connectivity in order to decrease the\\nnumber of parameters that needs to be learned, which in turn may relax training\\ndata size requirements. In this paper, we study the efficacy of transfer\\nlearning on StochasticNet frameworks. Experimental results show ~7% improvement\\non StochasticNet performance when the transfer learning is applied in training\\nstep.\\n',\n",
       "  'title': u'Domain Adaptation and Transfer Learning in StochasticNets'},\n",
       " u'1511.03771': {'arxivid': u'1511.03771',\n",
       "  'authorsaffil': [[u'Sachin S. Talathi', None], [u'Aniket Vartak', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG'],\n",
       "  'comment': u'10 pages 6 figures; under consideration for publication with ICLR\\n  2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03771v2',\n",
       "  'published': u'2015-11-12T04:35:41Z',\n",
       "  'summary': u'  In recent years significant progress has been made in successfully training\\nrecurrent neural networks (RNNs) on sequence learning problems involving long\\nrange temporal dependencies. The progress has been made on three fronts: (a)\\nAlgorithmic improvements involving sophisticated optimization techniques, (b)\\nnetwork design involving complex hidden layer nodes and specialized recurrent\\nlayer connections and (c) weight initialization methods. In this paper, we\\nfocus on recently proposed weight initialization with identity matrix for the\\nrecurrent weights in a RNN. This initialization is specifically proposed for\\nhidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple\\ndynamical systems perspective on weight initialization process, which allows us\\nto propose a modified weight initialization strategy. We show that this\\ninitialization technique leads to successfully training RNNs composed of ReLUs.\\nWe demonstrate that our proposal produces comparable or better solution for\\nthree toy problems involving long range temporal structure: the addition\\nproblem, the multiplication problem and the MNIST classification problem using\\nsequence of pixels. In addition, we present results for a benchmark action\\nrecognition problem.\\n',\n",
       "  'title': u'Improving performance of recurrent neural network with relu nonlinearity'},\n",
       " u'1603.07057': {'arxivid': u'1603.07057',\n",
       "  'authorsaffil': [[u'Iacopo Masi', None],\n",
       "   [u'Anh Tuan Tran', None],\n",
       "   [u'Jatuporn Toy Leksut', None],\n",
       "   [u'Tal Hassner', None],\n",
       "   [u'Gerard Medioni', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07057v2',\n",
       "  'published': u'2016-03-23T02:57:15Z',\n",
       "  'summary': u'  Face recognition capabilities have recently made extraordinary leaps. Though\\nthis progress is at least partially due to ballooning training set sizes --\\nhuge numbers of face images downloaded and labeled for identity -- it is not\\nclear if the formidable task of collecting so many images is truly necessary.\\nWe propose a far more accessible means of increasing training data sizes for\\nface recognition systems. Rather than manually harvesting and labeling more\\nfaces, we simply synthesize them. We describe novel methods of enriching an\\nexisting dataset with important facial appearance variations by manipulating\\nthe faces it contains. We further apply this synthesis approach when matching\\nquery images represented using a standard convolutional neural network. The\\neffect of training and testing with synthesized images is extensively tested on\\nthe LFW and IJB-A (verification and identification) benchmarks and Janus CS2.\\nThe performances obtained by our approach match state of the art results\\nreported by systems trained on millions of downloaded images.\\n',\n",
       "  'title': u'Do We Really Need to Collect Millions of Faces for Effective Face\\n  Recognition?'},\n",
       " u'1512.05849': {'arxivid': u'1512.05849',\n",
       "  'authorsaffil': [[u'Miles Brundage', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'AAAI 2016 Workshop on AI, Ethics, and Society',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05849v1',\n",
       "  'published': u'2015-12-18T04:17:39Z',\n",
       "  'summary': u'  Participants in recent discussions of AI-related issues ranging from\\nintelligence explosion to technological unemployment have made diverse claims\\nabout the nature, pace, and drivers of progress in AI. However, these theories\\nare rarely specified in enough detail to enable systematic evaluation of their\\nassumptions or to extrapolate progress quantitatively, as is often done with\\nsome success in other technological domains. After reviewing relevant\\nliteratures and justifying the need for more rigorous modeling of AI progress,\\nthis paper contributes to that research program by suggesting ways to account\\nfor the relationship between hardware speed increases and algorithmic\\nimprovements in AI, the role of human inputs in enabling AI capabilities, and\\nthe relationships between different sub-fields of AI. It then outlines ways of\\ntailoring AI progress models to generate insights on the specific issue of\\ntechnological unemployment, and outlines future directions for research on AI\\nprogress.\\n',\n",
       "  'title': u'Modeling Progress in AI'},\n",
       " u'1603.09050': {'arxivid': u'1603.09050',\n",
       "  'authorsaffil': [[u'Nguyen Viet Cuong', None],\n",
       "   [u'Nan Ye', None],\n",
       "   [u'Wee Sun Lee', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'This paper is published at AAAI Conference on Artificial Intelligence\\n  (AAAI 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09050v1',\n",
       "  'published': u'2016-03-30T06:21:42Z',\n",
       "  'summary': u'  We study the robustness of active learning (AL) algorithms against prior\\nmisspecification: whether an algorithm achieves similar performance using a\\nperturbed prior as compared to using the true prior. In both the average and\\nworst cases of the maximum coverage setting, we prove that all\\n$\\\\alpha$-approximate algorithms are robust (i.e., near $\\\\alpha$-approximate) if\\nthe utility is Lipschitz continuous in the prior. We further show that\\nrobustness may not be achieved if the utility is non-Lipschitz. This suggests\\nwe should use a Lipschitz utility for AL if robustness is required. For the\\nminimum cost setting, we can also obtain a robustness result for approximate AL\\nalgorithms. Our results imply that many commonly used AL algorithms are robust\\nagainst perturbed priors. We then propose the use of a mixture prior to\\nalleviate the problem of prior misspecification. We analyze the robustness of\\nthe uniform mixture prior and show experimentally that it performs reasonably\\nwell in practice.\\n',\n",
       "  'title': u'Robustness of Bayesian Pool-based Active Learning Against Prior\\n  Misspecification'},\n",
       " u'1603.09051': {'arxivid': u'1603.09051',\n",
       "  'authorsaffil': [[u'Rahul A R', None], [u'G Srinivasaraghavan', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09051v1',\n",
       "  'published': u'2016-03-30T06:41:04Z',\n",
       "  'summary': u\"  Since the advent of computers, many tasks which required humans to spend a\\nlot of time and energy have been trivialized by the computers' ability to\\nperform repetitive tasks extremely quickly. However there are still many areas\\nin which humans excel in comparison with the machines. One such area is chess.\\nEven with great advances in the speed and computational power of modern\\nmachines, Grandmasters often beat the best chess programs in the world with\\nrelative ease. This may be due to the fact that a game of chess cannot be won\\nby pure calculation. There is more to the goodness of a chess position than\\nsome numerical value which apparently can be seen only by the human brain. Here\\nan effort has been made to improve current chess engines by letting themselves\\nevolve over a period of time. Firstly, the problem of learning is reduced into\\nan optimization problem by defining Position Evaluation in terms of Positional\\nValue Tables (PVTs). Next, the PVTs are optimized using Multi-Niche Crowding\\nwhich successfully identifies the optima in a multimodal function, thereby\\narriving at distinctly different solutions which are close to the global\\noptimum.\\n\",\n",
       "  'title': u'Phoenix: A Self-Optimizing Chess Engine'},\n",
       " u'1603.09056': {'arxivid': u'1603.09056',\n",
       "  'authorsaffil': [[u'Xiao-Jiao Mao', None],\n",
       "   [u'Chunhua Shen', None],\n",
       "   [u'Yu-Bin Yang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'15 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09056v1',\n",
       "  'published': u'2016-03-30T07:16:05Z',\n",
       "  'summary': u'  Image denoising is a long-standing problem in computer vision and image\\nprocessing, as well as a test bed for low-level image modeling algorithms. In\\nthis paper, we propose a very deep encoding-decoding framework for image\\ndenoising. Instead of using image priors, the proposed framework learns\\nend-to-end fully convolutional mappings from noisy images to the clean ones.\\nThe network is composed of multiple layers of convolution and de-convolution\\noperators. With the observation that deeper networks improve denoising\\nperformance, we propose to use significantly deeper networks than those\\nemployed previously for low-level image processing tasks such as denoising. We\\npropose to symmetrically link convolutional and de-convolutional layers with\\nskip-layer connections, with which the training converges much faster and\\nattains a higher-quality local optimum. From the image processing point of\\nview, those symmetric connections help preserve image details.\\n',\n",
       "  'title': u'Image Denoising Using Very Deep Fully Convolutional Encoder-Decoder\\n  Networks with Symmetric Skip Connections'},\n",
       " u'1508.03337': {'arxivid': u'1508.03337',\n",
       "  'authorsaffil': [[u'Kimon Fountoulakis', None],\n",
       "   [u'Abhisek Kundu', None],\n",
       "   [u'Eugenia-Maria Kontopoulou', None],\n",
       "   [u'Petros Drineas', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'22 pages, 31 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.03337v4',\n",
       "  'published': u'2015-08-13T20:06:59Z',\n",
       "  'summary': u'  We present and analyze a simple, two-step algorithm to approximate the\\noptimal solution of the sparse PCA problem. Our approach first solves a L1\\npenalized version of the NP-hard sparse PCA optimization problem and then uses\\na randomized rounding strategy to sparsify the resulting dense solution. Our\\nmain theoretical result guarantees an additive error approximation and provides\\na tradeoff between sparsity and accuracy. Our experimental evaluation indicates\\nthat our approach is competitive in practice, even compared to state-of-the-art\\ntoolboxes such as Spasm.\\n',\n",
       "  'title': u'A Randomized Rounding Algorithm for Sparse PCA'},\n",
       " u'1603.08092': {'arxivid': u'1603.08092',\n",
       "  'authorsaffil': [[u'Jianyu Tang', None],\n",
       "   [u'Hanzi Wang', None],\n",
       "   [u'Yan Yan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1016/j.neucom.2014.10.071',\n",
       "  'journalref': u'Neurocomputing, 2015,152(3):236-249',\n",
       "  'link': u'http://arxiv.org/abs/1603.08092v1',\n",
       "  'published': u'2016-03-26T09:33:30Z',\n",
       "  'summary': u'  Popular Hough Transform-based object detection approaches usually construct\\nan appearance codebook by clustering local image features. However, how to\\nchoose appropriate values for the parameters used in the clustering step\\nremains an open problem. Moreover, some popular histogram features extracted\\nfrom overlapping image blocks may cause a high degree of redundancy and\\nmulticollinearity. In this paper, we propose a novel Hough Transform-based\\nobject detection approach. First, to address the above issues, we exploit a\\nBridge Partial Least Squares (BPLS) technique to establish context-encoded\\nHough Regression Models (HRMs), which are linear regression models that cast\\nprobabilistic Hough votes to predict object locations. BPLS is an efficient\\nvariant of Partial Least Squares (PLS). PLS-based regression techniques\\n(including BPLS) can reduce the redundancy and eliminate the multicollinearity\\nof a feature set. And the appropriate value of the only parameter used in PLS\\n(i.e., the number of latent components) can be determined by using a\\ncross-validation procedure. Second, to efficiently handle object scale changes,\\nwe propose a novel multi-scale voting scheme. In this scheme, multiple Hough\\nimages corresponding to multiple object scales can be obtained simultaneously.\\nThird, an object in a test image may correspond to multiple true and false\\npositive hypotheses at different scales. Based on the proposed multi-scale\\nvoting scheme, a principled strategy is proposed to fuse hypotheses to reduce\\nfalse positives by evaluating normalized pointwise mutual information between\\nhypotheses. In the experiments, we also compare the proposed HRM approach with\\nits several variants to evaluate the influences of its components on its\\nperformance. Experimental results show that the proposed HRM approach has\\nachieved desirable performances on popular benchmark datasets.\\n',\n",
       "  'title': u'Learning Hough Regression Models via Bridge Partial Least Squares for\\n  Object Detection'},\n",
       " u'1604.02245': {'arxivid': u'1604.02245',\n",
       "  'authorsaffil': [[u'Matthias Limmer', None],\n",
       "   [u'Hendrik P. A. Lensch', None]],\n",
       "  'categoryterms': [u'cs.CV',\n",
       "   u'cs.GR',\n",
       "   u'(Primary) 82C32 (Secondary) 68T45',\n",
       "   u'H.5.1; I.4.8; I.5.1'],\n",
       "  'comment': u'6 pages, 8 figures, submitted to ICPR2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02245v2',\n",
       "  'published': u'2016-04-08T07:10:47Z',\n",
       "  'summary': u'  This paper proposes a method for transferring the RGB color spectrum to\\nnear-infrared (NIR) images using deep multi-scale convolutional neural\\nnetworks. A direct and integrated transfer between NIR and RGB pixels is\\ntrained. The trained model does not require any user guidance or a reference\\nimage database in the recall phase to produce images with a natural appearance.\\nTo preserve the rich details of the NIR image, its high frequency features are\\ntransferred to the estimated RGB image. The presented approach is trained and\\nevaluated on a real-world dataset containing a large amount of road scene\\nimages in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which\\nensures a perfect pixel to pixel registration.\\n',\n",
       "  'title': u'Infrared Colorization Using Deep Convolutional Neural Networks'},\n",
       " u'1602.07714': {'arxivid': u'1602.07714',\n",
       "  'authorsaffil': [[u'Hado van Hasselt', None],\n",
       "   [u'Arthur Guez', None],\n",
       "   [u'Matteo Hessel', None],\n",
       "   [u'David Silver', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.NE', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07714v1',\n",
       "  'published': u'2016-02-24T21:14:52Z',\n",
       "  'summary': u'  Learning non-linear functions can be hard when the magnitude of the target\\nfunction is unknown beforehand, as most learning algorithms are not scale\\ninvariant. We propose an algorithm to adaptively normalize these targets. This\\nis complementary to recent advances in input normalization. Importantly, the\\nproposed method preserves the unnormalized outputs whenever the normalization\\nis updated to avoid instability caused by non-stationarity. It can be combined\\nwith any learning algorithm and any non-linear function approximation,\\nincluding the important special case of deep learning. We empirically validate\\nthe method in supervised learning and reinforcement learning and apply it to\\nlearning how to play Atari 2600 games. Previous work on applying deep learning\\nto this domain relied on clipping the rewards to make learning in different\\ngames more homogeneous, but this uses the domain-specific knowledge that in\\nthese games counting rewards is often almost as informative as summing these.\\nUsing our adaptive normalization we can remove this heuristic without\\ndiminishing overall performance, and even improve performance on some games,\\nsuch as Ms. Pac-Man and Centipede, on which previous methods did not perform\\nwell.\\n',\n",
       "  'title': u'Learning functions across many orders of magnitudes'},\n",
       " u'1503.01313': {'arxivid': u'1503.01313',\n",
       "  'authorsaffil': [[u'Matej Kristan', None],\n",
       "   [u'Jiri Matas', None],\n",
       "   [u'Ales Leonardis', None],\n",
       "   [u'Tomas Vojir', None],\n",
       "   [u'Roman Pflugfelder', None],\n",
       "   [u'Gustavo Fernandez', None],\n",
       "   [u'Georg Nebehay', None],\n",
       "   [u'Fatih Porikli', None],\n",
       "   [u'Luka Cehovin', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Final version (Accepted), IEEE Pattern Analysis and Machine\\n  Intelligence, 2016',\n",
       "  'doi': u'10.1109/TPAMI.2016.2516982',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.01313v3',\n",
       "  'published': u'2015-03-04T14:12:17Z',\n",
       "  'summary': u'  This paper addresses the problem of single-target tracker performance\\nevaluation. We consider the performance measures, the dataset and the\\nevaluation system to be the most important components of tracker evaluation and\\npropose requirements for each of them. The requirements are the basis of a new\\nevaluation methodology that aims at a simple and easily interpretable tracker\\ncomparison. The ranking-based methodology addresses tracker equivalence in\\nterms of statistical significance and practical differences. A fully-annotated\\ndataset with per-frame annotations with several visual attributes is\\nintroduced. The diversity of its visual properties is maximized in a novel way\\nby clustering a large number of videos according to their visual attributes.\\nThis makes it the most sophistically constructed and annotated dataset to date.\\nA multi-platform evaluation system allowing easy integration of third-party\\ntrackers is presented as well. The proposed evaluation methodology was tested\\non the VOT2014 challenge on the new dataset and 38 trackers, making it the\\nlargest benchmark to date. Most of the tested trackers are indeed\\nstate-of-the-art since they outperform the standard baselines, resulting in a\\nhighly-challenging benchmark. An exhaustive analysis of the dataset from the\\nperspective of tracking difficulty is carried out. To facilitate tracker\\ncomparison a new performance visualization technique is proposed.\\n',\n",
       "  'title': u'A Novel Performance Evaluation Methodology for Single-Target Trackers'},\n",
       " u'1603.08124': {'arxivid': u'1603.08124',\n",
       "  'authorsaffil': [[u'Wenbin Li', None], [u'Darren Cosker', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08124v1',\n",
       "  'published': u'2016-03-26T17:13:25Z',\n",
       "  'summary': u'  Non-rigid video interpolation is a common computer vision task. In this paper\\nwe present an optical flow approach which adopts a Laplacian Cotangent Mesh\\nconstraint to enhance the local smoothness. Similar to Li et al., our approach\\nadopts a mesh to the image with a resolution up to one vertex per pixel and\\nuses angle constraints to ensure sensible local deformations between image\\npairs. The Laplacian Mesh constraints are expressed wholly inside the optical\\nflow optimization, and can be applied in a straightforward manner to a wide\\nrange of image tracking and registration problems. We evaluate our approach by\\ntesting on several benchmark datasets, including the Middlebury and Garg et al.\\ndatasets. In addition, we show application of our method for constructing 3D\\nMorphable Facial Models from dynamic 3D data.\\n',\n",
       "  'title': u'Video Interpolation using Optical Flow and Laplacian Smoothness'},\n",
       " u'1511.04776': {'arxivid': u'1511.04776',\n",
       "  'authorsaffil': [[u'Marc Goessling', None], [u'Yali Amit', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04776v4',\n",
       "  'published': u'2015-11-15T22:54:02Z',\n",
       "  'summary': u'  We consider high-dimensional distribution estimation through autoregressive\\nnetworks. By combining the concepts of sparsity, mixtures and parameter sharing\\nwe obtain a simple model which is fast to train and which achieves\\nstate-of-the-art or better results on several standard benchmark datasets.\\nSpecifically, we use an L1-penalty to regularize the conditional distributions\\nand introduce a procedure for automatic parameter sharing between mixture\\ncomponents. Moreover, we propose a simple distributed representation which\\npermits exact likelihood evaluations since the latent variables are interleaved\\nwith the observable variables and can be easily integrated out. Our model\\nachieves excellent generalization performance and scales well to extremely high\\ndimensions.\\n',\n",
       "  'title': u'Mixtures of Sparse Autoregressive Networks'},\n",
       " u'1509.08102': {'arxivid': u'1509.08102',\n",
       "  'authorsaffil': [[u'Shin Ando', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.08102v4',\n",
       "  'published': u'2015-09-27T15:43:33Z',\n",
       "  'summary': u'  The nearest neighbor rule is one of the most widely used models for\\nclassification and selecting a compact set of prototype instances is an\\nimportant problem for its applications. Many existing approaches on the\\nprototype selection problem have relied on instance-based analyses of the class\\ndistribution, which can be computationally expensive for large datasets. In\\nthis paper, we revisit this problem to explore a parametric approach, which\\napproximates the violation of the nearest neighbor rule over the training set\\nand learns the prioritization of prototypes that minimizes the violation. We\\nshow that our approach reduces the problem to large-margin learning and\\ndemonstrate its advantage by empirical comparisons using public benchmark data.\\n',\n",
       "  'title': u'Discriminative Learning of the Prototype Set for Nearest Neighbor\\n  Classification'},\n",
       " u'1602.06667': {'arxivid': u'1602.06667',\n",
       "  'authorsaffil': [[u'Manikandasriram Srinivasan Ramanagopal', None],\n",
       "   [u'Jerome Le Ny', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.AI', u'cs.SY'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06667v1',\n",
       "  'published': u'2016-02-22T07:05:49Z',\n",
       "  'summary': u'  This paper presents a system capable of autonomously mapping the visible part\\nof a bounded three-dimensional structure using a mobile ground robot equipped\\nwith a depth sensor. We describe motion planning strategies to determine\\nappropriate successive viewpoints and attempt to fill holes automatically in a\\npoint cloud produced by the sensing and perception layer. We develop a local\\nmotion planner using potential fields to maintain a desired distance from the\\nstructure. The emphasis is on accurately reconstructing a 3D model of a\\nstructure of moderate size rather than mapping large open environments, with\\napplications for example in architecture, construction and inspection. The\\nproposed algorithms do not require any initialization in the form of a mesh\\nmodel or a bounding box. We compare via simulations the performance of our\\npolicies to the classic frontier based exploration algorithm. We illustrate the\\nefficacy of our approach for different structure sizes, levels of localization\\naccuracy and range of the depth sensor.\\n',\n",
       "  'title': u'Motion Planning Strategies for Autonomously Mapping 3D Structures'},\n",
       " u'1506.04365': {'arxivid': u'1506.04365',\n",
       "  'authorsaffil': [[u'Kuan-Yu Chen', None],\n",
       "   [u'Shih-Hung Liu', None],\n",
       "   [u'Hsin-Min Wang', None],\n",
       "   [u'Berlin Chen', None],\n",
       "   [u'Hsin-Hsi Chen', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.04365v1',\n",
       "  'published': u'2015-06-14T09:18:36Z',\n",
       "  'summary': u'  Owing to the rapidly growing multimedia content available on the Internet,\\nextractive spoken document summarization, with the purpose of automatically\\nselecting a set of representative sentences from a spoken document to concisely\\nexpress the most important theme of the document, has been an active area of\\nresearch and experimentation. On the other hand, word embedding has emerged as\\na newly favorite research subject because of its excellent performance in many\\nnatural language processing (NLP)-related tasks. However, as far as we are\\naware, there are relatively few studies investigating its use in extractive\\ntext or speech summarization. A common thread of leveraging word embeddings in\\nthe summarization process is to represent the document (or sentence) by\\naveraging the word embeddings of the words occurring in the document (or\\nsentence). Then, intuitively, the cosine similarity measure can be employed to\\ndetermine the relevance degree between a pair of representations. Beyond the\\ncontinued efforts made to improve the representation of words, this paper\\nfocuses on building novel and efficient ranking models based on the general\\nword embedding methods for extractive speech summarization. Experimental\\nresults demonstrate the effectiveness of our proposed methods, compared to\\nexisting state-of-the-art methods.\\n',\n",
       "  'title': u'Leveraging Word Embeddings for Spoken Document Summarization'},\n",
       " u'1511.04773': {'arxivid': u'1511.04773',\n",
       "  'authorsaffil': [[u'Weiran Wang', None], [u'Karen Livescu', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Published as a conference paper at International Conference on\\n  Learning Representations (ICLR) 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04773v4',\n",
       "  'published': u'2015-11-15T22:20:02Z',\n",
       "  'summary': u'  Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view\\nrepresentation learning technique with broad applicability in statistics and\\nmachine learning. Although there is a closed-form solution for the KCCA\\nobjective, it involves solving an $N\\\\times N$ eigenvalue system where $N$ is\\nthe training set size, making its computational requirements in both memory and\\ntime prohibitive for large-scale problems. Various approximation techniques\\nhave been developed for KCCA. A commonly used approach is to first transform\\nthe original inputs to an $M$-dimensional random feature space so that inner\\nproducts in the feature space approximate kernel evaluations, and then apply\\nlinear CCA to the transformed inputs. In many applications, however, the\\ndimensionality $M$ of the random feature space may need to be very large in\\norder to obtain a sufficiently good approximation; it then becomes challenging\\nto perform the linear CCA step on the resulting very high-dimensional data\\nmatrices. We show how to use a stochastic optimization algorithm, recently\\nproposed for linear CCA and its neural-network extension, to further alleviate\\nthe computation requirements of approximate KCCA. This approach allows us to\\nrun approximate KCCA on a speech dataset with $1.4$ million training samples\\nand a random feature space of dimensionality $M=100000$ on a typical\\nworkstation.\\n',\n",
       "  'title': u'Large-Scale Approximate Kernel Canonical Correlation Analysis'},\n",
       " u'1602.06662': {'arxivid': u'1602.06662',\n",
       "  'authorsaffil': [[u'Mikael Henaff', None],\n",
       "   [u'Arthur Szlam', None],\n",
       "   [u'Yann LeCun', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.AI', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06662v1',\n",
       "  'published': u'2016-02-22T06:51:25Z',\n",
       "  'summary': u'  Although RNNs have been shown to be powerful tools for processing sequential\\ndata, finding architectures or optimization strategies that allow them to model\\nvery long term dependencies is still an active area of research. In this work,\\nwe carefully analyze two synthetic datasets originally outlined in (Hochreiter\\nand Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store\\ninformation over many time steps. We explicitly construct RNN solutions to\\nthese problems, and using these constructions, illuminate both the problems\\nthemselves and the way in which RNNs store different types of information in\\ntheir hidden states. These constructions furthermore explain the success of\\nrecent methods that specify unitary initializations or constraints on the\\ntransition matrices.\\n',\n",
       "  'title': u'Orthogonal RNNs and Long-Memory Tasks'},\n",
       " u'1601.05994': {'arxivid': u'1601.05994',\n",
       "  'authorsaffil': [[u'Wei Wang', None], [u'Chuanjiang He', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05994v1',\n",
       "  'published': u'2016-01-22T13:36:20Z',\n",
       "  'summary': u'  Haze removal has been a very challenging problem due to its ill-posedness,\\nwhich is more ill-posed if the input data is only a single hazy image. In this\\npaper, we present a new approach for removing haze from a single input image.\\nThe proposed method combines the model widely used to describe the formation of\\na haze image with the assumption in Retinex that an image is the product of the\\nillumination and the reflection. We assume that the depth and reflection\\nfunctions are spatially piecewise smooth in the model, where the total\\nvariation is used for the regularization. The proposed model is defined as a\\nconstrained optimization problem, which is solved by an alternating\\nminimization scheme and the fast gradient projection algorithm. Some theoretic\\nanalyses are given for the proposed model and algorithm. Finally, numerical\\nexamples are presented to demonstrate that our method can restore vivid and\\ncontrastive hazy images effectively.\\n',\n",
       "  'title': u'Depth and Reflection Total Variation for Single Image Dehazing'},\n",
       " u'1412.6856': {'arxivid': u'1412.6856',\n",
       "  'authorsaffil': [[u'Bolei Zhou', None],\n",
       "   [u'Aditya Khosla', None],\n",
       "   [u'Agata Lapedriza', None],\n",
       "   [u'Aude Oliva', None],\n",
       "   [u'Antonio Torralba', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.NE'],\n",
       "  'comment': u'12 pages, ICLR 2015 conference paper',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1412.6856v2',\n",
       "  'published': u'2014-12-22T01:14:01Z',\n",
       "  'summary': u'  With the success of new computational architectures for visual processing,\\nsuch as convolutional neural networks (CNN) and access to image databases with\\nmillions of labeled examples (e.g., ImageNet, Places), the state of the art in\\ncomputer vision is advancing rapidly. One important factor for continued\\nprogress is to understand the representations that are learned by the inner\\nlayers of these deep architectures. Here we show that object detectors emerge\\nfrom training CNNs to perform scene classification. As scenes are composed of\\nobjects, the CNN for scene classification automatically discovers meaningful\\nobjects detectors, representative of the learned scene categories. With object\\ndetectors emerging as a result of learning to recognize scenes, our work\\ndemonstrates that the same network can perform both scene recognition and\\nobject localization in a single forward-pass, without ever having been\\nexplicitly taught the notion of objects.\\n',\n",
       "  'title': u'Object Detectors Emerge in Deep Scene CNNs'},\n",
       " u'1603.01870': {'arxivid': u'1603.01870',\n",
       "  'authorsaffil': [[u'Sougata Chaudhuri', None],\n",
       "   [u'Georgios Theocharous', None],\n",
       "   [u'Mohammad Ghavamzadeh', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IR'],\n",
       "  'comment': u'Under review',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01870v1',\n",
       "  'published': u'2016-03-06T20:26:41Z',\n",
       "  'summary': u\"  We study the problem of personalized advertisement recommendation (PAR),\\nwhich consist of a user visiting a system (website) and the system displaying\\none of $K$ ads to the user. The system uses an internal ad recommendation\\npolicy to map the user's profile (context) to one of the ads. The user either\\nclicks or ignores the ad and correspondingly, the system updates its\\nrecommendation policy. PAR problem is usually tackled by scalable\\n\\\\emph{contextual bandit} algorithms, where the policies are generally based on\\nclassifiers. A practical problem in PAR is extreme click sparsity, due to very\\nfew users actually clicking on ads. We systematically study the drawback of\\nusing contextual bandit algorithms based on classifier-based policies, in face\\nof extreme click sparsity. We then suggest an alternate policy, based on\\nrankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss,\\nwhich can significantly alleviate the problem of click sparsity. We conduct\\nextensive experiments on public datasets, as well as three industry proprietary\\ndatasets, to illustrate the improvement in click-through-rate (CTR) obtained by\\nusing the ranker-based policy over classifier-based policies.\\n\",\n",
       "  'title': u'Personalized Advertisement Recommendation: A Ranking Approach to Address\\n  the Ubiquitous Click Sparsity Problem'},\n",
       " u'1604.01753': {'arxivid': u'1604.01753',\n",
       "  'authorsaffil': [[u'Gunnar A. Sigurdsson', None],\n",
       "   [u'G\\xfcl Varol', None],\n",
       "   [u'Xiaolong Wang', None],\n",
       "   [u'Ali Farhadi', None],\n",
       "   [u'Ivan Laptev', None],\n",
       "   [u'Abhinav Gupta', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01753v1',\n",
       "  'published': u'2016-04-06T19:56:04Z',\n",
       "  'summary': u'  Computer vision has a great potential to help our daily lives by searching\\nfor lost keys, watering flowers or reminding us to take a pill. To succeed with\\nsuch tasks, computer vision methods need to be trained from real and diverse\\nexamples of our daily dynamic scenes. While most of such scenes are not\\nparticularly exciting, they typically do not appear on YouTube, in movies or TV\\nbroadcasts. So how do we collect sufficiently many diverse but boring samples\\nrepresenting our lives? We propose a novel Hollywood in Homes approach to\\ncollect such data. Instead of shooting videos in the lab, we ensure diversity\\nby distributing and crowdsourcing the whole process of video creation from\\nscript writing to video recording and annotation. Following this procedure we\\ncollect a new dataset, Charades, with hundreds of people recording videos in\\ntheir own homes, acting out casual everyday activities. The dataset is composed\\nof 9,850 annotated videos with an average length of 30 seconds, showing\\nactivities of 267 people from three continents. Each video is annotated by\\nmultiple free-text descriptions, action labels, action intervals and classes of\\ninteracted objects. In total, Charades provides 27,847 video descriptions,\\n37,972 temporally localized intervals for 160 action classes and 24,623 labels\\nfor 40 object classes. Using this rich data, we evaluate and provide baseline\\nresults for several tasks including action recognition and automatic\\ndescription generation. We believe that the realism, diversity, and casual\\nnature of this dataset will present unique challenges and new opportunities for\\ncomputer vision community.\\n',\n",
       "  'title': u'Hollywood in Homes: Crowdsourcing Data Collection for Activity\\n  Understanding'},\n",
       " u'1503.00593': {'arxivid': u'1503.00593',\n",
       "  'authorsaffil': [[u'Jian Sun', None],\n",
       "   [u'Wenfei Cao', None],\n",
       "   [u'Zongben Xu', None],\n",
       "   [u'Jean Ponce', None]],\n",
       "  'categoryterms': [u'cs.CV', u'I.4'],\n",
       "  'comment': u'This is a final version accepted by CVPR 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.00593v3',\n",
       "  'published': u'2015-03-02T16:22:51Z',\n",
       "  'summary': u'  In this paper, we address the problem of estimating and removing non-uniform\\nmotion blur from a single blurry image. We propose a deep learning approach to\\npredicting the probabilistic distribution of motion blur at the patch level\\nusing a convolutional neural network (CNN). We further extend the candidate set\\nof motion kernels predicted by the CNN using carefully designed image\\nrotations. A Markov random field model is then used to infer a dense\\nnon-uniform motion blur field enforcing motion smoothness. Finally, motion blur\\nis removed by a non-uniform deblurring model using patch-level image prior.\\nExperimental evaluations show that our approach can effectively estimate and\\nremove complex non-uniform motion blur that is not handled well by previous\\napproaches.\\n',\n",
       "  'title': u'Learning a Convolutional Neural Network for Non-uniform Motion Blur\\n  Removal'},\n",
       " u'1604.00117': {'arxivid': u'1604.00117',\n",
       "  'authorsaffil': [[u'Aaron Jaech', None],\n",
       "   [u'Larry Heck', None],\n",
       "   [u'Mari Ostendorf', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00117v1',\n",
       "  'published': u'2016-04-01T03:24:32Z',\n",
       "  'summary': u'  The goal of this paper is to use multi-task learning to efficiently scale\\nslot filling models for natural language understanding to handle multiple\\ntarget tasks or domains. The key to scalability is reducing the amount of\\ntraining data needed to learn a model for a new task. The proposed multi-task\\nmodel delivers better performance with less data by leveraging patterns that it\\nlearns from the other tasks. The approach supports an open vocabulary, which\\nallows the models to generalize to unseen words, which is particularly\\nimportant when very little training data is used. A newly collected\\ncrowd-sourced data set, covering four different domains, is used to demonstrate\\nthe effectiveness of the domain adaptation and open vocabulary techniques.\\n',\n",
       "  'title': u'Domain Adaptation of Recurrent Neural Networks for Natural Language\\n  Understanding'},\n",
       " u'1603.05145': {'arxivid': u'1603.05145',\n",
       "  'authorsaffil': [[u'Qiyang Zhao', None], [u'Lewis D Griffin', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'11 pages, 12 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05145v1',\n",
       "  'published': u'2016-03-16T15:35:07Z',\n",
       "  'summary': u'  Many deep Convolutional Neural Networks (CNN) make incorrect predictions on\\nadversarial samples obtained by imperceptible perturbations of clean samples.\\nWe hypothesize that this is caused by a failure to suppress unusual signals\\nwithin network layers. As remedy we propose the use of Symmetric Activation\\nFunctions (SAF) in non-linear signal transducer units. These units suppress\\nsignals of exceptional magnitude. We prove that SAF networks can perform\\nclassification tasks to arbitrary precision in a simplified situation. In\\npractice, rather than use SAFs alone, we add them into CNNs to improve their\\nrobustness. The modified CNNs can be easily trained using popular strategies\\nwith the moderate training load. Our experiments on MNIST and CIFAR-10 show\\nthat the modified CNNs perform similarly to plain ones on clean samples, and\\nare remarkably more robust against adversarial and nonsense samples.\\n',\n",
       "  'title': u'Suppressing the Unusual: towards Robust CNNs using Symmetric Activation\\n  Functions'},\n",
       " u'1602.07394': {'arxivid': u'1602.07394',\n",
       "  'authorsaffil': [[u'Zhenhao Ge', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.CL'],\n",
       "  'comment': u'International Congress on Image and Signal Processing (CISP) 2015',\n",
       "  'doi': u'10.1109/CISP.2015.7408064',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07394v1',\n",
       "  'published': u'2016-02-24T04:33:49Z',\n",
       "  'summary': u'  Researches have shown accent classification can be improved by integrating\\nsemantic information into pure acoustic approach. In this work, we combine\\nphonetic knowledge, such as vowels, with enhanced acoustic features to build an\\nimproved accent classification system. The classifier is based on Gaussian\\nMixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual\\nLinear Predictive (PLP) features. The features are further optimized by\\nPrinciple Component Analysis (PCA) and Hetroscedastic Linear Discriminant\\nAnalysis (HLDA). Using 7 major types of accented speech from the Foreign\\nAccented English (FAE) corpus, the system achieves classification accuracy 54%\\nwith input test data as short as 20 seconds, which is competitive to the state\\nof the art in this field.\\n',\n",
       "  'title': u'Improved Accent Classification Combining Phonetic Vowels with Acoustic\\n  Features'},\n",
       " u'1603.09188': {'arxivid': u'1603.09188',\n",
       "  'authorsaffil': [[u'Spandana Gella', None],\n",
       "   [u'Mirella Lapata', None],\n",
       "   [u'Frank Keller', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.CV'],\n",
       "  'comment': u'11 pages, NAACL-HLT 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09188v1',\n",
       "  'published': u'2016-03-30T13:43:38Z',\n",
       "  'summary': u'  We introduce a new task, visual sense disambiguation for verbs: given an\\nimage and a verb, assign the correct sense of the verb, i.e., the one that\\ndescribes the action depicted in the image. Just as textual word sense\\ndisambiguation is useful for a wide range of NLP tasks, visual sense\\ndisambiguation can be useful for multimodal tasks such as image retrieval,\\nimage description, and text illustration. We introduce VerSe, a new dataset\\nthat augments existing multimodal datasets (COCO and TUHOI) with sense labels.\\nWe propose an unsupervised algorithm based on Lesk which performs visual sense\\ndisambiguation using textual, visual, or multimodal embeddings. We find that\\ntextual embeddings perform well when gold-standard textual annotations (object\\nlabels and image descriptions) are available, while multimodal embeddings\\nperform well on unannotated images. We also verify our findings by using the\\ntextual and multimodal embeddings as features in a supervised setting and\\nanalyse the performance of visual sense disambiguation task. VerSe is made\\npublicly available and can be downloaded at:\\nhttps://github.com/spandanagella/verse.\\n',\n",
       "  'title': u'Unsupervised Visual Sense Disambiguation for Verbs using Multimodal\\n  Embeddings'},\n",
       " u'1602.07393': {'arxivid': u'1602.07393',\n",
       "  'authorsaffil': [[u'Zhenhao Ge', None], [u'Yufang Sun', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'International Conference on Pattern Recognition Application and\\n  Methods (ICPRAM) 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07393v1',\n",
       "  'published': u'2016-02-24T04:32:34Z',\n",
       "  'summary': u'  Authorship attribution refers to the task of automatically determining the\\nauthor based on a given sample of text. It is a problem with a long history and\\nhas a wide range of application. Building author profiles using language models\\nis one of the most successful methods to automate this task. New language\\nmodeling methods based on neural networks alleviate the curse of dimensionality\\nand usually outperform conventional N-gram methods. However, there have not\\nbeen much research applying them to authorship attribution. In this paper, we\\npresent a novel setup of a Neural Network Language Model (NNLM) and apply it to\\na database of text samples from different authors. We investigate how the NNLM\\nperforms on a task with moderate author set size and relatively limited\\ntraining and test data, and how the topics of the text samples affect the\\naccuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of\\nfitness of a trained language model to the test data. Given 5 random test\\nsentences, it also increases the author classification accuracy by 3.43% on\\naverage, compared with the N-gram methods using SRILM tools. An open source\\nimplementation of our methodology is freely available at\\nhttps://github.com/zge/authorship-attribution/.\\n',\n",
       "  'title': u'Domain Specific Author Attribution Based on Feedforward Neural Network\\n  Language Models'},\n",
       " u'1507.07595': {'arxivid': u'1507.07595',\n",
       "  'authorsaffil': [[u'Jason D. Lee', None],\n",
       "   [u'Qihang Lin', None],\n",
       "   [u'Tengyu Ma', None],\n",
       "   [u'Tianbao Yang', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'significant addition to both theory and experimental results',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.07595v2',\n",
       "  'published': u'2015-07-27T22:09:57Z',\n",
       "  'summary': u'  We study distributed optimization algorithms for minimizing the average of\\nconvex functions. The applications include empirical risk minimization problems\\nin statistical machine learning where the datasets are large and have to be\\nstored on different machines. We design a distributed stochastic variance\\nreduced gradient algorithm that, under certain conditions on the condition\\nnumber, simultaneously achieves the optimal parallel runtime, amount of\\ncommunication and rounds of communication among all distributed first-order\\nmethods up to constant factors. Our method and its accelerated extension also\\noutperform existing distributed algorithms in terms of the rounds of\\ncommunication as long as the condition number is not too large compared to the\\nsize of data in each machine. We also prove a lower bound for the number of\\nrounds of communication for a broad class of distributed first-order methods\\nincluding the proposed algorithms in this paper. We show that our accelerated\\ndistributed stochastic variance reduced gradient algorithm achieves this lower\\nbound so that it uses the fewest rounds of communication among all distributed\\nfirst-order algorithms.\\n',\n",
       "  'title': u'Distributed Stochastic Variance Reduced Gradient Methods and A Lower\\n  Bound for Communication Complexity'},\n",
       " u'1511.06739': {'arxivid': u'1511.06739',\n",
       "  'authorsaffil': [[u'Raghudeep Gadde', None],\n",
       "   [u'Varun Jampani', None],\n",
       "   [u'Martin Kiefel', None],\n",
       "   [u'Peter V. Gehler', None]],\n",
       "  'categoryterms': [u'cs.CV', u'I.2.10; I.2.6'],\n",
       "  'comment': u'Conference track submission to ICLR-2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06739v3',\n",
       "  'published': u'2015-11-20T19:58:38Z',\n",
       "  'summary': u'  In this paper we propose a CNN architecture for image segmentation. We\\nintroduce a new \"bilateral inception\" layer that is used on top of a\\nconvolutional architecture. The bilateral inception performs a filtering\\nbetween superpixels in an image. This addresses two problems that arise with\\nCNN segmentation architectures. First, this layer propagates information\\nbetween (super) pixels while respecting image edges, thus using the structured\\ninformation of the problem for improved results. Second, the layer recovers a\\nfull resolution segmentation result from the lower resolution solution of a\\nCNN. In the experiments we replace the deconvolution networks and Dense-CRF\\nthat have previously been proposed to address these problems with bilateral\\ninception layers. The reduction to superpixels reduces the amount of\\ncomputations and simplifies the network design. Further, we report better\\nempirical results by replacing De-convolutional and CNN+Dense-CRF steps in four\\ndifferent semantic segmentation CNN architecutres, even with-out re-training\\ntheir filter weights.\\n',\n",
       "  'title': u'Superpixel Convolutional Networks using Bilateral Inceptions'},\n",
       " u'1604.03443': {'arxivid': u'1604.03443',\n",
       "  'authorsaffil': [[u'Jinxing Li', None],\n",
       "   [u'David Zhang', None],\n",
       "   [u'Yongcheng Li', None],\n",
       "   [u'Jian Wu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages, 8 figures, 30 conference',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03443v1',\n",
       "  'published': u'2016-04-12T15:31:52Z',\n",
       "  'summary': u'  Effective and accurate diagnosis of Diabetes Mellitus (DM), as well as its\\nearly stage Impaired Glucose Regulation (IGR), has attracted much attention\\nrecently. Traditional Chinese Medicine (TCM) [3], [5] etc. has proved that\\ntongue, face and sublingual diagnosis as a noninvasive method is a reasonable\\nway for disease detection. However, most previous works only focus on a single\\nmodality (tongue, face or sublingual) for diagnosis, although different\\nmodalities may provide complementary information for the diagnosis of DM and\\nIGR. In this paper, we propose a novel multi-modal classification method to\\ndiscriminate between DM (or IGR) and healthy controls. Specially, the tongue,\\nfacial and sublingual images are first collected by using a non-invasive\\ncapture device. The color, texture and geometry features of these three types\\nof images are then extracted, respectively. Finally, our so-called multi-modal\\nsimilar and specific learning (MMSSL) approach is proposed to combine features\\nof tongue, face and sublingual, which not only exploits the correlation but\\nalso extracts individual components among them. Experimental results on a\\ndataset consisting of 192 Healthy, 198 DM and 114 IGR samples (all samples were\\nobtained from Guangdong Provincial Hospital of Traditional Chinese Medicine)\\nsubstantiate the effectiveness and superiority of our proposed method for the\\ndiagnosis of DM and IGR, compared to the case of using a single modality.\\n',\n",
       "  'title': u'Multi-modal Fusion for Diabetes Mellitus and Impaired Glucose Regulation\\n  Detection'},\n",
       " u'1601.01356': {'arxivid': u'1601.01356',\n",
       "  'authorsaffil': [[u'Makbule Gulcin Ozsoy', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL', u'cs.IR', u'cs.SI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01356v2',\n",
       "  'published': u'2016-01-07T00:09:37Z',\n",
       "  'summary': u\"  Social network platforms can archive data produced by their users. Then, the\\narchived data is used to provide better services to the users. One of the\\nservices that these platforms provide is the recommendation service.\\nRecommendation systems can predict the future preferences of users using\\nvarious different techniques. One of the most popular technique for\\nrecommendation is matrix-factorization, which uses low-rank approximation of\\ninput data. Similarly, word embedding methods from natural language processing\\nliterature learn low-dimensional vector space representation of input elements.\\nNoticing the similarities among word embedding and matrix factorization\\ntechniques and based on the previous works that apply techniques from text\\nprocessing to recommendation, Word2Vec's skip-gram technique is employed to\\nmake recommendations. The aim of this work is to make recommendation on next\\ncheck-in venues. Unlike previous works that use Word2Vec for recommendation, in\\nthis work non-textual features are used. For the experiments, a Foursquare\\ncheck-in dataset is used. The results show that use of vector space\\nrepresentations of items modeled by skip-gram technique is promising for making\\nrecommendations.\\n\",\n",
       "  'title': u'From Word Embeddings to Item Recommendation'},\n",
       " u'1601.07804': {'arxivid': u'1601.07804',\n",
       "  'authorsaffil': [[u'Xin Ding', None],\n",
       "   [u'Wei Chen', None],\n",
       "   [u'Ian J. Wassell', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IT', u'math.IT'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07804v1',\n",
       "  'published': u'2016-01-28T15:35:34Z',\n",
       "  'summary': u'  Tensor Compressive Sensing (TCS) is a multidimensional framework of\\nCompressive Sensing (CS), and it is advantageous in terms of reducing the\\namount of storage, easing hardware implementations and preserving\\nmultidimensional structures of signals in comparison to a conventional CS\\nsystem. In a TCS system, instead of using a random sensing matrix and a\\npredefined dictionary, the average-case performance can be further improved by\\nemploying an optimized multidimensional sensing matrix and a learned\\nmultilinear sparsifying dictionary. In this paper, we propose a joint\\noptimization approach of the sensing matrix and dictionary for a TCS system.\\nFor the sensing matrix design in TCS, an extended separable approach with a\\nclosed form solution and a novel iterative non-separable method are proposed\\nwhen the multilinear dictionary is fixed. In addition, a multidimensional\\ndictionary learning method that takes advantages of the multidimensional\\nstructure is derived, and the influence of sensing matrices is taken into\\naccount in the learning process. A joint optimization is achieved via\\nalternately iterating the optimization of the sensing matrix and dictionary.\\nNumerical experiments using both synthetic data and real images demonstrate the\\nsuperiority of the proposed approaches.\\n',\n",
       "  'title': u'Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor\\n  Compressive Sensing'},\n",
       " u'1511.06732': {'arxivid': u'1511.06732',\n",
       "  'authorsaffil': [[u\"Marc'Aurelio Ranzato\", None],\n",
       "   [u'Sumit Chopra', None],\n",
       "   [u'Michael Auli', None],\n",
       "   [u'Wojciech Zaremba', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06732v7',\n",
       "  'published': u'2015-11-20T19:25:54Z',\n",
       "  'summary': u'  Many natural language processing applications use language models to generate\\ntext. These models are typically trained to predict the next word in a\\nsequence, given the previous words and some context such as an image. However,\\nat test time the model is expected to generate the entire sequence from\\nscratch. This discrepancy makes generation brittle, as errors may accumulate\\nalong the way. We address this issue by proposing a novel sequence level\\ntraining algorithm that directly optimizes the metric used at test time, such\\nas BLEU or ROUGE. On three different tasks, our approach outperforms several\\nstrong baselines for greedy generation. The method is also competitive when\\nthese baselines employ beam search, while being several times faster.\\n',\n",
       "  'title': u'Sequence Level Training with Recurrent Neural Networks'},\n",
       " u'1603.03116': {'arxivid': u'1603.03116',\n",
       "  'authorsaffil': [[u'Antonio Valerio Miceli Barone', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'17 pages, 8 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03116v2',\n",
       "  'published': u'2016-03-10T01:04:07Z',\n",
       "  'summary': u'  Deep learning consists in training neural networks to perform computations\\nthat sequentially unfold in many steps over a time dimension or an intrinsic\\ndepth dimension. Effective learning in this setting is usually accomplished by\\nspecialized network architectures that are designed to mitigate the vanishing\\ngradient problem of naive deep networks. Many of these architectures, such as\\nLSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single\\nstructural principle: the state passthrough.\\n  We observe that these architectures, hereby characterized as Passthrough\\nNetworks, in addition to the mitigation of the vanishing gradient problem,\\nenable the decoupling of the network state size from the number of parameters\\nof the network, a possibility that is exploited in some recent works but not\\nthoroughly explored.\\n  In this work we propose simple, yet effective, low-rank and low-rank plus\\ndiagonal matrix parametrizations for Passthrough Networks which exploit this\\ndecoupling property, reducing the data complexity and memory requirements of\\nthe network while preserving its memory capacity. We present competitive\\nexperimental results on synthetic tasks and a near state of the art result on\\nsequential randomly-permuted MNIST classification, a hard task on natural data.\\n',\n",
       "  'title': u'Low-rank passthrough neural networks'},\n",
       " u'1603.03112': {'arxivid': u'1603.03112',\n",
       "  'authorsaffil': [[u'Lifu Huang', None],\n",
       "   [u'Jonathan May', None],\n",
       "   [u'Xiaoman Pan', None],\n",
       "   [u'Heng Ji', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03112v1',\n",
       "  'published': u'2016-03-10T00:33:28Z',\n",
       "  'summary': u\"  Recent research has shown great progress on fine-grained entity typing. Most\\nexisting methods require pre-defining a set of types and training a multi-class\\nclassifier from a large labeled data set based on multi-level linguistic\\nfeatures. They are thus limited to certain domains, genres and languages. In\\nthis paper, we propose a novel unsupervised entity typing framework by\\ncombining symbolic and distributional semantics. We start from learning general\\nembeddings for each entity mention, compose the embeddings of specific contexts\\nusing linguistic structures, link the mention to knowledge bases and learn its\\nrelated knowledge representations. Then we develop a novel joint hierarchical\\nclustering and linking algorithm to type all mentions using these\\nrepresentations. This framework doesn't rely on any annotated data, predefined\\ntyping schema, or hand-crafted features, therefore it can be quickly adapted to\\na new domain, genre and language. Furthermore, it has great flexibility at\\nincorporating linguistic structures (e.g., Abstract Meaning Representation\\n(AMR), dependency relations) to improve specific context representation.\\nExperiments on genres (news and discussion forum) show comparable performance\\nwith state-of-the-art supervised typing systems trained from a large amount of\\nlabeled data. Results on various languages (English, Chinese, Japanese, Hausa,\\nand Yoruba) and domains (general and biomedical) demonstrate the portability of\\nour framework.\\n\",\n",
       "  'title': u'Building a Fine-Grained Entity Typing System Overnight for a New X (X =\\n  Language, Domain, Genre)'},\n",
       " u'1603.00709': {'arxivid': u'1603.00709',\n",
       "  'authorsaffil': [[u'Mouna Ben Ishak', u'LARODEC'],\n",
       "   [u'Rajani Chulyadyo', u'LINA'],\n",
       "   [u'Philippe Leray', u'LINA']],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00709v1',\n",
       "  'published': u'2016-03-02T13:46:31Z',\n",
       "  'summary': u'  The validation of any database mining methodology goes through an evaluation\\nprocess where benchmarks availability is essential. In this paper, we aim to\\nrandomly generate relational database benchmarks that allow to check\\nprobabilistic dependencies among the attributes. We are particularly interested\\nin Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs)\\nto a relational data mining context and enable effective and robust reasoning\\nover relational data. Even though a panoply of works have focused, separately ,\\non the generation of random Bayesian networks and relational databases, no work\\nhas been identified for PRMs on that track. This paper provides an algorithmic\\napproach for generating random PRMs from scratch to fill this gap. The proposed\\nmethod allows to generate PRMs as well as synthetic relational data from a\\nrandomly generated relational schema and a random set of probabilistic\\ndependencies. This can be of interest not only for machine learning researchers\\nto evaluate their proposals in a common framework, but also for databases\\ndesigners to evaluate the effectiveness of the components of a database\\nmanagement system.\\n',\n",
       "  'title': u'Probabilistic Relational Model Benchmark Generation'},\n",
       " u'1511.08343': {'arxivid': u'1511.08343',\n",
       "  'authorsaffil': [[u'Yunseong Hwang', None],\n",
       "   [u'Anh Tong', None],\n",
       "   [u'Jaesik Choi', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.08343v2',\n",
       "  'published': u'2015-11-26T10:26:51Z',\n",
       "  'summary': u'  Gaussian Processes (GPs) provide a general and analytically tractable way of\\nmodeling complex time-varying, nonparametric functions. The Automatic Bayesian\\nCovariance Discovery (ABCD) system constructs natural-language description of\\ntime-series data by treating unknown time-series data nonparametrically using\\nGP with a composite covariance kernel function. Unfortunately, learning a\\ncomposite covariance kernel with a single time-series data set often results in\\nless informative kernel that may not give qualitative, distinctive descriptions\\nof data. We address this challenge by proposing two relational kernel learning\\nmethods which can model multiple time-series data sets by finding common,\\nshared causes of changes. We show that the relational kernel learning methods\\nfind more accurate models for regression problems on several real-world data\\nsets; US stock data, US house price index data and currency exchange rate data.\\n',\n",
       "  'title': u'The Automatic Statistician: A Relational Perspective'},\n",
       " u'1511.07394': {'arxivid': u'1511.07394',\n",
       "  'authorsaffil': [[u'Kevin J. Shih', None],\n",
       "   [u'Saurabh Singh', None],\n",
       "   [u'Derek Hoiem', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Submitted to CVPR2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07394v2',\n",
       "  'published': u'2015-11-23T20:17:18Z',\n",
       "  'summary': u'  We present a method that learns to answer visual questions by selecting image\\nregions relevant to the text-based query. Our method exhibits significant\\nimprovements in answering questions such as \"what color,\" where it is necessary\\nto evaluate a specific location, and \"what room,\" where it selectively\\nidentifies informative image regions. Our model is tested on the VQA dataset\\nwhich is the largest human-annotated visual question answering dataset to our\\nknowledge.\\n',\n",
       "  'title': u'Where To Look: Focus Regions for Visual Question Answering'},\n",
       " u'1508.06073': {'arxivid': u'1508.06073',\n",
       "  'authorsaffil': [[u'Hilde Kuehne', None],\n",
       "   [u'Juergen Gall', None],\n",
       "   [u'Thomas Serre', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'15 pages, 12 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.06073v2',\n",
       "  'published': u'2015-08-25T08:59:46Z',\n",
       "  'summary': u'  As research on action recognition matures, the focus is shifting away from\\ncategorizing basic task-oriented actions using hand-segmented video datasets to\\nunderstanding complex goal-oriented daily human activities in real-world\\nsettings. Temporally structured models would seem obvious to tackle this set of\\nproblems, but so far, cases where these models have outperformed simpler\\nunstructured bag-of-word types of models are scarce. With the increasing\\navailability of large human activity datasets, combined with the development of\\nnovel feature coding techniques that yield more compact representations, it is\\ntime to revisit structured generative approaches.\\n  Here, we describe an end-to-end generative approach from the encoding of\\nfeatures to the structural modeling of complex human activities by applying\\nFisher vectors and temporal models for the analysis of video sequences.\\n  We systematically evaluate the proposed approach on several available\\ndatasets (ADL, MPIICooking, and Breakfast datasets) using a variety of\\nperformance metrics. Through extensive system evaluations, we demonstrate that\\ncombining compact video representations based on Fisher Vectors with HMM-based\\nmodeling yields very significant gains in accuracy and when properly trained\\nwith sufficient training samples, structured temporal models outperform\\nunstructured bag-of-word types of models by a large margin on the tested\\nperformance metric.\\n',\n",
       "  'title': u'Cooking in the kitchen: Recognizing and Segmenting Human Activities in\\n  Videos'},\n",
       " u'1603.07123': {'arxivid': u'1603.07123',\n",
       "  'authorsaffil': [[u'R. M. Farouk', None], [u'M. A. SayedElahl', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'13 Pages,12 figures,FSP JOURNAL ISSN:1955-2068,Vol.9, Issue.7, part.1',\n",
       "  'doi': None,\n",
       "  'journalref': u'HFSP JOURNAL ISSN:1955-2068,Vol.9, Issue.7, part.1 Human Frontier\\n  Science Program , HFSP JOURNAL ISSN:1955-2068',\n",
       "  'link': u'http://arxiv.org/abs/1603.07123v1',\n",
       "  'published': u'2016-03-23T10:25:20Z',\n",
       "  'summary': u'  One of the most challenging tasks in microarray image analysis is spot\\nsegmentation. A solution to this problem is to provide an algorithm than can be\\nused to find any spot within the microarray image. Circular Hough\\nTransformation (CHT) is a powerful feature extraction technique used in image\\nanalysis, computer vision, and digital image processing. CHT algorithm is\\napplied on the cDNA microarray images to develop the accuracy and the\\nefficiency of the spots localization, addressing and segmentation process. The\\npurpose of the applied technique is to find imperfect instances of spots within\\na certain class of circles by applying a voting procedure on the cDNA\\nmicroarray images for spots localization, addressing and characterizing the\\npixels of each spot into foreground pixels and background simultaneously.\\nIntensive experiments on the University of North Carolina (UNC) microarray\\ndatabase indicate that the proposed method is superior to the K-means method\\nand the Support vector machine (SVM). Keywords: Hough circle transformation,\\ncDNA microarray image analysis, cDNA microarray image segmentation, spots\\nlocalization and addressing, spots segmentation\\n',\n",
       "  'title': u'Robust cDNA microarray image segmentation and analysis technique based\\n  on Hough circle transform'},\n",
       " u'1509.09308': {'arxivid': u'1509.09308',\n",
       "  'authorsaffil': [[u'Andrew Lavin', None], [u'Scott Gray', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'I.2.6; F.2.1'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.09308v2',\n",
       "  'published': u'2015-09-30T19:39:20Z',\n",
       "  'summary': u\"  Deep convolutional neural networks take GPU days of compute time to train on\\nlarge data sets. Pedestrian detection for self driving cars requires very low\\nlatency. Image recognition for mobile phones is constrained by limited\\nprocessing resources. The success of convolutional neural networks in these\\nsituations is limited by how fast we can compute them. Conventional FFT based\\nconvolution is fast for large filters, but state of the art convolutional\\nneural networks use small, 3x3 filters. We introduce a new class of fast\\nalgorithms for convolutional neural networks using Winograd's minimal filtering\\nalgorithms. The algorithms compute minimal complexity convolution over small\\ntiles, which makes them fast with small filters and small batch sizes. We\\nbenchmark a GPU implementation of our algorithm with the VGG network and show\\nstate of the art throughput at batch sizes from 1 to 64.\\n\",\n",
       "  'title': u'Fast Algorithms for Convolutional Neural Networks'},\n",
       " u'1604.01734': {'arxivid': u'1604.01734',\n",
       "  'authorsaffil': [[u'Sylvain Bouveret', u'Toulouse'],\n",
       "   [u'Michel Lema\\xeetre', u'Toulouse']],\n",
       "  'categoryterms': [u'cs.GT', u'cs.AI'],\n",
       "  'comment': u'COMSOC-2016, Jun 2016, Toulouse, France',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01734v1',\n",
       "  'published': u'2016-04-06T19:08:34Z',\n",
       "  'summary': u'  In fair division of indivisible goods, using sequences of sincere choices (or\\npicking sequences) is a natural way to allocate the objects. The idea is the\\nfollowing: at each stage, a designated agent picks one object among those that\\nremain. This paper, restricted to the case where the agents have numerical\\nadditive preferences over objects, revisits to some extent the seminal paper by\\nBrams and King [9] which was specific to ordinal and linear order preferences\\nover items. We point out similarities and differences with this latter context.\\nIn particular, we show that any Pareto-optimal allocation (under additive\\npreferences) is sequenceable, but that the converse is not true anymore. This\\nasymmetry leads naturally to the definition of a \"scale of efficiency\" having\\nthree steps: Pareto-optimality, sequenceability without Pareto-optimality, and\\nnon-sequenceability. Finally, we investigate the links between these efficiency\\nproperties and the \"scale of fairness\" we have described in an earlier work\\n[7]: we first show that an allocation can be envy-free and non-sequenceable,\\nbut that every competitive equilibrium with equal incomes is sequenceable. Then\\nwe experimentally explore the links between the scales of efficiency and\\nfairness.\\n',\n",
       "  'title': u'Efficiency and Sequenceability in Fair Division of Indivisible Goods\\n  with Additive Preferences'},\n",
       " u'1505.04467': {'arxivid': u'1505.04467',\n",
       "  'authorsaffil': [[u'Jacob Devlin', None],\n",
       "   [u'Saurabh Gupta', None],\n",
       "   [u'Ross Girshick', None],\n",
       "   [u'Margaret Mitchell', None],\n",
       "   [u'C. Lawrence Zitnick', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.04467v1',\n",
       "  'published': u'2015-05-17T22:14:27Z',\n",
       "  'summary': u'  We explore a variety of nearest neighbor baseline approaches for image\\ncaptioning. These approaches find a set of nearest neighbor images in the\\ntraining set from which a caption may be borrowed for the query image. We\\nselect a caption for the query image by finding the caption that best\\nrepresents the \"consensus\" of the set of candidate captions gathered from the\\nnearest neighbor images. When measured by automatic evaluation metrics on the\\nMS COCO caption evaluation server, these approaches perform as well as many\\nrecent approaches that generate novel captions. However, human studies show\\nthat a method that generates novel captions is still preferred over the nearest\\nneighbor approach.\\n',\n",
       "  'title': u'Exploring Nearest Neighbor Approaches for Image Captioning'},\n",
       " u'1603.04146': {'arxivid': u'1603.04146',\n",
       "  'authorsaffil': [[u'Shuhan Chen', None],\n",
       "   [u'Jindong Li', None],\n",
       "   [u'Xuelong Hu', None],\n",
       "   [u'Ping Zhou', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'submitted to IEEE ICIP 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04146v2',\n",
       "  'published': u'2016-03-14T06:44:43Z',\n",
       "  'summary': u'  Object proposals greatly benefit object detection task in recent\\nstate-of-the-art works, such as R-CNN [2]. However, the existing object\\nproposals usually have low localization accuracy at high intersection over\\nunion threshold. To address it, we apply saliency detection to each bounding\\nbox to improve their quality in this paper. We first present a geodesic\\nsaliency detection method in contour, which is designed to find closed\\ncontours. Then, we apply it to each candidate box with multi-sizes, and refined\\nboxes can be easily produced in the obtained saliency maps which are further\\nused to calculate saliency scores for proposal ranking. Experiments on PASCAL\\nVOC 2007 test dataset demonstrate the proposed refinement approach can greatly\\nimprove existing models.\\n',\n",
       "  'title': u'Saliency Detection for Improving Object Proposals'},\n",
       " u'1511.04601': {'arxivid': u'1511.04601',\n",
       "  'authorsaffil': [[u'Weiyang Liu', None],\n",
       "   [u'Zhiding Yu', None],\n",
       "   [u'Yingzhen Yang', None],\n",
       "   [u'Meng Yang', None],\n",
       "   [u'Thomas S. Huang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04601v3',\n",
       "  'published': u'2015-11-14T19:11:13Z',\n",
       "  'summary': u'  Dictionary learning (DL) for sparse coding has shown impressive performance\\nin classification tasks. But how to select a feature that can best work with\\nthe learned dictionary remains an open question. Current prevailing DL methods\\nusually adopt existing well-performing features, ignoring the inner\\nrelationship between dictionaries and features. To address the problem, we\\npropose a joint non-negative projection and dictionary learning (JNPDL) method.\\nNon-negative projection learning and dictionary learning are complementary to\\neach other, since the former leads to the intrinsic discriminative parts-based\\nfeatures for objects while the latter searches a suitable representation in the\\nprojected feature space. Specifically, discrimination of projection and\\ndictionary is achieved by imposing to both projection and coding coefficients a\\ngraph constraint that maximizes the intra-class compactness and inter-class\\nseparability. Experimental results on both image classification and image set\\nclassification show the excellent performance of JNPDL by being comparable or\\noutperforming many state-of-the-art approaches.\\n',\n",
       "  'title': u'Jointly Learning Non-negative Projection and Dictionary with\\n  Discriminative Graph Constraints for Classification'},\n",
       " u'1602.08557': {'arxivid': u'1602.08557',\n",
       "  'authorsaffil': [[u'Syed Shakib Sarwar', None],\n",
       "   [u'Swagath Venkataramani', None],\n",
       "   [u'Anand Raghunathan', None],\n",
       "   [u'Kaushik Roy', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'Accepted in Design, Automation and Test in Europe 2016 conference\\n  (DATE-2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08557v1',\n",
       "  'published': u'2016-02-27T05:37:44Z',\n",
       "  'summary': u'  Large-scale artificial neural networks have shown significant promise in\\naddressing a wide range of classification and recognition applications.\\nHowever, their large computational requirements stretch the capabilities of\\ncomputing platforms. The fundamental components of these neural networks are\\nthe neurons and its synapses. The core of a digital hardware neuron consists of\\nmultiplier, accumulator and activation function. Multipliers consume most of\\nthe processing energy in the digital neurons, and thereby in the hardware\\nimplementations of artificial neural networks. We propose an approximate\\nmultiplier that utilizes the notion of computation sharing and exploits error\\nresilience of neural network applications to achieve improved energy\\nconsumption. We also propose Multiplier-less Artificial Neuron (MAN) for even\\nlarger improvement in energy consumption and adapt the training process to\\nensure minimal degradation in accuracy. We evaluated the proposed design on 5\\nrecognition applications. The results show, 35% and 60% reduction in energy\\nconsumption, for neuron sizes of 8 bits and 12 bits, respectively, with a\\nmaximum of ~2.83% loss in network accuracy, compared to a conventional neuron\\nimplementation. We also achieve 37% and 62% reduction in area for a neuron size\\nof 8 bits and 12 bits, respectively, under iso-speed conditions.\\n',\n",
       "  'title': u'Multiplier-less Artificial Neurons Exploiting Error Resiliency for\\n  Energy-Efficient Neural Computing'},\n",
       " u'1601.01750': {'arxivid': u'1601.01750',\n",
       "  'authorsaffil': [[u'Kilho Son', None],\n",
       "   [u'Ming-Yu Liu', None],\n",
       "   [u'Yuichi Taguchi', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.RO'],\n",
       "  'comment': u'8 pages, 11 figures, will be presented to ICRA 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01750v3',\n",
       "  'published': u'2016-01-08T02:25:10Z',\n",
       "  'summary': u'  Range images captured by Time-of-Flight (ToF) cameras are corrupted with\\nmultipath distortions due to interaction between modulated light signals and\\nscenes. The interaction is often complicated, which makes a model-based\\nsolution elusive. We propose a learning-based approach for removing the\\nmultipath distortions for a ToF camera in a robotic arm setup. Our approach is\\nbased on deep learning. We use the robotic arm to automatically collect a large\\namount of ToF range images containing various multipath distortions. The\\ntraining images are automatically labeled by leveraging a high precision\\nstructured light sensor available only in the training time. In the test time,\\nwe apply the learned model to remove the multipath distortions. This allows our\\nrobotic arm setup to enjoy the speed and compact form of the ToF camera without\\ncompromising with its range measurement errors. We conduct extensive\\nexperimental validations and compare the proposed method to several baseline\\nalgorithms. The experiment results show that our method achieves 55% error\\nreduction in range estimation and largely outperforms the baseline algorithms.\\n',\n",
       "  'title': u'Learning to Remove Multipath Distortions in Time-of-Flight Range Images\\n  for a Robotic Arm Setup'},\n",
       " u'1604.03489': {'arxivid': u'1604.03489',\n",
       "  'authorsaffil': [[u'Victor Campos', None],\n",
       "   [u'Brendan Jou', None],\n",
       "   [u'Xavier Giro-i-Nieto', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.MM'],\n",
       "  'comment': u'Models and source code available at\\n  https://github.com/imatge-upc/sentiment-2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03489v1',\n",
       "  'published': u'2016-04-12T17:24:39Z',\n",
       "  'summary': u\"  Visual media have become a crucial part of our social lives. The throughput\\nof generated multimedia content, together with its richness for conveying\\nsentiments and feelings, highlights the need of automated visual sentiment\\nanalysis tools. We explore how Convolutional Neural Networks (CNNs), a\\ncomputational learning paradigm that has shown outstanding performance in\\nseveral vision tasks, can be applied to the task of visual sentiment prediction\\nby fine-tuning a state-of-the-art CNN. We analyze its architecture, studying\\nseveral performance boosting techniques, which led to a network tuned to\\nachieve a 6.1 % absolute accuracy improvement over the previous\\nstate-of-the-art on a dataset of images from a popular social media platform.\\nFinally, we present visualizations of local patterns that the network\\nassociates to each image's sentiment.\\n\",\n",
       "  'title': u'From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment\\n  Prediction'},\n",
       " u'1603.08695': {'arxivid': u'1603.08695',\n",
       "  'authorsaffil': [[u'Pedro O. Pinheiro', None],\n",
       "   [u'Tsung-Yi Lin', None],\n",
       "   [u'Ronan Collobert', None],\n",
       "   [u'Piotr Doll\\xe0r', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08695v1',\n",
       "  'published': u'2016-03-29T09:33:44Z',\n",
       "  'summary': u\"  Object segmentation requires both object-level information and low-level\\npixel data. This presents a challenge for feedforward networks: lower layers in\\nconvolutional nets capture rich spatial information, while upper layers encode\\nobject-level knowledge but are invariant to factors such as pose and\\nappearance. In this work we propose to augment feedforward nets for object\\nsegmentation with a novel top-down refinement approach. The resulting\\nbottom-up/top-down architecture is capable of efficiently generating\\nhigh-fidelity object masks. Similarly to skip connections, our approach\\nleverages features at all layers of the net. Unlike skip connections, our\\napproach does not attempt to output independent predictions at each layer.\\nInstead, we first output a coarse `mask encoding' in a feedforward pass, then\\nrefine this mask encoding in a top-down pass utilizing features at successively\\nlower layers. The approach is simple, fast, and effective. Building on the\\nrecent DeepMask network for generating object proposals, we show accuracy\\nimprovements of 10-20% in average recall for various setups and for small\\nobjects we improve recall by nearly 2 times. Additionally, by optimizing the\\noverall network architecture, our approach, which we call SharpMask, is 50\\\\%\\nfaster than the original DeepMask network (under .8s per image).\\n\",\n",
       "  'title': u'Learning to Refine Object Segments'},\n",
       " u'1406.7842': {'arxivid': u'1406.7842',\n",
       "  'authorsaffil': [[u'Xiaowen Dong', None],\n",
       "   [u'Dorina Thanou', None],\n",
       "   [u'Pascal Frossard', None],\n",
       "   [u'Pierre Vandergheynst', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.SI', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1406.7842v3',\n",
       "  'published': u'2014-06-30T18:33:59Z',\n",
       "  'summary': u'  The construction of a meaningful graph plays a crucial role in the success of\\nmany graph-based representations and algorithms for handling structured data,\\nespecially in the emerging field of graph signal processing. However, a\\nmeaningful graph is not always readily available from the data, nor easy to\\ndefine depending on the application domain. In particular, it is often\\ndesirable in graph signal processing applications that a graph is chosen such\\nthat the data admit certain regularity or smoothness on the graph. In this\\npaper, we address the problem of learning graph Laplacians, which is equivalent\\nto learning graph topologies, such that the input data form graph signals with\\nsmooth variations on the resulting topology. To this end, we adopt a factor\\nanalysis model for the graph signals and impose a Gaussian probabilistic prior\\non the latent variables that control these signals. We show that the Gaussian\\nprior leads to an efficient representation that favors the smoothness property\\nof the graph signals. We then propose an algorithm for learning graphs that\\nenforces such property and is based on minimizing the variations of the signals\\non the learned graph. Experiments on both synthetic and real world data\\ndemonstrate that the proposed graph learning framework can efficiently infer\\nmeaningful graph topologies from signal observations under the smoothness\\nprior.\\n',\n",
       "  'title': u'Learning Laplacian Matrix in Smooth Graph Signal Representations'},\n",
       " u'1604.01376': {'arxivid': u'1604.01376',\n",
       "  'authorsaffil': [[u'Valentina Zantedeschi', None],\n",
       "   [u'R\\xe9mi Emonet', None],\n",
       "   [u'Marc Sebban', None]],\n",
       "  'categoryterms': [u'cs.NA', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01376v1',\n",
       "  'published': u'2016-04-04T12:39:26Z',\n",
       "  'summary': u'  Many theoretical results in the machine learning domain stand only for\\nfunctions that are Lipschitz continuous. Lipschitz continuity is a strong form\\nof continuity that linearly bounds the variations of a function. In this paper,\\nwe derive tight Lipschitz constants for two families of metrics: Mahalanobis\\ndistances and bounded-space bilinear forms. To our knowledge, this is the first\\ntime the Mahalanobis distance is formally proved to be Lipschitz continuous and\\nthat such tight Lipschitz constants are derived.\\n',\n",
       "  'title': u'Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms'},\n",
       " u'1512.07962': {'arxivid': u'1512.07962',\n",
       "  'authorsaffil': [[u'Changyou Chen', None],\n",
       "   [u'David Carlson', None],\n",
       "   [u'Zhe Gan', None],\n",
       "   [u'Chunyuan Li', None],\n",
       "   [u'Lawrence Carin', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'Merry Christmas from the Santa (algorithm). AISTATS 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07962v2',\n",
       "  'published': u'2015-12-25T06:01:44Z',\n",
       "  'summary': u'  Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian\\nanalogs to popular stochastic optimization methods; however, this connection is\\nnot well studied. We explore this relationship by applying simulated annealing\\nto an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two\\nkey components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)\\nadaptive element-wise momentum weights. The zero-temperature limit gives a\\nnovel stochastic optimization method with adaptive element-wise momentum\\nweights, while conventional optimization methods only have a shared, static\\nmomentum weight. Under certain assumptions, our theoretical analysis suggests\\nthe proposed simulated annealing approach converges close to the global optima.\\nExperiments on several deep neural network models show state-of-the-art results\\ncompared to related stochastic optimization algorithms.\\n',\n",
       "  'title': u'Bridging the Gap between Stochastic Gradient MCMC and Stochastic\\n  Optimization'},\n",
       " u'1603.02738': {'arxivid': u'1603.02738',\n",
       "  'authorsaffil': [[u'Matthew Guzdial', None], [u'Mark Riedl', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'8 pages, 11 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02738v1',\n",
       "  'published': u'2016-03-08T23:19:50Z',\n",
       "  'summary': u'  We present an approach to generate novel computer game levels that blend\\ndifferent game concepts in an unsupervised fashion. Our primary contribution is\\nan analogical reasoning process to construct blends between level design models\\nlearned from gameplay videos. The models represent probabilistic relationships\\nbetween elements in the game. An analogical reasoning process maps features\\nbetween two models to produce blended models that can then generate new level\\nchunks. As a proof-of-concept we train our system on the classic platformer\\ngame Super Mario Bros. due to its highly-regarded and well understood level\\ndesign. We evaluate the extent to which the models represent stylistic level\\ndesign knowledge and demonstrate the ability of our system to explain levels\\nthat were blended by human expert designers.\\n',\n",
       "  'title': u'Learning to Blend Computer Game Levels'},\n",
       " u'1603.02736': {'arxivid': u'1603.02736',\n",
       "  'authorsaffil': [[u'Umamahesh Srinivas', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV'],\n",
       "  'comment': u'Doctoral dissertation, Department of Electrical Engineering, The\\n  Pennsylvania State University, 2013',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02736v1',\n",
       "  'published': u'2016-03-08T23:15:18Z',\n",
       "  'summary': u'  A variety of real-world tasks involve the classification of images into\\npre-determined categories. Designing image classification algorithms that\\nexhibit robustness to acquisition noise and image distortions, particularly\\nwhen the available training data are insufficient to learn accurate models, is\\na significant challenge. This dissertation explores the development of\\ndiscriminative models for robust image classification that exploit underlying\\nsignal structure, via probabilistic graphical models and sparse signal\\nrepresentations.\\n  Probabilistic graphical models are widely used in many applications to\\napproximate high-dimensional data in a reduced complexity set-up. Learning\\ngraphical structures to approximate probability distributions is an area of\\nactive research. Recent work has focused on learning graphs in a discriminative\\nmanner with the goal of minimizing classification error. In the first part of\\nthe dissertation, we develop a discriminative learning framework that exploits\\nthe complementary yet correlated information offered by multiple\\nrepresentations (or projections) of a given signal/image. Specifically, we\\npropose a discriminative tree-based scheme for feature fusion by explicitly\\nlearning the conditional correlations among such multiple projections in an\\niterative manner. Experiments reveal the robustness of the resulting graphical\\nmodel classifier to training insufficiency.\\n',\n",
       "  'title': u'Discriminative models for robust image classification'},\n",
       " u'1602.04847': {'arxivid': u'1602.04847',\n",
       "  'authorsaffil': [[u'S\\xe9bastien Bubeck', None], [u'Yin-Tat Lee', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.DS', u'cs.LG', u'cs.NA'],\n",
       "  'comment': u'19 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04847v1',\n",
       "  'published': u'2016-02-15T21:35:58Z',\n",
       "  'summary': u'  We propose a new framework for black-box convex optimization which is\\nwell-suited for situations where gradient computations are expensive. We derive\\na new method for this framework which leverages several concepts from convex\\noptimization, from standard first-order methods (e.g. gradient descent or\\nquasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant\\nbarriers). We demonstrate empirically that our new technique compares favorably\\nwith state of the art algorithms (such as BFGS).\\n',\n",
       "  'title': u'Black-box optimization with a politician'},\n",
       " u'1603.08474': {'arxivid': u'1603.08474',\n",
       "  'authorsaffil': [[u'Oswaldo Ludwig', None],\n",
       "   [u'Xiao Liu', None],\n",
       "   [u'Parisa Kordjamshidi', None],\n",
       "   [u'Marie-Francine Moens', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08474v1',\n",
       "  'published': u'2016-03-28T18:38:46Z',\n",
       "  'summary': u'  This paper introduces the visually informed embedding of word (VIEW), a\\ncontinuous vector representation for a word extracted from a deep neural model\\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\\nbetween visual objects, given a textual description. The model is composed of a\\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\\napplied to transferring multimodal background knowledge to Spatial Role\\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\\nmentioned in the text. This work also contributes with a new method to select\\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.\\n',\n",
       "  'title': u'Deep Embedding for Spatial Role Labeling'},\n",
       " u'1603.08678': {'arxivid': u'1603.08678',\n",
       "  'authorsaffil': [[u'Jifeng Dai', None],\n",
       "   [u'Kaiming He', None],\n",
       "   [u'Yi Li', None],\n",
       "   [u'Shaoqing Ren', None],\n",
       "   [u'Jian Sun', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08678v1',\n",
       "  'published': u'2016-03-29T08:37:26Z',\n",
       "  'summary': u'  Fully convolutional networks (FCNs) have been proven very successful for\\nsemantic segmentation, but the FCN outputs are unaware of object instances. In\\nthis paper, we develop FCNs that are capable of proposing instance-level\\nsegment candidates. In contrast to the previous FCN that generates one score\\nmap, our FCN is designed to compute a small set of instance-sensitive score\\nmaps, each of which is the outcome of a pixel-wise classifier of a relative\\nposition to instances. On top of these instance-sensitive score maps, a simple\\nassembling module is able to output instance candidate at each position. In\\ncontrast to the recent DeepMask method for segmenting instances, our method\\ndoes not have any high-dimensional layer related to the mask resolution, but\\ninstead exploits image local coherence for estimating instances. We present\\ncompetitive results of instance segment proposal on both PASCAL VOC and MS\\nCOCO.\\n',\n",
       "  'title': u'Instance-sensitive Fully Convolutional Networks'},\n",
       " u'1601.07932': {'arxivid': u'1601.07932',\n",
       "  'authorsaffil': [[u'Keehwan Park', None], [u'Jean Honorio', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IT', u'math.IT', u'stat.ML'],\n",
       "  'comment': u\"ISIT'16\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07932v2',\n",
       "  'published': u'2016-01-28T22:12:06Z',\n",
       "  'summary': u'  We study the information-theoretic lower bound of the sample complexity of\\nthe correct recovery of diffusion network structures. We introduce a\\ndiscrete-time diffusion model based on the Independent Cascade model for which\\nwe obtain a lower bound of order $\\\\Omega(k \\\\log p)$, for directed graphs of $p$\\nnodes, and at most $k$ parents per node. Next, we introduce a continuous-time\\ndiffusion model, for which a similar lower bound of order $\\\\Omega(k \\\\log p)$ is\\nobtained. Our results show that the algorithm of Pouget-Abadie et al. is\\nstatistically optimal for the discrete-time regime. Our work also opens the\\nquestion of whether it is possible to devise an optimal algorithm for the\\ncontinuous-time regime.\\n',\n",
       "  'title': u'Information-Theoretic Lower Bounds for Recovery of Diffusion Network\\n  Structures'},\n",
       " u'1602.00715': {'arxivid': u'1602.00715',\n",
       "  'authorsaffil': [[u'Stanley H. Chan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00715v1',\n",
       "  'published': u'2016-02-01T21:24:55Z',\n",
       "  'summary': u'  This paper studies a type of image priors that are constructed implicitly\\nthrough the alternating direction method of multiplier (ADMM) algorithm, called\\nthe algorithm-induced prior. Different from classical image priors which are\\ndefined before running the reconstruction algorithm, algorithm-induced priors\\nare defined by the denoising procedure used to replace one of the two modules\\nin the ADMM algorithm. Since such prior is not explicitly defined, analyzing\\nthe performance has been difficult in the past.\\n  Focusing on the class of symmetric smoothing filters, this paper presents an\\nexplicit expression of the prior induced by the ADMM algorithm. The new prior\\nis reminiscent to the conventional graph Laplacian but with stronger\\nreconstruction performance. It can also be shown that the overall\\nreconstruction has an efficient closed-form implementation if the associated\\nsymmetric smoothing filter is low rank. The results are validated with\\nexperiments on image inpainting.\\n',\n",
       "  'title': u'Algorithm-Induced Prior for Image Restoration'},\n",
       " u'1603.08120': {'arxivid': u'1603.08120',\n",
       "  'authorsaffil': [[u'Wenbin Li', None],\n",
       "   [u'Darren Cosker', None],\n",
       "   [u'Zhihan Lv', None],\n",
       "   [u'Matthew Brown', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08120v1',\n",
       "  'published': u'2016-03-26T16:08:13Z',\n",
       "  'summary': u'  In this paper we present the first ground truth dataset of nonrigidly\\ndeforming real-world scenes (both long and short video sequences) in order to\\nquantitatively evaluate RGB based tracking and registration methods. To\\nconstruct ground truth for the RGB sequences, we simultaneously capture\\nNear-Infrared (NIR) image sequences where dense markers - visible only in NIR -\\nrepresent ground truth positions. This allows for comparison with automatically\\ntracked RGB positions and the formation of error metrics. Most previous\\ndatasets containing nonrigidly deforming sequences are based on synthetic data.\\nOur capture protocol enables us to acquire real-world deforming objects with\\nrealistic photometric effects - such as blur and illumination change - as well\\nas occlusion and complex deformations. A public evaluation website is\\nconstructed to allow for ranking of RGB image based optical flow and other\\ndense tracking algorithms, with various statistical measures. Furthermore, we\\npresent the first RGB-NIR multispectral optical flow model allowing for energy\\noptimization by combining information from both the RGB and the complementary\\nNIR channels. In our experiments we evaluate eight existing RGB based optical\\nflow methods on our new dataset. We also evaluate our multispectral optical\\nflow algorithm in real-world scenes by varying the input channels across RGB,\\nNIR and RGB-NIR.\\n',\n",
       "  'title': u'Dense Nonrigid Ground Truth for Optical Flow in Real-World Scenes'},\n",
       " u'1406.5370': {'arxivid': u'1406.5370',\n",
       "  'authorsaffil': [[u'Fajwel Fogel', None],\n",
       "   [u\"Alexandre d'Aspremont\", None],\n",
       "   [u'Milan Vojnovic', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'stat.ML', u'62F07, 06A07, 90C27'],\n",
       "  'comment': u'Substantially revised. Accepted by JMLR',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1406.5370v4',\n",
       "  'published': u'2014-06-20T12:58:46Z',\n",
       "  'summary': u'  We describe a seriation algorithm for ranking a set of items given pairwise\\ncomparisons between these items. Intuitively, the algorithm assigns similar\\nrankings to items that compare similarly with all others. It does so by\\nconstructing a similarity matrix from pairwise comparisons, using seriation\\nmethods to reorder this matrix and construct a ranking. We first show that this\\nspectral seriation algorithm recovers the true ranking when all pairwise\\ncomparisons are observed and consistent with a total order. We then show that\\nranking reconstruction is still exact when some pairwise comparisons are\\ncorrupted or missing, and that seriation based spectral ranking is more robust\\nto noise than classical scoring methods. Finally, we bound the ranking error\\nwhen only a random subset of the comparions are observed. An additional benefit\\nof the seriation formulation is that it allows us to solve semi-supervised\\nranking problems. Experiments on both synthetic and real datasets demonstrate\\nthat seriation based spectral ranking achieves competitive and in some cases\\nsuperior performance compared to classical ranking methods.\\n',\n",
       "  'title': u'Spectral Ranking using Seriation'},\n",
       " u'1511.06606': {'arxivid': u'1511.06606',\n",
       "  'authorsaffil': [[u'Hristo S. Paskov', None],\n",
       "   [u'John C. Mitchell', None],\n",
       "   [u'Trevor J. Hastie', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06606v5',\n",
       "  'published': u'2015-11-20T14:21:44Z',\n",
       "  'summary': u\"  We propose `Dracula', a new framework for unsupervised feature selection from\\nsequential data such as text. Dracula learns a dictionary of $n$-grams that\\nefficiently compresses a given corpus and recursively compresses its own\\ndictionary; in effect, Dracula is a `deep' extension of Compressive Feature\\nLearning. It requires solving a binary linear program that may be relaxed to a\\nlinear program. Both problems exhibit considerable structure, their solution\\npaths are well behaved, and we identify parameters which control the depth and\\ndiversity of the dictionary. We also discuss how to derive features from the\\ncompressed documents and show that while certain unregularized linear models\\nare invariant to the structure of the compressed dictionary, this structure may\\nbe used to regularize learning. Experiments are presented that demonstrate the\\nefficacy of Dracula's features.\\n\",\n",
       "  'title': u'Data Representation and Compression Using Linear-Programming\\n  Approximations'},\n",
       " u'1601.08068': {'arxivid': u'1601.08068',\n",
       "  'authorsaffil': [[u'Hildo Bijl', None],\n",
       "   [u'Thomas B. Sch\\xf6n', None],\n",
       "   [u'Jan-Willem van Wingerden', None],\n",
       "   [u'Michel Verhaegen', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.SY'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.08068v1',\n",
       "  'published': u'2016-01-29T11:55:26Z',\n",
       "  'summary': u'  Gaussian process regression traditionally has three important downsides. (1)\\nIt is computationally intensive, (2) it cannot efficiently implement newly\\nobtained measurements online, and (3) it cannot deal with stochastic (noisy)\\ninput points. In this paper we present an algorithm tackling all these three\\nissues simultaneously. The resulting Sparse Online Noisy Input GP (SONIG)\\nregression algorithm can incorporate new measurements in constant runtime. A\\ncomparison has shown that it is more accurate than similar existing regression\\nalgorithms. In addition, the algorithm can be applied to non-linear black-box\\nsystem modeling, where its performance is competitive with non-linear ARX\\nmodels.\\n',\n",
       "  'title': u'Online Sparse Gaussian Process Training with Input Noise'},\n",
       " u'1604.00990': {'arxivid': u'1604.00990',\n",
       "  'authorsaffil': [[u'Hatem Alismail', None],\n",
       "   [u'Brett Browning', None],\n",
       "   [u'Simon Lucey', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00990v1',\n",
       "  'published': u'2016-04-04T19:02:45Z',\n",
       "  'summary': u'  Feature descriptors, such as SIFT and ORB, are well-known for their\\nrobustness to illumination changes, which has made them popular for\\nfeature-based VSLAM\\\\@. However, in degraded imaging conditions such as low\\nlight, low texture, blur and specular reflections, feature extraction is often\\nunreliable. In contrast, direct VSLAM methods which estimate the camera pose by\\nminimizing the photometric error using raw pixel intensities are often more\\nrobust to low textured environments and blur. Nonetheless, at the core of\\ndirect VSLAM is the reliance on a consistent photometric appearance across\\nimages, otherwise known as the brightness constancy assumption. Unfortunately,\\nbrightness constancy seldom holds in real world applications.\\n  In this work, we overcome brightness constancy by incorporating feature\\ndescriptors into a direct visual odometry framework. This combination results\\nin an efficient algorithm that combines the strength of both feature-based\\nalgorithms and direct methods. Namely, we achieve robustness to arbitrary\\nphotometric variations while operating in low-textured and poorly lit\\nenvironments. Our approach utilizes an efficient binary descriptor, which we\\ncall Bit-Planes, and show how it can be used in the gradient-based optimization\\nrequired by direct methods. Moreover, we show that the squared Euclidean\\ndistance between Bit-Planes is equivalent to the Hamming distance. Hence, the\\ndescriptor may be used in least squares optimization without sacrificing its\\nphotometric invariance. Finally, we present empirical results that demonstrate\\nthe robustness of the approach in poorly lit underground environments.\\n',\n",
       "  'title': u'Direct Visual Odometry using Bit-Planes'},\n",
       " u'1510.07727': {'arxivid': u'1510.07727',\n",
       "  'authorsaffil': [[u'Art B. Owen', None]],\n",
       "  'categoryterms': [u'stat.CO', u'cs.LG', u'stat.ML', u'65C40, 62M05'],\n",
       "  'comment': u'10 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.07727v4',\n",
       "  'published': u'2015-10-27T00:00:54Z',\n",
       "  'summary': u'  It is common to subsample Markov chain samples to reduce the storage burden\\nof the output. It is also well known that discarding $k-1$ out of every $k$\\nobservations will not improve statistical efficiency. It is less frequently\\nremarked that subsampling a Markov chain allows one to omit some of the\\ncomputation beyond that needed to simply advance the chain. When this reduced\\ncomputation is accounted for, thinning the Markov chain by subsampling it can\\nimprove statistical efficiency. Given an autocorrelation parameter $\\\\rho$ and a\\ncost ratio $\\\\theta$, this paper shows how to compute the most efficient\\nsubsampling frequency $k$. The optimal $k$ grows rapidly as $\\\\rho$ increases\\ntowards $1$. The resulting efficiency gain depends primarily on $\\\\theta$, not\\n$\\\\rho$. Taking $k=1$ (no thinning) is optimal when $\\\\rho\\\\le0$. For $\\\\rho>0$ it\\nis optimal if and only if $\\\\theta \\\\le (1-\\\\rho)^2/(2\\\\rho)$. The efficiency gain\\nnever exceeds $1+\\\\theta$. The derivations are exact for an AR(1)\\nautocorrelation which is often a good approximation to the autocorrelations one\\nsees in practice.\\n',\n",
       "  'title': u'Statistically efficient thinning of a Markov chain sampler'},\n",
       " u'1602.07535': {'arxivid': u'1602.07535',\n",
       "  'authorsaffil': [[u'Alireza Ghasemi', None],\n",
       "   [u'Adam Scholefield', None],\n",
       "   [u'Martin Vetterli', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07535v1',\n",
       "  'published': u'2016-02-24T14:53:29Z',\n",
       "  'summary': u'  We propose a novel camera pose estimation or perspective-n-point (PnP)\\nalgorithm, based on the idea of consistency regions and half-space\\nintersections. Our algorithm has linear time-complexity and a squared\\nreconstruction error that decreases at least quadratically, as the number of\\nfeature point correspondences increase.\\n  Inspired by ideas from triangulation and frame quantisation theory, we define\\nconsistent reconstruction and then present SHAPE, our proposed consistent pose\\nestimation algorithm. We compare this algorithm with state-of-the-art pose\\nestimation techniques in terms of accuracy and error decay rate. The\\nexperimental results verify our hypothesis on the optimal worst-case quadratic\\ndecay and demonstrate its promising performance compared to other approaches.\\n',\n",
       "  'title': u'SHAPE: Linear-Time Camera Pose Estimation With Quadratic Error-Decay'},\n",
       " u'1409.3970': {'arxivid': u'1409.3970',\n",
       "  'authorsaffil': [[u'Yin Zheng', None],\n",
       "   [u'Yu-Jin Zhang', None],\n",
       "   [u'Hugo Larochelle', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.IR', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'24 pages, 10 figures. A version has been accepted by TPAMI on Aug\\n  4th, 2015. Add footnote about how to train the model in practice in Section\\n  5.1. arXiv admin note: substantial text overlap with arXiv:1305.5306',\n",
       "  'doi': u'10.1109/TPAMI.2015.2476802',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1409.3970v3',\n",
       "  'published': u'2014-09-13T17:17:05Z',\n",
       "  'summary': u'  Topic modeling based on latent Dirichlet allocation (LDA) has been a\\nframework of choice to deal with multimodal data, such as in image annotation\\ntasks. Another popular approach to model the multimodal data is through deep\\nneural networks, such as the deep Boltzmann machine (DBM). Recently, a new type\\nof topic model called the Document Neural Autoregressive Distribution Estimator\\n(DocNADE) was proposed and demonstrated state-of-the-art performance for text\\ndocument modeling. In this work, we show how to successfully apply and extend\\nthis model to multimodal data, such as simultaneous image classification and\\nannotation. First, we propose SupDocNADE, a supervised extension of DocNADE,\\nthat increases the discriminative power of the learned hidden topic features\\nand show how to employ it to learn a joint representation from image visual\\nwords, annotation words and class label information. We test our model on the\\nLabelMe and UIUC-Sports data sets and show that it compares favorably to other\\ntopic models. Second, we propose a deep extension of our model and provide an\\nefficient way of training the deep model. Experimental results show that our\\ndeep model outperforms its shallow version and reaches state-of-the-art\\nperformance on the Multimedia Information Retrieval (MIR) Flickr data set.\\n',\n",
       "  'title': u'A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data'},\n",
       " u'1603.04037': {'arxivid': u'1603.04037',\n",
       "  'authorsaffil': [[u'Umar Iqbal', None],\n",
       "   [u'Martin Garbade', None],\n",
       "   [u'Juergen Gall', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04037v1',\n",
       "  'published': u'2016-03-13T15:09:35Z',\n",
       "  'summary': u'  In this work we propose to utilize information about human actions to improve\\npose estimation in monocular videos. To this end, we present a pictorial\\nstructure model that exploits high-level information about activities to\\nincorporate higher-order part dependencies by modeling action specific\\nappearance models and pose priors. However, instead of using an additional\\nexpensive action recognition framework, the action priors are efficiently\\nestimated by our pose estimation framework. This is achieved by starting with a\\nuniform action prior and updating the action prior during pose estimation. We\\nalso show that learning the right amount of appearance sharing among action\\nclasses improves the pose estimation. Our proposed model achieves\\nstate-of-the-art performance on two challenging datasets for pose estimation\\nand action recognition with over 80,000 test images.\\n',\n",
       "  'title': u'Pose for Action - Action for Pose'},\n",
       " u'1604.03692': {'arxivid': u'1604.03692',\n",
       "  'authorsaffil': [[u'Tianmin Shu', None],\n",
       "   [u'M. S. Ryoo', None],\n",
       "   [u'Song-Chun Zhu', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.AI', u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'International Joint Conference on Artificial Intelligence (IJCAI),\\n  2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03692v2',\n",
       "  'published': u'2016-04-13T08:40:06Z',\n",
       "  'summary': u'  In this paper, we present an approach for robot learning of social affordance\\nfrom human activity videos. We consider the problem in the context of\\nhuman-robot interaction: Our approach learns structural representations of\\nhuman-human (and human-object-human) interactions, describing how body-parts of\\neach agent move with respect to each other and what spatial relations they\\nshould maintain to complete each sub-event (i.e., sub-goal). This enables the\\nrobot to infer its own movement in reaction to the human body motion, allowing\\nit to naturally replicate such interactions.\\n  We introduce the representation of social affordance and propose a generative\\nmodel for its weakly supervised learning from human demonstration videos. Our\\napproach discovers critical steps (i.e., latent sub-events) in an interaction\\nand the typical motion associated with them, learning what body-parts should be\\ninvolved and how. The experimental results demonstrate that our Markov Chain\\nMonte Carlo (MCMC) based learning algorithm automatically discovers\\nsemantically meaningful interactive affordance from RGB-D videos, which allows\\nus to generate appropriate full body motion for an agent.\\n',\n",
       "  'title': u'Learning Social Affordance for Human-Robot Interaction'},\n",
       " u'1506.07503': {'arxivid': u'1506.07503',\n",
       "  'authorsaffil': [[u'Jan Chorowski', None],\n",
       "   [u'Dzmitry Bahdanau', None],\n",
       "   [u'Dmitriy Serdyuk', None],\n",
       "   [u'Kyunghyun Cho', None],\n",
       "   [u'Yoshua Bengio', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.07503v1',\n",
       "  'published': u'2015-06-24T19:10:33Z',\n",
       "  'summary': u'  Recurrent sequence generators conditioned on input data through an attention\\nmechanism have recently shown very good performance on a range of tasks in-\\ncluding machine translation, handwriting synthesis and image caption gen-\\neration. We extend the attention-mechanism with features needed for speech\\nrecognition. We show that while an adaptation of the model used for machine\\ntranslation in reaches a competitive 18.7% phoneme error rate (PER) on the\\nTIMIT phoneme recognition task, it can only be applied to utterances which are\\nroughly as long as the ones it was trained on. We offer a qualitative\\nexplanation of this failure and propose a novel and generic method of adding\\nlocation-awareness to the attention mechanism to alleviate this issue. The new\\nmethod yields a model that is robust to long inputs and achieves 18% PER in\\nsingle utterances and 20% in 10-times longer (repeated) utterances. Finally, we\\npropose a change to the at- tention mechanism that prevents it from\\nconcentrating too much on single frames, which further reduces PER to 17.6%\\nlevel.\\n',\n",
       "  'title': u'Attention-Based Models for Speech Recognition'},\n",
       " u'1207.0580': {'arxivid': u'1207.0580',\n",
       "  'authorsaffil': [[u'Geoffrey E. Hinton', None],\n",
       "   [u'Nitish Srivastava', None],\n",
       "   [u'Alex Krizhevsky', None],\n",
       "   [u'Ilya Sutskever', None],\n",
       "   [u'Ruslan R. Salakhutdinov', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1207.0580v1',\n",
       "  'published': u'2012-07-03T06:35:15Z',\n",
       "  'summary': u'  When a large feedforward neural network is trained on a small training set,\\nit typically performs poorly on held-out test data. This \"overfitting\" is\\ngreatly reduced by randomly omitting half of the feature detectors on each\\ntraining case. This prevents complex co-adaptations in which a feature detector\\nis only helpful in the context of several other specific feature detectors.\\nInstead, each neuron learns to detect a feature that is generally helpful for\\nproducing the correct answer given the combinatorially large variety of\\ninternal contexts in which it must operate. Random \"dropout\" gives big\\nimprovements on many benchmark tasks and sets new records for speech and object\\nrecognition.\\n',\n",
       "  'title': u'Improving neural networks by preventing co-adaptation of feature\\n  detectors'},\n",
       " u'1603.03281': {'arxivid': u'1603.03281',\n",
       "  'authorsaffil': [[u'Yelipe UshaRani', None], [u'P. Sammulal', None]],\n",
       "  'categoryterms': [u'cs.DB', u'cs.IR', u'cs.LG'],\n",
       "  'comment': u'Special Issue of Journal IJCSIS indexed in Web of Science and Thomson\\n  Reuters ISI. https://sites.google.com/site/ijcsis/vol-14-s1-feb-2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03281v1',\n",
       "  'published': u'2016-03-10T14:31:33Z',\n",
       "  'summary': u'  Imputation of missing attribute values in medical datasets for extracting\\nhidden knowledge from medical datasets is an interesting research topic of\\ninterest which is very challenging. One cannot eliminate missing values in\\nmedical records. The reason may be because some tests may not been conducted as\\nthey are cost effective, values missed when conducting clinical trials, values\\nmay not have been recorded to name some of the reasons. Data mining researchers\\nhave been proposing various approaches to find and impute missing values to\\nincrease classification accuracies so that disease may be predicted accurately.\\nIn this paper, we propose a novel imputation approach for imputation of missing\\nvalues and performing classification after fixing missing values. The approach\\nis based on clustering concept and aims at dimensionality reduction of the\\nrecords. The case study discussed shows that missing values can be fixed and\\nimputed efficiently by achieving dimensionality reduction. The importance of\\nproposed approach for classification is visible in the case study which assigns\\nsingle class label in contrary to multi-label assignment if dimensionality\\nreduction is not performed.\\n',\n",
       "  'title': u'An Innovative Imputation and Classification Approach for Accurate\\n  Disease Prediction'},\n",
       " u'1603.04619': {'arxivid': u'1603.04619',\n",
       "  'authorsaffil': [[u'Yao Li', None],\n",
       "   [u'Linqiao Liu', None],\n",
       "   [u'Chunhua Shen', None],\n",
       "   [u'Anton van den Hengel', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'16pages, under review',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04619v1',\n",
       "  'published': u'2016-03-15T10:21:47Z',\n",
       "  'summary': u'  Given a set of images containing objects from the same category, the task of\\nimage co-localization is to identify and localize each instance. This paper\\nshows that this problem can be solved by a simple but intriguing idea, that is,\\na common object detector can be learnt by making its detection confidence\\nscores distributed like those of a strongly supervised detector. More\\nspecifically, we observe that given a set of object proposals extracted from an\\nimage that contains the object of interest, an accurate strongly supervised\\nobject detector should give high scores to only a small minority of proposals,\\nand low scores to most of them. Thus, we devise an entropy-based objective\\nfunction to enforce the above property when learning the common object\\ndetector. Once the detector is learnt, we resort to a segmentation approach to\\nrefine the localization. We show that despite its simplicity, our approach\\noutperforms state-of-the-art methods.\\n',\n",
       "  'title': u\"Image Co-localization by Mimicking a Good Detector's Confidence Score\\n  Distribution\"},\n",
       " u'1603.07292': {'arxivid': u'1603.07292',\n",
       "  'authorsaffil': [[u'Aleksandar Chakarov', None],\n",
       "   [u'Aditya Nori', None],\n",
       "   [u'Sriram Rajamani', None],\n",
       "   [u'Shayak Sen', None],\n",
       "   [u'Deepak Vijaykeerthy', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.PL', u'stat.ML', u'D.2.5; I.2.3'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07292v1',\n",
       "  'published': u'2016-03-23T18:30:37Z',\n",
       "  'summary': u\"  Unlike traditional programs (such as operating systems or word processors)\\nwhich have large amounts of code, machine learning tasks use programs with\\nrelatively small amounts of code (written in machine learning libraries), but\\nvoluminous amounts of data. Just like developers of traditional programs debug\\nerrors in their code, developers of machine learning tasks debug and fix errors\\nin their data. However, algorithms and tools for debugging and fixing errors in\\ndata are less common, when compared to their counterparts for detecting and\\nfixing errors in code. In this paper, we consider classification tasks where\\nerrors in training data lead to misclassifications in test points, and propose\\nan automated method to find the root causes of such misclassifications. Our\\nroot cause analysis is based on Pearl's theory of causation, and uses Pearl's\\nPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\\nencodes the computation of PS as a probabilistic program, and uses recent work\\non probabilistic programs and transformations on probabilistic programs (along\\nwith gray-box models of machine learning algorithms) to efficiently compute PS.\\nPsi is able to identify root causes of data errors in interesting data sets.\\n\",\n",
       "  'title': u'Debugging Machine Learning Tasks'},\n",
       " u'1601.02166': {'arxivid': u'1601.02166',\n",
       "  'authorsaffil': [[u'Anders S\\xf8gaard', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learning',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02166v1',\n",
       "  'published': u'2016-01-09T23:34:05Z',\n",
       "  'summary': u'  Sequence model learning algorithms typically maximize log-likelihood minus\\nthe norm of the model (or minimize Hamming loss + norm). In cross-lingual\\npart-of-speech (POS) tagging, our target language training data consists of\\nsequences of sentences with word-by-word labels projected from translations in\\n$k$ languages for which we have labeled data, via word alignments. Our training\\ndata is therefore very noisy, and if Rademacher complexity is high, learning\\nalgorithms are prone to overfit. Norm-based regularization assumes a constant\\nwidth and zero mean prior. We instead propose to use the $k$ source language\\nmodels to estimate the parameters of a Gaussian prior for learning new POS\\ntaggers. This leads to significantly better performance in multi-source\\ntransfer set-ups. We also present a drop-out version that injects (empirical)\\nGaussian noise during online learning. Finally, we note that using empirical\\nGaussian priors leads to much lower Rademacher complexity, and is superior to\\noptimally weighted model interpolation.\\n',\n",
       "  'title': u'Empirical Gaussian priors for cross-lingual transfer learning'},\n",
       " u'1603.06093': {'arxivid': u'1603.06093',\n",
       "  'authorsaffil': [[u'Sergei Fedorov', None], [u'Olga Kacher', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06093v1',\n",
       "  'published': u'2016-03-19T13:36:02Z',\n",
       "  'summary': u'  Most approaches to large-scale image retrieval are based on the construction\\nof the inverted index of local image descriptors or visual words. A search in\\nsuch an index usually results in a large number of candidates. This list of\\ncandidates is then re-ranked with the help of a geometric verification, using a\\nRANSAC algorithm, for example. In this paper we propose a feature\\nrepresentation, which is built as a combination of three local descriptors. It\\nallows one to significantly decrease the number of false matches and to shorten\\nthe list of candidates after the initial search in the inverted index. This\\ncombination of local descriptors is both reproducible and highly\\ndiscriminative, and thus can be efficiently used for large-scale near-duplicate\\nimage retrieval.\\n',\n",
       "  'title': u'Large scale near-duplicate image retrieval using Triples of Adjacent\\n  Ranked Features (TARF) with embedded geometric information'},\n",
       " u'1412.6071': {'arxivid': u'1412.6071',\n",
       "  'authorsaffil': [[u'Benjamin Graham', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1412.6071v4',\n",
       "  'published': u'2014-12-18T20:45:11Z',\n",
       "  'summary': u'  Convolutional networks almost always incorporate some form of spatial\\npooling, and very often it is alpha times alpha max-pooling with alpha=2.\\nMax-pooling act on the hidden layers of the network, reducing their size by an\\ninteger multiplicative factor alpha. The amazing by-product of discarding 75%\\nof your data is that you build into the network a degree of invariance with\\nrespect to translations and elastic distortions. However, if you simply\\nalternate convolutional layers with max-pooling layers, performance is limited\\ndue to the rapid reduction in spatial size, and the disjoint nature of the\\npooling regions. We have formulated a fractional version of max-pooling where\\nalpha is allowed to take non-integer values. Our version of max-pooling is\\nstochastic as there are lots of different ways of constructing suitable pooling\\nregions. We find that our form of fractional max-pooling reduces overfitting on\\na variety of datasets: for instance, we improve on the state-of-the art for\\nCIFAR-100 without even using dropout.\\n',\n",
       "  'title': u'Fractional Max-Pooling'},\n",
       " u'1603.05962': {'arxivid': u'1603.05962',\n",
       "  'authorsaffil': [[u'Stanislas Lauly', None],\n",
       "   [u'Yin Zheng', None],\n",
       "   [u'Alexandre Allauzen', None],\n",
       "   [u'Hugo Larochelle', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05962v1',\n",
       "  'published': u'2016-03-18T19:24:44Z',\n",
       "  'summary': u'  We present an approach based on feed-forward neural networks for learning the\\ndistribution of textual documents. This approach is inspired by the Neural\\nAutoregressive Distribution Estimator(NADE) model, which has been shown to be a\\ngood estimator of the distribution of discrete-valued igh-dimensional vectors.\\nIn this paper, we present how NADE can successfully be adapted to the case of\\ntextual data, retaining from NADE the property that sampling or computing the\\nprobability of observations can be done exactly and efficiently. The approach\\ncan also be used to learn deep representations of documents that are\\ncompetitive to those learned by the alternative topic modeling approaches.\\nFinally, we describe how the approach can be combined with a regular neural\\nnetwork N-gram model and substantially improve its performance, by making its\\nlearned representation sensitive to the larger, document-specific context.\\n',\n",
       "  'title': u'Document Neural Autoregressive Distribution Estimation'},\n",
       " u'1604.03629': {'arxivid': u'1604.03629',\n",
       "  'authorsaffil': [[u'Eva L. Dyer', None],\n",
       "   [u'William Gray Roncal', None],\n",
       "   [u'Hugo L. Fernandes', None],\n",
       "   [u'Doga G\\xfcrsoy', None],\n",
       "   [u'Xianghui Xiao', None],\n",
       "   [u'Joshua T. Vogelstein', None],\n",
       "   [u'Chris Jacobsen', None],\n",
       "   [u'Konrad P. K\\xf6rding', None],\n",
       "   [u'Narayanan Kasthuri', None]],\n",
       "  'categoryterms': [u'q-bio.QM', u'cs.CV'],\n",
       "  'comment': u'24 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03629v1',\n",
       "  'published': u'2016-04-13T01:46:54Z',\n",
       "  'summary': u'  Common methods for imaging the 3D microstructure of the brain often require\\nslicing the brain, imaging these slices, and stitching the images back\\ntogether. In contrast, X-rays allow access into centimeter-thick samples\\nwithout sectioning, providing an unique and largely untapped approach for\\nproducing large 3D mesoscale brain maps. Here we demonstrate the use of\\nsynchrotron X-ray microtomography ($\\\\mu$CT) for brain science and provide a\\nmuch needed toolkit for analyzing the large datasets afforded by this approach.\\nWe introduce methods for sample preparation, imaging, and analyzing the\\nresultant neural structures. This pipeline provides methods for automated cell\\ndetection and segmentation of the vasculature, in addition to large-scale\\nanalyses of the spatial statistics of cells and blood vessels. We applied our\\nmethods to produce dense micron-scale maps of the cells and blood vessels in a\\nmillimeter-scale volume of mouse brain with $\\\\mu$CT. Our results demonstrate\\nthat X-ray tomography promises rapid reconstructions over brain large volumes,\\nin a way that is complementary to other brain mapping and connectomics efforts.\\n',\n",
       "  'title': u'Quantifying mesoscale neuroanatomy using X-ray microtomography'},\n",
       " u'1604.00933': {'arxivid': u'1604.00933',\n",
       "  'authorsaffil': [[u'Walid Shalaby', None],\n",
       "   [u'Khalifeh Al Jadda', None],\n",
       "   [u'Mohammed Korayem', None],\n",
       "   [u'Trey Grainger', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.IR'],\n",
       "  'comment': u'A short version of this paper has been accepted in \"COMPSAC 2016: The\\n  40th IEEE Computer Society International Conference on Computers, Software &\\n  Applications\"',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00933v1',\n",
       "  'published': u'2016-04-04T16:18:44Z',\n",
       "  'summary': u'  We present an ensemble approach for categorizing search query entities in the\\nrecruitment domain. Understanding the types of entities expressed in a search\\nquery (Company, Skill, Job Title, etc.) enables more intelligent information\\nretrieval based upon those entities compared to a traditional keyword-based\\nsearch. Because search queries are typically very short, leveraging a\\ntraditional bag-of-words model to identify entity types would be inappropriate\\ndue to the lack of contextual information. Our approach instead combines clues\\nfrom different sources of varying complexity in order to collect real-world\\nknowledge about query entities. We employ distributional semantic\\nrepresentations of query entities through two models: 1) contextual vectors\\ngenerated from encyclopedic corpora like Wikipedia, and 2) high dimensional\\nword embedding vectors generated from millions of job postings using word2vec.\\nAdditionally, our approach utilizes both entity linguistic properties obtained\\nfrom WordNet and ontological properties extracted from DBpedia. We evaluate our\\napproach on a data set created at CareerBuilder; the largest job board in the\\nUS. The data set contains entities extracted from millions of job\\nseekers/recruiters search queries, job postings, and resume documents. After\\nconstructing the distributional vectors of search entities, we use supervised\\nmachine learning to infer search entity types. Empirical results show that our\\napproach outperforms the state-of-the-art word2vec distributional semantics\\nmodel trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of\\n97% using the proposed distributional representations ensemble.\\n',\n",
       "  'title': u'Entity Type Recognition using an Ensemble of Distributional Semantic\\n  Models to Enhance Query Understanding'},\n",
       " u'1604.00932': {'arxivid': u'1604.00932',\n",
       "  'authorsaffil': [[u'Hubie Chen', None], [u'Benoit Larose', None]],\n",
       "  'categoryterms': [u'cs.CC', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00932v1',\n",
       "  'published': u'2016-04-04T16:18:16Z',\n",
       "  'summary': u'  The constraint satisfaction problem (CSP) involves deciding, given a set of\\nvariables and a set of constraints on the variables, whether or not there is an\\nassignment to the variables satisfying all of the constraints. One formulation\\nof the CSP is as the problem of deciding, given a pair (G,H) of relational\\nstructures, whether or not there is a homomorphism from the first structure to\\nthe second structure. The CSP is in general NP-hard; a common way to restrict\\nthis problem is to fix the second structure H, so that each structure H gives\\nrise to a problem CSP(H). The problem family CSP(H) has been studied using an\\nalgebraic approach, which links the algorithmic and complexity properties of\\neach problem CSP(H) to a set of operations, the so-called polymorphisms of H.\\nCertain types of polymorphisms are known to imply the polynomial-time\\ntractability of $CSP(H)$, and others are conjectured to do so. This article\\nsystematically studies---for various classes of polymorphisms---the\\ncomputational complexity of deciding whether or not a given structure H admits\\na polymorphism from the class. Among other results, we prove the\\nNP-completeness of deciding a condition conjectured to characterize the\\ntractable problems CSP(H), as well as the NP-completeness of deciding if CSP(H)\\nhas bounded width.\\n',\n",
       "  'title': u'Asking the metaquestions in constraint tractability'},\n",
       " u'1602.02215': {'arxivid': u'1602.02215',\n",
       "  'authorsaffil': [[u'Noam Shazeer', None],\n",
       "   [u'Ryan Doherty', None],\n",
       "   [u'Colin Evans', None],\n",
       "   [u'Chris Waterson', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'9 pages, 4 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02215v1',\n",
       "  'published': u'2016-02-06T04:39:41Z',\n",
       "  'summary': u'  We present Submatrix-wise Vector Embedding Learner (Swivel), a method for\\ngenerating low-dimensional feature embeddings from a feature co-occurrence\\nmatrix. Swivel performs approximate factorization of the point-wise mutual\\ninformation matrix via stochastic gradient descent. It uses a piecewise loss\\nwith special handling for unobserved co-occurrences, and thus makes use of all\\nthe information in the matrix. While this requires computation proportional to\\nthe size of the entire matrix, we make use of vectorized multiplication to\\nprocess thousands of rows and columns at once to compute millions of predicted\\nvalues. Furthermore, we partition the matrix into shards in order to\\nparallelize the computation across many nodes. This approach results in more\\naccurate embeddings than can be achieved with methods that consider only\\nobserved co-occurrences, and can scale to much larger corpora than can be\\nhandled with sampling methods.\\n',\n",
       "  'title': u\"Swivel: Improving Embeddings by Noticing What's Missing\"},\n",
       " u'1601.00119': {'arxivid': u'1601.00119',\n",
       "  'authorsaffil': [[u'John McKay', None],\n",
       "   [u'Raghu Raj', None],\n",
       "   [u'Vishal Monga', None],\n",
       "   [u'Jason Isaacs', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Conference Paper for Oceans 2015 Washington DC (IEEE and MTS\\n  organizers)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00119v1',\n",
       "  'published': u'2016-01-01T21:57:45Z',\n",
       "  'summary': u'  Advancements in Sonar image capture have enabled researchers to apply\\nsophisticated object identification algorithms in order to locate targets of\\ninterest in images such as mines. Despite progress in this field, modern sonar\\nautomatic target recognition (ATR) approaches lack robustness to the amount of\\nnoise one would expect in real-world scenarios, the capability to handle\\nblurring incurred from the physics of image capture, and the ability to excel\\nwith relatively few training samples. We address these challenges by adapting\\nmodern sparsity-based techniques with dictionaries comprising of training from\\neach class. We develop new discriminative (as opposed to generative) sparse\\nrepresentations which can help automatically classify targets in Sonar imaging.\\nUsing a simulated SAS data set from the Naval Surface Warfare Center (NSWC), we\\nobtained compelling classification rates for multi-class problems even in cases\\nwith considerable noise and sparsity in training samples.\\n',\n",
       "  'title': u'Discriminative Sparsity for Sonar ATR'},\n",
       " u'1603.00806': {'arxivid': u'1603.00806',\n",
       "  'authorsaffil': [[u'Florian Strub', u'SEQUEL, CRIStAL'],\n",
       "   [u'Jeremie Mary', u'CRIStAL, SEQUEL'],\n",
       "   [u'Romaric Gaudel', u'LIFL']],\n",
       "  'categoryterms': [u'cs.IR', u'cs.AI', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00806v2',\n",
       "  'published': u'2016-03-02T17:48:25Z',\n",
       "  'summary': u'  Collaborative Filtering aims at exploiting the feedback of users to provide\\npersonalised recommendations. Such algorithms look for latent variables in a\\nlarge sparse matrix of ratings. They can be enhanced by adding side information\\nto tackle the well-known cold start problem. While Neu-ral Networks have\\ntremendous success in image and speech recognition, they have received less\\nattention in Collaborative Filtering. This is all the more surprising that\\nNeural Networks are able to discover latent variables in large and\\nheterogeneous datasets. In this paper, we introduce a Collaborative Filtering\\nNeural network architecture aka CFN which computes a non-linear Matrix\\nFactorization from sparse rating inputs and side information. We show\\nexperimentally on the MovieLens and Douban dataset that CFN outper-forms the\\nstate of the art and benefits from side information. We provide an\\nimplementation of the algorithm as a reusable plugin for Torch, a popular\\nNeural Network framework.\\n',\n",
       "  'title': u'Hybrid Collaborative Filtering with Neural Networks'},\n",
       " u'1512.04077': {'arxivid': u'1512.04077',\n",
       "  'authorsaffil': [[u'Mojmir Mutny', None],\n",
       "   [u'Rahul Nair', None],\n",
       "   [u'Jens-Malte Gottfried', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.04077v2',\n",
       "  'published': u'2015-12-13T16:31:14Z',\n",
       "  'summary': u'  The Multipath effect in Time-of-Flight(ToF) cameras still remains to be a\\nchallenging problem that hinders further processing of 3D data information.\\nBased on the evidence from previous literature, we explored the possibility of\\nusing machine learning techniques to correct this effect. Firstly, we created\\ntwo new datasets of of ToF images rendered via ToF simulator of LuxRender.\\nThese two datasets contain corners in multiple orientations and with different\\nmaterial properties. We chose scenes with corners as multipath effects are most\\npronounced in corners. Secondly, we used this dataset to construct a learning\\nmodel to predict real valued corrections to the ToF data using Random Forests.\\nWe found out that in our smaller dataset we were able to predict real valued\\ncorrection and improve the quality of depth images significantly by removing\\nmultipath bias. With our algorithm, we improved relative per-pixel error from\\naverage value of 19% to 3%. Additionally, variance of the error was lowered by\\nan order of magnitude.\\n',\n",
       "  'title': u'Learning the Correction for Multi-Path Deviations in Time-of-Flight\\n  Cameras'},\n",
       " u'1603.02649': {'arxivid': u'1603.02649',\n",
       "  'authorsaffil': [[u'Aleksandar Dimitriev', None], [u'Matej Kristan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02649v1',\n",
       "  'published': u'2016-03-08T20:02:28Z',\n",
       "  'summary': u'  We propose a novel unsupervised image segmentation algorithm, which aims to\\nsegment an image into several coherent parts. It requires no user input, no\\nsupervised learning phase and assumes an unknown number of segments. It\\nachieves this by first over-segmenting the image into several hundred\\nsuperpixels. These are iteratively joined on the basis of a discriminative\\nclassifier trained on color and texture information obtained from each\\nsuperpixel. The output of the classifier is regularized by a Markov random\\nfield that lends more influence to neighbouring superpixels that are more\\nsimilar. In each iteration, similar superpixels fall under the same label,\\nuntil only a few coherent regions remain in the image. The algorithm was tested\\non a standard evaluation data set, where it performs on par with\\nstate-of-the-art algorithms in term of precision and greatly outperforms the\\nstate of the art by reducing the oversegmentation of the object of interest.\\n',\n",
       "  'title': u'A regularization-based approach for unsupervised image segmentation'},\n",
       " u'1601.05472': {'arxivid': u'1601.05472',\n",
       "  'authorsaffil': [[u'Halid Ziya Yerebakan', None],\n",
       "   [u'Fitsum Reda', None],\n",
       "   [u'Yiqiang Zhan', None],\n",
       "   [u'Yoshihisa Shinagawa', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05472v1',\n",
       "  'published': u'2016-01-20T23:31:58Z',\n",
       "  'summary': u'  This paper presents a new Bayesian non-parametric model by extending the\\nusage of Hierarchical Dirichlet Allocation to extract tree structured word\\nclusters from text data. The inference algorithm of the model collects words in\\na cluster if they share similar distribution over documents. In our\\nexperiments, we observed meaningful hierarchical structures on NIPS corpus and\\nradiology reports collected from public repositories.\\n',\n",
       "  'title': u'Hierarchical Latent Word Clustering'},\n",
       " u'1602.08141': {'arxivid': u'1602.08141',\n",
       "  'authorsaffil': [[u'Thomas Castelli', None],\n",
       "   [u'Aidean Sharghi', None],\n",
       "   [u'Don Harper', None],\n",
       "   [u'Alain Tremeau', None],\n",
       "   [u'Mubarak Shah', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08141v1',\n",
       "  'published': u'2016-02-25T22:43:14Z',\n",
       "  'summary': u\"  In recent years, consumer Unmanned Aerial Vehicles have become very popular,\\neveryone can buy and fly a drone without previous experience, which raises\\nconcern in regards to regulations and public safety. In this paper, we present\\na novel approach towards enabling safe operation of such vehicles in urban\\nareas. Our method uses geodetically accurate dataset images with Geographical\\nInformation System (GIS) data of road networks and buildings provided by Google\\nMaps, to compute a weighted A* shortest path from start to end locations of a\\nmission. Weights represent the potential risk of injuries for individuals in\\nall categories of land-use, i.e. flying over buildings is considered safer than\\nabove roads. We enable safe UAV operation in regards to 1- land-use by\\ncomputing a static global path dependent on environmental structures, and 2-\\navoiding flying over moving objects such as cars and pedestrians by dynamically\\noptimizing the path locally during the flight. As all input sources are first\\ngeo-registered, pixels and GPS coordinates are equivalent, it therefore allows\\nus to generate an automated and user-friendly mission with GPS waypoints\\nreadable by consumer drones' autopilots. We simulated 54 missions and show\\nsignificant improvement in maximizing UAV's standoff distance to moving objects\\nwith a quantified safety parameter over 40 times better than the naive straight\\nline navigation.\\n\",\n",
       "  'title': u'Autonomous navigation for low-altitude UAVs in urban areas'},\n",
       " u'1604.03468': {'arxivid': u'1604.03468',\n",
       "  'authorsaffil': [[u'Caelan Reed Garrett', None],\n",
       "   [u'Tomas Lozano-Perez', None],\n",
       "   [u'Leslie Pack Kaelbling', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.AI'],\n",
       "  'comment': u'8 pages in IEEE/RSJ International Conference on Intelligent Robots\\n  and Systems (IROS), 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03468v1',\n",
       "  'published': u'2016-04-12T16:22:29Z',\n",
       "  'summary': u'  In this paper we address planning problems in high-dimensional hybrid\\nconfiguration spaces, with a particular focus on manipulation planning problems\\ninvolving many objects. We present the hybrid backward-forward (HBF) planning\\nalgorithm that uses a backward identification of constraints to direct the\\nsampling of the infinite action space in a forward search from the initial\\nstate towards a goal configuration. The resulting planner is probabilistically\\ncomplete and can effectively construct long manipulation plans requiring both\\nprehensile and nonprehensile actions in cluttered environments.\\n',\n",
       "  'title': u'Backward-Forward Search for Manipulation Planning'},\n",
       " u'1511.06040': {'arxivid': u'1511.06040',\n",
       "  'authorsaffil': [[u'Moustafa Ibrahim', None],\n",
       "   [u'Srikanth Muralidharan', None],\n",
       "   [u'Zhiwei Deng', None],\n",
       "   [u'Arash Vahdat', None],\n",
       "   [u'Greg Mori', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'cs.cv Accepted to CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06040v2',\n",
       "  'published': u'2015-11-19T01:33:35Z',\n",
       "  'summary': u'  In group activity recognition, the temporal dynamics of the whole activity\\ncan be inferred based on the dynamics of the individual people representing the\\nactivity. We build a deep model to capture these dynamics based on LSTM\\n(long-short term memory) models. To make use of these ob- servations, we\\npresent a 2-stage deep temporal model for the group activity recognition\\nproblem. In our model, a LSTM model is designed to represent action dynamics of\\nin- dividual people in a sequence and another LSTM model is designed to\\naggregate human-level information for whole activity understanding. We evaluate\\nour model over two datasets: the collective activity dataset and a new volley-\\nball dataset. Experimental results demonstrate that our proposed model improves\\ngroup activity recognition perfor- mance with compared to baseline methods.\\n',\n",
       "  'title': u'A Hierarchical Deep Temporal Model for Group Activity Recognition'},\n",
       " u'1602.06920': {'arxivid': u'1602.06920',\n",
       "  'authorsaffil': [[u'R\\xe9mi Cura', None],\n",
       "   [u'Julien Perret', None],\n",
       "   [u'Nicolas Paparoditis', None]],\n",
       "  'categoryterms': [u'cs.CG', u'cs.CV', u'cs.SE'],\n",
       "  'comment': u'to be submitted, 18 pages, 24 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06920v2',\n",
       "  'published': u'2016-02-22T20:19:23Z',\n",
       "  'summary': u'  We propose a new paradigm to effortlessly get a portable geometric Level Of\\nDetails (LOD) for a point cloud inside a Point Cloud Server. The point cloud is\\ndivided into groups of points (patch), then each patch is reordered (MidOc\\nordering) so that reading points following this order provides more and more\\ndetails on the patch. This LOD have then multiple applications: point cloud\\nsize reduction for visualisation (point cloud streaming) or speeding of slow\\nalgorithm, fast density peak detection and correction as well as safeguard for\\nmethods that may be sensible to density variations. The LOD method also embeds\\ninformation about the sensed object geometric nature, and thus can be used as a\\ncrude multi-scale dimensionality descriptor, enabling fast classification and\\non-the-fly filtering for basic classes.\\n',\n",
       "  'title': u'Implicit LOD for processing, visualisation and classification in Point\\n  Cloud Servers'},\n",
       " u'1511.02258': {'arxivid': u'1511.02258',\n",
       "  'authorsaffil': [[u'Z. Zhang', None],\n",
       "   [u'K. Duraisamy', None],\n",
       "   [u'N. A. Gumerov', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'22 pages, 9 figures. Preprint. Submitted to Machine Learning Mar.\\n  2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.02258v2',\n",
       "  'published': u'2015-11-06T23:18:13Z',\n",
       "  'summary': u'  Standard Gaussian Process (GP) regression, a powerful machine learning tool,\\nis computationally expensive when it is applied to large datasets, and\\npotentially inaccurate when data points are sparsely distributed in a\\nhigh-dimensional feature space. To address these challenges, a new multiscale,\\nsparsified GP algorithm is formulated, with the goal of application to large\\nscientific computing datasets. In this approach, the data is partitioned into\\nclusters and the cluster centers are used to define a reduced training set,\\nresulting in an improvement over standard GPs in terms of training and\\nevaluation costs. Further, a hierarchical technique is used to adaptively map\\nthe local covariance representation to the underlying sparsity of the feature\\nspace, leading to improved prediction accuracy when the data distribution is\\nhighly non-uniform. A theoretical investigation of the computational complexity\\nof the algorithm is presented. The efficacy of this method is then demonstrated\\non smooth and discontinuous analytical functions and on data from a direct\\nnumerical simulation of turbulent combustion.\\n',\n",
       "  'title': u'Efficient Multiscale Gaussian Process Regression using Hierarchical\\n  Clustering'},\n",
       " u'1602.02950': {'arxivid': u'1602.02950',\n",
       "  'authorsaffil': [[u'Xiaohai Tian', None],\n",
       "   [u'Zhizheng Wu', None],\n",
       "   [u'Xiong Xiao', None],\n",
       "   [u'Eng Siong Chng', None],\n",
       "   [u'Haizhou Li', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.SD'],\n",
       "  'comment': u'Submitted to Odyssey: The Speaker and Language Recognition Workshop\\n  2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02950v1',\n",
       "  'published': u'2016-02-09T12:00:56Z',\n",
       "  'summary': u'  Spoofing detection for automatic speaker verification (ASV), which is to\\ndiscriminate between live speech and attacks, has received increasing\\nattentions recently. However, all the previous studies have been done on the\\nclean data without significant additive noise. To simulate the real-life\\nscenarios, we perform a preliminary investigation of spoofing detection under\\nadditive noisy conditions, and also describe an initial database for this task.\\nThe noisy database is based on the ASVspoof challenge 2015 database and\\ngenerated by artificially adding background noises at different signal-to-noise\\nratios (SNRs). Five different additive noises are included. Our preliminary\\nresults show that using the model trained from clean data, the system\\nperformance degrades significantly in noisy conditions. Phase-based feature is\\nmore noise robust than magnitude-based features. And the systems perform\\nsignificantly differ under different noise scenarios.\\n',\n",
       "  'title': u'Spoofing detection under noisy conditions: a preliminary investigation\\n  and an initial database'},\n",
       " u'1511.03677': {'arxivid': u'1511.03677',\n",
       "  'authorsaffil': [[u'Zachary C. Lipton', None],\n",
       "   [u'David C. Kale', None],\n",
       "   [u'Charles Elkan', None],\n",
       "   [u'Randall Wetzell', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03677v6',\n",
       "  'published': u'2015-11-11T21:01:28Z',\n",
       "  'summary': u\"  Clinical medical data, especially in the intensive care unit (ICU), consist\\nof multivariate time series of observations. For each patient visit (or\\nepisode), sensor data and lab test results are recorded in the patient's\\nElectronic Health Record (EHR). While potentially containing a wealth of\\ninsights, the data is difficult to mine effectively, owing to varying length,\\nirregular sampling and missing data. Recurrent Neural Networks (RNNs),\\nparticularly those using Long Short-Term Memory (LSTM) hidden units, are\\npowerful and increasingly popular models for learning from sequence data. They\\neffectively model varying length sequences and capture long range dependencies.\\nWe present the first study to empirically evaluate the ability of LSTMs to\\nrecognize patterns in multivariate time series of clinical measurements.\\nSpecifically, we consider multilabel classification of diagnoses, training a\\nmodel to classify 128 diagnoses given 13 frequently but irregularly sampled\\nclinical measurements. First, we establish the effectiveness of a simple LSTM\\nnetwork for modeling clinical data. Then we demonstrate a straightforward and\\neffective training strategy in which we replicate targets at each sequence\\nstep. Trained only on raw time series, our models outperform several strong\\nbaselines, including a multilayer perceptron trained on hand-engineered\\nfeatures.\\n\",\n",
       "  'title': u'Learning to Diagnose with LSTM Recurrent Neural Networks'},\n",
       " u'1411.2158': {'arxivid': u'1411.2158',\n",
       "  'authorsaffil': [[u'Norbert Binkiewicz', None],\n",
       "   [u'Joshua T. Vogelstein', None],\n",
       "   [u'Karl Rohe', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'math.ST', u'stat.ME', u'stat.TH'],\n",
       "  'comment': u'30 pages, updated acknowledgments',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1411.2158v4',\n",
       "  'published': u'2014-11-08T20:14:59Z',\n",
       "  'summary': u'  Biological and social systems consist of myriad interacting units. The\\ninteractions can be represented in the form of a graph or network. Measurements\\nof these graphs can reveal the underlying structure of these interactions,\\nwhich provides insight into the systems that generated the graphs. Moreover, in\\napplications such as connectomics, social networks, and genomics, graph data\\nare accompanied by contextualizing measures on each node. We utilize these node\\ncovariates to help uncover latent communities in a graph, using a modification\\nof spectral clustering. Statistical guarantees are provided under a joint\\nmixture model that we call the node-contextualized stochastic blockmodel,\\nincluding a bound on the mis-clustering rate. For most simulated conditions,\\ncovariate-assisted spectral clustering yields results superior to regularized\\nspectral clustering without node covariates and to an adaptation of canonical\\ncorrelation analysis. We apply our clustering method to large brain graphs\\nderived from diffusion MRI data, using the node locations or neurological\\nregion membership as covariates. In both cases, covariate-assisted spectral\\nclustering yields clusters that are easier to interpret neurologically.\\n',\n",
       "  'title': u'Covariate-assisted spectral clustering'},\n",
       " u'1602.04921': {'arxivid': u'1602.04921',\n",
       "  'authorsaffil': [[u'Weiyao Lin', None],\n",
       "   [u'Yang Mi', None],\n",
       "   [u'Weiyue Wang', None],\n",
       "   [u'Jianxin Wu', None],\n",
       "   [u'Jingdong Wang', None],\n",
       "   [u'Tao Mei', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.MM'],\n",
       "  'comment': u'This manuscript is the accepted version for TIP (IEEE Transactions on\\n  Image Processing), 2016',\n",
       "  'doi': u'10.1109/TIP.2016.2531281',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04921v1',\n",
       "  'published': u'2016-02-16T06:25:30Z',\n",
       "  'summary': u'  This paper addresses the problem of detecting coherent motions in crowd\\nscenes and presents its two applications in crowd scene understanding: semantic\\nregion detection and recurrent activity mining. It processes input motion\\nfields (e.g., optical flow fields) and produces a coherent motion filed, named\\nas thermal energy field. The thermal energy field is able to capture both\\nmotion correlation among particles and the motion trends of individual\\nparticles which are helpful to discover coherency among them. We further\\nintroduce a two-step clustering process to construct stable semantic regions\\nfrom the extracted time-varying coherent motions. These semantic regions can be\\nused to recognize pre-defined activities in crowd scenes. Finally, we introduce\\na cluster-and-merge process which automatically discovers recurrent activities\\nin crowd scenes by clustering and merging the extracted coherent motions.\\nExperiments on various videos demonstrate the effectiveness of our approach.\\n',\n",
       "  'title': u'A diffusion and clustering-based approach for finding coherent motions\\n  and understanding crowd scenes'},\n",
       " u'1602.04924': {'arxivid': u'1602.04924',\n",
       "  'authorsaffil': [[u'Dhruv Arya', None],\n",
       "   [u'Viet Ha-Thuc', None],\n",
       "   [u'Shakti Sinha', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.LG'],\n",
       "  'comment': u'in Proceedings of the 24th ACM International on Conference on\\n  Information and Knowledge Management (CIKM 2015)',\n",
       "  'doi': u'10.1145/2806416.2806615',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.04924v1',\n",
       "  'published': u'2016-02-16T07:10:42Z',\n",
       "  'summary': u'  LinkedIn has grown to become a platform hosting diverse sources of\\ninformation ranging from member profiles, jobs, professional groups, slideshows\\netc. Given the existence of multiple sources, when a member issues a query like\\n\"software engineer\", the member could look for software engineer profiles, jobs\\nor professional groups. To tackle this problem, we exploit a data-driven\\napproach that extracts searcher intents from their profile data and recent\\nactivities at a large scale. The intents such as job seeking, hiring, content\\nconsuming are used to construct features to personalize federated search\\nexperience. We tested the approach on the LinkedIn homepage and A/B tests show\\nsignificant improvements in member engagement. As of writing this paper, the\\napproach powers all of federated search on LinkedIn homepage.\\n',\n",
       "  'title': u'Personalized Federated Search at LinkedIn'},\n",
       " u'1508.01577': {'arxivid': u'1508.01577',\n",
       "  'authorsaffil': [[u'Javier Vera', None],\n",
       "   [u'Felipe Urbina', None],\n",
       "   [u'Eric Goles', None]],\n",
       "  'categoryterms': [u'cs.CL', u'physics.soc-ph'],\n",
       "  'comment': u'This paper has been withdrawn due one author has declined its\\n  participation',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.01577v2',\n",
       "  'published': u'2015-08-07T00:57:18Z',\n",
       "  'summary': u'  Can artificial communities of agents develop language with scaling relations\\nclose to the Zipf law? As a preliminary answer to this question, we propose an\\nAutomata Networks model of the formation of a vocabulary on a population of\\nindividuals, under two in principle opposite strategies: the alignment and the\\nleast effort principle. Within the previous account to the emergence of\\nlinguistic conventions (specially, the Naming Game), we focus on modeling\\nspeaker and hearer efforts as actions over their vocabularies and we study the\\nimpact of these actions on the formation of a shared language. The numerical\\nsimulations are essentially based on an energy function, that measures the\\namount of local agreement between the vocabularies. The results suggests that\\non one dimensional lattices the best strategy to the formation of shared\\nlanguages is the one that minimizes the efforts of speakers on communicative\\ntasks.\\n',\n",
       "  'title': u'Automata networks model for alignment and least effort on vocabulary\\n  formation'},\n",
       " u'1602.01407': {'arxivid': u'1602.01407',\n",
       "  'authorsaffil': [[u'Roger Grosse', None], [u'James Martens', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01407v2',\n",
       "  'published': u'2016-02-03T18:45:07Z',\n",
       "  'summary': u'  Second-order optimization methods such as natural gradient descent have the\\npotential to speed up training of neural networks by correcting for the\\ncurvature of the loss function. Unfortunately, the exact natural gradient is\\nimpractical to compute for large models, and most approximations either require\\nan expensive iterative procedure or make crude approximations to the curvature.\\nWe present Kronecker Factors for Convolution (KFC), a tractable approximation\\nto the Fisher matrix for convolutional networks based on a structured\\nprobabilistic model for the distribution over backpropagated derivatives.\\nSimilarly to the recently proposed Kronecker-Factored Approximate Curvature\\n(K-FAC), each block of the approximate Fisher matrix decomposes as the\\nKronecker product of small matrices, allowing for efficient inversion. KFC\\ncaptures important curvature information while still yielding comparably\\nefficient updates to stochastic gradient descent (SGD). We show that the\\nupdates are invariant to commonly used reparameterizations, such as centering\\nof the activations. In our experiments, approximate natural gradient descent\\nwith KFC was able to train convolutional networks several times faster than\\ncarefully tuned SGD. Furthermore, it was able to train the networks in 10-20\\ntimes fewer iterations than SGD, suggesting its potential applicability in a\\ndistributed setting.\\n',\n",
       "  'title': u'A Kronecker-factored approximate Fisher matrix for convolution layers'},\n",
       " u'1603.08604': {'arxivid': u'1603.08604',\n",
       "  'authorsaffil': [[u'Matthew Dixon', None],\n",
       "   [u'Diego Klabjan', None],\n",
       "   [u'Jin Hoon Bang', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08604v1',\n",
       "  'published': u'2016-03-29T01:26:04Z',\n",
       "  'summary': u'  Deep neural networks (DNNs) are powerful types of artificial neural networks\\n(ANNs) that use several hidden layers. They have recently gained considerable\\nattention in the speech transcription and image recognition community\\n(Krizhevsky et al., 2012) for their superior predictive properties including\\nrobustness to overfitting. However their application to algorithmic trading has\\nnot been previously researched, partly because of their computational\\ncomplexity. This paper describes the application of DNNs to predicting\\nfinancial market movement directions. In particular we describe the\\nconfiguration and training approach and then demonstrate their application to\\nbacktesting a simple trading strategy over 43 different Commodity and FX future\\nmid-prices at 5-minute intervals. All results in this paper are generated using\\na C++ implementation on the Intel Xeon Phi co-processor which is 11.4x faster\\nthan the serial version and a Python strategy backtesting environment both of\\nwhich are available as open source code written by the authors.\\n',\n",
       "  'title': u'Classiffication-based Financial Markets Prediction using Deep Neural\\n  Networks'},\n",
       " u'1603.02466': {'arxivid': u'1603.02466',\n",
       "  'authorsaffil': [[u'Seba Susan', None], [u'Madasu Hanmandlu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1016/j.neucom.2012.09.043',\n",
       "  'journalref': u'Neurocomputing 120 (2013): 214-225',\n",
       "  'link': u'http://arxiv.org/abs/1603.02466v1',\n",
       "  'published': u'2016-03-08T10:31:55Z',\n",
       "  'summary': u'  This paper proposes a new probabilistic non-extensive entropy feature for\\ntexture characterization, based on a Gaussian information measure. The\\nhighlights of the new entropy are that it is bounded by finite limits and that\\nit is non additive in nature. The non additive property of the proposed entropy\\nmakes it useful for the representation of information content in the\\nnon-extensive systems containing some degree of regularity or correlation. The\\neffectiveness of the proposed entropy in representing the correlated random\\nvariables is demonstrated by applying it for the texture classification problem\\nsince textures found in nature are random and at the same time contain some\\ndegree of correlation or regularity at some scale. The gray level co-occurrence\\nprobabilities (GLCP) are used for computing the entropy function. The\\nexperimental results indicate high degree of the classification accuracy. The\\nperformance of the new entropy function is found superior to other forms of\\nentropy such as Shannon, Renyi, Tsallis and Pal and Pal entropies on\\ncomparison. Using the feature based polar interaction maps (FBIM) the proposed\\nentropy is shown to be the best measure among the entropies compared for\\nrepresenting the correlated textures.\\n',\n",
       "  'title': u'A non-extensive entropy feature and its application to texture\\n  classification'},\n",
       " u'1511.03814': {'arxivid': u'1511.03814',\n",
       "  'authorsaffil': [[u'Amir Rosenfeld', None], [u'Shimon Ullman', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Minor changes: title and abstract',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03814v2',\n",
       "  'published': u'2015-11-12T08:41:47Z',\n",
       "  'summary': u'  Action recognition in still images has seen major improvement in recent years\\ndue to advances in human pose estimation, object recognition and stronger\\nfeature representations produced by deep neural networks. However, there are\\nstill many cases in which performance remains far from that of humans. A major\\ndifficulty arises in distinguishing between transitive actions in which the\\noverall actor pose is similar, and recognition therefore depends on details of\\nthe grasp and the object, which may be largely occluded. In this paper we\\ndemonstrate how recognition is improved by obtaining precise localization of\\nthe action-object and consequently extracting details of the object shape\\ntogether with the actor-object interaction. To obtain exact localization of the\\naction object and its interaction with the actor, we employ a coarse-to-fine\\napproach which combines semantic segmentation and contextual features, in\\nsuccessive stages. We focus on (but are not limited) to face-related actions, a\\nset of actions that includes several currently challenging categories. We\\npresent an average relative improvement of 35% over state-of-the art and\\nvalidate through experimentation the effectiveness of our approach.\\n',\n",
       "  'title': u'Hand-Object Interaction and Precise Localization in Transitive Action\\n  Recognition'},\n",
       " u'1604.02477': {'arxivid': u'1604.02477',\n",
       "  'authorsaffil': [[u'Lorenzo Livi', None], [u'Cesare Alippi', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'cs.IT', u'math.IT'],\n",
       "  'comment': u'Extended and revised version of the paper \"One-Class Classification\\n  Through Mutual Information Minimization\", to appear in 2016 IEEE IJCNN,\\n  Vancouver, Canada',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02477v1',\n",
       "  'published': u'2016-04-08T20:41:54Z',\n",
       "  'summary': u'  One-class classifiers offer valuable tools to assess the presence of outliers\\nin data. In this paper, we propose a design methodology for one-class\\nclassifiers based on entropic spanning graphs. The spanning graph is learned on\\nthe embedded input data, with the aim to generate a partition of the vertices.\\nThe final partition is derived by exploiting a criterion based on mutual\\ninformation minimization. Here, we compute the mutual information by using a\\nconvenient formulation provided in terms of the $\\\\alpha$-Jensen difference.\\nOnce training is completed, in order to associate a confidence level with the\\nclassifier decision, a graph-based fuzzy model is constructed. The\\nfuzzification process is based only on topological information of the vertices\\nof the entropic spanning graph. As such, the proposed one-class classifier is\\nsuitable also for datasets with complex geometric structures. We provide\\nexperiments on well-known benchmarking datasets containing both feature vectors\\nand labeled graphs. In addition, we apply the method on the problem of protein\\nsolubility recognition by considering several data representations for the\\nsamples. Experimental results demonstrate the effectiveness and versatility of\\nthe proposed method with respect to other state-of-the-art approaches.\\n',\n",
       "  'title': u'One-class classifiers based on entropic spanning graphs'},\n",
       " u'1604.00162': {'arxivid': u'1604.00162',\n",
       "  'authorsaffil': [[u'Jesse Heyninck', None], [u'Christian Stra\\xdfer', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LO', u'68T27', u'I.2.3; I.2.4'],\n",
       "  'comment': u\"Contribution to the 16th International Workshop on Non-Monotonic\\n  Reasoning (NMR'16), Cape Town\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00162v1',\n",
       "  'published': u'2016-04-01T08:14:30Z',\n",
       "  'summary': u\"  In this paper we make a contribution to the unification of formal models of\\ndefeasible reasoning. We present several translations between formal\\nargumentation frameworks and nonmonotonic logics for reasoning with plausible\\nassumptions. More specifically, we translate adaptive logics into\\nassumption-based argumentation and ASPIC+, ASPIC+ into assumption-based\\nargumentation and a fragment of assumption-based argumentation into adaptive\\nlogics. Adaptive logics are closely related to Makinson's default assumptions\\nand to a significant class of systems within the tradition of preferential\\nsemantics in the vein of KLM and Shoham. Thus, our results also provide close\\nlinks between formal argumentation and the latter approaches.\\n\",\n",
       "  'title': u'Relations between assumption-based approaches in nonmonotonic logic and\\n  formal argumentation'},\n",
       " u'1512.00177': {'arxivid': u'1512.00177',\n",
       "  'authorsaffil': [[u'Yiming Cui', None],\n",
       "   [u'Shijin Wang', None],\n",
       "   [u'Jianfeng Li', None],\n",
       "   [u'Yuguang Wang', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.NE'],\n",
       "  'comment': u'6 pages, withdrawn by the author due to a error in formula',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.00177v2',\n",
       "  'published': u'2015-12-01T08:43:19Z',\n",
       "  'summary': u'  Artificial neural networks are powerful models, which have been widely\\napplied into many aspects of machine translation, such as language modeling and\\ntranslation modeling. Though notable improvements have been made in these\\nareas, the reordering problem still remains a challenge in statistical machine\\ntranslations. In this paper, we present a novel neural reordering model that\\ndirectly models word pairs and alignment. By utilizing LSTM recurrent neural\\nnetworks, much longer context could be learned for reordering prediction.\\nExperimental results on NIST OpenMT12 Arabic-English and Chinese-English\\n1000-best rescoring task show that our LSTM neural reordering feature is robust\\nand achieves significant improvements over various baseline systems.\\n',\n",
       "  'title': u'LSTM Neural Reordering Feature for Statistical Machine Translation'},\n",
       " u'1511.05635': {'arxivid': u'1511.05635',\n",
       "  'authorsaffil': [[u'Zhibin Liao', None], [u'Gustavo Carneiro', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05635v1',\n",
       "  'published': u'2015-11-18T01:19:00Z',\n",
       "  'summary': u'  In this paper, we introduce a new deep convolutional neural network (ConvNet)\\nmodule that promotes competition among a set of multi-scale convolutional\\nfilters. This new module is inspired by the inception module, where we replace\\nthe original collaborative pooling stage (consisting of a concatenation of the\\nmulti-scale filter outputs) by a competitive pooling represented by a maxout\\nactivation unit. This extension has the following two objectives: 1) the\\nselection of the maximum response among the multi-scale filters prevents filter\\nco-adaptation and allows the formation of multiple sub-networks within the same\\nmodel, which has been shown to facilitate the training of complex learning\\nproblems; and 2) the maxout unit reduces the dimensionality of the outputs from\\nthe multi-scale filters. We show that the use of our proposed module in typical\\ndeep ConvNets produces classification results that are either better than or\\ncomparable to the state of the art on the following benchmark datasets: MNIST,\\nCIFAR-10, CIFAR-100 and SVHN.\\n',\n",
       "  'title': u'Competitive Multi-scale Convolution'},\n",
       " u'1601.00022': {'arxivid': u'1601.00022',\n",
       "  'authorsaffil': [[u'Hongzhi Li', None],\n",
       "   [u'Joseph G. Ellis', None],\n",
       "   [u'Shih-Fu Chang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00022v2',\n",
       "  'published': u'2015-12-31T22:14:22Z',\n",
       "  'summary': u'  In this paper we describe a novel framework and algorithms for discovering\\nimage patch patterns from a large corpus of weakly supervised image-caption\\npairs generated from news events. Current pattern mining techniques attempt to\\nfind patterns that are representative and discriminative, we stipulate that our\\ndiscovered patterns must also be recognizable by humans and preferably with\\nmeaningful names. We propose a new multimodal pattern mining approach that\\nleverages the descriptive captions often accompanying news images to learn\\nsemantically meaningful image patch patterns. The mutltimodal patterns are then\\nnamed using words mined from the associated image captions for each pattern. A\\nnovel evaluation framework is provided that demonstrates our patterns are 26.2%\\nmore semantically meaningful than those discovered by the state of the art\\nvision only pipeline, and that we can provide tags for the discovered images\\npatches with 54.5% accuracy with no direct supervision. Our methods also\\ndiscover named patterns beyond those covered by the existing image datasets\\nlike ImageNet. To the best of our knowledge this is the first algorithm\\ndeveloped to automatically mine image patch patterns that have strong semantic\\nmeaning specific to high-level news events, and then evaluate these patterns\\nbased on that criteria.\\n',\n",
       "  'title': u'Event Specific Multimodal Pattern Mining with Image-Caption Pairs'},\n",
       " u'1508.05328': {'arxivid': u'1508.05328',\n",
       "  'authorsaffil': [[u'Luowei Zhou', None],\n",
       "   [u'Pei Yang', None],\n",
       "   [u'Chunlin Chen', None],\n",
       "   [u'Yang Gao', None]],\n",
       "  'categoryterms': [u'cs.MA', u'cs.AI'],\n",
       "  'comment': u'13 pages, 15 figures',\n",
       "  'doi': u'10.1109/TCYB.2016.2543238',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.05328v2',\n",
       "  'published': u'2015-08-21T16:30:25Z',\n",
       "  'summary': u'  Reinforcement learning has significant applications for multi-agent systems,\\nespecially in unknown dynamic environments. However, most multi-agent\\nreinforcement learning (MARL) algorithms suffer from such problems as\\nexponential computation complexity in the joint state-action space, which makes\\nit difficult to scale up to realistic multi-agent problems. In this paper, a\\nnovel algorithm named negotiation-based MARL with sparse interactions (NegoSI)\\nis presented. In contrast to traditional sparse-interaction based MARL\\nalgorithms, NegoSI adopts the equilibrium concept and makes it possible for\\nagents to select the non-strict Equilibrium Dominating Strategy Profile\\n(non-strict EDSP) or Meta equilibrium for their joint actions. The presented\\nNegoSI algorithm consists of four parts: the equilibrium-based framework for\\nsparse interactions, the negotiation for the equilibrium set, the minimum\\nvariance method for selecting one joint action and the knowledge transfer of\\nlocal Q-values. In this integrated algorithm, three techniques, i.e., unshared\\nvalue functions, equilibrium solutions and sparse interactions are adopted to\\nachieve privacy protection, better coordination and lower computational\\ncomplexity, respectively. To evaluate the performance of the presented NegoSI\\nalgorithm, two groups of experiments are carried out regarding three criteria:\\nsteps of each episode (SEE), rewards of each episode (REE) and average runtime\\n(AR). The first group of experiments is conducted using six grid world games\\nand shows fast convergence and high scalability of the presented algorithm.\\nThen in the second group of experiments NegoSI is applied to an intelligent\\nwarehouse problem and simulated results demonstrate the effectiveness of the\\npresented NegoSI algorithm compared with other state-of-the-art MARL\\nalgorithms.\\n',\n",
       "  'title': u'Multi-agent Reinforcement Learning with Sparse Interactions by\\n  Negotiation and Knowledge Transfer'},\n",
       " u'1601.00027': {'arxivid': u'1601.00027',\n",
       "  'authorsaffil': [[u'Thomas J. Fuchs', None], [u'Joachim M. Buhmann', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1016/j.compmedimag.2011.02.006',\n",
       "  'journalref': u'Computerized Medical Imaging and Graphics, vol. 35, 7-8, p.\\n  515-530, 2011',\n",
       "  'link': u'http://arxiv.org/abs/1601.00027v1',\n",
       "  'published': u'2015-12-31T22:33:44Z',\n",
       "  'summary': u'  The histological assessment of human tissue has emerged as the key challenge\\nfor detection and treatment of cancer. A plethora of different data sources\\nranging from tissue microarray data to gene expression, proteomics or\\nmetabolomics data provide a detailed overview of the health status of a\\npatient. Medical doctors need to assess these information sources and they rely\\non data driven automatic analysis tools. Methods for classification, grouping\\nand segmentation of heterogeneous data sources as well as regression of noisy\\ndependencies and estimation of survival probabilities enter the processing\\nworkflow of a pathology diagnosis system at various stages. This paper reports\\non state-of-the-art of the design and effectiveness of computational pathology\\nworkflows and it discusses future research directions in this emergent field of\\nmedical informatics and diagnostic machine learning.\\n',\n",
       "  'title': u'Computational Pathology: Challenges and Promises for Tissue Analysis'},\n",
       " u'1604.03540': {'arxivid': u'1604.03540',\n",
       "  'authorsaffil': [[u'Abhinav Shrivastava', None],\n",
       "   [u'Abhinav Gupta', None],\n",
       "   [u'Ross Girshick', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'To appear in Proceedings of IEEE Conference on Computer Vision and\\n  Pattern Recognition (CVPR), 2016. (oral)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03540v1',\n",
       "  'published': u'2016-04-12T19:44:13Z',\n",
       "  'summary': u'  The field of object detection has made significant advances riding on the\\nwave of region-based ConvNets, but their training procedure still includes many\\nheuristics and hyperparameters that are costly to tune. We present a simple yet\\nsurprisingly effective online hard example mining (OHEM) algorithm for training\\nregion-based ConvNet detectors. Our motivation is the same as it has always\\nbeen -- detection datasets contain an overwhelming number of easy examples and\\na small number of hard examples. Automatic selection of these hard examples can\\nmake training more effective and efficient. OHEM is a simple and intuitive\\nalgorithm that eliminates several heuristics and hyperparameters in common use.\\nBut more importantly, it yields consistent and significant boosts in detection\\nperformance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness\\nincreases as datasets become larger and more difficult, as demonstrated by the\\nresults on the MS COCO dataset. Moreover, combined with complementary advances\\nin the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on\\nPASCAL VOC 2007 and 2012 respectively.\\n',\n",
       "  'title': u'Training Region-based Object Detectors with Online Hard Example Mining'},\n",
       " u'1603.06141': {'arxivid': u'1603.06141',\n",
       "  'authorsaffil': [[u'Joshua Brul\\xe9', None],\n",
       "   [u'Kevin Engel', None],\n",
       "   [u'Nick Fung', None],\n",
       "   [u'Isaac Julien', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06141v1',\n",
       "  'published': u'2016-03-19T20:36:44Z',\n",
       "  'summary': u\"  We apply genetic programming techniques to the `shepherding' problem, in\\nwhich a group of one type of animal (sheep dogs) attempts to control the\\nmovements of a second group of animals (sheep) obeying flocking behavior. Our\\ngenetic programming algorithm evolves an expression tree that governs the\\nmovements of each dog. The operands of the tree are hand-selected features of\\nthe simulation environment that may allow the dogs to herd the sheep\\neffectively. The algorithm uses tournament-style selection, crossover\\nreproduction, and a point mutation. We find that the evolved solutions\\ngeneralize well and outperform a (naive) human-designed algorithm.\\n\",\n",
       "  'title': u'Evolving Shepherding Behavior with Genetic Programming Algorithms'},\n",
       " u'1603.06140': {'arxivid': u'1603.06140',\n",
       "  'authorsaffil': [[u'Brendan Alvey', None],\n",
       "   [u'Alina Zare', None],\n",
       "   [u'Matthew Cook', None],\n",
       "   [u'Dominic K. Ho', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Proceedings of the SPIE, 2016, Corrected reference formatting and\\n  figure',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06140v3',\n",
       "  'published': u'2016-03-19T20:33:50Z',\n",
       "  'summary': u'  The adaptive coherence estimator (ACE) estimates the squared cosine of the\\nangle between a known target vector and a sample vector in a whitened\\ncoordinate space. The space is whitened according to an estimation of the\\nbackground statistics, which directly effects the performance of the statistic\\nas a target detector. In this paper, the ACE detection statistic is used to\\ndetect buried explosive hazards with data from a Wideband Electromagnetic\\nInduction (WEMI) sensor. Target signatures are based on a dictionary defined\\nusing a Discrete Spectrum of Relaxation Frequencies (DSRF) model. Results are\\nsummarized as a receiver operator curve (ROC) and compared to other leading\\nmethods.\\n',\n",
       "  'title': u'Adaptive coherence estimator (ACE) for explosive hazard detection using\\n  wideband electromagnetic induction (WEMI)'},\n",
       " u'1603.06143': {'arxivid': u'1603.06143',\n",
       "  'authorsaffil': [[u'Daniel Ritchie', None],\n",
       "   [u'Anna Thomas', None],\n",
       "   [u'Pat Hanrahan', None],\n",
       "   [u'Noah D. Goodman', None]],\n",
       "  'categoryterms': [u'cs.GR', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06143v1',\n",
       "  'published': u'2016-03-19T20:58:47Z',\n",
       "  'summary': u'  We present a deep learning approach for speeding up constrained procedural\\nmodeling. Probabilistic inference algorithms such as Sequential Monte Carlo\\n(SMC) provide powerful tools for constraining procedural models, but they\\nrequire many samples to produce desirable results. In this paper, we show how\\nto create procedural models which learn how to satisfy constraints. We augment\\nprocedural models with neural networks: these networks control how the model\\nmakes random choices based on what output it has generated thus far. We call\\nsuch a model a neurally-guided procedural model. As a pre-computation, we train\\nthese models on constraint-satisfying example outputs generated via SMC. They\\nare then used as efficient importance samplers for SMC, generating high-quality\\nresults with very few samples. We evaluate our method on L-system-like models\\nwith image-based constraints. Given a desired quality threshold,\\nneurally-guided models can generate satisfactory results up to 10x faster than\\nunguided models.\\n',\n",
       "  'title': u'Neurally-Guided Procedural Models: Learning to Guide Procedural Models\\n  with Deep Neural Networks'},\n",
       " u'1602.07480': {'arxivid': u'1602.07480',\n",
       "  'authorsaffil': [[u'Lluis Gomez', None],\n",
       "   [u'Anguelos Nicolaou', None],\n",
       "   [u'Dimosthenis Karatzas', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07480v1',\n",
       "  'published': u'2016-02-24T12:33:25Z',\n",
       "  'summary': u'  This paper focuses on the problem of script identification in scene text\\nimages. Facing this problem with state of the art CNN classifiers is not\\nstraightforward, as they fail to address a key characteristic of scene text\\ninstances: their extremely variable aspect ratio. Instead of resizing input\\nimages to a fixed aspect ratio as in the typical use of holistic CNN\\nclassifiers, we propose here a patch-based classification framework in order to\\npreserve discriminative parts of the image that are characteristic of its\\nclass.\\n  We describe a novel method based on the use of ensembles of conjoined\\nnetworks to jointly learn discriminative stroke-parts representations and their\\nrelative importance in a patch-based classification scheme. Our experiments\\nwith this learning procedure demonstrate state-of-the-art results in two public\\nscript identification datasets.\\n  In addition, we propose a new public benchmark dataset for the evaluation of\\nmulti-lingual scene text end-to-end reading systems. Experiments done in this\\ndataset demonstrate the key role of script identification in a complete\\nend-to-end system that combines our script identification method with a\\npreviously published text detector and an off-the-shelf OCR engine.\\n',\n",
       "  'title': u'Boosting patch-based scene text script identification with ensembles of\\n  conjoined networks'},\n",
       " u'1603.06147': {'arxivid': u'1603.06147',\n",
       "  'authorsaffil': [[u'Junyoung Chung', None],\n",
       "   [u'Kyunghyun Cho', None],\n",
       "   [u'Yoshua Bengio', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06147v2',\n",
       "  'published': u'2016-03-19T21:35:04Z',\n",
       "  'summary': u\"  The existing machine translation systems, whether phrase-based or neural,\\nhave relied almost exclusively on word-level modelling with explicit\\nsegmentation. In this paper, we ask a fundamental question: can neural machine\\ntranslation generate a character sequence without any explicit segmentation? To\\nanswer this question, we evaluate an attention-based encoder-decoder with a\\nsubword-level encoder and a character-level decoder on four language\\npairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.\\nOur experiments show that the models with a character-level decoder outperform\\nthe ones with a subword-level decoder on all of the four language pairs.\\nFurthermore, the ensembles of neural models with a character-level decoder\\noutperform the state-of-the-art non-neural machine translation systems on\\nEn-Cs, En-De and En-Fi and perform comparably on En-Ru.\\n\",\n",
       "  'title': u'A Character-level Decoder without Explicit Segmentation for Neural\\n  Machine Translation'},\n",
       " u'1402.4303': {'arxivid': u'1402.4303',\n",
       "  'authorsaffil': [[u'Christian Geist', None]],\n",
       "  'categoryterms': [u'cs.MA', u'cs.AI', u'cs.LO'],\n",
       "  'comment': u'Corrected typos, updated references, and added conclusion',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1402.4303v2',\n",
       "  'published': u'2014-02-18T11:31:08Z',\n",
       "  'summary': u'  Condorcet winning sets are a set-valued generalization of the well-known\\nconcept of a Condorcet winner. As supersets of Condorcet winning sets are\\nalways Condorcet winning sets themselves, an interesting property of preference\\nprofiles is the size of the smallest Condorcet winning set they admit. This\\nsmallest size is called the Condorcet dimension of a preference profile. Since\\nlittle is known about profiles that have a certain Condorcet dimension, we show\\nin this paper how the problem of finding a preference profile that has a given\\nCondorcet dimension can be encoded as a satisfiability problem and solved by a\\nSAT solver. Initial results include a minimal example of a preference profile\\nof Condorcet dimension 3, improving previously known examples both in terms of\\nthe number of agents as well as alternatives. Due to the high complexity of\\nsuch problems it remains open whether a preference profile of Condorcet\\ndimension 4 exists.\\n',\n",
       "  'title': u'Finding Preference Profiles of Condorcet Dimension $k$ via SAT'},\n",
       " u'1603.06288': {'arxivid': u'1603.06288',\n",
       "  'authorsaffil': [[u'Kirthevasan Kandasamy', None],\n",
       "   [u'Gautam Dasarathy', None],\n",
       "   [u'Junier B. Oliva', None],\n",
       "   [u'Jeff Schneider', None],\n",
       "   [u'Barnabas Poczos', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.AI', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06288v1',\n",
       "  'published': u'2016-03-20T22:58:43Z',\n",
       "  'summary': u'  In many scientific and engineering applications, we are tasked with the\\noptimisation of an expensive to evaluate black box function $f$. Traditional\\nmethods for this problem assume just the availability of this single function.\\nHowever, in many cases, cheap approximations to $f$ may be obtainable. For\\nexample, the expensive real world behaviour of a robot can be approximated by a\\ncheap computer simulation. We can use these approximations to eliminate low\\nfunction value regions and use the expensive evaluations to $f$ in a small\\npromising region and speedily identify the optimum. We formalise this task as a\\n\\\\emph{multi-fidelity} bandit problem where the target function and its\\napproximations are sampled from a Gaussian process. We develop a method based\\non upper confidence bound techniques and prove that it exhibits precisely the\\nabove behaviour, hence achieving better regret than strategies which ignore\\nmulti-fidelity information. Our method outperforms such naive strategies on\\nseveral synthetic and real experiments.\\n',\n",
       "  'title': u'Multi-fidelity Gaussian Process Bandit Optimisation'},\n",
       " u'1509.05142': {'arxivid': u'1509.05142',\n",
       "  'authorsaffil': [[u'Sourish Das', None],\n",
       "   [u'Sasanka Roy', None],\n",
       "   [u'Rajiv Sambasivan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.05142v4',\n",
       "  'published': u'2015-09-17T06:18:08Z',\n",
       "  'summary': u'  Gaussian Processes are widely used for regression tasks. A known limitation\\nin the application of Gaussian Processes to regression tasks is that the\\ncomputation of the solution requires performing a matrix inversion. The\\nsolution also requires the storage of a large matrix in memory. These factors\\nrestrict the application of Gaussian Process regression to small and moderate\\nsize data sets. We present an algorithm based on empirically determined subset\\nselection that works well on both real world and synthetic datasets. We also\\ncompare the performance of this algorithm with two other methods that are used\\nto apply Gaussian Processes Regression on large datasets. On the synthetic and\\nreal world datasets used in this study, the algorithm demonstrated sub-linear\\ntime and space complexity. The accuracy obtained with this algorithm on the\\ndatasets used for this study is comparable to what is achieved with the two\\nother methods commonly used to apply Gaussian Processes to large datasets.\\n',\n",
       "  'title': u'Fast Gaussian Process Regression for Big Data'},\n",
       " u'1510.07945': {'arxivid': u'1510.07945',\n",
       "  'authorsaffil': [[u'Hyeonseob Nam', None], [u'Bohyung Han', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.07945v2',\n",
       "  'published': u'2015-10-27T15:53:00Z',\n",
       "  'summary': u'  We propose a novel visual tracking algorithm based on the representations\\nfrom a discriminatively trained Convolutional Neural Network (CNN). Our\\nalgorithm pretrains a CNN using a large set of videos with tracking\\nground-truths to obtain a generic target representation. Our network is\\ncomposed of shared layers and multiple branches of domain-specific layers,\\nwhere domains correspond to individual training sequences and each branch is\\nresponsible for binary classification to identify the target in each domain. We\\ntrain the network with respect to each domain iteratively to obtain generic\\ntarget representations in the shared layers. When tracking a target in a new\\nsequence, we construct a new network by combining the shared layers in the\\npretrained CNN with a new binary classification layer, which is updated online.\\nOnline tracking is performed by evaluating the candidate windows randomly\\nsampled around the previous target state. The proposed algorithm illustrates\\noutstanding performance compared with state-of-the-art methods in existing\\ntracking benchmarks.\\n',\n",
       "  'title': u'Learning Multi-Domain Convolutional Neural Networks for Visual Tracking'},\n",
       " u'1604.03136': {'arxivid': u'1604.03136',\n",
       "  'authorsaffil': [[u'Arnav Sharma', None],\n",
       "   [u'Sakshi Gupta', None],\n",
       "   [u'Raveesh Motlani', None],\n",
       "   [u'Piyush Bansal', None],\n",
       "   [u'Manish Srivastava', None],\n",
       "   [u'Radhika Mamidi', None],\n",
       "   [u'Dipti M. Sharma', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03136v1',\n",
       "  'published': u'2016-04-11T20:24:52Z',\n",
       "  'summary': u'  In this study, the problem of shallow parsing of Hindi-English code-mixed\\nsocial media text (CSMT) has been addressed. We have annotated the data,\\ndeveloped a language identifier, a normalizer, a part-of-speech tagger and a\\nshallow parser. To the best of our knowledge, we are the first to attempt\\nshallow parsing on CSMT. The pipeline developed has been made available to the\\nresearch community with the goal of enabling better text analysis of Hindi\\nEnglish CSMT. The pipeline is accessible at http://bit.ly/csmt-parser-api .\\n',\n",
       "  'title': u'Shallow Parsing Pipeline for Hindi-English Code-Mixed Social Media Text'},\n",
       " u'1511.05389': {'arxivid': u'1511.05389',\n",
       "  'authorsaffil': [[u'Imran Sheikh', None],\n",
       "   [u'Irina Illina', None],\n",
       "   [u'Dominique Fohr', None],\n",
       "   [u'Georges Linar\\xe8s', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Updated references, added appendix discussing more results; added\\n  more discussion, replaced simple phone search results with KWS results; added\\n  KWS results for both training phase, probably last update',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05389v4',\n",
       "  'published': u'2015-11-17T13:18:07Z',\n",
       "  'summary': u'  Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech\\nrecognition systems used to process diachronic audio data. To help recovery of\\nthe PNs missed by the system, relevant OOV PNs can be retrieved out of the many\\nOOVs by exploiting semantic context of the spoken content. In this paper, we\\npropose two neural network models targeted to retrieve OOV PNs relevant to an\\naudio document: (a) Document level Continuous Bag of Words (D-CBOW), (b)\\nDocument level Continuous Bag of Weighted Words (D-CBOW2). Both these models\\ntake document words as input and learn with an objective to maximise the\\nretrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a new\\napproach in which the input embedding layer is augmented with a context anchor\\nlayer. This layer learns to assign importance to input words and has the\\nability to capture (task specific) key-words in a bag-of-word neural network\\nmodel. With experiments on French broadcast news videos we show that these two\\nmodels outperform the baseline methods based on raw embeddings from LDA,\\nSkip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models gives\\nfaster convergence during training.\\n',\n",
       "  'title': u'Learning to retrieve out-of-vocabulary words in speech recognition'},\n",
       " u'1603.06059': {'arxivid': u'1603.06059',\n",
       "  'authorsaffil': [[u'Nasrin Mostafazadeh', None],\n",
       "   [u'Ishan Misra', None],\n",
       "   [u'Jacob Devlin', None],\n",
       "   [u'Margaret Mitchell', None],\n",
       "   [u'Xiaodong He', None],\n",
       "   [u'Lucy Vanderwende', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.CV'],\n",
       "  'comment': u'Proceedings of the 54th Annual Meeting of the Association for\\n  Computational Linguistics',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06059v3',\n",
       "  'published': u'2016-03-19T07:27:15Z',\n",
       "  'summary': u'  There has been an explosion of work in the vision & language community during\\nthe past few years from image captioning to video transcription, and answering\\nquestions about images. These tasks have focused on literal descriptions of the\\nimage. To move beyond the literal, we choose to explore how questions about an\\nimage are often directed at commonsense inference and the abstract events\\nevoked by objects in the image. In this paper, we introduce the novel task of\\nVisual Question Generation (VQG), where the system is tasked with asking a\\nnatural and engaging question when shown an image. We provide three datasets\\nwhich cover a variety of images from object-centric to event-centric, with\\nconsiderably more abstract training data than provided to state-of-the-art\\ncaptioning systems thus far. We train and test several generative and retrieval\\nmodels to tackle the task of VQG. Evaluation results show that while such\\nmodels ask reasonable questions for a variety of images, there is still a wide\\ngap with human performance which motivates further work on connecting images\\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\\nchallenge to the community which we hope furthers interest in exploring deeper\\nconnections between vision & language.\\n',\n",
       "  'title': u'Generating Natural Questions About an Image'},\n",
       " u'1512.05135': {'arxivid': u'1512.05135',\n",
       "  'authorsaffil': [[u'Byunghan Lee', None],\n",
       "   [u'Taehoon Lee', None],\n",
       "   [u'Byunggook Na', None],\n",
       "   [u'Sungroh Yoon', None]],\n",
       "  'categoryterms': [u'cs.LG', u'q-bio.GN'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05135v1',\n",
       "  'published': u'2015-12-16T11:41:00Z',\n",
       "  'summary': u'  A eukaryotic gene consists of multiple exons (protein coding regions) and\\nintrons (non-coding regions), and a splice junction refers to the boundary\\nbetween a pair of exon and intron. Precise identification of spice junctions on\\na gene is important for deciphering its primary structure, function, and\\ninteraction. Experimental techniques for determining exon/intron boundaries\\ninclude RNA-seq, which is often accompanied by computational approaches.\\nCanonical splicing signals are known, but computational junction prediction\\nstill remains challenging because of a large number of false positives and\\nother complications. In this paper, we exploit deep recurrent neural networks\\n(RNNs) to model DNA sequences and to detect splice junctions thereon. We test\\nvarious RNN units and architectures including long short-term memory units,\\ngated recurrent units, and recently proposed iRNN for in-depth design space\\nexploration. According to our experimental results, the proposed approach\\nsignificantly outperforms not only conventional machine learning-based methods\\nbut also a recent state-of-the-art deep belief network-based technique in terms\\nof prediction accuracy.\\n',\n",
       "  'title': u'DNA-Level Splice Junction Prediction using Deep Recurrent Neural\\n  Networks'},\n",
       " u'1411.2738': {'arxivid': u'1411.2738',\n",
       "  'authorsaffil': [[u'Xin Rong', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1411.2738v4',\n",
       "  'published': u'2014-11-11T09:24:00Z',\n",
       "  'summary': u'  The word2vec model and application by Mikolov et al. have attracted a great\\namount of attention in recent two years. The vector representations of words\\nlearned by word2vec models have been shown to carry semantic meanings and are\\nuseful in various NLP tasks. As an increasing number of researchers would like\\nto experiment with word2vec or similar techniques, I notice that there lacks a\\nmaterial that comprehensively explains the parameter learning process of word\\nembedding models in details, thus preventing researchers that are non-experts\\nin neural networks from understanding the working mechanism of such models.\\n  This note provides detailed derivations and explanations of the parameter\\nupdate equations of the word2vec models, including the original continuous\\nbag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization\\ntechniques, including hierarchical softmax and negative sampling. Intuitive\\ninterpretations of the gradient equations are also provided alongside\\nmathematical derivations.\\n  In the appendix, a review on the basics of neuron networks and\\nbackpropagation is provided. I also created an interactive demo, wevi, to\\nfacilitate the intuitive understanding of the model.\\n',\n",
       "  'title': u'word2vec Parameter Learning Explained'},\n",
       " u'1604.00980': {'arxivid': u'1604.00980',\n",
       "  'authorsaffil': [[u'Mohd Anuar Mat Isa', None],\n",
       "   [u'Ramlan Mahmod', None],\n",
       "   [u'Nur Izura Udzir', None],\n",
       "   [u'Jamalul-lail Ab Manan', None],\n",
       "   [u'Ali Dehghan Tanha', None]],\n",
       "  'categoryterms': [u'cs.CR', u'cs.AI'],\n",
       "  'comment': u'Keywords Trust Algebra, International Relations, Trust Computation,\\n  Foreign Policy, Politics, Dempster-Shafer, Subjective Logic, Common Criteria,\\n  Defense, National Security, Terrorism, Counter Terrorism, Trust Perception,\\n  Foreign Ministry, Intelligent',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00980v1',\n",
       "  'published': u'2016-02-13T05:26:08Z',\n",
       "  'summary': u'  This paper presents a trust computation for international relations and its\\ncalculus, which related to Bayesian inference, Dempster Shafer theory and\\nsubjective logic. We proposed a method that allows a trust computation which is\\npreviously subjective and incomputable. An example of case study for the trust\\ncomputation is the United States of America Great Britain relations. The method\\nsupports decision makers in a government such as foreign ministry, defense\\nministry, presidential or prime minister office. The Department of Defense\\n(DoD) may use our method to determine a nation that can be known as a friendly,\\nneutral or hostile nation.\\n',\n",
       "  'title': u'A Mathematical Trust Algebra for International Nation Relations\\n  Computation and Evaluation'},\n",
       " u'1603.07454': {'arxivid': u'1603.07454',\n",
       "  'authorsaffil': [[u'Chao Ma', None],\n",
       "   [u' Tianchenghou', None],\n",
       "   [u'Bin Lan', None],\n",
       "   [u'Jinhui Xu', None],\n",
       "   [u'Zhenhua Zhang', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'20 pages, 9 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07454v1',\n",
       "  'published': u'2016-03-24T07:12:20Z',\n",
       "  'summary': u\"  In this paper, we present Deep Extreme Feature Extraction (DEFE), a new\\nensemble MVA method for searching $\\\\tau^{+}\\\\tau^{-}$ channel of Higgs bosons in\\nhigh energy physics. DEFE can be viewed as a deep ensemble learning scheme that\\ntrains a strongly diverse set of neural feature learners without explicitly\\nencouraging diversity and penalizing correlations. This is achieved by adopting\\nan implicit neural controller (not involved in feedforward compuation) that\\ndirectly controls and distributes gradient flows from higher level deep\\nprediction network. Such model-independent controller results in that every\\nsingle local feature learned are used in the feature-to-output mapping stage,\\navoiding the blind averaging of features. DEFE makes the ensembles 'deep' in\\nthe sense that it allows deep post-process of these features that tries to\\nlearn to select and abstract the ensemble of neural feature learners. With the\\napplication of this model, a selection regions full of signal process can be\\nobtained through the training of a miniature collision events set. In\\ncomparison of the Classic Deep Neural Network, DEFE shows a state-of-the-art\\nperformance: the error rate has decreased by about 37\\\\%, the accuracy has\\nbroken through 90\\\\% for the first time, along with the discovery significance\\nhas reached a standard deviation of 6.0 $\\\\sigma$. Experimental data shows that,\\nDEFE is able to train an ensemble of discriminative feature learners that\\nboosts the overperformance of final prediction.\\n\",\n",
       "  'title': u'Deep Extreme Feature Extraction: New MVA Method for Searching Particles\\n  in High Energy Physics'},\n",
       " u'1504.01046': {'arxivid': u'1504.01046',\n",
       "  'authorsaffil': [[u'Yining Wang', None],\n",
       "   [u'Yu-Xiang Wang', None],\n",
       "   [u'Aarti Singh', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'14 pages. To appear in The 19th International Conference on\\n  Artificial Intelligence and Statistics, held at Cadiz, Spain in 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1504.01046v2',\n",
       "  'published': u'2015-04-04T20:05:17Z',\n",
       "  'summary': u'  Subspace clustering is the problem of clustering data points into a union of\\nlow-dimensional linear/affine subspaces. It is the mathematical abstraction of\\nmany important problems in computer vision, image processing and machine\\nlearning. A line of recent work (4, 19, 24, 20) provided strong theoretical\\nguarantee for sparse subspace clustering (4), the state-of-the-art algorithm\\nfor subspace clustering, on both noiseless and noisy data sets. It was shown\\nthat under mild conditions, with high probability no two points from different\\nsubspaces are clustered together. Such guarantee, however, is not sufficient\\nfor the clustering to be correct, due to the notorious \"graph connectivity\\nproblem\" (15). In this paper, we investigate the graph connectivity problem for\\nnoisy sparse subspace clustering and show that a simple post-processing\\nprocedure is capable of delivering consistent clustering under certain \"general\\nposition\" or \"restricted eigenvalue\" assumptions. We also show that our\\ncondition is almost tight with adversarial noise perturbation by constructing a\\ncounter-example. These results provide the first exact clustering guarantee of\\nnoisy SSC for subspaces of dimension greater then 3.\\n',\n",
       "  'title': u'Graph Connectivity in Noisy Sparse Subspace Clustering'},\n",
       " u'1602.01711': {'arxivid': u'1602.01711',\n",
       "  'authorsaffil': [[u'Anthony Bagnall', None],\n",
       "   [u'Aaron Bostrom', None],\n",
       "   [u'James Large', None],\n",
       "   [u'Jason Lines', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01711v1',\n",
       "  'published': u'2016-02-04T15:24:22Z',\n",
       "  'summary': u'  In the last five years there have been a large number of new time series\\nclassification algorithms proposed in the literature. These algorithms have\\nbeen evaluated on subsets of the 47 data sets in the University of California,\\nRiverside time series classification archive. The archive has recently been\\nexpanded to 85 data sets, over half of which have been donated by researchers\\nat the University of East Anglia. Aspects of previous evaluations have made\\ncomparisons between algorithms difficult. For example, several different\\nprogramming languages have been used, experiments involved a single train/test\\nsplit and some used normalised data whilst others did not. The relaunch of the\\narchive provides a timely opportunity to thoroughly evaluate algorithms on a\\nlarger number of datasets. We have implemented 18 recently proposed algorithms\\nin a common Java framework and compared them against two standard benchmark\\nclassifiers (and each other) by performing 100 resampling experiments on each\\nof the 85 datasets. We use these results to test several hypotheses relating to\\nwhether the algorithms are significantly more accurate than the benchmarks and\\neach other. Our results indicate that only 9 of these algorithms are\\nsignificantly more accurate than both benchmarks and that one classifier, the\\nCollective of Transformation Ensembles, is significantly more accurate than all\\nof the others. All of our experiments and results are reproducible: we release\\nall of our code, results and experimental details and we hope these experiments\\nform the basis for more rigorous testing of new algorithms in the future.\\n',\n",
       "  'title': u'The Great Time Series Classification Bake Off: An Experimental\\n  Evaluation of Recently Proposed Algorithms. Extended Version'},\n",
       " u'1602.01718': {'arxivid': u'1602.01718',\n",
       "  'authorsaffil': [[u'Maryam Kamali', None],\n",
       "   [u'Louise A. Dennis', None],\n",
       "   [u'Owen McAree', None],\n",
       "   [u'Michael Fisher', None],\n",
       "   [u'Sandor M. Veres', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.SE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01718v1',\n",
       "  'published': u'2016-02-04T15:50:22Z',\n",
       "  'summary': u'  The coordination of multiple autonomous vehicles into convoys or platoons is\\nexpected on our highways in the near future. However, before such platoons can\\nbe deployed, the new autonomous behaviors of the vehicles in these platoons\\nmust be certified. An appropriate representation for vehicle platooning is as a\\nmulti-agent system in which each agent captures the \"autonomous decisions\"\\ncarried out by each vehicle. In order to ensure that these autonomous\\ndecision-making agents in vehicle platoons never violate safety requirements,\\nwe use formal verification. However, as the formal verification technique used\\nto verify the agent code does not scale to the full system and as the global\\nverification technique does not capture the essential verification of\\nautonomous behavior, we use a combination of the two approaches. This mixed\\nstrategy allows us to verify safety requirements not only of a model of the\\nsystem, but of the actual agent code used to program the autonomous vehicles.\\n',\n",
       "  'title': u'Formal Verification of Autonomous Vehicle Platooning'},\n",
       " u'1406.7444': {'arxivid': u'1406.7444',\n",
       "  'authorsaffil': [[u'Christian J. Schuler', None],\n",
       "   [u'Michael Hirsch', None],\n",
       "   [u'Stefan Harmeling', None],\n",
       "   [u'Bernhard Sch\\xf6lkopf', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1406.7444v1',\n",
       "  'published': u'2014-06-28T21:56:31Z',\n",
       "  'summary': u'  We describe a learning-based approach to blind image deconvolution. It uses a\\ndeep layered architecture, parts of which are borrowed from recent work on\\nneural network learning, and parts of which incorporate computations that are\\nspecific to image deconvolution. The system is trained end-to-end on a set of\\nartificially generated training examples, enabling competitive performance in\\nblind deconvolution, both with respect to quality and runtime.\\n',\n",
       "  'title': u'Learning to Deblur'},\n",
       " u'1602.05419': {'arxivid': u'1602.05419',\n",
       "  'authorsaffil': [[u'Aymeric Dieuleveut', u'SIERRA, LIENS'],\n",
       "   [u'Nicolas Flammarion', u'LIENS, SIERRA'],\n",
       "   [u'Francis Bach', u'SIERRA, LIENS']],\n",
       "  'categoryterms': [u'math.OC', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05419v2',\n",
       "  'published': u'2016-02-17T14:06:34Z',\n",
       "  'summary': u'  We consider the optimization of a quadratic objective function whose\\ngradients are only accessible through a stochastic oracle that returns the\\ngradient at any given point plus a zero-mean finite variance random error. We\\npresent the first algorithm that achieves jointly the optimal prediction error\\nrates for least-squares regression, both in terms of forgetting of initial\\nconditions in O(1/n 2), and in terms of dependence on the noise and dimension d\\nof the problem, as O(d/n). Our new algorithm is based on averaged accelerated\\nregularized gradient descent, and may also be analyzed through finer\\nassumptions on initial conditions and the Hessian matrix, leading to\\ndimension-free quantities that may still be small while the \" optimal \" terms\\nabove are large. In order to characterize the tightness of these new bounds, we\\nconsider an application to non-parametric regression and use the known lower\\nbounds on the statistical performance (without computational limits), which\\nhappen to match our bounds obtained from a single pass on the data and thus\\nshow optimality of our algorithm in a wide variety of particular trade-offs\\nbetween bias and variance.\\n',\n",
       "  'title': u'Harder, Better, Faster, Stronger Convergence Rates for Least-Squares\\n  Regression'},\n",
       " u'1603.09631': {'arxivid': u'1603.09631',\n",
       "  'authorsaffil': [[u'Miroslav Vodol\\xe1n', None],\n",
       "   [u'Filip Jur\\u010d\\xed\\u010dek', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09631v2',\n",
       "  'published': u'2016-03-31T15:13:51Z',\n",
       "  'summary': u'  This paper presents a dataset collected from natural dialogs which enables to\\ntest the ability of dialog systems to learn new facts from user utterances\\nthroughout the dialog. This interactive learning will help with one of the most\\nprevailing problems of open domain dialog system, which is the sparsity of\\nfacts a dialog system can reason about. The proposed dataset, consisting of\\n1900 collected dialogs, allows simulation of an interactive gaining of\\ndenotations and questions explanations from users which can be used for the\\ninteractive learning.\\n',\n",
       "  'title': u'Data Collection for Interactive Learning through the Dialog'},\n",
       " u'1604.02506': {'arxivid': u'1604.02506',\n",
       "  'authorsaffil': [[u'Antonio Jimeno Yepes', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02506v1',\n",
       "  'published': u'2016-04-09T01:14:05Z',\n",
       "  'summary': u'  Word sense disambiguation helps identifying the proper sense of ambiguous\\nwords in text. With large terminologies such as the UMLS Metathesaurus\\nambiguities appear and highly effective disambiguation methods are required.\\nSupervised learning algorithm methods are used as one of the approaches to\\nperform disambiguation. Features extracted from the context of an ambiguous\\nword are used to identify the proper sense of such a word. The type of features\\nhave an impact on machine learning methods, thus affect disambiguation\\nperformance. In this work, we have evaluated several types of features derived\\nfrom the context of the ambiguous word and we have explored as well more global\\nfeatures derived from MEDLINE using word embeddings. Results show that word\\nembeddings improve the performance of more traditional features and allow as\\nwell using recurrent neural networks based on Long-Short Term Memory (LSTM)\\nnodes, which further improve the disambiguation performance. The combination of\\nunigrams and word embeddings set a new state of the art performance with an\\naccuracy of 95.97 in the MSH WSD data set.\\n',\n",
       "  'title': u'Higher order features and recurrent neural networks based on Long-Short\\n  Term Memory nodes in supervised biomedical word sense disambiguation'},\n",
       " u'1602.08761': {'arxivid': u'1602.08761',\n",
       "  'authorsaffil': [[u'Tolga Bolukbasi', None],\n",
       "   [u'Kai-Wei Chang', None],\n",
       "   [u'Joseph Wang', None],\n",
       "   [u'Venkatesh Saligrama', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CL', u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08761v2',\n",
       "  'published': u'2016-02-28T19:44:57Z',\n",
       "  'summary': u'  We study the problem of structured prediction under test-time budget\\nconstraints. We propose a novel approach applicable to a wide range of\\nstructured prediction problems in computer vision and natural language\\nprocessing. Our approach seeks to adaptively generate computationally costly\\nfeatures during test-time in order to reduce the computational cost of\\nprediction while maintaining prediction performance. We show that training the\\nadaptive feature generation system can be reduced to a series of structured\\nlearning problems, resulting in efficient training using existing structured\\nlearning algorithms. This framework provides theoretical justification for\\nseveral existing heuristic approaches found in literature. We evaluate our\\nproposed adaptive system on two structured prediction tasks, optical character\\nrecognition (OCR) and dependency parsing and show strong performance in\\nreduction of the feature costs without degrading accuracy.\\n',\n",
       "  'title': u'Resource Constrained Structured Prediction'},\n",
       " u'1510.07867': {'arxivid': u'1510.07867',\n",
       "  'authorsaffil': [[u'Rasmus Rothe', None],\n",
       "   [u'Radu Timofte', None],\n",
       "   [u'Luc Van Gool', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'accepted for publication at CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.07867v2',\n",
       "  'published': u'2015-10-27T11:17:46Z',\n",
       "  'summary': u\"  For people first impressions of someone are of determining importance. They\\nare hard to alter through further information. This begs the question if a\\ncomputer can reach the same judgement. Earlier research has already pointed out\\nthat age, gender, and average attractiveness can be estimated with reasonable\\nprecision. We improve the state-of-the-art, but also predict - based on\\nsomeone's known preferences - how much that particular person is attracted to a\\nnovel face. Our computational pipeline comprises a face detector, convolutional\\nneural networks for the extraction of deep features, standard support vector\\nregression for gender, age and facial beauty, and - as the main novelties -\\nvisual regularized collaborative filtering to infer inter-person preferences as\\nwell as a novel regression technique for handling visual queries without rating\\nhistory. We validate the method using a very large dataset from a dating site\\nas well as images from celebrities. Our experiments yield convincing results,\\ni.e. we predict 76% of the ratings correctly solely based on an image, and\\nreveal some sociologically relevant conclusions. We also validate our\\ncollaborative filtering solution on the standard MovieLens rating dataset,\\naugmented with movie posters, to predict an individual's movie rating. We\\ndemonstrate our algorithms on howhot.io which went viral around the Internet\\nwith more than 50 million pictures evaluated in the first month.\\n\",\n",
       "  'title': u'Some like it hot - visual guidance for preference prediction'},\n",
       " u'1502.04617': {'arxivid': u'1502.04617',\n",
       "  'authorsaffil': [[u'Andrew J. R. Simpson', None]],\n",
       "  'categoryterms': [u'cs.LG', u'68Txx'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.04617v1',\n",
       "  'published': u'2015-02-16T16:41:26Z',\n",
       "  'summary': u'  Errors in data are usually unwelcome and so some means to correct them is\\nuseful. However, it is difficult to define, detect or correct errors in an\\nunsupervised way. Here, we train a deep neural network to re-synthesize its\\ninputs at its output layer for a given class of data. We then exploit the fact\\nthat this abstract transformation, which we call a deep transform (DT),\\ninherently rejects information (errors) existing outside of the abstract\\nfeature space. Using the DT to perform probabilistic re-synthesis, we\\ndemonstrate the recovery of data that has been subject to extreme degradation.\\n',\n",
       "  'title': u'Deep Transform: Error Correction via Probabilistic Re-Synthesis'},\n",
       " u'1601.06971': {'arxivid': u'1601.06971',\n",
       "  'authorsaffil': [[u'Vishal. A. Kharde', None],\n",
       "   [u'Prof. Sheetal. Sonawane', None]],\n",
       "  'categoryterms': [u'cs.CL', u'I.2.7'],\n",
       "  'comment': u'7 figures, 10 tables',\n",
       "  'doi': u'10.5120/ijca2016908625',\n",
       "  'journalref': u'International Journal of Computer Applications 139(11):5-15, April\\n  2016. Published by Foundation of Computer Science (FCS), NY, USA',\n",
       "  'link': u'http://arxiv.org/abs/1601.06971v3',\n",
       "  'published': u'2016-01-26T10:44:30Z',\n",
       "  'summary': u'  With the advancement of web technology and its growth, there is a huge volume\\nof data present in the web for internet users and a lot of data is generated\\ntoo. Internet has become a platform for online learning, exchanging ideas and\\nsharing opinions. Social networking sites like Twitter, Facebook, Google+ are\\nrapidly gaining popularity as they allow people to share and express their\\nviews about topics,have discussion with different communities, or post messages\\nacross the world. There has been lot of work in the field of sentiment analysis\\nof twitter data. This survey focuses mainly on sentiment analysis of twitter\\ndata which is helpful to analyze the information in the tweets where opinions\\nare highly unstructured, heterogeneous and are either positive or negative, or\\nneutral in some cases. In this paper, we provide a survey and a comparative\\nanalyses of existing techniques for opinion mining like machine learning and\\nlexicon-based approaches, together with evaluation metrics. Using various\\nmachine learning algorithms like Naive Bayes, Max Entropy, and Support Vector\\nMachine, we provide a research on twitter data streams.General challenges and\\napplications of Sentiment Analysis on Twitter are also discussed in this paper.\\n',\n",
       "  'title': u'Sentiment Analysis of Twitter Data: A Survey of Techniques'},\n",
       " u'1512.08562': {'arxivid': u'1512.08562',\n",
       "  'authorsaffil': [[u'Roy Fox', None],\n",
       "   [u'Ari Pakman', None],\n",
       "   [u'Naftali Tishby', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IT', u'math.IT'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.08562v2',\n",
       "  'published': u'2015-12-28T23:59:12Z',\n",
       "  'summary': u'  Model-free reinforcement learning algorithms, such as Q-learning, perform\\npoorly in the early stages of learning in noisy environments, because much\\neffort is spent unlearning biased estimates of the state-action value function.\\nThe bias results from selecting, among several noisy estimates, the apparent\\noptimum, which may actually be suboptimal. We propose G-learning, a new\\noff-policy learning algorithm that regularizes the value estimates by\\npenalizing deterministic policies in the beginning of the learning process. We\\nshow that this method reduces the bias of the value-function estimation,\\nleading to faster convergence to the optimal value and the optimal policy.\\nMoreover, G-learning enables the natural incorporation of prior domain\\nknowledge, when available. The stochastic nature of G-learning also makes it\\navoid some exploration costs, a property usually attributed only to on-policy\\nalgorithms. We illustrate these ideas in several examples, where G-learning\\nresults in significant improvements of the convergence rate and the cost of the\\nlearning process.\\n',\n",
       "  'title': u'Taming the Noise in Reinforcement Learning via Soft Updates'},\n",
       " u'1508.01774': {'arxivid': u'1508.01774',\n",
       "  'authorsaffil': [[u'Siddharth Sigtia', None],\n",
       "   [u'Emmanouil Benetos', None],\n",
       "   [u'Simon Dixon', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.SD'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.01774v2',\n",
       "  'published': u'2015-08-07T18:16:32Z',\n",
       "  'summary': u'  We present a supervised neural network model for polyphonic piano music\\ntranscription. The architecture of the proposed model is analogous to speech\\nrecognition systems and comprises an acoustic model and a music language model.\\nThe acoustic model is a neural network used for estimating the probabilities of\\npitches in a frame of audio. The language model is a recurrent neural network\\nthat models the correlations between pitch combinations over time. The proposed\\nmodel is general and can be used to transcribe polyphonic music without\\nimposing any constraints on the polyphony. The acoustic and language model\\npredictions are combined using a probabilistic graphical model. Inference over\\nthe output variables is performed using the beam search algorithm. We perform\\ntwo sets of experiments. We investigate various neural network architectures\\nfor the acoustic models and also investigate the effect of combining acoustic\\nand music language model predictions using the proposed architecture. We\\ncompare performance of the neural network based acoustic models with two\\npopular unsupervised acoustic models. Results show that convolutional neural\\nnetwork acoustic models yields the best performance across all evaluation\\nmetrics. We also observe improved performance with the application of the music\\nlanguage models. Finally, we present an efficient variant of beam search that\\nimproves performance and reduces run-times by an order of magnitude, making the\\nmodel suitable for real-time applications.\\n',\n",
       "  'title': u'An End-to-End Neural Network for Polyphonic Piano Music Transcription'},\n",
       " u'1412.7062': {'arxivid': u'1412.7062',\n",
       "  'authorsaffil': [[u'Liang-Chieh Chen', None],\n",
       "   [u'George Papandreou', None],\n",
       "   [u'Iasonas Kokkinos', None],\n",
       "   [u'Kevin Murphy', None],\n",
       "   [u'Alan L. Yuille', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'14 pages. Updated related work',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1412.7062v4',\n",
       "  'published': u'2014-12-22T17:18:33Z',\n",
       "  'summary': u'  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the\\nart performance in high level vision tasks, such as image classification and\\nobject detection. This work brings together methods from DCNNs and\\nprobabilistic graphical models for addressing the task of pixel-level\\nclassification (also called \"semantic image segmentation\"). We show that\\nresponses at the final layer of DCNNs are not sufficiently localized for\\naccurate object segmentation. This is due to the very invariance properties\\nthat make DCNNs good for high level tasks. We overcome this poor localization\\nproperty of deep networks by combining the responses at the final DCNN layer\\nwith a fully connected Conditional Random Field (CRF). Qualitatively, our\\n\"DeepLab\" system is able to localize segment boundaries at a level of accuracy\\nwhich is beyond previous methods. Quantitatively, our method sets the new\\nstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching\\n71.6% IOU accuracy in the test set. We show how these results can be obtained\\nefficiently: Careful network re-purposing and a novel application of the \\'hole\\'\\nalgorithm from the wavelet community allow dense computation of neural net\\nresponses at 8 frames per second on a modern GPU.\\n',\n",
       "  'title': u'Semantic Image Segmentation with Deep Convolutional Nets and Fully\\n  Connected CRFs'},\n",
       " u'1602.01398': {'arxivid': u'1602.01398',\n",
       "  'authorsaffil': [[u'Usman Habib', None], [u'Gerhard Zucker', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1109/FIT.2015.60',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01398v1',\n",
       "  'published': u'2016-02-03T18:11:32Z',\n",
       "  'summary': u'  The understanding of the buildings operation has become a challenging task\\ndue to the large amount of data recorded in energy efficient buildings. Still,\\ntoday the experts use visual tools for analyzing the data. In order to make the\\ntask realistic, a method has been proposed in this paper to automatically\\ndetect the different patterns in buildings. The K Means clustering is used to\\nautomatically identify the ON (operational) cycles of the chiller. In the next\\nstep the ON cycles are transformed to symbolic representation by using Symbolic\\nAggregate Approximation (SAX) method. Then the SAX symbols are converted to bag\\nof words representation for hierarchical clustering. Moreover, the proposed\\ntechnique is applied to real life data of adsorption chiller. Additionally, the\\nresults from the proposed method and dynamic time warping (DTW) approach are\\nalso discussed and compared.\\n',\n",
       "  'title': u'Finding the different patterns in buildings data using bag of words\\n  representation with clustering'},\n",
       " u'1604.02878': {'arxivid': u'1604.02878',\n",
       "  'authorsaffil': [[u'Kaipeng Zhang', None],\n",
       "   [u'Zhanpeng Zhang', None],\n",
       "   [u'Zhifeng Li', None],\n",
       "   [u'Yu Qiao', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Submitted to IEEE Signal Processing Letters',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02878v1',\n",
       "  'published': u'2016-04-11T10:47:14Z',\n",
       "  'summary': u'  Face detection and alignment in unconstrained environment are challenging due\\nto various poses, illuminations and occlusions. Recent studies show that deep\\nlearning approaches can achieve impressive performance on these two tasks. In\\nthis paper, we propose a deep cascaded multi-task framework which exploits the\\ninherent correlation between them to boost up their performance. In particular,\\nour framework adopts a cascaded structure with three stages of carefully\\ndesigned deep convolutional networks that predict face and landmark location in\\na coarse-to-fine manner. In addition, in the learning process, we propose a new\\nonline hard sample mining strategy that can improve the performance\\nautomatically without manual sample selection. Our method achieves superior\\naccuracy over the state-of-the-art techniques on the challenging FDDB and WIDER\\nFACE benchmark for face detection, and AFLW benchmark for face alignment, while\\nkeeps real time performance.\\n',\n",
       "  'title': u'Joint Face Detection and Alignment using Multi-task Cascaded\\n  Convolutional Networks'},\n",
       " u'1602.02377': {'arxivid': u'1602.02377',\n",
       "  'authorsaffil': [[u'Yong Tan', None]],\n",
       "  'categoryterms': [u'cs.DS',\n",
       "   u'cs.AI',\n",
       "   u'cs.DM',\n",
       "   u'cs.RO',\n",
       "   u'math.DS',\n",
       "   u'37HXX, 70Q05, 70G60, 93C85, 91B06,',\n",
       "   u'F.2.2; G.2.2; I.2.9; J.7'],\n",
       "  'comment': u'27 pages, 9720 words,10 figures,5 trials',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02377v1',\n",
       "  'published': u'2016-02-07T14:50:45Z',\n",
       "  'summary': u'  We study an ancient problem that in a static or dynamical system, sought an\\noptimal path, which the context always means within an extremal condition. In\\nfact, through those discussions about this theme, we established a universal\\nessential calculated model to serve for these complex systems. Meanwhile we\\nutilize the sample space to character the system. These contents in this paper\\nwould involve in several major areas including the geometry, probability, graph\\nalgorithms and some prior approaches, which stands the ultimately subtle linear\\nalgorithm to solve this class problem. Along with our progress, our discussion\\nwould demonstrate more general meaning and robust character, which provides\\nclear ideas or notion to support our concrete applications, who work in a more\\npopular complex system.\\n',\n",
       "  'title': u'Find an Optimal Path in Static System and Dynamical System within\\n  Polynomial Runtime'},\n",
       " u'1603.01514': {'arxivid': u'1603.01514',\n",
       "  'authorsaffil': [[u'Nikhil Garg', None], [u'James Henderson', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01514v1',\n",
       "  'published': u'2016-03-04T16:03:53Z',\n",
       "  'summary': u'  We propose a Bayesian model of unsupervised semantic role induction in\\nmultiple languages, and use it to explore the usefulness of parallel corpora\\nfor this task. Our joint Bayesian model consists of individual models for each\\nlanguage plus additional latent variables that capture alignments between roles\\nacross languages. Because it is a generative Bayesian model, we can do\\nevaluations in a variety of scenarios just by varying the inference procedure,\\nwithout changing the model, thereby comparing the scenarios directly. We\\ncompare using only monolingual data, using a parallel corpus, using a parallel\\ncorpus with annotations in the other language, and using small amounts of\\nannotation in the target language. We find that the biggest impact of adding a\\nparallel corpus to training is actually the increase in mono-lingual data, with\\nthe alignments to another language resulting in small improvements, even with\\nlabeled data for the other language.\\n',\n",
       "  'title': u'A Bayesian Model of Multilingual Unsupervised Semantic Role Induction'},\n",
       " u'1602.02373': {'arxivid': u'1602.02373',\n",
       "  'authorsaffil': [[u'Rie Johnson', None], [u'Tong Zhang', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CL', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02373v2',\n",
       "  'published': u'2016-02-07T14:05:58Z',\n",
       "  'summary': u\"  One-hot CNN (convolutional neural network) has been shown to be effective for\\ntext categorization (Johnson & Zhang, 2015). We view it as a special case of a\\ngeneral framework which jointly trains a linear model with a non-linear feature\\ngenerator consisting of `text region embedding + pooling'. Under this\\nframework, we explore a more sophisticated region embedding method using Long\\nShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly\\nlarge) sizes, whereas the region size needs to be fixed in a CNN. We seek\\neffective and efficient use of LSTM for this purpose in the supervised and\\nsemi-supervised settings. The best results were obtained by combining region\\nembeddings in the form of LSTM and convolution layers trained on unlabeled\\ndata. The results indicate that on this task, embeddings of text regions, which\\ncan convey complex concepts, are more useful than embeddings of single words in\\nisolation. We report performances exceeding the previous best results on four\\nbenchmark datasets.\\n\",\n",
       "  'title': u'Supervised and Semi-Supervised Text Categorization using LSTM for Region\\n  Embeddings'},\n",
       " u'1512.05430': {'arxivid': u'1512.05430',\n",
       "  'authorsaffil': [[u'Qian Yu', None],\n",
       "   [u'Christian Szegedy', None],\n",
       "   [u'Martin C. Stumpe', None],\n",
       "   [u'Liron Yatziv', None],\n",
       "   [u'Vinay Shet', None],\n",
       "   [u'Julian Ibarz', None],\n",
       "   [u'Sacha Arnoud', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05430v2',\n",
       "  'published': u'2015-12-17T01:15:11Z',\n",
       "  'summary': u'  Search with local intent is becoming increasingly useful due to the\\npopularity of the mobile device. The creation and maintenance of accurate\\nlistings of local businesses worldwide is time consuming and expensive. In this\\npaper, we propose an approach to automatically discover businesses that are\\nvisible on street level imagery. Precise business store front detection enables\\naccurate geo-location of businesses, and further provides input for business\\ncategorization, listing generation, etc. The large variety of business\\ncategories in different countries makes this a very challenging problem.\\nMoreover, manual annotation is prohibitive due to the scale of this problem. We\\npropose the use of a MultiBox based approach that takes input image pixels and\\ndirectly outputs store front bounding boxes. This end-to-end learning approach\\ninstead preempts the need for hand modeling either the proposal generation\\nphase or the post-processing phase, leveraging large labelled training\\ndatasets. We demonstrate our approach outperforms the state of the art\\ndetection techniques with a large margin in terms of performance and run-time\\nefficiency. In the evaluation, we show this approach achieves human accuracy in\\nthe low-recall settings. We also provide an end-to-end evaluation of business\\ndiscovery in the real world.\\n',\n",
       "  'title': u'Large Scale Business Discovery from Street Level Imagery'},\n",
       " u'1603.01046': {'arxivid': u'1603.01046',\n",
       "  'authorsaffil': [[u'Teemu Helenius', None], [u'Samuli Siltanen', None]],\n",
       "  'categoryterms': [u'physics.data-an', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01046v1',\n",
       "  'published': u'2016-03-03T10:24:07Z',\n",
       "  'summary': u'  This is a photographic dataset collected for testing image processing\\nalgorithms. The idea is to have sets of different but statistically similar\\nimages. In this work the images show randomly distributed peppercorns. The\\ndataset is made available at www.fips.fi/photographic_dataset.php .\\n',\n",
       "  'title': u'Photographic dataset: random peppercorns'},\n",
       " u'1509.07308': {'arxivid': u'1509.07308',\n",
       "  'authorsaffil': [[u'Ivan Vuli\\u0107', None],\n",
       "   [u'Marie-Francine Moens', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.07308v2',\n",
       "  'published': u'2015-09-24T11:00:04Z',\n",
       "  'summary': u'  We propose a new model for learning bilingual word representations from\\nnon-parallel document-aligned data. Following the recent advances in word\\nrepresentation learning, our model learns dense real-valued word vectors, that\\nis, bilingual word embeddings (BWEs). Unlike prior work on inducing BWEs which\\nheavily relied on parallel sentence-aligned corpora and/or readily available\\ntranslation resources such as dictionaries, the article reveals that BWEs may\\nbe learned solely on the basis of document-aligned comparable data without any\\nadditional lexical resources nor syntactic information. We present a comparison\\nof our approach with previous state-of-the-art models for learning bilingual\\nword representations from comparable data that rely on the framework of\\nmultilingual probabilistic topic modeling (MuPTM), as well as with\\ndistributional local context-counting models. We demonstrate the utility of the\\ninduced BWEs in two semantic tasks: (1) bilingual lexicon extraction, (2)\\nsuggesting word translations in context for polysemous words. Our simple yet\\neffective BWE-based models significantly outperform the MuPTM-based and\\ncontext-counting representation models from comparable data as well as prior\\nBWE-based models, and acquire the best reported results on both tasks for all\\nthree tested language pairs.\\n',\n",
       "  'title': u'Bilingual Distributed Word Representations from Document-Aligned\\n  Comparable Data'},\n",
       " u'1602.01576': {'arxivid': u'1602.01576',\n",
       "  'authorsaffil': [[u'Anantharaman Palacode Narayana Iyer', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI'],\n",
       "  'comment': u'8 pages',\n",
       "  'doi': u'10.1109/ICSC.2016.37',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01576v1',\n",
       "  'published': u'2016-02-04T07:53:11Z',\n",
       "  'summary': u'  Statistical language models are central to many applications that use\\nsemantics. Recurrent Neural Networks (RNN) are known to produce state of the\\nart results for language modelling, outperforming their traditional n-gram\\ncounterparts in many cases. To generate a probability distribution across a\\nvocabulary, these models require a softmax output layer that linearly increases\\nin size with the size of the vocabulary. Large vocabularies need a\\ncommensurately large softmax layer and training them on typical laptops/PCs\\nrequires significant time and machine resources. In this paper we present a new\\ntechnique for implementing RNN based large vocabulary language models that\\nsubstantially speeds up computation while optimally using the limited memory\\nresources. Our technique, while building on the notion of factorizing the\\noutput layer by having multiple output layers, improves on the earlier work by\\nsubstantially optimizing on the individual output layer size and also\\neliminating the need for a multistep prediction process.\\n',\n",
       "  'title': u'A Factorized Recurrent Neural Network based architecture for medium to\\n  large vocabulary Language Modelling'},\n",
       " u'1507.03292': {'arxivid': u'1507.03292',\n",
       "  'authorsaffil': [[u'Jaeseong Jeong', None],\n",
       "   [u'Mathieu Leconte', None],\n",
       "   [u'Alexandre Proutiere', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.03292v4',\n",
       "  'published': u'2015-07-12T23:27:50Z',\n",
       "  'summary': u\"  Predicting the future location of users in wireless net- works has numerous\\napplications, and can help service providers to improve the quality of service\\nperceived by their clients. The location predictors proposed so far estimate\\nthe next location of a specific user by inspecting the past individual\\ntrajectories of this user. As a consequence, when the training data collected\\nfor a given user is limited, the resulting prediction is inaccurate. In this\\npaper, we develop cluster-aided predictors that exploit past trajectories\\ncollected from all users to predict the next location of a given user. These\\npredictors rely on clustering techniques and extract from the training data\\nsimilarities among the mobility patterns of the various users to improve the\\nprediction accuracy. Specifically, we present CAMP (Cluster-Aided Mobility\\nPredictor), a cluster-aided predictor whose design is based on recent\\nnon-parametric bayesian statistical tools. CAMP is robust and adaptive in the\\nsense that it exploits similarities in users' mobility only if such\\nsimilarities are really present in the training data. We analytically prove the\\nconsistency of the predictions provided by CAMP, and investigate its\\nperformance using two large-scale datasets. CAMP significantly outperforms\\nexisting predictors, and in particular those that only exploit individual past\\ntrajectories.\\n\",\n",
       "  'title': u'Cluster-Aided Mobility Predictions'},\n",
       " u'1511.05099': {'arxivid': u'1511.05099',\n",
       "  'authorsaffil': [[u'Peng Zhang', None],\n",
       "   [u'Yash Goyal', None],\n",
       "   [u'Douglas Summers-Stay', None],\n",
       "   [u'Dhruv Batra', None],\n",
       "   [u'Devi Parikh', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05099v5',\n",
       "  'published': u'2015-11-16T19:38:14Z',\n",
       "  'summary': u'  The complex compositional structure of language makes problems at the\\nintersection of vision and language challenging. But language also provides a\\nstrong prior that can result in good superficial performance, without the\\nunderlying models truly understanding the visual content. This can hinder\\nprogress in pushing state of art in the computer vision aspects of multi-modal\\nAI. In this paper, we address binary Visual Question Answering (VQA) on\\nabstract scenes. We formulate this problem as visual verification of concepts\\ninquired in the questions. Specifically, we convert the question to a tuple\\nthat concisely summarizes the visual concept to be detected in the image. If\\nthe concept can be found in the image, the answer to the question is \"yes\", and\\notherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on\\nthe high-level semantics of the VQA task as opposed to the low-level\\nrecognition problems, and perhaps more importantly, (2) They provide us the\\nmodality to balance the dataset such that language priors are controlled, and\\nthe role of vision is essential. In particular, we collect fine-grained pairs\\nof scenes for every question, such that the answer to the question is \"yes\" for\\none scene, and \"no\" for the other for the exact same question. Indeed, language\\npriors alone do not perform better than chance on our balanced dataset.\\nMoreover, our proposed approach matches the performance of a state-of-the-art\\nVQA approach on the unbalanced dataset, and outperforms it on the balanced\\ndataset.\\n',\n",
       "  'title': u'Yin and Yang: Balancing and Answering Binary Visual Questions'},\n",
       " u'1603.05600': {'arxivid': u'1603.05600',\n",
       "  'authorsaffil': [[u'Roozbeh Mottaghi', None],\n",
       "   [u'Mohammad Rastegari', None],\n",
       "   [u'Abhinav Gupta', None],\n",
       "   [u'Ali Farhadi', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05600v1',\n",
       "  'published': u'2016-03-17T18:12:33Z',\n",
       "  'summary': u'  What happens if one pushes a cup sitting on a table toward the edge of the\\ntable? How about pushing a desk against a wall? In this paper, we study the\\nproblem of understanding the movements of objects as a result of applying\\nexternal forces to them. For a given force vector applied to a specific\\nlocation in an image, our goal is to predict long-term sequential movements\\ncaused by that force. Doing so entails reasoning about scene geometry, objects,\\ntheir attributes, and the physical rules that govern the movements of objects.\\nWe design a deep neural network model that learns long-term sequential\\ndependencies of object movements while taking into account the geometry and\\nappearance of the scene by combining Convolutional and Recurrent Neural\\nNetworks. Training our model requires a large-scale dataset of object movements\\ncaused by external forces. To build a dataset of forces in scenes, we\\nreconstructed all images in SUN RGB-D dataset in a physics simulator to\\nestimate the physical movements of objects caused by external forces applied to\\nthem. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which a\\nvariety of external forces are applied to different types of objects resulting\\nin more than 65,000 object movements represented in 3D. Our experimental\\nevaluations show that the challenging task of predicting long-term movements of\\nobjects as their reaction to external forces is possible from a single image.\\n',\n",
       "  'title': u'\"What happens if...\" Learning to Predict the Effect of Forces in Images'},\n",
       " u'1603.06554': {'arxivid': u'1603.06554',\n",
       "  'authorsaffil': [[u'Timothy J. Shields', None],\n",
       "   [u'Mohamed R. Amer', None],\n",
       "   [u'Max Ehrlich', None],\n",
       "   [u'Amir Tamrakar', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.HC', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06554v1',\n",
       "  'published': u'2016-03-21T19:38:07Z',\n",
       "  'summary': u'  Most recent work focused on affect from facial expressions, and not as much\\non body. This work focuses on body affect analysis. Affect does not occur in\\nisolation. Humans usually couple affect with an action in natural interactions;\\nfor example, a person could be talking and smiling. Recognizing body affect in\\nsequences requires efficient algorithms to capture both the micro movements\\nthat differentiate between happy and sad and the macro variations between\\ndifferent actions. We depart from traditional approaches for time-series data\\nanalytics by proposing a multi-task learning model that learns a shared\\nrepresentation that is well-suited for action-affect classification as well as\\ngeneration. For this paper we choose Conditional Restricted Boltzmann Machines\\nto be our building block. We propose a new model that enhances the CRBM model\\nwith a factored multi-task component to become Multi-Task Conditional\\nRestricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two\\npublicly available datasets, the Body Affect dataset and the Tower Game\\ndataset, and show superior classification performance improvement over the\\nstate-of-the-art, as well as the generative abilities of our model.\\n',\n",
       "  'title': u'Action-Affect Classification and Morphing using Multi-Task\\n  Representation Learning'},\n",
       " u'1602.05388': {'arxivid': u'1602.05388',\n",
       "  'authorsaffil': [[u'Muhammad Imran', None],\n",
       "   [u'Prasenjit Mitra', None],\n",
       "   [u'Jaideep Srivastava', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'ISCRAM 2016, 10 pages, 4 tables',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05388v2',\n",
       "  'published': u'2016-02-17T12:29:56Z',\n",
       "  'summary': u'  Rapid crisis response requires real-time analysis of messages. After a\\ndisaster happens, volunteers attempt to classify tweets to determine needs,\\ne.g., supplies, infrastructure damage, etc. Given labeled data, supervised\\nmachine learning can help classify these messages. Scarcity of labeled data\\ncauses poor performance in machine training. Can we reuse old tweets to train\\nclassifiers? How can we choose labeled tweets for training? Specifically, we\\nstudy the usefulness of labeled data of past events. Do labeled tweets in\\ndifferent language help? We observe the performance of our classifiers trained\\nusing different combinations of training sets obtained from past disasters. We\\nperform extensive experimentation on real crisis datasets and show that the\\npast labels are useful when both source and target events are of the same type\\n(e.g. both earthquakes). For similar languages (e.g., Italian and Spanish),\\ncross-language domain adaptation was useful, however, when for different\\nlanguages (e.g., Italian and English), the performance decreased.\\n',\n",
       "  'title': u'Cross-Language Domain Adaptation for Classifying Crisis-Related Short\\n  Messages'},\n",
       " u'1603.06398': {'arxivid': u'1603.06398',\n",
       "  'authorsaffil': [[u'Liqian Ma', None],\n",
       "   [u'Jue Wang', None],\n",
       "   [u'Eli Shechtman', None],\n",
       "   [u'Kalyan Sunkavalli', None],\n",
       "   [u'Shimin Hu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06398v1',\n",
       "  'published': u'2016-03-21T12:01:36Z',\n",
       "  'summary': u'  Shadows often create unwanted artifacts in photographs, and removing them can\\nbe very challenging. Previous shadow removal methods often produce de-shadowed\\nregions that are visually inconsistent with the rest of the image. In this work\\nwe propose a fully automatic shadow region harmonization approach that improves\\nthe appearance compatibility of the de-shadowed region as typically produced by\\nprevious methods. It is based on a shadow-guided patch-based image synthesis\\napproach that reconstructs the shadow region using patches sampled from\\nnon-shadowed regions. The result is then refined based on the reconstruction\\nconfidence to handle unique image patterns. Many shadow removal results and\\ncomparisons are show the effectiveness of our improvement. Quantitative\\nevaluation on a benchmark dataset suggests that our automatic shadow\\nharmonization approach effectively improves upon the state-of-the-art.\\n',\n",
       "  'title': u'Appearance Harmonization for Single Image Shadow Removal'},\n",
       " u'1603.06393': {'arxivid': u'1603.06393',\n",
       "  'authorsaffil': [[u'Jiatao Gu', None],\n",
       "   [u'Zhengdong Lu', None],\n",
       "   [u'Hang Li', None],\n",
       "   [u'Victor O. K. Li', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'10 pages, 5 figures, accepted by ACL2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06393v3',\n",
       "  'published': u'2016-03-21T11:35:08Z',\n",
       "  'summary': u'  We address an important problem in sequence-to-sequence (Seq2Seq) learning\\nreferred to as copying, in which certain segments in the input sequence are\\nselectively replicated in the output sequence. A similar phenomenon is\\nobservable in human language communication. For example, humans tend to repeat\\nentity names or even long phrases in conversation. The challenge with regard to\\ncopying in Seq2Seq is that new machinery is needed to decide when to perform\\nthe operation. In this paper, we incorporate copying into neural network-based\\nSeq2Seq learning and propose a new model called CopyNet with encoder-decoder\\nstructure. CopyNet can nicely integrate the regular way of word generation in\\nthe decoder with the new copying mechanism which can choose sub-sequences in\\nthe input sequence and put them at proper places in the output sequence. Our\\nempirical study on both synthetic data sets and real world data sets\\ndemonstrates the efficacy of CopyNet. For example, CopyNet can outperform\\nregular RNN-based model with remarkable margins on text summarization tasks.\\n',\n",
       "  'title': u'Incorporating Copying Mechanism in Sequence-to-Sequence Learning'},\n",
       " u'1601.07233': {'arxivid': u'1601.07233',\n",
       "  'authorsaffil': [[u'Andrew Schaumberg', None],\n",
       "   [u'Angela Yu', None],\n",
       "   [u'Tatsuhiro Koshi', None],\n",
       "   [u'Xiaochan Zong', None],\n",
       "   [u'Santoshkalyan Rayadhurgam', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'I.2.1; J.3'],\n",
       "  'comment': u'7 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07233v1',\n",
       "  'published': u'2016-01-27T00:31:02Z',\n",
       "  'summary': u'  In this study, we intend to solve a mutual information problem in interacting\\nmolecules of any type, such as proteins, nucleic acids, and small molecules.\\nUsing machine learning techniques, we accurately predict pairwise interactions,\\nwhich can be of medical and biological importance. Graphs are are useful in\\nthis problem for their generality to all types of molecules, due to the\\ninherent association of atoms through atomic bonds. Subgraphs can represent\\ndifferent molecular domains. These domains can be biologically significant as\\nmost molecules only have portions that are of functional significance and can\\ninteract with other domains. Thus, we use subgraphs as features in different\\nmachine learning algorithms to predict if two drugs interact and predict\\npotential single molecule effects.\\n',\n",
       "  'title': u'Predicting Drug Interactions and Mutagenicity with Ensemble Classifiers\\n  on Subgraphs of Molecules'},\n",
       " u'1601.00248': {'arxivid': u'1601.00248',\n",
       "  'authorsaffil': [[u'Kushal Arora', None], [u'Anand Rangarajan', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'submitted to INTERSPEECH 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00248v2',\n",
       "  'published': u'2016-01-03T05:47:42Z',\n",
       "  'summary': u'  Perplexity (per word) is the most widely used metric for evaluating language\\nmodels. Despite this, there has been no dearth of criticism for this metric.\\nMost of these criticisms center around lack of correlation with extrinsic\\nmetrics like word error rate (WER), dependence upon shared vocabulary for model\\ncomparison and unsuitability for unnormalized language model evaluation. In\\nthis paper, we address the last problem and propose a new discriminative\\nentropy based intrinsic metric that works for both traditional word level\\nmodels and unnormalized language models like sentence level models. We also\\npropose a discriminatively trained sentence level interpretation of recurrent\\nneural network based language model (RNN) as an example of unnormalized\\nsentence level model. We demonstrate that for word level models, contrastive\\nentropy shows a strong correlation with perplexity. We also observe that when\\ntrained at lower distortion levels, sentence level RNN considerably outperforms\\ntraditional RNNs on this new metric.\\n',\n",
       "  'title': u'Contrastive Entropy: A new evaluation metric for unnormalized language\\n  models'},\n",
       " u'1601.04902': {'arxivid': u'1601.04902',\n",
       "  'authorsaffil': [[u'Wolfgang Fuhl', None],\n",
       "   [u'Thiago Santini', None],\n",
       "   [u'Gjergji Kasneci', None],\n",
       "   [u'Enkelejda Kasneci', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages, 11 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04902v1',\n",
       "  'published': u'2016-01-19T13:06:16Z',\n",
       "  'summary': u'  Real-time, accurate, and robust pupil detection is an essential prerequisite\\nfor pervasive video-based eye-tracking. However, automated pupil detection in\\nreal-world scenarios has proven to be an intricate challenge due to fast\\nillumination changes, pupil occlusion, non centered and off-axis eye recording,\\nand physiological eye characteristics. In this paper, we propose and evaluate a\\nmethod based on a novel dual convolutional neural network pipeline. In its\\nfirst stage the pipeline performs coarse pupil position identification using a\\nconvolutional neural network and subregions from a downscaled input image to\\ndecrease computational costs. Using subregions derived from a small window\\naround the initial pupil position estimate, the second pipeline stage employs\\nanother convolutional neural network to refine this position, resulting in an\\nincreased pupil detection rate up to 25% in comparison with the best performing\\nstate-of-the-art algorithm. Annotated data sets can be made available upon\\nrequest.\\n',\n",
       "  'title': u'PupilNet: Convolutional Neural Networks for Robust Pupil Detection'},\n",
       " u'1602.06872': {'arxivid': u'1602.06872',\n",
       "  'authorsaffil': [[u'Roy Frostig', None],\n",
       "   [u'Cameron Musco', None],\n",
       "   [u'Christopher Musco', None],\n",
       "   [u'Aaron Sidford', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06872v1',\n",
       "  'published': u'2016-02-22T17:52:02Z',\n",
       "  'summary': u'  We show how to efficiently project a vector onto the top principal components\\nof a matrix, without explicitly computing these components. Specifically, we\\nintroduce an iterative algorithm that provably computes the projection using\\nfew calls to any black-box routine for ridge regression.\\n  By avoiding explicit principal component analysis (PCA), our algorithm is the\\nfirst with no runtime dependence on the number of top principal components. We\\nshow that it can be used to give a fast iterative method for the popular\\nprincipal component regression problem, giving the first major runtime\\nimprovement over the naive method of combining PCA with regression.\\n  To achieve our results, we first observe that ridge regression can be used to\\nobtain a \"smooth projection\" onto the top principal components. We then sharpen\\nthis approximation to true projection using a low-degree polynomial\\napproximation to the matrix step function. Step function approximation is a\\ntopic of long-term interest in scientific computing. We extend prior theory by\\nconstructing polynomials with simple iterative structure and rigorously\\nanalyzing their behavior under limited precision.\\n',\n",
       "  'title': u'Principal Component Projection Without Principal Component Analysis'},\n",
       " u'1503.08895': {'arxivid': u'1503.08895',\n",
       "  'authorsaffil': [[u'Sainbayar Sukhbaatar', None],\n",
       "   [u'Arthur Szlam', None],\n",
       "   [u'Jason Weston', None],\n",
       "   [u'Rob Fergus', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.CL'],\n",
       "  'comment': u'Accepted to NIPS 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.08895v5',\n",
       "  'published': u'2015-03-31T03:05:37Z',\n",
       "  'summary': u'  We introduce a neural network with a recurrent attention model over a\\npossibly large external memory. The architecture is a form of Memory Network\\n(Weston et al., 2015) but unlike the model in that work, it is trained\\nend-to-end, and hence requires significantly less supervision during training,\\nmaking it more generally applicable in realistic settings. It can also be seen\\nas an extension of RNNsearch to the case where multiple computational steps\\n(hops) are performed per output symbol. The flexibility of the model allows us\\nto apply it to tasks as diverse as (synthetic) question answering and to\\nlanguage modeling. For the former our approach is competitive with Memory\\nNetworks, but with less supervision. For the latter, on the Penn TreeBank and\\nText8 datasets our approach demonstrates comparable performance to RNNs and\\nLSTMs. In both cases we show that the key concept of multiple computational\\nhops yields improved results.\\n',\n",
       "  'title': u'End-To-End Memory Networks'},\n",
       " u'1602.08960': {'arxivid': u'1602.08960',\n",
       "  'authorsaffil': [[u'Roberto P. Palomares', None],\n",
       "   [u'Gloria Haro', None],\n",
       "   [u'Coloma Ballester', None],\n",
       "   [u'Enric Meinhardt-Llopis', None]],\n",
       "  'categoryterms': [u'cs.CV', u'68U10, 49M29, 65K10'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08960v1',\n",
       "  'published': u'2016-02-29T13:54:39Z',\n",
       "  'summary': u'  We propose a large displacement optical flow method that introduces a new\\nstrategy to compute a good local minimum of any optical flow energy functional.\\nThe method requires a given set of discrete matches, which can be extremely\\nsparse, and an energy functional. The matches are used to guide a structured\\ncoordinate-descent of the energy functional around these keypoints. It results\\nin a two-step minimization method at the finest scale which is very robust to\\nthe inevitable outliers of the sparse matcher, and it is better than the multi-\\nscale methods, especially when there are small objects with very large\\ndisplacements, that the multi-scale methods are incapable to find. Indeed, the\\nproposed method recovers the correct motion field of any object which has at\\nleast one correct match, regardless of the magnitude of the displacement. We\\nvalidate our proposal using several optical flow variational models. The\\nresults consistently outperform the coarse-to-fine approaches and achieve good\\nqualitative and quantitative performance on the standard optical flow\\nbenchmarks.\\n',\n",
       "  'title': u'FALDOI: Large Displacement Optical Flow by Astute Initialization'},\n",
       " u'1602.05257': {'arxivid': u'1602.05257',\n",
       "  'authorsaffil': [[u'Deovrat Kakde', None],\n",
       "   [u'Arin Chaudhuri', None],\n",
       "   [u'Seunghyun Kong', None],\n",
       "   [u'Maria Jahja', None],\n",
       "   [u'Hansi Jiang', None],\n",
       "   [u'Jorge Silva', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05257v2',\n",
       "  'published': u'2016-02-17T00:51:18Z',\n",
       "  'summary': u'  Support Vector Data Description (SVDD) is a machine-learning technique used\\nfor single class classification and outlier detection. SVDD formulation with\\nkernel function provides a flexible boundary around data. The value of kernel\\nfunction parameters affects the nature of the data boundary. For example, it is\\nobserved that with a Gaussian kernel, as the value of kernel bandwidth is\\nlowered, the data boundary changes from spherical to wiggly. The spherical data\\nboundary leads to underfitting, and an extremely wiggly data boundary leads to\\noverfitting. In this paper, we propose empirical criterion to obtain good\\nvalues of the Gaussian kernel bandwidth parameter. This criterion provides a\\nsmooth boundary that captures the essential geometric features of the data.\\n',\n",
       "  'title': u'Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector\\n  Data Description'},\n",
       " u'1602.05256': {'arxivid': u'1602.05256',\n",
       "  'authorsaffil': [[u'Wichai Shanklin', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.GR'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05256v1',\n",
       "  'published': u'2016-02-17T00:41:58Z',\n",
       "  'summary': u'  The scanning electron microscopy (SEM) is probably one the most fascinating\\nexamination approach that has been used since more than two decades to detailed\\ninspection of micro scale objects. Most of the scanning electron microscopes\\ncould only produce 2D images that could not assist operational analysis of\\nmicroscopic surface properties. Computer vision algorithms combined with very\\nadvanced geometry and mathematical approaches turn any SEM into a full 3D\\nmeasurement device. This work focuses on a methodical literature review for\\nautomatic 3D surface reconstruction of scanning electron microscope images.\\n',\n",
       "  'title': u'2D SEM images turn into 3D object models'},\n",
       " u'1602.05703': {'arxivid': u'1602.05703',\n",
       "  'authorsaffil': [[u'Paolo Di Lorenzo', None],\n",
       "   [u'Sergio Barbarossa', None],\n",
       "   [u'Paolo Banelli', None],\n",
       "   [u'Stefania Sardellitti', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.SY'],\n",
       "  'comment': u'Submitted to IEEE Transactions on Signal and Information Processing\\n  over Networks',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05703v2',\n",
       "  'published': u'2016-02-18T07:34:04Z',\n",
       "  'summary': u'  In many applications spanning from sensor to social networks, transportation\\nsystems, gene regulatory networks or big data, the signals of interest are\\ndefined over the vertices of a graph. The aim of this paper is to propose a\\nleast mean square (LMS) strategy for adaptive estimation of signals defined\\nover graphs. Assuming the graph signal to be band-limited, over a known\\nbandwidth, the method enables reconstruction, with guaranteed performance in\\nterms of mean-square error, and tracking from a limited number of observations\\nover a subset of vertices. A detailed mean square analysis provides the\\nperformance of the proposed method, and leads to several insights for designing\\nuseful sampling strategies for graph signals. Numerical results validate our\\ntheoretical findings, and illustrate the performance of the proposed method.\\nFurthermore, to cope with the case where the bandwidth is not known beforehand,\\nwe propose a method that performs a sparse online estimation of the signal\\nsupport in the (graph) frequency domain, which enables online adaptation of the\\ngraph sampling strategy. Finally, we apply the proposed method to build the\\npower spatial density cartography of a given operational region in a cognitive\\nnetwork environment.\\n',\n",
       "  'title': u'Least Mean Squares Estimation of Graph Signals'},\n",
       " u'1510.02358': {'arxivid': u'1510.02358',\n",
       "  'authorsaffil': [[u'Javier Vera', None],\n",
       "   [u'Pedro Montealegre', None],\n",
       "   [u'Eric Goles', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Conflict between recently published and arxiv versions',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.02358v2',\n",
       "  'published': u'2015-10-08T15:15:09Z',\n",
       "  'summary': u'  The Naming Game has been studied to explore the role of self-organization in\\nthe development and negotiation of linguistic conventions. In this paper, we\\ndefine an automata networks approach to the Naming Game. Two problems are\\nfaced: (1) the definition of an automata networks for multi-party communicative\\ninteractions; and (2) the proof of convergence for three different orders in\\nwhich the individuals are updated (updating schemes). Finally, computer\\nsimulations are explored in two-dimensional lattices with the purpose to\\nrecover the main features of the Naming Game and to describe the dynamics under\\ndifferent updating schemes.\\n',\n",
       "  'title': u'Automata networks for multi-party communication in the Naming Game'},\n",
       " u'1510.05830': {'arxivid': u'1510.05830',\n",
       "  'authorsaffil': [[u'Ariel Jaffe', None],\n",
       "   [u'Ethan Fetaya', None],\n",
       "   [u'Boaz Nadler', None],\n",
       "   [u'Tingting Jiang', None],\n",
       "   [u'Yuval Kluger', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.05830v2',\n",
       "  'published': u'2015-10-20T10:48:40Z',\n",
       "  'summary': u'  In unsupervised ensemble learning, one obtains predictions from multiple\\nsources or classifiers, yet without knowing the reliability and expertise of\\neach source, and with no labeled data to assess it. The task is to combine\\nthese possibly conflicting predictions into an accurate meta-learner. Most\\nworks to date assumed perfect diversity between the different sources, a\\nproperty known as conditional independence. In realistic scenarios, however,\\nthis assumption is often violated, and ensemble learners based on it can be\\nseverely sub-optimal. The key challenges we address in this paper are:\\\\ (i) how\\nto detect, in an unsupervised manner, strong violations of conditional\\nindependence; and (ii) construct a suitable meta-learner. To this end we\\nintroduce a statistical model that allows for dependencies between classifiers.\\nOur main contributions are the development of novel unsupervised methods to\\ndetect strongly dependent classifiers, better estimate their accuracies, and\\nconstruct an improved meta-learner. Using both artificial and real datasets, we\\nshowcase the importance of taking classifier dependencies into account and the\\ncompetitive performance of our approach.\\n',\n",
       "  'title': u'Unsupervised Ensemble Learning with Dependent Classifiers'},\n",
       " u'1511.04103': {'arxivid': u'1511.04103',\n",
       "  'authorsaffil': [[u'Panqu Wang', None], [u'Garrison W. Cottrell', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'ICLR 2016 submission R1',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04103v3',\n",
       "  'published': u'2015-11-12T21:41:35Z',\n",
       "  'summary': u\"  Recent advances in deep learning have led to significant progress in the\\ncomputer vision field, especially for visual object recognition tasks. The\\nfeatures useful for object classification are learned by feed-forward deep\\nconvolutional neural networks (CNNs) automatically, and they are shown to be\\nable to predict and decode neural representations in the ventral visual pathway\\nof humans and monkeys. However, despite the huge amount of work on optimizing\\nCNNs, there has not been much research focused on linking CNNs with guiding\\nprinciples from the human visual cortex. In this work, we propose a network\\noptimization strategy inspired by both of the developmental trajectory of\\nchildren's visual object recognition capabilities, and Bar (2003), who\\nhypothesized that basic level information is carried in the fast magnocellular\\npathway through the prefrontal cortex (PFC) and then projected back to inferior\\ntemporal cortex (IT), where subordinate level categorization is achieved. We\\ninstantiate this idea by training a deep CNN to perform basic level object\\ncategorization first, and then train it on subordinate level categorization. We\\napply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC\\n2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,\\ndemonstrating the effectiveness of the method. We also show that subsequent\\ntransfer learning on smaller datasets gives superior results.\\n\",\n",
       "  'title': u'Basic Level Categorization Facilitates Visual Object Recognition'},\n",
       " u'1604.03832': {'arxivid': u'1604.03832',\n",
       "  'authorsaffil': [[u'Mikhail Kharinov', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'5 pages, 3 figures, 6 formulas, submitted to the 13th International\\n  Conference on Pattern Recognition and Information Processing October 3-5,\\n  2016, Minsk, Belarus',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03832v1',\n",
       "  'published': u'2016-04-13T15:31:24Z',\n",
       "  'summary': u'  In this paper a hierarchical model for pixel clustering and image\\nsegmentation is developed. In the model an image is hierarchically structured.\\nThe original image is treated as a set of nested images, which are capable to\\nreversibly merge with each other. An object is defined as a structural element\\nof an image, so that, an image is regarded as a maximal object. The simulating\\nof none-hierarchical optimal pixel clustering by hierarchical clustering is\\nstudied. To generate a hierarchy of optimized piecewise constant image\\napproximations, estimated by the standard deviation of approximation from the\\nimage, the conversion of any hierarchy of approximations into the hierarchy\\ndescribed in relation to the number of intensity levels by convex sequence of\\ntotal squared errors is proposed.\\n',\n",
       "  'title': u'Reversible Image Merging for Low-level Machine Vision'},\n",
       " u'1601.03916': {'arxivid': u'1601.03916',\n",
       "  'authorsaffil': [[u'Julian Hitschler', None],\n",
       "   [u'Shigehiko Schamoni', None],\n",
       "   [u'Stefan Riezler', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Final version, accepted at ACL 2016. New section on Human Evaluation',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03916v3',\n",
       "  'published': u'2016-01-15T13:42:04Z',\n",
       "  'summary': u'  We present an approach to improve statistical machine translation of image\\ndescriptions by multimodal pivots defined in visual space. The key idea is to\\nperform image retrieval over a database of images that are captioned in the\\ntarget language, and use the captions of the most similar images for\\ncrosslingual reranking of translation outputs. Our approach does not depend on\\nthe availability of large amounts of in-domain parallel data, but only relies\\non available large datasets of monolingually captioned images, and on\\nstate-of-the-art convolutional neural networks to compute image similarities.\\nOur experimental evaluation shows improvements of 1 BLEU point over strong\\nbaselines.\\n',\n",
       "  'title': u'Multimodal Pivots for Image Caption Translation'},\n",
       " u'1603.00964': {'arxivid': u'1603.00964',\n",
       "  'authorsaffil': [[u'Zhen Zeng', None], [u'Benjamin Kuipers', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00964v1',\n",
       "  'published': u'2016-03-03T03:49:02Z',\n",
       "  'summary': u\"  We aim to enable robot to learn tabletop object manipulation by imitation.\\nGiven external observations of demonstrations on object manipulations, we\\nbelieve that two underlying problems to address in learning by imitation is 1)\\nsegment a given demonstration into skills that can be individually learned and\\nreused, and 2) formulate the correct RL (Reinforcement Learning) problem that\\nonly considers the relevant aspects of each skill so that the policy for each\\nskill can be effectively learned. Previous works made certain progress in this\\ndirection, but none has taken private information into account. The public\\ninformation is the information that is available in the external observations\\nof demonstration, and the private information is the information that are only\\navailable to the agent that executes the actions, such as tactile sensations.\\nOur contribution is that we provide a method for the robot to automatically\\nsegment the demonstration into multiple skills, and formulate the correct RL\\nproblem for each skill, and automatically decide whether the private\\ninformation is an important aspect of each skill based on interaction with the\\nworld. Our motivating example is for a real robot to play the shape sorter game\\nby imitating other's behavior, and we will show the results in a simulated 2D\\nenvironment that captures the important properties of the shape sorter game.\\nThe evaluation is based on whether the demonstration is reasonably segmented,\\nand whether the correct RL problems are formulated. In the end, we will show\\nthat robot can imitate the demonstrated behavior based on learned policies.\\n\",\n",
       "  'title': u'Learning Tabletop Object Manipulation by Imitation'},\n",
       " u'1512.05726': {'arxivid': u'1512.05726',\n",
       "  'authorsaffil': [[u'Tao Lei', None],\n",
       "   [u'Hrishikesh Joshi', None],\n",
       "   [u'Regina Barzilay', None],\n",
       "   [u'Tommi Jaakkola', None],\n",
       "   [u'Katerina Tymoshenko', None],\n",
       "   [u'Alessandro Moschitti', None],\n",
       "   [u'Lluis Marquez', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.NE'],\n",
       "  'comment': u'NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05726v2',\n",
       "  'published': u'2015-12-17T19:14:20Z',\n",
       "  'summary': u'  Question answering forums are rapidly growing in size with no effective\\nautomated ability to refer to and reuse answers already available for previous\\nposted questions. In this paper, we develop a methodology for finding\\nsemantically related questions. The task is difficult since 1) key pieces of\\ninformation are often buried in extraneous details in the question body and 2)\\navailable annotations on similar questions are scarce and fragmented. We design\\na recurrent and convolutional model (gated convolution) to effectively map\\nquestions to their semantic representations. The models are pre-trained within\\nan encoder-decoder framework (from body to title) on the basis of the entire\\nraw corpus, and fine-tuned discriminatively from limited annotations. Our\\nevaluation demonstrates that our model yields substantial gains over a standard\\nIR baseline and various neural network architectures (including CNNs, LSTMs and\\nGRUs).\\n',\n",
       "  'title': u'Semi-supervised Question Retrieval with Gated Convolutions'},\n",
       " u'1509.03877': {'arxivid': u'1509.03877',\n",
       "  'authorsaffil': [[u'Zhen Zuo', None],\n",
       "   [u'Bing Shuai', None],\n",
       "   [u'Gang Wang', None],\n",
       "   [u'Xiao Liu', None],\n",
       "   [u'Xingxing Wang', None],\n",
       "   [u'Bing Wang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.03877v2',\n",
       "  'published': u'2015-09-13T18:33:15Z',\n",
       "  'summary': u'  Existing deep convolutional neural networks (CNNs) have shown their great\\nsuccess on image classification. CNNs mainly consist of convolutional and\\npooling layers, both of which are performed on local image areas without\\nconsidering the dependencies among different image regions. However, such\\ndependencies are very important for generating explicit image representation.\\nIn contrast, recurrent neural networks (RNNs) are well known for their ability\\nof encoding contextual information among sequential data, and they only require\\na limited number of network parameters. General RNNs can hardly be directly\\napplied on non-sequential data. Thus, we proposed the hierarchical RNNs\\n(HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependencies\\namong image regions from the same scale but different locations. While the\\ncross RNN scale connections target on modeling scale dependencies among regions\\nfrom the same location but different scales. Specifically, we propose two\\nrecurrent neural network models: 1) hierarchical simple recurrent network\\n(HSRN), which is fast and has low computational cost; and 2) hierarchical\\nlong-short term memory recurrent network (HLSTM), which performs better than\\nHSRN with the price of more computational cost.\\n  In this manuscript, we integrate CNNs with HRNNs, and develop end-to-end\\nconvolutional hierarchical recurrent neural networks (C-HRNNs). C-HRNNs not\\nonly make use of the representation power of CNNs, but also efficiently encodes\\nspatial and scale dependencies among different image regions. On four of the\\nmost challenging object/scene image classification benchmarks, our C-HRNNs\\nachieve state-of-the-art results on Places 205, SUN 397, MIT indoor, and\\ncompetitive results on ILSVRC 2012.\\n',\n",
       "  'title': u'Learning Contextual Dependencies with Convolutional Hierarchical\\n  Recurrent Neural Networks'},\n",
       " u'1511.05897': {'arxivid': u'1511.05897',\n",
       "  'authorsaffil': [[u'Harrison Edwards', None], [u'Amos Storkey', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'stat.ML'],\n",
       "  'comment': u'Paper accepted to ICLR',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05897v3',\n",
       "  'published': u'2015-11-18T18:06:24Z',\n",
       "  'summary': u'  In practice, there are often explicit constraints on what representations or\\ndecisions are acceptable in an application of machine learning. For example it\\nmay be a legal requirement that a decision must not favour a particular group.\\nAlternatively it can be that that representation of data must not have\\nidentifying information. We address these two related issues by learning\\nflexible representations that minimize the capability of an adversarial critic.\\nThis adversary is trying to predict the relevant sensitive variable from the\\nrepresentation, and so minimizing the performance of the adversary ensures\\nthere is little or no information in the representation about the sensitive\\nvariable. We demonstrate this adversarial approach on two problems: making\\ndecisions free from discrimination and removing private information from\\nimages. We formulate the adversarial model as a minimax problem, and optimize\\nthat minimax objective using a stochastic gradient alternate min-max optimizer.\\nWe demonstrate the ability to provide discriminant free representations for\\nstandard test problems, and compare with previous state of the art methods for\\nfairness, showing statistically significant improvement across most cases. The\\nflexibility of this method is shown via a novel problem: removing annotations\\nfrom images, from unaligned training examples of annotated and unannotated\\nimages, and with no a priori knowledge of the form of annotation provided to\\nthe model.\\n',\n",
       "  'title': u'Censoring Representations with an Adversary'},\n",
       " u'1601.01085': {'arxivid': u'1601.01085',\n",
       "  'authorsaffil': [[u'Trevor Cohn', None],\n",
       "   [u'Cong Duy Vu Hoang', None],\n",
       "   [u'Ekaterina Vymolova', None],\n",
       "   [u'Kaisheng Yao', None],\n",
       "   [u'Chris Dyer', None],\n",
       "   [u'Gholamreza Haffari', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'10 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01085v1',\n",
       "  'published': u'2016-01-06T06:03:17Z',\n",
       "  'summary': u'  Neural encoder-decoder models of machine translation have achieved impressive\\nresults, rivalling traditional translation models. However their modelling\\nformulation is overly simplistic, and omits several key inductive biases built\\ninto traditional models. In this paper we extend the attentional neural\\ntranslation model to include structural biases from word based alignment\\nmodels, including positional bias, Markov conditioning, fertility and agreement\\nover translation directions. We show improvements over a baseline attentional\\nmodel and standard phrase-based model over several language pairs, evaluating\\non difficult languages in a low resource setting.\\n',\n",
       "  'title': u'Incorporating Structural Alignment Biases into an Attentional Neural\\n  Translation Model'},\n",
       " u'1602.02743': {'arxivid': u'1602.02743',\n",
       "  'authorsaffil': [[u'Michael Brand', None], [u'David L. Dowe', None]],\n",
       "  'categoryterms': [u'cs.LO',\n",
       "   u'cs.AI',\n",
       "   u'cs.CC',\n",
       "   u'cs.FL',\n",
       "   u'68Q32 (Primary) 68Q05, 68T42, 68T05',\n",
       "   u'F.1.1; F.4.1; I.2.6'],\n",
       "  'comment': u'23 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02743v1',\n",
       "  'published': u'2016-02-07T04:17:17Z',\n",
       "  'summary': u'  We introduce a problem set-up we call the Iterated Matching Pennies (IMP)\\ngame and show that it is a powerful framework for the study of three problems:\\nadversarial learnability, conventional (i.e., non-adversarial) learnability and\\napproximability. Using it, we are able to derive the following theorems. (1) It\\nis possible to learn by example all of $\\\\Sigma^0_1 \\\\cup \\\\Pi^0_1$ as well as\\nsome supersets; (2) in adversarial learning (which we describe as a\\npursuit-evasion game), the pursuer has a winning strategy (in other words,\\n$\\\\Sigma^0_1$ can be learned adversarially, but $\\\\Pi^0_1$ not); (3) some\\nlanguages in $\\\\Pi^0_1$ cannot be approximated by any language in $\\\\Sigma^0_1$.\\n  We show corresponding results also for $\\\\Sigma^0_i$ and $\\\\Pi^0_i$ for\\narbitrary $i$.\\n',\n",
       "  'title': u'The IMP game: Learnability, approximability and adversarial learning\\n  beyond $\\u03a3^0_1$'},\n",
       " u'1511.09120': {'arxivid': u'1511.09120',\n",
       "  'authorsaffil': [[u'Soliman Nasser', None],\n",
       "   [u'Ibrahim Jubran', None],\n",
       "   [u'Dan Feldman', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.09120v2',\n",
       "  'published': u'2015-11-30T00:44:41Z',\n",
       "  'summary': u'  How can a \\\\$20 toy quadcopter hover using a weak \"Internet of Things\"\\nmini-board and a web-cam? In the pose-estimation problem we need to rotate a\\nset of $n$ marker (points) and choose one of their n! permutations, so that the\\nsum of squared corresponding distances to another ordered set of $n$ markers is\\nminimized. A popular heuristic for this problem is ICP.\\n  We prove that \\\\emph{every} set has a weighted subset (core-set) of constant\\nsize (independent of $n$), such that computing the optimal orientation of the\\nsmall core-set would yield \\\\emph{exactly} the same result as using the full set\\nof $n$ markers. A deterministic algorithm for computing this core-set in $O(n)$\\ntime is provided, using the Caratheodory Theorem from computational geometry.\\n  We developed a system that enables low-cost and real-time tracking by\\ncomputing this core-set on the cloud in one thread, and uses the last computed\\ncore-set locally in a parallel thread. Our experimental results show how these\\ncore-sets can boost the tracking time and quality for large and even small sets\\nof both IR and RGB markers on a toy quadcopter. Open source code for the system\\nand algorithm is provided\\n',\n",
       "  'title': u'Low-cost and Faster Tracking Systems Using Core-sets for Pose-Estimation'},\n",
       " u'1503.07027': {'arxivid': u'1503.07027',\n",
       "  'authorsaffil': [[u'Karin Schnass', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.IT', u'math.IT'],\n",
       "  'comment': u'32 pages, 1 figure, revised and corrected version including new\\n  numerical section',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.07027v3',\n",
       "  'published': u'2015-03-24T13:29:12Z',\n",
       "  'summary': u'  In this work we show that iterative thresholding and K-means (ITKM)\\nalgorithms can recover a generating dictionary with K atoms from noisy $S$\\nsparse signals up to an error $\\\\tilde \\\\varepsilon$ as long as the\\ninitialisation is within a convergence radius, that is up to a $\\\\log K$ factor\\ninversely proportional to the dynamic range of the signals, and the sample size\\nis proportional to $K \\\\log K \\\\tilde \\\\varepsilon^{-2}$. The results are valid\\nfor arbitrary target errors if the sparsity level is of the order of the square\\nof the signal dimension $d$ and for target errors down to $K^{-\\\\ell}$ if $S$\\nscales as $S \\\\leq d/(\\\\ell \\\\log K)$.\\n',\n",
       "  'title': u'Convergence radius and sample complexity of ITKM algorithms for\\n  dictionary learning'},\n",
       " u'1511.04383': {'arxivid': u'1511.04383',\n",
       "  'authorsaffil': [[u'Bopeng Li', None],\n",
       "   [u'Sougata Chaudhuri', None],\n",
       "   [u'Ambuj Tewari', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.SI'],\n",
       "  'comment': u'The paper has been withdrawn due to a baseline implementation error\\n  in experiments',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04383v2',\n",
       "  'published': u'2015-11-13T18:06:15Z',\n",
       "  'summary': u'  We consider the link prediction problem in a partially observed network,\\nwhere the objective is to make predictions in the unobserved portion of the\\nnetwork. Many existing methods reduce link prediction to binary classification\\nproblem. However, the dominance of absent links in real world networks makes\\nmisclassification error a poor performance metric. Instead, researchers have\\nargued for using ranking performance measures, like AUC, AP and NDCG, for\\nevaluation. Our main contribution is to recast the link prediction problem as a\\nlearning to rank problem and use effective learning to rank techniques directly\\nduring training. This is in contrast to existing work that uses ranking\\nmeasures only during evaluation. Our approach is able to deal with the class\\nimbalance problem by using effective, scalable learning to rank techniques\\nduring training. Furthermore, our approach allows us to combine network\\ntopology and node features. As a demonstration of our general approach, we\\ndevelop a link prediction method by optimizing the cross-entropy surrogate,\\noriginally used in the popular ListNet ranking algorithm. We conduct extensive\\nexperiments on publicly available co-authorship, citation and metabolic\\nnetworks to demonstrate the merits of our method.\\n',\n",
       "  'title': u'Handling Class Imbalance in Link Prediction using Learning to Rank\\n  Techniques'},\n",
       " u'1604.00239': {'arxivid': u'1604.00239',\n",
       "  'authorsaffil': [[u'Piotr Koniusz', None],\n",
       "   [u'Anoop Cherian', None],\n",
       "   [u'Fatih Porikli', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00239v1',\n",
       "  'published': u'2016-04-01T13:41:49Z',\n",
       "  'summary': u'  In this paper, we explore tensor representations that can compactly capture\\nhigher-order relationships between skeleton joints for 3D action recognition.\\nWe first define RBF kernels on 3D joint sequences, which are then linearized to\\nform kernel descriptors. The higher-order outer-products of these kernel\\ndescriptors form our tensor representations. We present two different kernels\\nfor action recognition, namely (i) a sequence compatibility kernel that\\ncaptures the spatio-temporal compatibility of joints in one sequence against\\nthose in the other, and (ii) a dynamics compatibility kernel that explicitly\\nmodels the action dynamics of a sequence. Tensors formed from these kernels are\\nthen used to train an SVM. We present experiments on several benchmark datasets\\nand demonstrate state of the art results, substantiating the effectiveness of\\nour representations.\\n',\n",
       "  'title': u'Tensor Representations via Kernel Linearization for Action Recognition\\n  from 3D Skeletons'},\n",
       " u'1601.02068': {'arxivid': u'1601.02068',\n",
       "  'authorsaffil': [[u'Yining Wang', None], [u'Aarti Singh', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'math.ST', u'stat.TH'],\n",
       "  'comment': u'35 pages, 3 figures, 1 table',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02068v2',\n",
       "  'published': u'2016-01-09T03:05:31Z',\n",
       "  'summary': u'  Subsampling strategies are derived to sample a small portion of design (data)\\npoints in a low-dimensional linear regression model $y=X\\\\beta+\\\\varepsilon$ with\\nnear-optimal statistical rates. Our results apply to both problems of\\nestimation of the underlying linear model $\\\\beta$ and predicting the\\nreal-valued response $y$ of a new data point $x$. The derived subsampling\\nstrategies are minimax optimal under the fixed design setting, up to a small\\n$(1+\\\\epsilon)$ relative factor. We also give interpretable subsampling\\nprobabilities for the random design setting and demonstrate explicit gaps in\\nstatistial rates between optimal and baseline (e.g., uniform) subsampling\\nmethods.\\n',\n",
       "  'title': u'Minimax Subsampling for Estimation and Prediction in Low-Dimensional\\n  Linear Regression'},\n",
       " u'1604.01879': {'arxivid': u'1604.01879',\n",
       "  'authorsaffil': [[u'Song Bai', None],\n",
       "   [u'Xiang Bai', None],\n",
       "   [u'Zhichao Zhou', None],\n",
       "   [u'Zhaoxiang Zhang', None],\n",
       "   [u'Longin Jan Latecki', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'accepted by CVPR16, achieved the first place in Shrec2016\\n  competition: Large-Scale 3D Shape Retrieval under the perturbed case',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01879v1',\n",
       "  'published': u'2016-04-07T05:45:56Z',\n",
       "  'summary': u'  Projective analysis is an important solution for 3D shape retrieval, since\\nhuman visual perceptions of 3D shapes rely on various 2D observations from\\ndifferent view points. Although multiple informative and discriminative views\\nare utilized, most projection-based retrieval systems suffer from heavy\\ncomputational cost, thus cannot satisfy the basic requirement of scalability\\nfor search engines. In this paper, we present a real-time 3D shape search\\nengine based on the projective images of 3D shapes. The real-time property of\\nour search engine results from the following aspects: (1) efficient projection\\nand view feature extraction using GPU acceleration; (2) the first inverted\\nfile, referred as F-IF, is utilized to speed up the procedure of multi-view\\nmatching; (3) the second inverted file (S-IF), which captures a local\\ndistribution of 3D shapes in the feature manifold, is adopted for efficient\\ncontext-based re-ranking. As a result, for each query the retrieval task can be\\nfinished within one second despite the necessary cost of IO overhead. We name\\nthe proposed 3D shape search engine, which combines GPU acceleration and\\nInverted File Twice, as GIFT. Besides its high efficiency, GIFT also\\noutperforms the state-of-the-art methods significantly in retrieval accuracy on\\nvarious shape benchmarks and competitions.\\n',\n",
       "  'title': u'GIFT: A Real-time and Scalable 3D Shape Search Engine'},\n",
       " u'1602.01971': {'arxivid': u'1602.01971',\n",
       "  'authorsaffil': [[u'Erik Andresen', None],\n",
       "   [u'David Haensel', None],\n",
       "   [u'Mohcine Chraibi', None],\n",
       "   [u'Armin Seyfried', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u\"8 pages, 3 figures, TGF'15 Conference, 2015\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01971v1',\n",
       "  'published': u'2016-02-05T10:25:15Z',\n",
       "  'summary': u\"  Usually, routing models in pedestrian dynamics assume that agents have\\nfulfilled and global knowledge about the building's structure. However, they\\nneglect the fact that pedestrians possess no or only parts of information about\\ntheir position relative to final exits and possible routes leading to them. To\\nget a more realistic description we introduce the systematics of gathering and\\nusing spatial knowledge. A new wayfinding model for pedestrian dynamics is\\nproposed. The model defines for every pedestrian an individual knowledge\\nrepresentation implying inaccuracies and uncertainties. In addition,\\nknowledge-driven search strategies are introduced. The presented concept is\\ntested on a fictive example scenario.\\n\",\n",
       "  'title': u'Wayfinding and cognitive maps for pedestrian models'},\n",
       " u'1604.03901': {'arxivid': u'1604.03901',\n",
       "  'authorsaffil': [[u'Weifeng Chen', None],\n",
       "   [u'Zhao Fu', None],\n",
       "   [u'Dawei Yang', None],\n",
       "   [u'Jia Deng', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03901v1',\n",
       "  'published': u'2016-04-13T18:19:35Z',\n",
       "  'summary': u'  This paper studies single-image depth perception in the wild, i.e.,\\nrecovering depth from a single image taken in unconstrained settings. We\\nintroduce a new dataset \"Depth in the Wild\" consisting of images in the wild\\nannotated with relative depth between pairs of random points. We also propose a\\nnew algorithm that learns to estimate metric depth using annotations of\\nrelative depth. Compared to the state of the art, our algorithm is simpler and\\nperforms better. Experiments show that our algorithm, combined with existing\\nRGB-D data and our new relative depth annotations, significantly improves\\nsingle-image depth perception in the wild.\\n',\n",
       "  'title': u'Single-Image Depth Perception in the Wild'},\n",
       " u'1603.07954': {'arxivid': u'1603.07954',\n",
       "  'authorsaffil': [[u'Karthik Narasimhan', None],\n",
       "   [u'Adam Yala', None],\n",
       "   [u'Regina Barzilay', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'10 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07954v1',\n",
       "  'published': u'2016-03-25T16:38:54Z',\n",
       "  'summary': u'  In traditional formulations, information extraction systems operate on a\\nfixed collection of documents. In this work, we explore the task of acquiring\\nand incorporating external evidence to improve extraction accuracy. This\\nprocess entails query reformulation for search, extraction from new sources and\\nreconciliation of extracted values, which are repeated until sufficient\\nevidence is collected. We approach the problem using a reinforcement learning\\nframework where our model learns to select optimal actions based on contextual\\ninformation. We employ a deep Q-network, trained to optimize a reward function\\nthat reflects extraction accuracy while penalizing extra effort. Our\\nexperiments on a publicly available database of shooting incidents demonstrate\\nthat our system outperforms traditional extractors by 7.2% on average.\\n',\n",
       "  'title': u'Improving Information Extraction by Acquiring External Evidence with\\n  Reinforcement Learning'},\n",
       " u'1603.07957': {'arxivid': u'1603.07957',\n",
       "  'authorsaffil': [[u'Fuqiang Liu', None],\n",
       "   [u'Fukun Bi', None],\n",
       "   [u'Liang Chen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'16 pages, 6 figures, 2 tables',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07957v1',\n",
       "  'published': u'2016-03-25T16:44:35Z',\n",
       "  'summary': u'  This paper proposes a novel semi-supervised method on object recognition.\\nFirst, based on Boost Picking, a universal algorithm, Boost Picking Teaching\\n(BPT), is proposed to train an effective binary-classifier just using a few\\nlabeled data and amounts of unlabeled data. Then, an ensemble strategy is\\ndetailed to synthesize multiple BPT-trained binary-classifiers to be a\\nhigh-performance multi-classifier. The rationality of the strategy is also\\nanalyzed in theory. Finally, the proposed method is tested on two databases,\\nCIFAR-10 and CIFAR-100. Using 2% labeled data and 98% unlabeled data, the\\naccuracies of the proposed method on the two data sets are 78.39% and 50.77%\\nrespectively.\\n',\n",
       "  'title': u'Object Recognition Based on Amounts of Unlabeled Data'},\n",
       " u'1503.06866': {'arxivid': u'1503.06866',\n",
       "  'authorsaffil': [[u'Serge Alain Eb\\xe9l\\xe9', None],\n",
       "   [u'Ren\\xe8 Ndoundam', None]],\n",
       "  'categoryterms': [u'cs.NE', u'92B20', u'F.1.1'],\n",
       "  'comment': u'22 pages',\n",
       "  'doi': None,\n",
       "  'journalref': u'Complex Systems, 24, 2015',\n",
       "  'link': u'http://arxiv.org/abs/1503.06866v4',\n",
       "  'published': u'2015-03-23T22:30:39Z',\n",
       "  'summary': u'  We characterize the structure of the periods of a neuronal recurrence\\nequation. Firstly, we give a characterization of k-chains in 0-1 periodic\\nsequences. Secondly, we characterize the periods of all cycles of some neuronal\\nrecurrence equation. Thirdly, we explain how these results can be used to\\ndeduce the existence of the generalized period-halving bifurcation.\\n',\n",
       "  'title': u'Study of all the periods of a Neuronal Recurrence Equation'},\n",
       " u'1511.00736': {'arxivid': u'1511.00736',\n",
       "  'authorsaffil': [[u'Wajdi Dhifli', None],\n",
       "   [u'Abdoulaye Banir\\xe9 Diallo', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.SI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.00736v2',\n",
       "  'published': u'2015-11-02T23:02:48Z',\n",
       "  'summary': u'  Studying the function of proteins is important for understanding the\\nmolecular mechanisms of life. The number of publicly available protein\\nstructures has increasingly become extremely large. Still, the determination of\\nthe function of a protein structure remains a difficult, costly, and time\\nconsuming task. The difficulties are often due to the essential role of spatial\\nand topological structures in the determination of protein functions in living\\ncells. In this paper, we propose ProtNN, a novel approach for protein function\\nprediction. Given an unannotated protein structure and a set of annotated\\nproteins, ProtNN finds the nearest neighbor annotated structures based on\\nprotein-graph pairwise similarities. Given a query protein, ProtNN finds the\\nnearest neighbor reference proteins based on a graph representation model and a\\npairwise similarity between vector embedding of both query and reference\\nprotein-graphs in structural and topological spaces. ProtNN assigns to the\\nquery protein the function with the highest number of votes across the set of k\\nnearest neighbor reference proteins, where k is a user-defined parameter.\\nExperimental evaluation demonstrates that ProtNN is able to accurately classify\\nseveral datasets in an extremely fast runtime compared to state-of-the-art\\napproaches. We further show that ProtNN is able to scale up to a whole PDB\\ndataset in a single-process mode with no parallelization, with a gain of\\nthousands order of magnitude of runtime compared to state-of-the-art\\napproaches.\\n',\n",
       "  'title': u'ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction\\n  based on Graph Embedding in Structural and Topological Space'},\n",
       " u'1601.03822': {'arxivid': u'1601.03822',\n",
       "  'authorsaffil': [[u'Hossein Keshavarz', None],\n",
       "   [u'Clayton Scott', None],\n",
       "   [u'XuanLong Nguyen', None]],\n",
       "  'categoryterms': [u'math.ST', u'cs.LG', u'stat.ML', u'stat.TH'],\n",
       "  'comment': u'34 pages, 2 Figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03822v1',\n",
       "  'published': u'2016-01-15T05:47:29Z',\n",
       "  'summary': u\"  Gaussian random fields are a powerful tool for modeling environmental\\nprocesses. For high dimensional samples, classical approaches for estimating\\nthe covariance parameters require highly challenging and massive computations,\\nsuch as the evaluation of the Cholesky factorization or solving linear systems.\\nRecently, Anitescu, Chen and Stein \\\\cite{M.Anitescu} proposed a fast and\\nscalable algorithm which does not need such burdensome computations. The main\\nfocus of this article is to study the asymptotic behavior of the algorithm of\\nAnitescu et al. (ACS) for regular and irregular grids in the increasing domain\\nsetting. Consistency, minimax optimality and asymptotic normality of this\\nalgorithm are proved under mild differentiability conditions on the covariance\\nfunction. Despite the fact that ACS's method entails a non-concave\\nmaximization, our results hold for any stationary point of the objective\\nfunction. A numerical study is presented to evaluate the efficiency of this\\nalgorithm for large data sets.\\n\",\n",
       "  'title': u'On the consistency of inversion-free parameter estimation for Gaussian\\n  random fields'},\n",
       " u'1601.03821': {'arxivid': u'1601.03821',\n",
       "  'authorsaffil': [[u'Guangcong Zhang', None],\n",
       "   [u'Mason J. Lilly', None],\n",
       "   [u'Patricio A. Vela', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted to 2016 ICRA (IEEE International Conference on Robotics and\\n  Automation)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03821v2',\n",
       "  'published': u'2016-01-15T05:40:13Z',\n",
       "  'summary': u'  This paper proposes a simple yet effective approach to learn visual features\\nonline for improving loop-closure detection and place recognition, based on\\nbag-of-words frameworks. The approach learns a codeword in bag-of-words model\\nfrom a pair of matched features from two consecutive frames, such that the\\ncodeword has temporally-derived perspective invariance to camera motion. The\\nlearning algorithm is efficient: the binary descriptor is generated from the\\nmean image patch, and the mask is learned based on discriminative projection by\\nminimizing the intra-class distances among the learned feature and the two\\noriginal features. A codeword for bag-of-words models is generated by packaging\\nthe learned descriptor and mask, with a masked Hamming distance defined to\\nmeasure the distance between two codewords. The geometric properties of the\\nlearned codewords are then mathematically justified. In addition, hypothesis\\nconstraints are imposed through temporal consistency in matched codewords,\\nwhich improves precision. The approach, integrated in an incremental\\nbag-of-words system, is validated on multiple benchmark data sets and compared\\nto state-of-the-art methods. Experiments demonstrate improved precision/recall\\noutperforming state of the art with little loss in runtime.\\n',\n",
       "  'title': u'Learning Binary Features Online from Motion Dynamics for Incremental\\n  Loop-Closure Detection and Place Recognition'},\n",
       " u'1510.05940': {'arxivid': u'1510.05940',\n",
       "  'authorsaffil': [[u'Lantian Li', None],\n",
       "   [u'Dong Wang', None],\n",
       "   [u'Chao Xing', None],\n",
       "   [u'Thomas Fang Zheng', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.05940v2',\n",
       "  'published': u'2015-10-20T16:01:05Z',\n",
       "  'summary': u'  Probabilistic linear discriminant analysis (PLDA) is a popular normalization\\napproach for the i-vector model, and has delivered state-of-the-art performance\\nin speaker recognition. A potential problem of the PLDA model, however, is that\\nit essentially assumes Gaussian distributions over speaker vectors, which is\\nnot always true in practice. Additionally, the objective function is not\\ndirectly related to the goal of the task, e.g., discriminating true speakers\\nand imposters. In this paper, we propose a max-margin metric learning approach\\nto solve the problems. It learns a linear transform with a criterion that the\\nmargin between target and imposter trials are maximized. Experiments conducted\\non the SRE08 core test show that compared to PLDA, the new approach can obtain\\ncomparable or even better performance, though the scoring is simply a cosine\\ncomputation.\\n',\n",
       "  'title': u'Max-margin Metric Learning for Speaker Recognition'},\n",
       " u'1511.00394': {'arxivid': u'1511.00394',\n",
       "  'authorsaffil': [[u'Francis Bach', u'LIENS, SIERRA']],\n",
       "  'categoryterms': [u'cs.LG', u'math.OC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.00394v2',\n",
       "  'published': u'2015-11-02T06:33:59Z',\n",
       "  'summary': u'  Submodular set-functions have many applications in combinatorial\\noptimization, as they can be minimized and approximately maximized in\\npolynomial time. A key element in many of the algorithms and analyses is the\\npossibility of extending the submodular set-function to a convex function,\\nwhich opens up tools from convex optimization. Submodularity goes beyond\\nset-functions and has naturally been considered for problems with multiple\\nlabels or for functions defined on continuous domains, where it corresponds\\nessentially to cross second-derivatives being nonpositive. In this paper, we\\nshow that most results relating submodularity and convexity for set-functions\\ncan be extended to all submodular functions. In particular, (a) we naturally\\ndefine a continuous extension in a set of probability measures, (b) show that\\nthe extension is convex if and only if the original function is submodular, (c)\\nprove that the problem of minimizing a submodular function is equivalent to a\\ntypically non-smooth convex optimization problem, and (d) propose another\\nconvex optimization problem with better computational properties (e.g., a\\nsmooth dual problem). Most of these extensions from the set-function situation\\nare obtained by drawing links with the theory of multi-marginal optimal\\ntransport, which provides also a new interpretation of existing results for\\nset-functions. We then provide practical algorithms to minimize generic\\nsubmodular functions on discrete domains, with associated convergence rates.\\n',\n",
       "  'title': u'Submodular Functions: from Discrete to Continous Domains'},\n",
       " u'1601.02088': {'arxivid': u'1601.02088',\n",
       "  'authorsaffil': [[u'J\\xf6rg Hendrik Kappes', None],\n",
       "   [u'Paul Swoboda', None],\n",
       "   [u'Bogdan Savchynskyy', None],\n",
       "   [u'Tamir Hazan', None],\n",
       "   [u'Christoph Schn\\xf6rr', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02088v1',\n",
       "  'published': u'2016-01-09T09:29:31Z',\n",
       "  'summary': u'  We present a probabilistic graphical model formulation for the graph\\nclustering problem. This enables to locally represent uncertainty of image\\npartitions by approximate marginal distributions in a mathematically\\nsubstantiated way, and to rectify local data term cues so as to close contours\\nand to obtain valid partitions.\\n  We exploit recent progress on globally optimal MAP inference by integer\\nprogramming and on perturbation-based approximations of the log-partition\\nfunction, in order to sample clusterings and to estimate marginal distributions\\nof node-pairs both more accurately and more efficiently than state-of-the-art\\nmethods. Our approach works for any graphically represented problem instance.\\nThis is demonstrated for image segmentation and social network cluster\\nanalysis. Our mathematical ansatz should be relevant also for other\\ncombinatorial problems.\\n',\n",
       "  'title': u'Multicuts and Perturb & MAP for Probabilistic Graph Clustering'},\n",
       " u'1602.07019': {'arxivid': u'1602.07019',\n",
       "  'authorsaffil': [[u'Zhiguo Wang', None],\n",
       "   [u'Haitao Mi', None],\n",
       "   [u'Abraham Ittycheriah', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07019v1',\n",
       "  'published': u'2016-02-23T03:08:50Z',\n",
       "  'summary': u'  Most conventional sentence similarity methods only focus on similar parts of\\ntwo input sentences, and simply ignore the dissimilar parts, which usually give\\nus some clues and semantic meanings about the sentences. In this work, we\\npropose a model to take into account both the similarities and dissimilarities\\nby decomposing and composing lexical semantics over sentences. The model\\nrepresents each word as a vector, and calculates a semantic matching vector for\\neach word based on all words in the other sentence. Then, each word vector is\\ndecomposed into a similar component and a dissimilar component based on the\\nsemantic matching vector. After this, a two-channel CNN model is employed to\\ncapture features by composing the similar and dissimilar components. Finally, a\\nsimilarity score is estimated over the composed feature vectors. Experimental\\nresults show that our model gets the state-of-the-art performance on the answer\\nsentence selection task, and achieves a comparable result on the paraphrase\\nidentification task.\\n',\n",
       "  'title': u'Sentence Similarity Learning by Lexical Decomposition and Composition'},\n",
       " u'1603.06759': {'arxivid': u'1603.06759',\n",
       "  'authorsaffil': [[u'Yanwei Pang', None],\n",
       "   [u'Manli Sun', None],\n",
       "   [u'Xiaoheng Jiang', None],\n",
       "   [u'Xuelong Li', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'A method of Convolutional Neural Networks',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06759v1',\n",
       "  'published': u'2016-03-22T12:33:11Z',\n",
       "  'summary': u'  Network in Netwrok (NiN) is an effective instance and an important extension\\nof Convolutional Neural Network (CNN) consisting of alternating convolutional\\nlayers and pooling layers. Instead of using a linear filter for convolution,\\nNiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to\\nreplace the linear filter. Because of the powerfulness of MLP and $ 1\\\\times 1 $\\nconvolutions in spatial domain, NiN has stronger ability of feature\\nrepresentation and hence results in better recognition rate. However, MLP\\nitself consists of fully connected layers which give rise to a large number of\\nparameters. In this paper, we propose to replace dense shallow MLP with sparse\\nshallow MLP. One or more layers of the sparse shallow MLP are sparely connected\\nin the channel dimension or channel-spatial domain. The proposed method is\\nimplemented by applying unshared convolution across the channel dimension and\\napplying shared convolution across the spatial dimension in some computational\\nlayers. The proposed method is called CiC. Experimental results on the CIFAR10\\ndataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the\\neffectiveness of the proposed CiC method.\\n',\n",
       "  'title': u'Convolution in Convolution for Network in Network'},\n",
       " u'1510.03710': {'arxivid': u'1510.03710',\n",
       "  'authorsaffil': [[u'Miroslav Vodol\\xe1n', None],\n",
       "   [u'Rudolf Kadlec', None],\n",
       "   [u'Jan Kleindienst', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Accepted to Machine Learning for SLU & Interaction NIPS 2015\\n  Workshop. Model description in Section 2.1 simplified compared to the\\n  previous version',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.03710v3',\n",
       "  'published': u'2015-10-13T14:44:01Z',\n",
       "  'summary': u'  This paper presents a hybrid dialog state tracker that combines a rule based\\nand a machine learning based approach to belief state tracking. Therefore, we\\ncall it a hybrid tracker. The machine learning in our tracker is realized by a\\nLong Short Term Memory (LSTM) network. To our knowledge, our hybrid tracker\\nsets a new state-of-the-art result for the Dialog State Tracking Challenge\\n(DSTC) 2 dataset when the system uses only live SLU as its input.\\n',\n",
       "  'title': u'Hybrid Dialog State Tracker'},\n",
       " u'1310.5568': {'arxivid': u'1310.5568',\n",
       "  'authorsaffil': [[u'Larry Bull', None]],\n",
       "  'categoryterms': [u'cs.CE', u'cs.NE'],\n",
       "  'comment': u'arXiv admin note: substantial text overlap with arXiv:1306.4793,\\n  arXiv:1303.7220',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1310.5568v1',\n",
       "  'published': u'2013-10-21T14:31:20Z',\n",
       "  'summary': u'  The computational modeling of genetic regulatory networks is now common\\nplace, either by fitting a system to experimental data or by exploring the\\nbehaviour of abstract systems with the aim of identifying underlying\\nprinciples. This paper presents an approach to the latter, considering the\\nresponse to environmental changes of a well-known model placed upon tunable\\nfitness landscapes. The effects on genome size and gene connectivity are\\nexplored.\\n',\n",
       "  'title': u'Towards Application of the RBNK Model'},\n",
       " u'1602.02009': {'arxivid': u'1602.02009',\n",
       "  'authorsaffil': [[u'Sergei Dytckov', None], [u'Masoud Daneshtalab', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'Withdrawn by the author as the paper is rejected from the target\\n  conference',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02009v2',\n",
       "  'published': u'2016-02-05T13:06:44Z',\n",
       "  'summary': u'  While classical neural networks take a position of a leading method in the\\nmachine learning community, spiking neuromorphic systems bring attention and\\nlarge projects in neuroscience. Spiking neural networks were shown to be able\\nto substitute networks of classical neurons in applied tasks. This work\\nexplores recent hardware designs focusing on perspective applications (like\\nconvolutional neural networks) for both neuron types from the energy efficiency\\nside to analyse whether there is a possibility for spiking neuromorphic\\nhardware to grow up for a wider use. Our comparison shows that spiking hardware\\nis at least on the same level of energy efficiency or even higher than\\nnon-spiking on a level of basic operations. However, on a system level, spiking\\nsystems are outmatched and consume much more energy due to inefficient data\\nrepresentation with a long series of spikes. If spike-driven applications,\\nminimizing an amount of spikes, are developed, spiking neural systems may reach\\nthe energy efficiency level of classical neural systems. However, in the near\\nfuture, both type of neuromorphic systems may benefit from emerging memory\\ntechnologies, minimizing the energy consumption of computation and memory for\\nboth neuron types. That would make infrastructure and data transfer energy\\ndominant on the system level. We expect that spiking neurons have some\\nbenefits, which would allow achieving better energy results. Still the problem\\nof an amount of spikes will still be the major bottleneck for spiking hardware\\nsystems.\\n',\n",
       "  'title': u'Computing with hardware neurons: spiking or classical? Perspectives of\\n  applied Spiking Neural Networks from the hardware side'},\n",
       " u'1602.03534': {'arxivid': u'1602.03534',\n",
       "  'authorsaffil': [[u'Ozan Sener', None],\n",
       "   [u'Hyun Oh Song', None],\n",
       "   [u'Ashutosh Saxena', None],\n",
       "   [u'Silvio Savarese', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03534v3',\n",
       "  'published': u'2016-02-10T21:07:23Z',\n",
       "  'summary': u'  Supervised learning with large scale labeled datasets and deep layered models\\nhas made a paradigm shift in diverse areas in learning and recognition.\\nHowever, this approach still suffers generalization issues under the presence\\nof a domain shift between the training and the test data distribution. In this\\nregard, unsupervised domain adaptation algorithms have been proposed to\\ndirectly address the domain shift problem. In this paper, we approach the\\nproblem from a transductive perspective. We incorporate the domain shift and\\nthe transductive target inference into our framework by jointly solving for an\\nasymmetric similarity metric and the optimal transductive target label\\nassignment. We also show that our model can easily be extended for deep feature\\nlearning in order to learn features which are discriminative in the target\\ndomain. Our experiments show that the proposed method significantly outperforms\\nstate-of-the-art algorithms in both object recognition and digit classification\\nexperiments by a large margin.\\n',\n",
       "  'title': u'Unsupervised Transductive Domain Adaptation'},\n",
       " u'1502.04434': {'arxivid': u'1502.04434',\n",
       "  'authorsaffil': [[u'Sergey Demyanov', None],\n",
       "   [u'James Bailey', None],\n",
       "   [u'Ramamohanarao Kotagiri', None],\n",
       "   [u'Christopher Leckie', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.04434v3',\n",
       "  'published': u'2015-02-16T06:28:35Z',\n",
       "  'summary': u'  In many classification problems a classifier should be robust to small\\nvariations in the input vector. This is a desired property not only for\\nparticular transformations, such as translation and rotation in image\\nclassification problems, but also for all others for which the change is small\\nenough to retain the object perceptually indistinguishable. We propose two\\nextensions of the backpropagation algorithm that train a neural network to be\\nrobust to variations in the feature vector. While the first of them enforces\\nrobustness of the loss function to all variations, the second method trains the\\npredictions to be robust to a particular variation which changes the loss\\nfunction the most. The second methods demonstrates better results, but is\\nslightly slower. We analytically compare the proposed algorithm with two the\\nmost similar approaches (Tangent BP and Adversarial Training), and propose\\ntheir fast versions. In the experimental part we perform comparison of all\\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\\nproposed algorithm depends on the dataset size and data augmentation.\\n',\n",
       "  'title': u'Invariant backpropagation: how to train a transformation-invariant\\n  neural network'},\n",
       " u'1511.04166': {'arxivid': u'1511.04166',\n",
       "  'authorsaffil': [[u'Yin Li', None],\n",
       "   [u'Manohar Paluri', None],\n",
       "   [u'James M. Rehg', None],\n",
       "   [u'Piotr Doll\\xe1r', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Camera ready version for CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04166v2',\n",
       "  'published': u'2015-11-13T06:09:00Z',\n",
       "  'summary': u'  Data-driven approaches for edge detection have proven effective and achieve\\ntop results on modern benchmarks. However, all current data-driven edge\\ndetectors require manual supervision for training in the form of hand-labeled\\nregion segments or object boundaries. Specifically, human annotators mark\\nsemantically meaningful edges which are subsequently used for training. Is this\\nform of strong, high-level supervision actually necessary to learn to\\naccurately detect edges? In this work we present a simple yet effective\\napproach for training edge detectors without human supervision. To this end we\\nutilize motion, and more specifically, the only input to our method is noisy\\nsemi-dense matches between frames. We begin with only a rudimentary knowledge\\nof edges (in the form of image gradients), and alternate between improving\\nmotion estimation and edge detection in turn. Using a large corpus of video\\ndata, we show that edge detectors trained using our unsupervised scheme\\napproach the performance of the same methods trained with full supervision\\n(within 3-5%). Finally, we show that when using a deep network for the edge\\ndetector, our approach provides a novel pre-training scheme for object\\ndetection.\\n',\n",
       "  'title': u'Unsupervised Learning of Edges'},\n",
       " u'1601.01675': {'arxivid': u'1601.01675',\n",
       "  'authorsaffil': [[u'Alexei Zhukov', None],\n",
       "   [u'Victor Kurbatsky', None],\n",
       "   [u'Nikita Tomin', None],\n",
       "   [u'Denis Sidorov', None],\n",
       "   [u'Daniil Panasetsky', None],\n",
       "   [u'Aoife Foley', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'68T05'],\n",
       "  'comment': u'6 pages, 4 figures, 4 tables. Submitted to PSSC',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01675v1',\n",
       "  'published': u'2016-01-07T13:31:41Z',\n",
       "  'summary': u'  One of the most promising approaches for complex technical systems analysis\\nemploys ensemble methods of classification. Ensemble methods enable to build a\\nreliable decision rules for feature space classification in the presence of\\nmany possible states of the system. In this paper, novel techniques based on\\ndecision trees are used for evaluation of the reliability of the regime of\\nelectric power systems. We proposed hybrid approach based on random forests\\nmodels and boosting models. Such techniques can be applied to predict the\\ninteraction of increasing renewable power, storage devices and swiching of\\nsmart loads from intelligent domestic appliances, heaters and air-conditioning\\nunits and electric vehicles with grid for enhanced decision making. The\\nensemble classification methods were tested on the modified 118-bus IEEE power\\nsystem showing that proposed technique can be employed to examine whether the\\npower system is secured under steady-state operating conditions.\\n',\n",
       "  'title': u'Ensemble Methods of Classification for Power Systems Security Assessment'},\n",
       " u'1511.05960': {'arxivid': u'1511.05960',\n",
       "  'authorsaffil': [[u'Kan Chen', None],\n",
       "   [u'Jiang Wang', None],\n",
       "   [u'Liang-Chieh Chen', None],\n",
       "   [u'Haoyuan Gao', None],\n",
       "   [u'Wei Xu', None],\n",
       "   [u'Ram Nevatia', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.05960v2',\n",
       "  'published': u'2015-11-18T20:59:50Z',\n",
       "  'summary': u\"  We propose a novel attention based deep learning architecture for visual\\nquestion answering task (VQA). Given an image and an image related natural\\nlanguage question, VQA generates the natural language answer for the question.\\nGenerating the correct answers requires the model's attention to focus on the\\nregions corresponding to the question, because different questions inquire\\nabout the attributes of different image regions. We introduce an attention\\nbased configurable convolutional neural network (ABC-CNN) to learn such\\nquestion-guided attention. ABC-CNN determines an attention map for an\\nimage-question pair by convolving the image feature map with configurable\\nconvolutional kernels derived from the question's semantics. We evaluate the\\nABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR,\\nand VQA dataset. ABC-CNN model achieves significant improvements over\\nstate-of-the-art methods on these datasets. The question-guided attention\\ngenerated by ABC-CNN is also shown to reflect the regions that are highly\\nrelevant to the questions.\\n\",\n",
       "  'title': u'ABC-CNN: An Attention Based Convolutional Neural Network for Visual\\n  Question Answering'},\n",
       " u'1604.01350': {'arxivid': u'1604.01350',\n",
       "  'authorsaffil': [[u'Kenji Kawaguchi', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'In Proceedings of the 30th AAAI Conference on Artificial Intelligence\\n  (AAAI), 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01350v1',\n",
       "  'published': u'2016-04-05T18:00:02Z',\n",
       "  'summary': u'  Within the framework of probably approximately correct Markov decision\\nprocesses (PAC-MDP), much theoretical work has focused on methods to attain\\nnear optimality after a relatively long period of learning and exploration.\\nHowever, practical concerns require the attainment of satisfactory behavior\\nwithin a short period of time. In this paper, we relax the PAC-MDP conditions\\nto reconcile theoretically driven exploration methods and practical needs. We\\npropose simple algorithms for discrete and continuous state spaces, and\\nillustrate the benefits of our proposed relaxation via theoretical analyses and\\nnumerical examples. Our algorithms also maintain anytime error bounds and\\naverage loss bounds. Our approach accommodates both Bayesian and non-Bayesian\\nmethods.\\n',\n",
       "  'title': u'Bounded Optimal Exploration in MDP'},\n",
       " u'1603.04550': {'arxivid': u'1603.04550',\n",
       "  'authorsaffil': [[u'Bat-Erdene Batsukh', None], [u'Ganbat Tsend', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'5 pages in IJASCSE Volume 5, Issue 3 (March 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04550v1',\n",
       "  'published': u'2016-03-15T04:30:34Z',\n",
       "  'summary': u'  We are introducing new effective computer model for extracting nationality\\nfrom frontal image candidate using face part color, size and distances based on\\ndeep research. Determining face part size, color, and distances is depending on\\na variety of factors including image quality, lighting condition, rotation\\nangle, occlusion and facial emotion. Therefore, first we need to detect a face\\non the image then convert an image into the real input. After that, we can\\ndetermine image candidate gender, face shape, key points and face parts.\\nFinally, we will return the result, based on the comparison of sizes and\\ndistances with the sample measurement table database. While we were measuring\\nsamples, there were big differences between images by their gender and face\\nshapes. Input images must be the frontal face image that has smooth lighting\\nand does not have any rotation angle. The model can be used in military,\\npolice, defense, healthcare, and technology sectors. Finally, Computer can\\ndistinguish nationality from the face image.\\n',\n",
       "  'title': u'Effective Computer Model For Recognizing Nationality From Frontal Image'},\n",
       " u'1603.04553': {'arxivid': u'1603.04553',\n",
       "  'authorsaffil': [[u'Xuezhe Ma', None],\n",
       "   [u'Zhengzhong Liu', None],\n",
       "   [u'Eduard Hovy', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'Accepted by NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04553v1',\n",
       "  'published': u'2016-03-15T04:39:15Z',\n",
       "  'summary': u'  Coreference resolution is one of the first stages in deep language\\nunderstanding and its importance has been well recognized in the natural\\nlanguage processing community. In this paper, we propose a generative,\\nunsupervised ranking model for entity coreference resolution by introducing\\nresolution mode variables. Our unsupervised system achieves 58.44% F1 score of\\nthe CoNLL metric on the English data from the CoNLL-2012 shared task (Pradhan\\net al., 2012), outperforming the Stanford deterministic system (Lee et al.,\\n2013) by 3.01%.\\n',\n",
       "  'title': u'Unsupervised Ranking Model for Entity Coreference Resolution'},\n",
       " u'1512.07344': {'arxivid': u'1512.07344',\n",
       "  'authorsaffil': [[u'Yunchen Pu', None],\n",
       "   [u'Xin Yuan', None],\n",
       "   [u'Andrew Stevens', None],\n",
       "   [u'Chunyuan Li', None],\n",
       "   [u'Lawrence Carin', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'10 pages, 7 figures. Appearing in Proceedings of the 19th\\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\\n  2016, Cadiz, Spain. JMLR: W&CP volume 41',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07344v1',\n",
       "  'published': u'2015-12-23T03:10:29Z',\n",
       "  'summary': u'  A deep generative model is developed for representation and analysis of\\nimages, based on a hierarchical convolutional dictionary-learning framework.\\nStochastic {\\\\em unpooling} is employed to link consecutive layers in the model,\\nyielding top-down image generation. A Bayesian support vector machine is linked\\nto the top-layer features, yielding max-margin discrimination. Deep\\ndeconvolutional inference is employed when testing, to infer the latent\\nfeatures, and the top-layer features are connected with the max-margin\\nclassifier for discrimination tasks. The model is efficiently trained using a\\nMonte Carlo expectation-maximization (MCEM) algorithm, with implementation on\\ngraphical processor units (GPUs) for efficient large-scale learning, and fast\\ntesting. Excellent results are obtained on several benchmark datasets,\\nincluding ImageNet, demonstrating that the proposed model achieves results that\\nare highly competitive with similarly sized convolutional neural networks.\\n',\n",
       "  'title': u'A Deep Generative Deconvolutional Image Model'},\n",
       " u'1602.05875': {'arxivid': u'1602.05875',\n",
       "  'authorsaffil': [[u'Gil Keren', None], [u'Bj\\xf6rn Schuller', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05875v2',\n",
       "  'published': u'2016-02-18T16:55:30Z',\n",
       "  'summary': u'  Traditional convolutional layers extract features from patches of data by\\napplying a non-linearity on an affine function of the input. We propose a model\\nthat enhances this feature extraction process for the case of sequential data,\\nby feeding patches of the data into a recurrent neural network and using the\\noutputs or hidden states of the recurrent units to compute the extracted\\nfeatures. By doing so, we exploit the fact that a window containing a few\\nframes of the sequential data is a sequence itself and this additional\\nstructure might encapsulate valuable information. In addition, we allow for\\nmore steps of computation in the feature extraction process, which is\\npotentially beneficial as an affine function followed by a non-linearity can\\nresult in too simple features. Using our convolutional recurrent layers we\\nobtain an improvement in performance in two audio classification tasks,\\ncompared to traditional convolutional layers.\\n',\n",
       "  'title': u'Convolutional RNN: an Enhanced Model for Extracting Features from\\n  Sequential Data'},\n",
       " u'1603.06777': {'arxivid': u'1603.06777',\n",
       "  'authorsaffil': [[u'Bert Moons', None],\n",
       "   [u'Bert De Brabandere', None],\n",
       "   [u'Luc Van Gool', None],\n",
       "   [u'Marian Verhelst', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Published in IEEE Winter Conference on Applications of Computer\\n  Vision (WACV 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06777v1',\n",
       "  'published': u'2016-03-22T13:20:36Z',\n",
       "  'summary': u'  Recently ConvNets or convolutional neural networks (CNN) have come up as\\nstate-of-the-art classification and detection algorithms, achieving near-human\\nperformance in visual detection. However, ConvNet algorithms are typically very\\ncomputation and memory intensive. In order to be able to embed ConvNet-based\\nclassification into wearable platforms and embedded systems such as smartphones\\nor ubiquitous electronics for the internet-of-things, their energy consumption\\nshould be reduced drastically. This paper proposes methods based on approximate\\ncomputing to reduce energy consumption in state-of-the-art ConvNet\\naccelerators. By combining techniques both at the system- and circuit level, we\\ncan gain energy in the systems arithmetic: up to 30x without losing\\nclassification accuracy and more than 100x at 99% classification accuracy,\\ncompared to the commonly used 16-bit fixed point number format.\\n',\n",
       "  'title': u'Energy-Efficient ConvNets Through Approximate Computing'},\n",
       " u'1603.09335': {'arxivid': u'1603.09335',\n",
       "  'authorsaffil': [[u'Stephen Marsland', None], [u'Robert McLachlan', None]],\n",
       "  'categoryterms': [u'cs.CV', u'math.MG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09335v1',\n",
       "  'published': u'2016-03-30T05:13:59Z',\n",
       "  'summary': u'  Identifying when different images are of the same object despite changes\\ncaused by imaging technologies, or processes such as growth, has many\\napplications in fields such as computer vision and biological image analysis.\\nOne approach to this problem is to identify the group of possible\\ntransformations of the object and to find invariants to the action of that\\ngroup, meaning that the object has the same values of the invariants despite\\nthe action of the group. In this paper we study the invariants of planar shapes\\nand images under the M\\\\\"obius group $\\\\mathrm{PSL}(2,\\\\mathbb{C})$, which arises\\nin the conformal camera model of vision and may also correspond to neurological\\naspects of vision, such as grouping of lines and circles. We survey the\\ncomputational requirements of an invariant, and the known M\\\\\"obius invariants,\\nand then develop an algorithm by which shapes can be recognised that is\\nM\\\\\"obius- and parametrization-invariant, numerically stable, and robust to\\nnoise. We demonstrate the efficacy of this new invariant approach on sets of\\ncurves, and then develop a M\\\\\"obius-invariant signature of grey-scale images.\\n',\n",
       "  'title': u'M\\xf6bius invariants of shapes and images'},\n",
       " u'1604.03058': {'arxivid': u'1604.03058',\n",
       "  'authorsaffil': [[u'Xundong Wu', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'More testing and verification is needed',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03058v2',\n",
       "  'published': u'2016-04-11T18:39:33Z',\n",
       "  'summary': u'  We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet\\nLSVRC-2102 dataset classification task and achieved a good performance. With a\\nmoderate size network of 10 layers, we obtained top-5 classification accuracy\\nrate of 81 percent on validation set which is much better than previous\\npublished results. We expect training networks of a much better performance\\nthrough increase network depth would be straight forward by following our\\ncurrent strategies. A detailed discussion on strategies used in the network\\ntraining is included as well as preliminary analysis.\\n',\n",
       "  'title': u'High Performance Binarized Neural Networks trained on the ImageNet\\n  Classification Task'},\n",
       " u'1601.02403': {'arxivid': u'1601.02403',\n",
       "  'authorsaffil': [[u'Ivan Habernal', None], [u'Iryna Gurevych', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Under review in Computational Linguistics. First submission: 7 March\\n  2015. Revised submission (round 2): 10 January 2016. Revised submission\\n  (round 3): 29 April 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02403v3',\n",
       "  'published': u'2016-01-11T11:28:49Z',\n",
       "  'summary': u\"  The goal of argumentation mining, an evolving research field in computational\\nlinguistics, is to design methods capable of analyzing people's argumentation.\\nIn this article, we go beyond the state of the art in several ways. (i) We deal\\nwith actual Web data and take up the challenges given by the variety of\\nregisters, multiple domains, and unrestricted noisy user-generated Web\\ndiscourse. (ii) We bridge the gap between normative argumentation theories and\\nargumentation phenomena encountered in actual data by adapting an argumentation\\nmodel tested in an extensive annotation study. (iii) We create a new gold\\nstandard corpus (90k tokens in 340 documents) and experiment with several\\nmachine learning methods to identify argument components. We offer the data,\\nsource codes, and annotation guidelines to the community under free licenses.\\nOur findings show that argumentation mining in user-generated Web discourse is\\na feasible but challenging task.\\n\",\n",
       "  'title': u'Argumentation Mining in User-Generated Web Discourse'},\n",
       " u'1512.06293': {'arxivid': u'1512.06293',\n",
       "  'authorsaffil': [[u'Thomas Wiatowski', None], [u'Helmut B\\xf6lcskei', None]],\n",
       "  'categoryterms': [u'cs.IT',\n",
       "   u'cs.AI',\n",
       "   u'cs.LG',\n",
       "   u'math.FA',\n",
       "   u'math.IT',\n",
       "   u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.06293v1',\n",
       "  'published': u'2015-12-19T22:31:24Z',\n",
       "  'summary': u\"  Deep convolutional neural networks have led to breakthrough results in\\npractical feature extraction applications. The mathematical analysis of such\\nnetworks was initiated by Mallat, 2012. Specifically, Mallat considered\\nso-called scattering networks based on semi-discrete shift-invariant wavelet\\nframes and modulus non-linearities in each network layer, and proved\\ntranslation invariance (asymptotically in the wavelet scale parameter) and\\ndeformation stability of the corresponding feature extractor. The purpose of\\nthis paper is to develop Mallat's theory further by allowing for general\\nconvolution kernels, or in more technical parlance, general semi-discrete\\nshift-invariant frames (including Weyl-Heisenberg, curvelet, shearlet,\\nridgelet, and wavelet frames) and general Lipschitz-continuous non-linearities\\n(e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents,\\nand modulus functions), as well as pooling through sub-sampling, all of which\\ncan be different in different network layers. The resulting generalized network\\nenables extraction of significantly wider classes of features than those\\nresolved by Mallat's wavelet-modulus scattering network. We prove deformation\\nstability for a larger class of deformations than those considered by Mallat,\\nand we establish a new translation invariance result which is of vertical\\nnature in the sense of the network depth determining the amount of invariance.\\nMoreover, our results establish that deformation stability and vertical\\ntranslation invariance are guaranteed by the network structure per se rather\\nthan the specific convolution kernels and non-linearities. This offers an\\nexplanation for the tremendous success of deep convolutional neural networks in\\na wide variety of practical feature extraction applications. The mathematical\\ntechniques we employ are based on continuous frame theory.\\n\",\n",
       "  'title': u'A Mathematical Theory of Deep Convolutional Neural Networks for Feature\\n  Extraction'},\n",
       " u'1512.07587': {'arxivid': u'1512.07587',\n",
       "  'authorsaffil': [[u'Rajasekaran Masatran', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'stat.ML'],\n",
       "  'comment': u'6 pages, with 4 figures, 8 algorithms, and 1 table',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07587v7',\n",
       "  'published': u'2015-12-23T19:01:03Z',\n",
       "  'summary': u'  Markov random field (MRF) learning is intractable, and its approximation\\nalgorithms are computationally expensive. We target a small subset of MRF that\\nis used frequently in computer vision. We characterize this subset with three\\nconcepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as\\nan alternative. Our goal is robust learning from small datasets. Our learning\\nalgorithm uses vector quantization and, at time complexity O(U log U) for a\\ndataset of U pixels, is much faster than that of general-purpose MRF.\\n',\n",
       "  'title': u'A Latent-Variable Lattice Model'},\n",
       " u'1512.01249': {'arxivid': u'1512.01249',\n",
       "  'authorsaffil': [[u'Timber Kerkvliet', None], [u'Ronald Meester', None]],\n",
       "  'categoryterms': [u'math.PR', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.01249v1',\n",
       "  'published': u'2015-12-02T13:33:03Z',\n",
       "  'summary': u\"  We first show that there are practical situations in for instance forensic\\nand gambling settings, in which applying classical probability theory, that is,\\nbased on the axioms of Kolmogorov, is problematic. We then introduce and\\ndiscuss Shafer belief functions. Technically, Shafer belief functions\\ngeneralize probability distributions. Philosophically, they pertain to\\nindividual or shared knowledge of facts, rather than to facts themselves, and\\ntherefore can be interpreted as generalizing epistemic probability, that is,\\nprobability theory interpreted epistemologically. Belief functions are more\\nflexible and better suited to deal with certain types of uncertainty than\\nclassical probability distributions. We develop a new calculus for belief\\nfunctions which does not use the much criticized Dempster's rule of\\ncombination, by generalizing the classical notions of conditioning and\\nindependence in a natural and uncontroversial way. Using this calculus, we\\nexplain our rejection of Dempster's rule in detail. We apply the new theory to\\na number of examples, including a gambling example and an example in a forensic\\nsetting. We prove a law of large numbers for belief functions and offer a\\nbetting interpretation similar to the Dutch Book Theorem for probability\\ndistributions.\\n\",\n",
       "  'title': u'Quantifying knowledge with a new calculus for belief functions - a\\n  generalization of probability theory'},\n",
       " u'1603.09048': {'arxivid': u'1603.09048',\n",
       "  'authorsaffil': [[u'Kuan-Hao Huang', None], [u'Hsuan-Tien Lin', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09048v3',\n",
       "  'published': u'2016-03-30T06:19:02Z',\n",
       "  'summary': u'  Label embedding (LE) is an important family of multi-label classification\\nalgorithms that digest the label information jointly for better performance.\\nDifferent real-world applications evaluate performance by different cost\\nfunctions of interest. Current LE algorithms often aim to optimize one specific\\ncost function, but they can suffer from bad performance with respect to other\\ncost functions. In this paper, we resolve the performance issue by proposing a\\nnovel cost-sensitive LE algorithm that takes the cost function of interest into\\naccount. The proposed algorithm, cost-sensitive label embedding with\\nmultidimensional scaling (CLEMS), approximates the cost information with the\\ndistances of the embedded vectors using the classic multidimensional scaling\\napproach for manifold learning. CLEMS is able to deal with both symmetric and\\nasymmetric cost functions, and effectively makes cost-sensitive decisions by\\nnearest-neighbor decoding within the embedded vectors. Theoretical results\\njustify that CLEMS achieves the cost-sensitivity and extensive experimental\\nresults demonstrate that CLEMS is significantly better than a wide spectrum of\\nexisting LE algorithms and state-of-the-art cost-sensitive algorithms across\\ndifferent cost functions.\\n',\n",
       "  'title': u'Cost-sensitive Label Embedding for Multi-label Classification'},\n",
       " u'1603.07044': {'arxivid': u'1603.07044',\n",
       "  'authorsaffil': [[u'Wei-Ning Hsu', None],\n",
       "   [u'Yu Zhang', None],\n",
       "   [u'James Glass', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07044v1',\n",
       "  'published': u'2016-03-23T01:52:54Z',\n",
       "  'summary': u'  We apply a general recurrent neural network (RNN) encoder framework to\\ncommunity question answering (cQA) tasks. Our approach does not rely on any\\nlinguistic processing, and can be applied to different languages or domains.\\nFurther improvements are observed when we extend the RNN encoders with a neural\\nattention mechanism that encourages reasoning over entire sequences. To deal\\nwith practical issues such as data sparsity and imbalanced labels, we apply\\nvarious techniques such as transfer learning and multitask learning. Our\\nexperiments on the SemEval-2016 cQA task show 10% improvement on a MAP score\\ncompared to an information retrieval-based approach, and achieve comparable\\nperformance to a strong handcrafted feature-based method.\\n',\n",
       "  'title': u'Recurrent Neural Network Encoder with Attention for Community Question\\n  Answering'},\n",
       " u'1511.07838': {'arxivid': u'1511.07838',\n",
       "  'authorsaffil': [[u'Amjad Almahairi', None],\n",
       "   [u'Nicolas Ballas', None],\n",
       "   [u'Tim Cooijmans', None],\n",
       "   [u'Yin Zheng', None],\n",
       "   [u'Hugo Larochelle', None],\n",
       "   [u'Aaron Courville', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'ICML 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07838v7',\n",
       "  'published': u'2015-11-24T19:30:19Z',\n",
       "  'summary': u\"  We introduce the Dynamic Capacity Network (DCN), a neural network that can\\nadaptively assign its capacity across different portions of the input data.\\nThis is achieved by combining modules of two types: low-capacity sub-networks\\nand high-capacity sub-networks. The low-capacity sub-networks are applied\\nacross most of the input, but also provide a guide to select a few portions of\\nthe input on which to apply the high-capacity sub-networks. The selection is\\nmade using a novel gradient-based attention mechanism, that efficiently\\nidentifies input regions for which the DCN's output is most sensitive and to\\nwhich we should devote more capacity. We focus our empirical evaluation on the\\nCluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are\\nable to drastically reduce the number of computations, compared to traditional\\nconvolutional neural networks, while maintaining similar or even better\\nperformance.\\n\",\n",
       "  'title': u'Dynamic Capacity Networks'},\n",
       " u'1601.04451': {'arxivid': u'1601.04451',\n",
       "  'authorsaffil': [[u'Robert P. W. Duin', None], [u'Elzbieta Pekalska', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'5 pages. Paper originally written in 2003. Although it may proof an\\n  obvious fact, it is significant for understanding the essential conditions it\\n  is based on',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04451v1',\n",
       "  'published': u'2016-01-18T10:12:15Z',\n",
       "  'summary': u'  We consider general non-Euclidean distance measures between real world\\nobjects that need to be classified. It is assumed that objects are represented\\nby distances to other objects only. Conditions for zero-error dissimilarity\\nbased classifiers are derived. Additional conditions are given under which the\\nzero-error decision boundary is a continues function of the distances to a\\nfinite set of training samples. These conditions affect the objects as well as\\nthe distance measure used. It is argued that they can be met in practice.\\n',\n",
       "  'title': u'Zero-error dissimilarity based classifiers'},\n",
       " u'1604.00562': {'arxivid': u'1604.00562',\n",
       "  'authorsaffil': [[u'Jacob Andreas', None], [u'Dan Klein', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00562v1',\n",
       "  'published': u'2016-04-02T21:52:03Z',\n",
       "  'summary': u'  We present a model for pragmatically describing scenes, in which contrastive\\nbehavior results from a combination of inference-driven pragmatics and learned\\nsemantics. Like previous learned approaches to language generation, our model\\nuses a simple feature-driven architecture (here a pair of neural \"listener\" and\\n\"speaker\" models) to ground language in the world. Like inference-driven\\napproaches to pragmatics, our model actively reasons about listener behavior\\nwhen selecting utterances. For training, our approach requires only ordinary\\ncaptions, annotated _without_ demonstration of the pragmatic behavior the model\\nultimately exhibits. In human evaluations on a referring expression game, our\\napproach succeeds 81% of the time, compared to a 64% success rate using\\nexisting techniques.\\n',\n",
       "  'title': u'Reasoning About Pragmatics with Neural Listeners and Speakers'},\n",
       " u'1511.07837': {'arxivid': u'1511.07837',\n",
       "  'authorsaffil': [[u'Zhaosong Lu', None], [u'Xiaojun Chen', None]],\n",
       "  'categoryterms': [u'math.OC',\n",
       "   u'cs.LG',\n",
       "   u'math.NA',\n",
       "   u'stat.CO',\n",
       "   u'stat.ML',\n",
       "   u'65C60, 65K05, 65Y20, 90C06, 90C20, 90C25'],\n",
       "  'comment': u'36 pages, 2 tables',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07837v3',\n",
       "  'published': u'2015-11-24T19:28:09Z',\n",
       "  'summary': u'  The conjugate gradient (CG) method is an efficient iterative method for\\nsolving large-scale strongly convex quadratic programming (QP). In this paper\\nwe propose some generalized CG (GCG) methods for solving the\\n$\\\\ell_1$-regularized (possibly not strongly) convex QP that terminate at an\\noptimal solution in a finite number of iterations. At each iteration, our\\nmethods first identify a face of an orthant and then either perform an exact\\nline search along the direction of the negative projected minimum-norm\\nsubgradient of the objective function or execute a CG subroutine that conducts\\na sequence of CG iterations until a CG iterate crosses the boundary of this\\nface or an approximate minimizer of over this face or a subface is found. We\\ndetermine which type of step should be taken by comparing the magnitude of some\\ncomponents of the minimum-norm subgradient of the objective function to that of\\nits rest components. Our analysis on finite convergence of these methods makes\\nuse of an error bound result and some key properties of the aforementioned\\nexact line search and the CG subroutine. We also show that the proposed methods\\nare capable of finding an approximate solution of the problem by allowing some\\ninexactness on the execution of the CG subroutine. The overall arithmetic\\noperation cost of our GCG methods for finding an $\\\\epsilon$-optimal solution\\ndepends on $\\\\epsilon$ in $O(\\\\log(1/\\\\epsilon))$, which is superior to the\\naccelerated proximal gradient method [2,23] that depends on $\\\\epsilon$ in\\n$O(1/\\\\sqrt{\\\\epsilon})$. In addition, our GCG methods can be extended\\nstraightforwardly to solve box-constrained convex QP with finite convergence.\\nNumerical results demonstrate that our methods are very favorable for solving\\nill-conditioned problems.\\n',\n",
       "  'title': u'Generalized Conjugate Gradient Methods for $\\\\ell_1$ Regularized Convex\\n  Quadratic Programming with Finite Convergence'},\n",
       " u'1601.06569': {'arxivid': u'1601.06569',\n",
       "  'authorsaffil': [[u'Kareem Amin', None], [u'Satinder Singh', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06569v1',\n",
       "  'published': u'2016-01-25T11:50:43Z',\n",
       "  'summary': u\"  We consider a setting for Inverse Reinforcement Learning (IRL) where the\\nlearner is extended with the ability to actively select multiple environments,\\nobserving an agent's behavior on each environment. We first demonstrate that if\\nthe learner can experiment with any transition dynamics on some fixed set of\\nstates and actions, then there exists an algorithm that reconstructs the\\nagent's reward function to the fullest extent theoretically possible, and that\\nrequires only a small (logarithmic) number of experiments. We contrast this\\nresult to what is known about IRL in single fixed environments, namely that the\\ntrue reward function is fundamentally unidentifiable. We then extend this\\nsetting to the more realistic case where the learner may not select any\\ntransition dynamic, but rather is restricted to some fixed set of environments\\nthat it may try. We connect the problem of maximizing the information derived\\nfrom experiments to submodular function maximization and demonstrate that a\\ngreedy algorithm is near optimal (up to logarithmic factors). Finally, we\\nempirically validate our algorithm on an environment inspired by behavioral\\npsychology.\\n\",\n",
       "  'title': u'Towards Resolving Unidentifiability in Inverse Reinforcement Learning'},\n",
       " u'1603.09511': {'arxivid': u'1603.09511',\n",
       "  'authorsaffil': [[u'Adrian Haret', None],\n",
       "   [u'Jean-Guy Mailly', None],\n",
       "   [u'Stefan Woltran', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09511v1',\n",
       "  'published': u'2016-03-31T09:59:02Z',\n",
       "  'summary': u'  Understanding the behavior of belief change operators for fragments of\\nclassical logic has received increasing interest over the last years. Results\\nin this direction are mainly concerned with adapting representation theorems.\\nHowever, fragment-driven belief change also leads to novel research questions.\\nIn this paper we propose the concept of belief distribution, which can be\\nunderstood as the reverse task of merging. More specifically, we are interested\\nin the following question: given an arbitrary knowledge base $K$ and some\\nmerging operator $\\\\Delta$, can we find a profile $E$ and a constraint $\\\\mu$,\\nboth from a given fragment of classical logic, such that $\\\\Delta_\\\\mu(E)$ yields\\na result equivalent to $K$? In other words, we are interested in seeing if $K$\\ncan be distributed into knowledge bases of simpler structure, such that the\\ntask of merging allows for a reconstruction of the original knowledge. Our\\ninitial results show that merging based on drastic distance allows for an easy\\ndistribution of knowledge, while the power of distribution for operators based\\non Hamming distance relies heavily on the fragment of choice.\\n',\n",
       "  'title': u'Distributing Knowledge into Simple Bases'},\n",
       " u'1603.09046': {'arxivid': u'1603.09046',\n",
       "  'authorsaffil': [[u'Andrew Shin', None],\n",
       "   [u'Masataka Yamaguchi', None],\n",
       "   [u'Katsunori Ohnishi', None],\n",
       "   [u'Tatsuya Harada', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'submitted to ECCV2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09046v1',\n",
       "  'published': u'2016-03-30T05:48:05Z',\n",
       "  'summary': u'  The workflow of extracting features from images using convolutional neural\\nnetworks (CNN) and generating captions with recurrent neural networks (RNN) has\\nbecome a de-facto standard for image captioning task. However, since CNN\\nfeatures are originally designed for classification task, it is mostly\\nconcerned with the main conspicuous element of the image, and often fails to\\ncorrectly convey information on local, secondary elements. We propose to\\nincorporate coding with vector of locally aggregated descriptors (VLAD) on\\nspatial pyramid for CNN features of sub-regions in order to generate image\\nrepresentations that better reflect the local information of the images. Our\\nresults show that our method of compact VLAD coding can match CNN features with\\nas little as 3% of dimensionality and, when combined with spatial pyramid, it\\nresults in image captions that more accurately take local elements into\\naccount.\\n',\n",
       "  'title': u'Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for\\n  Locally Robust Captioning'},\n",
       " u'1604.02275': {'arxivid': u'1604.02275',\n",
       "  'authorsaffil': [[u'Rocco De Rosa', None],\n",
       "   [u'Thomas Mensink', None],\n",
       "   [u'Barbara Caputo', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'keywords{Open world recognition, Open set, Incremental Learning,\\n  Metric Learning, Nonparametric methods, Classification confidence}',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02275v1',\n",
       "  'published': u'2016-04-08T08:43:15Z',\n",
       "  'summary': u'  As we enter into the big data age and an avalanche of images have become\\nreadily available, recognition systems face the need to move from close, lab\\nsettings where the number of classes and training data are fixed, to dynamic\\nscenarios where the number of categories to be recognized grows continuously\\nover time, as well as new data providing useful information to update the\\nsystem. Recent attempts, like the open world recognition framework, tried to\\ninject dynamics into the system by detecting new unknown classes and adding\\nthem incrementally, while at the same time continuously updating the models for\\nthe known classes. incrementally adding new classes and detecting instances\\nfrom unknown classes, while at the same time continuously updating the models\\nfor the known classes. In this paper we argue that to properly capture the\\nintrinsic dynamic of open world recognition, it is necessary to add to these\\naspects (a) the incremental learning of the underlying metric, (b) the\\nincremental estimate of confidence thresholds for the unknown classes, and (c)\\nthe use of local learning to precisely describe the space of classes. We extend\\nthree existing metric learning algorithms towards these goals by using online\\nmetric learning. Experimentally we validate our approach on two large-scale\\ndatasets in different learning scenarios. For all these scenarios our proposed\\nmethods outperform their non-online counterparts. We conclude that local and\\nonline learning is important to capture the full dynamics of open world\\nrecognition.\\n',\n",
       "  'title': u'Online Open World Recognition'},\n",
       " u'1604.02270': {'arxivid': u'1604.02270',\n",
       "  'authorsaffil': [[u'Mikhail Kolmogorov', None],\n",
       "   [u'Eamonn Kennedy', None],\n",
       "   [u'Zhuxin Dong', None],\n",
       "   [u'Gregory Timp', None],\n",
       "   [u'Pavel Pevzner', None]],\n",
       "  'categoryterms': [u'q-bio.QM', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02270v1',\n",
       "  'published': u'2016-04-08T08:16:41Z',\n",
       "  'summary': u'  Recent advances in top-down mass spectrometry enabled identification of\\nintact proteins, but this technology still faces challenges. For example,\\ntop-down mass spectrometry suffers from a lack of sensitivity since the ion\\ncounts for a single fragmentation event are often low. In contrast, nanopore\\ntechnology is exquisitely sensitive to single intact molecules, but it has only\\nbeen successfully applied to DNA sequencing, so far. Here, we explore the\\npotential of sub-nanopores for single-molecule protein identification (SMPI)\\nand describe an algorithm for analyzing the electrical current blockade signal\\n(nanospectrum) resulting from the translocation of a denaturated, linearly\\ncharged protein through a sub-nanopore. We further describe the first SMPI\\nalgorithm, compute the p-values of Protein-Nanospectrum Matches, and discuss\\nthe promise and computational limitations of the current SMPI technology.\\n',\n",
       "  'title': u'Single-Molecule Protein Identification by Sub-Nanopore Sensors'},\n",
       " u'1604.02271': {'arxivid': u'1604.02271',\n",
       "  'authorsaffil': [[u'Liang Lin', None],\n",
       "   [u'Guangrun Wang', None],\n",
       "   [u'Rui Zhang', None],\n",
       "   [u'Ruimao Zhang', None],\n",
       "   [u'Xiaodan Liang', None],\n",
       "   [u'Wangmeng Zuo', None]],\n",
       "  'categoryterms': [u'cs.CV', u'68U10', u'I.4.8; I.5'],\n",
       "  'comment': u'To appear in Proceedings of IEEE Conference on Computer Vision and\\n  Pattern Recognition (CVPR), 2016. (oral)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02271v1',\n",
       "  'published': u'2016-04-08T08:17:37Z',\n",
       "  'summary': u'  This paper addresses a fundamental problem of scene understanding: How to\\nparse the scene image into a structured configuration (i.e., a semantic object\\nhierarchy with object interaction relations) that finely accords with human\\nperception. We propose a deep architecture consisting of two networks: i) a\\nconvolutional neural network (CNN) extracting the image representation for\\npixelwise object labeling and ii) a recursive neural network (RNN) discovering\\nthe hierarchical object structure and the inter-object relations. Rather than\\nrelying on elaborative user annotations (e.g., manually labeling semantic maps\\nand relations), we train our deep model in a weakly-supervised manner by\\nleveraging the descriptive sentences of the training images. Specifically, we\\ndecompose each sentence into a semantic tree consisting of nouns and verb\\nphrases, and facilitate these trees discovering the configurations of the\\ntraining images. Once these scene configurations are determined, then the\\nparameters of both the CNN and RNN are updated accordingly by back propagation.\\nThe entire model training is accomplished through an Expectation-Maximization\\nmethod. Extensive experiments suggest that our model is capable of producing\\nmeaningful and structured scene configurations and achieving more favorable\\nscene labeling performance on PASCAL VOC 2012 over other state-of-the-art\\nweakly-supervised methods.\\n',\n",
       "  'title': u'Deep Structured Scene Parsing by Learning with Image Descriptions'},\n",
       " u'1603.09240': {'arxivid': u'1603.09240',\n",
       "  'authorsaffil': [[u'Afshin Dehghan', None], [u'Mubarak Shah', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.RO'],\n",
       "  'comment': u'14 pages, 15 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09240v1',\n",
       "  'published': u'2016-03-30T15:11:38Z',\n",
       "  'summary': u\"  Multi-object tracking has been studied for decades. However, when it comes to\\ntracking pedestrians in extremely crowded scenes, we are limited to only few\\nworks. This is an important problem which gives rise to several challenges.\\nPre-trained object detectors fail to localize targets in crowded sequences.\\nThis consequently limits the use of data-association based multi-target\\ntracking methods which rely on the outcome of an object detector. Additionally,\\nthe small apparent target size makes it challenging to extract features to\\ndiscriminate targets from their surroundings. Finally, the large number of\\ntargets greatly increases computational complexity which in turn makes it hard\\nto extend existing multi-target tracking approaches to high-density crowd\\nscenarios. In this paper, we propose a tracker that addresses the\\naforementioned problems and is capable of tracking hundreds of people\\nefficiently. We formulate online crowd tracking as Binary Quadratic Programing.\\nOur formulation employs target's individual information in the form of\\nappearance and motion as well as contextual cues in the form of neighborhood\\nmotion, spatial proximity and grouping constraints, and solves detection and\\ndata association simultaneously. In order to solve the proposed quadratic\\noptimization efficiently, where state-of art commercial quadratic programing\\nsolvers fail to find the answer in a reasonable amount of time, we propose to\\nuse the most recent version of the Modified Frank Wolfe algorithm, which takes\\nadvantage of SWAP-steps to speed up the optimization. We show that the proposed\\nformulation can track hundreds of targets efficiently and improves state-of-art\\nresults by significant margins on eleven challenging high density crowd\\nsequences.\\n\",\n",
       "  'title': u'Binary Quadratic Programing for Online Tracking of Hundreds of People in\\n  Extremely Crowded Scenes'},\n",
       " u'1511.00060': {'arxivid': u'1511.00060',\n",
       "  'authorsaffil': [[u'Xingxing Zhang', None],\n",
       "   [u'Liang Lu', None],\n",
       "   [u'Mirella Lapata', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'to appear in NAACL 2016; code available at\\n  https://github.com/XingxingZhang/td-treelstm',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.00060v3',\n",
       "  'published': u'2015-10-31T02:05:28Z',\n",
       "  'summary': u'  Long Short-Term Memory (LSTM) networks, a type of recurrent neural network\\nwith a more complex computational unit, have been successfully applied to a\\nvariety of sequence modeling tasks. In this paper we develop Tree Long\\nShort-Term Memory (TreeLSTM), a neural network model based on LSTM, which is\\ndesigned to predict a tree rather than a linear sequence. TreeLSTM defines the\\nprobability of a sentence by estimating the generation probability of its\\ndependency tree. At each time step, a node is generated based on the\\nrepresentation of the generated sub-tree. We further enhance the modeling power\\nof TreeLSTM by explicitly representing the correlations between left and right\\ndependents. Application of our model to the MSR sentence completion challenge\\nachieves results beyond the current state of the art. We also report results on\\ndependency parsing reranking achieving competitive performance.\\n',\n",
       "  'title': u'Top-down Tree Long Short-Term Memory Networks'},\n",
       " u'1602.06654': {'arxivid': u'1602.06654',\n",
       "  'authorsaffil': [[u'Guosheng Lin', None],\n",
       "   [u'Fayao Liu', None],\n",
       "   [u'Chunhua Shen', None],\n",
       "   [u'Jianxin Wu', None],\n",
       "   [u'Heng Tao Shen', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'20 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06654v1',\n",
       "  'published': u'2016-02-22T06:02:25Z',\n",
       "  'summary': u'  Hashing methods aim to learn a set of hash functions which map the original\\nfeatures to compact binary codes with similarity preserving in the Hamming\\nspace. Hashing has proven a valuable tool for large-scale information\\nretrieval. We propose a column generation based binary code learning framework\\nfor data-dependent hash function learning. Given a set of triplets that encode\\nthe pairwise similarity comparison information, our column generation based\\nmethod learns hash functions that preserve the relative comparison relations\\nwithin the large-margin learning framework. Our method iteratively learns the\\nbest hash functions during the column generation procedure. Existing hashing\\nmethods optimize over simple objectives such as the reconstruction error or\\ngraph Laplacian related loss functions, instead of the performance evaluation\\ncriteria of interest---multivariate performance measures such as the AUC and\\nNDCG. Our column generation based method can be further generalized from the\\ntriplet loss to a general structured learning based framework that allows one\\nto directly optimize multivariate performance measures. For optimizing general\\nranking measures, the resulting optimization problem can involve exponentially\\nor infinitely many variables and constraints, which is more challenging than\\nstandard structured output learning. We use a combination of column generation\\nand cutting-plane techniques to solve the optimization problem. To speed-up the\\ntraining we further explore stage-wise training and propose to use a simplified\\nNDCG loss for efficient inference. We demonstrate the generality of our method\\nby applying it to ranking prediction and image retrieval, and show that it\\noutperforms a few state-of-the-art hashing methods.\\n',\n",
       "  'title': u'Structured Learning of Binary Codes with Column Generation'},\n",
       " u'1510.07545': {'arxivid': u'1510.07545',\n",
       "  'authorsaffil': [[u'Tobias Schnabel', None],\n",
       "   [u'Paul N. Bennett', None],\n",
       "   [u'Susan T. Dumais', None],\n",
       "   [u'Thorsten Joachims', None]],\n",
       "  'categoryterms': [u'cs.HC', u'cs.IR', u'cs.LG'],\n",
       "  'comment': u'11 pages in WWW 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.07545v2',\n",
       "  'published': u'2015-10-26T16:49:07Z',\n",
       "  'summary': u\"  In this paper, we study shortlists as an interface component for recommender\\nsystems with the dual goal of supporting the user's decision process, as well\\nas improving implicit feedback elicitation for increased recommendation\\nquality. A shortlist is a temporary list of candidates that the user is\\ncurrently considering, e.g., a list of a few movies the user is currently\\nconsidering for viewing. From a cognitive perspective, shortlists serve as\\ndigital short-term memory where users can off-load the items under\\nconsideration -- thereby decreasing their cognitive load. From a machine\\nlearning perspective, adding items to the shortlist generates a new implicit\\nfeedback signal as a by-product of exploration and decision making which can\\nimprove recommendation quality. Shortlisting therefore provides additional data\\nfor training recommendation systems without the increases in cognitive load\\nthat requesting explicit feedback would incur.\\n  We perform an user study with a movie recommendation setup to compare\\ninterfaces that offer shortlist support with those that do not. From the user\\nstudies we conclude: (i) users make better decisions with a shortlist; (ii)\\nusers prefer an interface with shortlist support; and (iii) the additional\\nimplicit feedback from sessions with a shortlist improves the quality of\\nrecommendations by nearly a factor of two.\\n\",\n",
       "  'title': u'Using Shortlists to Support Decision Making and Improve Recommender\\n  System Performance'},\n",
       " u'1601.02543': {'arxivid': u'1601.02543',\n",
       "  'authorsaffil': [[u'Vinod Kumar Pandey', None],\n",
       "   [u'Sunil Kumar Kopparapu', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.HC'],\n",
       "  'comment': u'7 pages, 2 figure, ACC 2011',\n",
       "  'doi': None,\n",
       "  'journalref': u'ACC (3) 2011: 230-238',\n",
       "  'link': u'http://arxiv.org/abs/1601.02543v1',\n",
       "  'published': u'2016-01-11T18:01:56Z',\n",
       "  'summary': u'  Speech based solutions have taken center stage with growth in the services\\nindustry where there is a need to cater to a very large number of people from\\nall strata of the society. While natural language speech interfaces are the\\ntalk in the research community, yet in practice, menu based speech solutions\\nthrive. Typically in a menu based speech solution the user is required to\\nrespond by speaking from a closed set of words when prompted by the system. A\\nsequence of human speech response to the IVR prompts results in the completion\\nof a transaction. A transaction is deemed successful if the speech solution can\\ncorrectly recognize all the spoken utterances of the user whenever prompted by\\nthe system. The usual mechanism to evaluate the performance of a speech\\nsolution is to do an extensive test of the system by putting it to actual\\npeople use and then evaluating the performance by analyzing the logs for\\nsuccessful transactions. This kind of evaluation could lead to dissatisfied\\ntest users especially if the performance of the system were to result in a poor\\ntransaction completion rate. To negate this the Wizard of Oz approach is\\nadopted during evaluation of a speech system. Overall this kind of evaluations\\nis an expensive proposition both in terms of time and cost. In this paper, we\\npropose a method to evaluate the performance of a speech solution without\\nactually putting it to people use. We first describe the methodology and then\\nshow experimentally that this can be used to identify the performance\\nbottlenecks of the speech solution even before the system is actually used thus\\nsaving evaluation time and expenses.\\n',\n",
       "  'title': u'Evaluating the Performance of a Speech Recognition based System'},\n",
       " u'1603.01860': {'arxivid': u'1603.01860',\n",
       "  'authorsaffil': [[u'Ambuj Tewari', None], [u'Sougata Chaudhuri', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Appeared in ICML 2015. arXiv admin note: substantial text overlap\\n  with arXiv:1405.0586',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01860v1',\n",
       "  'published': u'2016-03-06T19:01:53Z',\n",
       "  'summary': u'  We consider the generalization ability of algorithms for learning to rank at\\na query level, a problem also called subset ranking. Existing generalization\\nerror bounds necessarily degrade as the size of the document list associated\\nwith a query increases. We show that such a degradation is not intrinsic to the\\nproblem. For several loss functions, including the cross-entropy loss used in\\nthe well known ListNet method, there is \\\\emph{no} degradation in generalization\\nability as document lists become longer. We also provide novel generalization\\nerror bounds under $\\\\ell_1$ regularization and faster convergence rates if the\\nloss function is smooth.\\n',\n",
       "  'title': u'Generalization error bounds for learning to rank: Does the length of\\n  document lists matter?'},\n",
       " u'1602.05765': {'arxivid': u'1602.05765',\n",
       "  'authorsaffil': [[u'Shoaib Jameel', None], [u'Steven Schockaert', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05765v1',\n",
       "  'published': u'2016-02-18T11:37:50Z',\n",
       "  'summary': u'  Conceptual spaces are geometric representations of conceptual knowledge, in\\nwhich entities correspond to points, natural properties correspond to convex\\nregions, and the dimensions of the space correspond to salient features. While\\nconceptual spaces enable elegant models of various cognitive phenomena, the\\nlack of automated methods for constructing such representations have so far\\nlimited their application in artificial intelligence. To address this issue, we\\npropose a method which learns a vector-space embedding of entities from\\nWikipedia and constrains this embedding such that entities of the same semantic\\ntype are located in some lower-dimensional subspace. We experimentally\\ndemonstrate the usefulness of these subspaces as (approximate) conceptual space\\nrepresentations by showing, among others, that important features can be\\nmodelled as directions and that natural properties tend to correspond to convex\\nregions.\\n',\n",
       "  'title': u'Entity Embeddings with Conceptual Subspaces as a Basis for Plausible\\n  Reasoning'},\n",
       " u'1603.01864': {'arxivid': u'1603.01864',\n",
       "  'authorsaffil': [[u'Felipe Codevilla', None],\n",
       "   [u'Joel D. O. Gaya', None],\n",
       "   [u'Amanda C. Duarte', None],\n",
       "   [u'Silvia Botelho', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01864v1',\n",
       "  'published': u'2016-03-06T19:54:18Z',\n",
       "  'summary': u'  This paper describes a method to restore degraded images captured in general\\nparticipative media --- fog, turbid water, sand storm, etc. To obtain\\ngenerality, we, first, propose a novel interpretation of the participative\\nmedia image formation by considering the color variation of the media. Second,\\nwe introduce that joining different image priors is an effective alternative\\nfor image restoration. The proposed method contains a Composite Prior supported\\nby statistics collected on both haze-free and degraded participative\\nenvironment images. The key of the method is joining two complementary measures\\n--- local contrast and color. The results presented for a variety of underwater\\nand haze images demonstrate the power of the method. Moreover, we showed the\\npotential of our method using a special dataset for which a reference haze-free\\nimage is available for comparison.\\n',\n",
       "  'title': u'General Participative Media Single Image Restoration'},\n",
       " u'1603.05544': {'arxivid': u'1603.05544',\n",
       "  'authorsaffil': [[u'Linnan Wang', None],\n",
       "   [u'Yi Yang', None],\n",
       "   [u'Martin Renqiang Min', None],\n",
       "   [u'Srimat Chakradhar', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DC'],\n",
       "  'comment': u'The patent of ISGD belongs to NEC Labs',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05544v2',\n",
       "  'published': u'2016-03-17T15:49:48Z',\n",
       "  'summary': u'  SGD is the widely adopted method to train CNN. Conceptually it approximates\\nthe population with a randomly sampled batch; then it evenly trains batches by\\nconducting a gradient update on every batch in an epoch. In this paper, we\\ndemonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle Pseudo\\nRandom Sampling differentiate batches in training, which then affect learning\\nspeeds on them. Because of this, the unbiased treatment of batches involved in\\nSGD creates improper load balancing. To address this issue, we present\\nInconsistent Stochastic Gradient Descent (ISGD) to dynamically vary training\\neffort according to learning statuses on batches. Specifically ISGD leverages\\ntechniques in Statistical Process Control to identify a undertrained batch.\\nOnce a batch is undertrained, ISGD solves a new subproblem, a chasing logic\\nplus a conservative constraint, to accelerate the training on the batch while\\navoid drastic parameter changes. Extensive experiments on a variety of datasets\\ndemonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is\\n21.05\\\\% faster than SGD to reach 56\\\\% top1 accuracy under the exactly same\\nexperiment setup. We also extend ISGD to work on multiGPU or heterogeneous\\ndistributed system based on data parallelism, enabling the batch size to be the\\nkey to scalability. Then we present the study of ISGD batch size to the\\nlearning rate, parallelism, synchronization cost, system saturation and\\nscalability. We conclude the optimal ISGD batch size is machine dependent.\\nVarious experiments on a multiGPU system validate our claim. In particular,\\nISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4\\nNVIDIA TITAN X at the batch size of 1536.\\n',\n",
       "  'title': u'Accelerating Deep Neural Network Training with Inconsistent Stochastic\\n  Gradient Descent'},\n",
       " u'1505.04732': {'arxivid': u'1505.04732',\n",
       "  'authorsaffil': [[u'L. Martino', None],\n",
       "   [u'V. Elvira', None],\n",
       "   [u'D. Luengo', None],\n",
       "   [u'J. Corander', None]],\n",
       "  'categoryterms': [u'stat.CO', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Statistics and Computing, 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.04732v3',\n",
       "  'published': u'2015-05-18T17:40:51Z',\n",
       "  'summary': u'  Monte Carlo methods represent the \"de facto\" standard for approximating\\ncomplicated integrals involving multidimensional target distributions. In order\\nto generate random realizations from the target distribution, Monte Carlo\\ntechniques use simpler proposal probability densities to draw candidate\\nsamples. The performance of any such method is strictly related to the\\nspecification of the proposal distribution, such that unfortunate choices\\neasily wreak havoc on the resulting estimators. In this work, we introduce a\\nlayered (i.e., hierarchical) procedure to generate samples employed within a\\nMonte Carlo scheme. This approach ensures that an appropriate equivalent\\nproposal density is always obtained automatically (thus eliminating the risk of\\na catastrophic performance), although at the expense of a moderate increase in\\nthe complexity. Furthermore, we provide a general unified importance sampling\\n(IS) framework, where multiple proposal densities are employed and several IS\\nschemes are introduced by applying the so-called deterministic mixture\\napproach. Finally, given these schemes, we also propose a novel class of\\nadaptive importance samplers using a population of proposals, where the\\nadaptation is driven by independent parallel or interacting Markov Chain Monte\\nCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefits\\nof both IS and MCMC methods.\\n',\n",
       "  'title': u'Layered Adaptive Importance Sampling'},\n",
       " u'1506.03624': {'arxivid': u'1506.03624',\n",
       "  'authorsaffil': [[u'Daniel J. Mankowitz', None],\n",
       "   [u'Timothy A. Mann', None],\n",
       "   [u'Shie Mannor', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.03624v1',\n",
       "  'published': u'2015-06-11T11:06:40Z',\n",
       "  'summary': u'  The monolithic approach to policy representation in Markov Decision Processes\\n(MDPs) looks for a single policy that can be represented as a function from\\nstates to actions. For the monolithic approach to succeed (and this is not\\nalways possible), a complex feature representation is often necessary since the\\npolicy is a complex object that has to prescribe what actions to take all over\\nthe state space. This is especially true in large domains with complicated\\ndynamics. It is also computationally inefficient to both learn and plan in MDPs\\nusing a complex monolithic approach. We present a different approach where we\\nrestrict the policy space to policies that can be represented as combinations\\nof simpler, parameterized skills---a type of temporally extended action, with a\\nsimple policy representation. We introduce Learning Skills via Bootstrapping\\n(LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as\\na \"black box\" to iteratively learn parametrized skills. Initially, the learned\\nskills are short-sighted but each iteration of the algorithm allows the skills\\nto bootstrap off one another, improving each skill in the process. We prove\\nthat this bootstrapping process returns a near-optimal policy. Furthermore, our\\nexperiments demonstrate that LSB can solve MDPs that, given the same\\nrepresentational power, could not be solved by a monolithic approach. Thus,\\nplanning with learned skills results in better policies without requiring\\ncomplex policy representations.\\n',\n",
       "  'title': u'Bootstrapping Skills'},\n",
       " u'1603.05152': {'arxivid': u'1603.05152',\n",
       "  'authorsaffil': [[u'Kleanthis Malialis', None],\n",
       "   [u'Jun Wang', None],\n",
       "   [u'Gary Brooks', None],\n",
       "   [u'George Frangou', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'AAMAS-16 Workshop on Adaptive and Learning Agents (ALA-16)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05152v1',\n",
       "  'published': u'2016-03-16T15:49:37Z',\n",
       "  'summary': u'  Datasets with hundreds to tens of thousands features is the new norm. Feature\\nselection constitutes a central problem in machine learning, where the aim is\\nto derive a representative set of features from which to construct a\\nclassification (or prediction) model for a specific task. Our experimental\\nstudy involves microarray gene expression datasets, these are high-dimensional\\nand noisy datasets that contain genetic data typically used for distinguishing\\nbetween benign or malicious tissues or classifying different types of cancer.\\nIn this paper, we formulate feature selection as a multiagent coordination\\nproblem and propose a novel feature selection method using multiagent\\nreinforcement learning. The central idea of the proposed approach is to\\n\"assign\" a reinforcement learning agent to each feature where each agent learns\\nto control a single feature, we refer to this approach as MARL. Applying this\\nto microarray datasets creates an enormous multiagent coordination problem\\nbetween thousands of learning agents. To address the scalability challenge we\\napply a form of reward shaping called CLEAN rewards. We compare in total nine\\nfeature selection methods, including state-of-the-art methods, and show that\\nthe proposed method using CLEAN rewards can significantly scale-up, thus\\noutperforming the rest of learning-based methods. We further show that a hybrid\\nvariant of MARL achieves the best overall performance.\\n',\n",
       "  'title': u'Feature Selection as a Multiagent Coordination Problem'},\n",
       " u'1603.09429': {'arxivid': u'1603.09429',\n",
       "  'authorsaffil': [[u'Aaron Hunter', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'7 pages, 1 figure, presented at the International Workshop on\\n  Non-monotonic Reasoning 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09429v1',\n",
       "  'published': u'2016-03-31T00:48:40Z',\n",
       "  'summary': u'  We are interested in belief revision involving conditional statements where\\nthe antecedent is almost certainly false. To represent such problems, we use\\nOrdinal Conditional Functions that may take infinite values. We model belief\\nchange in this context through simple arithmetical operations that allow us to\\ncapture the intuition that certain antecedents can not be validated by any\\nnumber of observations. We frame our approach as a form of finite belief\\nimprovement, and we propose a model of conditional belief revision in which\\nonly the \"right\" hypothetical levels of implausibility are revised.\\n',\n",
       "  'title': u'Ordinal Conditional Functions for Nearly Counterfactual Revision'},\n",
       " u'1604.00367': {'arxivid': u'1604.00367',\n",
       "  'authorsaffil': [[u'Mengran Gou', None],\n",
       "   [u'Xikang Zhang', None],\n",
       "   [u'Angels Rates-Borras', None],\n",
       "   [u'Sadjad Asghari-Esfeden', None],\n",
       "   [u'Mario Sznaier', None],\n",
       "   [u'Octavia Camps', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'10 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00367v1',\n",
       "  'published': u'2016-04-01T19:20:03Z',\n",
       "  'summary': u'  Person re-identification is critical in surveillance applications. Current\\napproaches rely on appearance based features extracted from a single or\\nmultiple shots of the target and candidate matches. These approaches are at a\\ndisadvantage when trying to distinguish between candidates dressed in similar\\ncolors or when targets change their clothing. In this paper we propose a\\ndynamics-based feature to overcome this limitation. The main idea is to capture\\nsoft biometrics from gait and motion patterns by gathering dense short\\ntrajectories (tracklets) which are Fisher vector encoded. To illustrate the\\nmerits of the proposed features we introduce three new \"appearance-impaired\"\\ndatasets. Our experiments on the original and the appearance impaired datasets\\ndemonstrate the benefits of incorporating dynamics-based information with\\nappearance-based information to re-identification algorithms.\\n',\n",
       "  'title': u'Person Re-identification in Appearance Impaired Scenarios'},\n",
       " u'1603.05157': {'arxivid': u'1603.05157',\n",
       "  'authorsaffil': [[u'Heike Adel', None],\n",
       "   [u'Benjamin Roth', None],\n",
       "   [u'Hinrich Sch\\xfctze', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'NAACL 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05157v2',\n",
       "  'published': u'2016-03-16T16:02:03Z',\n",
       "  'summary': u'  We address relation classification in the context of slot filling, the task\\nof finding and evaluating fillers like \"Steve Jobs\" for the slot X in \"X\\nfounded Apple\". We propose a convolutional neural network which splits the\\ninput sentence into three parts according to the relation arguments and compare\\nit to state-of-the-art and traditional approaches of relation classification.\\nFinally, we combine different methods and show that the combination is better\\nthan individual approaches. We also analyze the effect of genre differences on\\nperformance.\\n',\n",
       "  'title': u'Comparing Convolutional Neural Networks to Traditional Models for Slot\\n  Filling'},\n",
       " u'1602.07388': {'arxivid': u'1602.07388',\n",
       "  'authorsaffil': [[u'Keith Burghardt', None],\n",
       "   [u'Emanuel F. Alsina', None],\n",
       "   [u'Michelle Girvan', None],\n",
       "   [u'William Rand', None],\n",
       "   [u'Kristina Lerman', None]],\n",
       "  'categoryterms': [u'cs.HC', u'cs.CY', u'cs.LG', u'physics.soc-ph'],\n",
       "  'comment': u'10 pages, 9 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07388v1',\n",
       "  'published': u'2016-02-24T03:55:15Z',\n",
       "  'summary': u'  Crowds can often make better decisions than individuals or small groups of\\nexperts by leveraging their ability to aggregate diverse information. Question\\nanswering sites, such as Stack Exchange, rely on the \"wisdom of crowds\" effect\\nto identify the best answers to questions asked by users. We analyze data from\\n250 communities on the Stack Exchange network to pinpoint factors affecting\\nwhich answers are chosen as the best answers. Our results suggest that, rather\\nthan evaluate all available answers to a question, users rely on simple\\ncognitive heuristics to choose an answer to vote for or accept. These cognitive\\nheuristics are linked to an answer\\'s salience, such as the order in which it is\\nlisted and how much screen space it occupies. While askers appear to depend\\nmore on heuristics, compared to voting users, when choosing an answer to accept\\nas the most helpful one, voters use acceptance itself as a heuristic: they are\\nmore likely to choose the answer after it is accepted than before that very\\nsame answer was accepted. These heuristics become more important in explaining\\nand predicting behavior as the number of available answers increases. Our\\nfindings suggest that crowd judgments may become less reliable as the number of\\nanswers grow.\\n',\n",
       "  'title': u'The Myopia of Crowds: A Study of Collective Evaluation on Stack Exchange'},\n",
       " u'1603.05154': {'arxivid': u'1603.05154',\n",
       "  'authorsaffil': [[u'Faisal Mahmood', None],\n",
       "   [u'M\\xe4rt Toots', None],\n",
       "   [u'Lars-G\\xf6ran \\xd6fverstedt', None],\n",
       "   [u'Ulf Skoglund', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AR'],\n",
       "  'comment': u'IEEE 2015 International Conference on Field Programmable Technology\\n  (FPT), Queenstown, New Zealand',\n",
       "  'doi': u'10.1109/FPT.2015.7393157',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05154v1',\n",
       "  'published': u'2016-03-16T15:52:13Z',\n",
       "  'summary': u'  Two-Dimensional (2D) Discrete Fourier Transform (DFT) is a basic and\\ncomputationally intensive algorithm, with a vast variety of applications. 2D\\nimages are, in general, non-periodic, but are assumed to be periodic while\\ncalculating their DFTs. This leads to cross-shaped artifacts in the frequency\\ndomain due to spectral leakage. These artifacts can have critical consequences\\nif the DFTs are being used for further processing. In this paper we present a\\nnovel FPGA-based design to calculate high-throughput 2D DFTs with simultaneous\\nedge artifact removal. Standard approaches for removing these artifacts using\\napodization functions or mirroring, either involve removing critical\\nfrequencies or a surge in computation by increasing image size. We use a\\nperiodic-plus-smooth decomposition based artifact removal algorithm optimized\\nfor FPGA implementation, while still achieving real-time ($\\\\ge$23 frames per\\nsecond) performance for a 512$\\\\times$512 size image stream. Our optimization\\napproach leads to a significant decrease in external memory utilization thereby\\navoiding memory conflicts and simplifies the design. We have tested our design\\non a PXIe based Xilinx Kintex 7 FPGA system communicating with a host PC which\\ngives us the advantage to further expand the design for industrial\\napplications.\\n',\n",
       "  'title': u'2D Discrete Fourier Transform with Simultaneous Edge Artifact Removal\\n  for Real-Time Applications'},\n",
       " u'1603.09423': {'arxivid': u'1603.09423',\n",
       "  'authorsaffil': [[u'Tong He', None],\n",
       "   [u'Weilin Huang', None],\n",
       "   [u'Yu Qiao', None],\n",
       "   [u'Jian Yao', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09423v1',\n",
       "  'published': u'2016-03-31T00:16:31Z',\n",
       "  'summary': u'  We introduce a new top-down pipeline for scene text detection. We propose a\\nnovel Cascaded Convolutional Text Network (CCTN) that joints two customized\\nconvolutional networks for coarse-to-fine text localization. The CCTN fast\\ndetects text regions roughly from a low-resolution image, and then accurately\\nlocalizes text lines from each enlarged region. We cast previous character\\nbased detection into direct text region estimation, avoiding multiple bottom-\\nup post-processing steps. It exhibits surprising robustness and discriminative\\npower by considering whole text region as detection object which provides\\nstrong semantic information. We customize convolutional network by develop- ing\\nrectangle convolutions and multiple in-network fusions. This enables it to\\nhandle multi-shape and multi-scale text efficiently. Furthermore, the CCTN is\\ncomputationally efficient by sharing convolutional computations, and high-level\\nproperty allows it to be invariant to various languages and multiple\\norientations. It achieves 0.84 and 0.86 F-measures on the ICDAR 2011 and ICDAR\\n2013, delivering substantial improvements over state-of-the-art results [23,\\n1].\\n',\n",
       "  'title': u'Accurate Text Localization in Natural Image with Cascaded Convolutional\\n  Text Network'},\n",
       " u'1603.07604': {'arxivid': u'1603.07604',\n",
       "  'authorsaffil': [[u'Yan Yan', None],\n",
       "   [u'Hanzi Wang', None],\n",
       "   [u'David Suter', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': u'Pattern Recognition, volume 47, 11, pages3487--3501, (2014)',\n",
       "  'link': u'http://arxiv.org/abs/1603.07604v1',\n",
       "  'published': u'2016-03-24T14:45:51Z',\n",
       "  'summary': u'  In this paper, we propose an effective feature extraction algorithm, called\\nMulti-Subregion based Correlation Filter Bank (MS-CFB), for robust face\\nrecognition. MS-CFB combines the benefits of global-based and local-based\\nfeature extraction algorithms, where multiple correlation filters correspond-\\ning to different face subregions are jointly designed to optimize the overall\\ncorrelation outputs. Furthermore, we reduce the computational complexi- ty of\\nMS-CFB by designing the correlation filter bank in the spatial domain and\\nimprove its generalization capability by capitalizing on the unconstrained form\\nduring the filter bank design process. MS-CFB not only takes the d- ifferences\\namong face subregions into account, but also effectively exploits the\\ndiscriminative information in face subregions. Experimental results on various\\npublic face databases demonstrate that the proposed algorithm pro- vides a\\nbetter feature representation for classification and achieves higher\\nrecognition rates compared with several state-of-the-art algorithms.\\n',\n",
       "  'title': u'Multi-Subregion Based Correlation Filter Bank for Robust Face\\n  Recognition'},\n",
       " u'1507.07583': {'arxivid': u'1507.07583',\n",
       "  'authorsaffil': [[u'David L. Richmond', None],\n",
       "   [u'Dagmar Kainmueller', None],\n",
       "   [u'Michael Y. Yang', None],\n",
       "   [u'Eugene W. Myers', None],\n",
       "   [u'Carsten Rother', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.07583v2',\n",
       "  'published': u'2015-07-27T20:44:51Z',\n",
       "  'summary': u'  We consider the task of pixel-wise semantic segmentation given a small set of\\nlabeled training images. Among two of the most popular techniques to address\\nthis task are Random Forests (RF) and Neural Networks (NN). The main\\ncontribution of this work is to explore the relationship between two special\\nforms of these techniques: stacked RFs and deep Convolutional Neural Networks\\n(CNN). We show that there exists a mapping from stacked RF to deep CNN, and an\\napproximate mapping back. This insight gives two major practical benefits:\\nFirstly, deep CNNs can be intelligently constructed and initialized, which is\\ncrucial when dealing with a limited amount of training data. Secondly, it can\\nbe utilized to create a new stacked RF with improved performance. Furthermore,\\nthis mapping yields a new CNN architecture, that is well suited for pixel-wise\\nsemantic labeling. We experimentally verify these practical benefits for two\\ndifferent application scenarios in computer vision and biology, where the\\nlayout of parts is important: Kinect-based body part labeling from depth\\nimages, and somite segmentation in microscopy images of developing zebrafish.\\n',\n",
       "  'title': u'Relating Cascaded Random Forests to Deep Convolutional Neural Networks\\n  for Semantic Segmentation'},\n",
       " u'1603.09420': {'arxivid': u'1603.09420',\n",
       "  'authorsaffil': [[u'Guo-Bing Zhou', None],\n",
       "   [u'Jianxin Wu', None],\n",
       "   [u'Chen-Lin Zhang', None],\n",
       "   [u'Zhi-Hua Zhou', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09420v1',\n",
       "  'published': u'2016-03-31T00:01:10Z',\n",
       "  'summary': u\"  Recently recurrent neural networks (RNN) has been very successful in handling\\nsequence data. However, understanding RNN and finding the best practices for\\nRNN is a difficult task, partly because there are many competing and complex\\nhidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as\\nMinimal Gated Unit (MGU), since it only contains one gate, which is a minimal\\ndesign among all gated hidden units. The design of MGU benefits from evaluation\\nresults on LSTM and GRU in the literature. Experiments on various sequence data\\nshow that MGU has comparable accuracy with GRU, but has a simpler structure,\\nfewer parameters, and faster training. Hence, MGU is suitable in RNN's\\napplications. Its simple architecture also means that it is easier to evaluate\\nand tune, and in principle it is easier to study MGU's properties theoretically\\nand empirically.\\n\",\n",
       "  'title': u'Minimal Gated Unit for Recurrent Neural Networks'},\n",
       " u'1602.07383': {'arxivid': u'1602.07383',\n",
       "  'authorsaffil': [[u'Weiguang Ding', None], [u'Graham Taylor', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'Preprints accepted by Computers and electronics in agriculture',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07383v1',\n",
       "  'published': u'2016-02-24T03:35:42Z',\n",
       "  'summary': u'  Monitoring the number of insect pests is a crucial component in\\npheromone-based pest management systems. In this paper, we propose an automatic\\ndetection pipeline based on deep learning for identifying and counting pests in\\nimages taken inside field traps. Applied to a commercial codling moth dataset,\\nour method shows promising performance both qualitatively and quantitatively.\\nCompared to previous attempts at pest detection, our approach uses no\\npest-specific engineering which enables it to adapt to other species and\\nenvironments with minimal human effort. It is amenable to implementation on\\nparallel hardware and therefore capable of deployment in settings where\\nreal-time performance is required.\\n',\n",
       "  'title': u'Automatic Moth Detection from Trap Images for Pest Management'},\n",
       " u'1603.08564': {'arxivid': u'1603.08564',\n",
       "  'authorsaffil': [[u'Satrajit Mukherjee', None],\n",
       "   [u'Bodhisattwa Prasad Majumder', None],\n",
       "   [u'Aritran Piplai', None],\n",
       "   [u'Swagatam Das', None]],\n",
       "  'categoryterms': [u'cs.CV', u'stat.ML'],\n",
       "  'comment': u'Journal Version',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08564v1',\n",
       "  'published': u'2016-03-28T21:09:52Z',\n",
       "  'summary': u'  The paper proposes a novel Kernelized image segmentation scheme for noisy\\nimages that utilizes the concept of Smallest Univalue Segment Assimilating\\nNucleus (SUSAN) and incorporates spatial constraints by computing circular\\ncolour map induced weights. Fuzzy damping coefficients are obtained for each\\nnucleus or center pixel on the basis of the corresponding weighted SUSAN area\\nvalues, the weights being equal to the inverse of the number of horizontal and\\nvertical moves required to reach a neighborhood pixel from the center pixel.\\nThese weights are used to vary the contributions of the different nuclei in the\\nKernel based framework. The paper also presents an edge quality metric obtained\\nby fuzzy decision based edge candidate selection and final computation of the\\nblurriness of the edges after their selection. The inability of existing\\nalgorithms to preserve edge information and structural details in their\\nsegmented maps necessitates the computation of the edge quality factor (EQF)\\nfor all the competing algorithms. Qualitative and quantitative analysis have\\nbeen rendered with respect to state-of-the-art algorithms and for images ridden\\nwith varying types of noises. Speckle noise ridden SAR images and Rician noise\\nridden Magnetic Resonance Images have also been considered for evaluating the\\neffectiveness of the proposed algorithm in extracting important segmentation\\ninformation.\\n',\n",
       "  'title': u'Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image\\n  Segmentation'},\n",
       " u'1604.00289': {'arxivid': u'1604.00289',\n",
       "  'authorsaffil': [[u'Brenden M. Lake', None],\n",
       "   [u'Tomer D. Ullman', None],\n",
       "   [u'Joshua B. Tenenbaum', None],\n",
       "   [u'Samuel J. Gershman', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CV', u'cs.LG', u'cs.NE', u'stat.ML'],\n",
       "  'comment': u'Added references. Updated Figure 3',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00289v2',\n",
       "  'published': u'2016-04-01T15:37:57Z',\n",
       "  'summary': u'  Recent progress in artificial intelligence (AI) has renewed interest in\\nbuilding systems that learn and think like people. Many advances have come from\\nusing deep neural networks trained end-to-end in tasks such as object\\nrecognition, video games, and board games, achieving performance that equals or\\neven beats humans in some respects. Despite their biological inspiration and\\nperformance achievements, these systems differ from human intelligence in\\ncrucial ways. We review progress in cognitive science suggesting that truly\\nhuman-like learning and thinking machines will have to reach beyond current\\nengineering trends in both what they learn, and how they learn it.\\nSpecifically, we argue that these machines should (a) build causal models of\\nthe world that support explanation and understanding, rather than merely\\nsolving pattern recognition problems; (b) ground learning in intuitive theories\\nof physics and psychology, to support and enrich the knowledge that is learned;\\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\\ngeneralize knowledge to new tasks and situations. We suggest concrete\\nchallenges and promising routes towards these goals that can combine the\\nstrengths of recent neural network advances with more structured cognitive\\nmodels.\\n',\n",
       "  'title': u'Building Machines That Learn and Think Like People'},\n",
       " u'1602.00386': {'arxivid': u'1602.00386',\n",
       "  'authorsaffil': [[u'Parthipan Siva', None],\n",
       "   [u'Mohammad Javad Shafiee', None],\n",
       "   [u'Mike Jamieson', None],\n",
       "   [u'Alexander Wong', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00386v1',\n",
       "  'published': u'2016-02-01T04:07:32Z',\n",
       "  'summary': u'  The problem of automated crowd segmentation and counting has garnered\\nsignificant interest in the field of video surveillance. This paper proposes a\\nnovel scene invariant crowd segmentation and counting algorithm designed with\\nhigh accuracy yet low computational complexity in mind, which is key for\\nwidespread industrial adoption. A novel low-complexity, scale-normalized\\nfeature called Histogram of Moving Gradients (HoMG) is introduced for highly\\neffective spatiotemporal representation of individuals and crowds within a\\nvideo. Real-time crowd segmentation is achieved via boosted cascade of weak\\nclassifiers based on sliding-window HoMG features, while linear SVM regression\\nof crowd-region HoMG features is employed for real-time crowd counting.\\nExperimental results using multi-camera crowd datasets show that the proposed\\nalgorithm significantly outperform state-of-the-art crowd counting algorithms,\\nas well as achieve very promising crowd segmentation results, thus\\ndemonstrating the efficacy of the proposed method for highly-accurate,\\nreal-time video-driven crowd analysis.\\n',\n",
       "  'title': u'Scene Invariant Crowd Segmentation and Counting Using Scale-Normalized\\n  Histogram of Moving Gradients (HoMG)'},\n",
       " u'1511.06727': {'arxivid': u'1511.06727',\n",
       "  'authorsaffil': [[u'Jelena Luketina', None],\n",
       "   [u'Mathias Berglund', None],\n",
       "   [u'Tapani Raiko', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'8 pages, 5 figures. added references, fixed typos',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06727v2',\n",
       "  'published': u'2015-11-20T19:10:16Z',\n",
       "  'summary': u'  Hyperparameter selection generally relies on running multiple full training\\ntrials, with hyperparameter selection based on validation set performance. We\\npropose a gradient-based approach for locally adjusting hyperparameters on the\\nfly in which we adjust the hyperparameters so as to make the model parameter\\ngradients, and hence updates, more advantageous for the validation cost. We\\nexplore the approach for tuning regularization hyperparameters and find that in\\nexperiments on MNIST the resulting regularization levels are within the optimal\\nregions. The method is less computationally demanding compared to similar\\ngradient-based approaches to hyperparameter selection, only requires a few\\ntrials, and consistently finds solid hyperparameter values which makes it a\\nuseful tool for training neural network models.\\n',\n",
       "  'title': u'Scalable Gradient-Based Tuning of Continuous Regularization\\n  Hyperparameters'},\n",
       " u'1604.01946': {'arxivid': u'1604.01946',\n",
       "  'authorsaffil': [[u'Jeremy Appleyard', None],\n",
       "   [u'Tomas Kocisky', None],\n",
       "   [u'Phil Blunsom', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01946v1',\n",
       "  'published': u'2016-04-07T10:31:01Z',\n",
       "  'summary': u\"  As recurrent neural networks become larger and deeper, training times for\\nsingle networks are rising into weeks or even months. As such there is a\\nsignificant incentive to improve the performance and scalability of these\\nnetworks. While GPUs have become the hardware of choice for training and\\ndeploying recurrent models, the implementations employed often make use of only\\nbasic optimizations for these architectures. In this article we demonstrate\\nthat by exposing parallelism between operations within the network, an order of\\nmagnitude speedup across a range of network sizes can be achieved over a naive\\nimplementation. We describe three stages of optimization that have been\\nincorporated into the fifth release of NVIDIA's cuDNN: firstly optimizing a\\nsingle cell, secondly a single layer, and thirdly the entire network.\\n\",\n",
       "  'title': u'Optimizing Performance of Recurrent Neural Networks on GPUs'},\n",
       " u'1604.02917': {'arxivid': u'1604.02917',\n",
       "  'authorsaffil': [[u'Stefanos Eleftheriadis', None],\n",
       "   [u'Ognjen Rudovic', None],\n",
       "   [u'Marc P. Deisenroth', None],\n",
       "   [u'Maja Pantic', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02917v2',\n",
       "  'published': u'2016-04-11T12:37:36Z',\n",
       "  'summary': u\"  We present a novel approach for supervised domain adaptation that is based\\nupon the probabilistic framework of Gaussian processes (GPs). Specifically, we\\nintroduce domain-specific GPs as local experts for facial expression\\nclassification from face images. The adaptation of the classifier is\\nfacilitated in probabilistic fashion by conditioning the target expert on\\nmultiple source experts. Furthermore, in contrast to existing adaptation\\napproaches, we also learn a target expert from available target data solely.\\nThen, a single and confident classifier is obtained by combining the\\npredictions from multiple experts based on their confidence. Learning of the\\nmodel is efficient and requires no retraining/reweighting of the source\\nclassifiers. We evaluate the proposed approach on two publicly available\\ndatasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression\\nclassification. To this end, we perform adaptation of two contextual factors:\\n'where' (view) and 'who' (subject). We show in our experiments that the\\nproposed approach consistently outperforms both source and target classifiers,\\nwhile using as few as 30 target examples. It also outperforms the\\nstate-of-the-art approaches for supervised domain adaptation.\\n\",\n",
       "  'title': u'Gaussian Process Domain Experts for Model Adaptation in Facial Behavior\\n  Analysis'},\n",
       " u'1604.02182': {'arxivid': u'1604.02182',\n",
       "  'authorsaffil': [[u'Joseph P Robinson', None],\n",
       "   [u'Ming Shao', None],\n",
       "   [u'Yue Wu', None],\n",
       "   [u'Yun Fu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02182v1',\n",
       "  'published': u'2016-04-07T21:45:53Z',\n",
       "  'summary': u'  We introduce a large-scale dataset for visual kin-based problems, the Family\\nin the Wild (FIW) dataset. Motivated by the lack of a single, unified image\\ndataset available for kinship tasks, our goal is to provide a dataset that\\ncaptivates the interest of the research community, i.e., large enough to\\nsupport multiple tasks for evaluation. For this, we collected and labelled the\\nlargest set of family images to date, with only a small team and an efficient\\nlabelling tool that was designed to optimize the process of marking complex\\nhierarchical relationships, attributes, and local label information in family\\nphotos. We experimentally compare our dataset the existing kinship image\\ndatasets, and demonstrate the practical value of the newly collected FIW\\ndataset. We also demonstrate that using a pre-trained convolutional neural\\nnetwork (CNN) as an off-the-shelf feature extractor as performing better than\\ntraditional feature types used for kinship based tasks in the visual domain. We\\nalso measure human performance and show their performance does not match up to\\nthat of machine vision algorithms.\\n',\n",
       "  'title': u'Family in the Wild (FIW): A Large-scale Kinship Recognition Database'},\n",
       " u'1604.03458': {'arxivid': u'1604.03458',\n",
       "  'authorsaffil': [[u'Jonathan Epperlein', None], [u'Jakub Marecek', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.AI', u'cs.MA'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03458v1',\n",
       "  'published': u'2016-04-12T15:53:25Z',\n",
       "  'summary': u'  Many analyses of resource-allocation problems employ simplistic models of the\\npopulation. Using the example of a resource-allocation problem of Marecek et\\nal. [arXiv:1406.7639], we introduce rather a general behavioural model, where\\nthe evolution of a heterogeneous population of agents is governed by a Markov\\nchain. Still, we are able to show that the distribution of agents across\\nresources converges in distribution, for suitable means of information\\nprovision, under certain assumptions. The model and proof techniques may have\\nwider applicability.\\n',\n",
       "  'title': u'Resource Allocation with Population Dynamics'},\n",
       " u'1603.07421': {'arxivid': u'1603.07421',\n",
       "  'authorsaffil': [[u'Ye Yuan', None],\n",
       "   [u'Mu Li', None],\n",
       "   [u'Claire J. Tomlin', None]],\n",
       "  'categoryterms': [u'cs.SY', u'cs.LG', u'math.OC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07421v2',\n",
       "  'published': u'2016-03-24T03:13:40Z',\n",
       "  'summary': u\"  We propose a new method to accelerate the convergence of optimization\\nalgorithms. This method simply adds a power coefficient $\\\\gamma\\\\in[0,1)$ to the\\ngradient during optimization. We call this the Powerball method after the\\nwell-known Heavy-ball method by Polyak. We analyze the convergence rate for the\\nPowerball method for strongly convex functions and show that it has a faster\\nconvergence rate than gradient descent and Newton's method in the initial\\niterations. We also demonstrate that the Powerball method provides a $10$-fold\\nspeed up of the convergence of both gradient descent and L-BFGS on multiple\\nreal datasets as well as accelerates the computation for Pagerank vector.\\n\",\n",
       "  'title': u'On the Powerball Method'},\n",
       " u'1512.04295': {'arxivid': u'1512.04295',\n",
       "  'authorsaffil': [[u'Lukas Cavigelli', None], [u'Luca Benini', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.LG', u'cs.NE', u'B.7.1; I.2.6'],\n",
       "  'comment': u'14 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.04295v2',\n",
       "  'published': u'2015-12-14T13:06:43Z',\n",
       "  'summary': u'  An ever increasing number of computer vision and image/video processing\\nchallenges are being approached using deep convolutional neural networks,\\nobtaining state-of-the-art results in object recognition and detection,\\nsemantic segmentation, action recognition, optical flow and superresolution.\\nHardware acceleration of these algorithms is essential to adopt these\\nimprovements in embedded and mobile computer vision systems. We present a new\\narchitecture, design and implementation as well as the first reported silicon\\nmeasurements of such an accelerator, outperforming previous work in terms of\\npower-, area- and I/O-efficiency. The manufactured device provides up to 196\\nGOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power\\nefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it\\nthe first architecture scalable to TOp/s performance.\\n',\n",
       "  'title': u'Origami: A 803 GOp/s/W Convolutional Network Accelerator'},\n",
       " u'1603.06182': {'arxivid': u'1603.06182',\n",
       "  'authorsaffil': [[u'Haimin Zhang', None],\n",
       "   [u'Min Xu', None],\n",
       "   [u'Changsheng Xu', None],\n",
       "   [u'Ramesh Jain', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06182v2',\n",
       "  'published': u'2016-03-20T04:28:21Z',\n",
       "  'summary': u'  Recently, video classification attracts intensive research efforts. However,\\nmost existing works are based on framelevel visual features, which might fail\\nto model the temporal information, e.g. characteristics accumulated along time.\\nIn order to capture video temporal information, we propose to analyse features\\nin frequency domain transformed by discrete Fourier transform (DFT features).\\nFrame-level features are firstly extract by a pre-trained deep convolutional\\nneural network (CNN). Then, time domain features are transformed and\\ninterpolated into DFT features. CNN and DFT features are further encoded by\\nusing different pooling methods and fused for video classification. In this\\nway, static image features extracted from a pre-trained deep CNN and temporal\\ninformation represented by DFT features are jointly considered for video\\nclassification. We test our method for video emotion classification and action\\nrecognition. Experimental results demonstrate that combining DFT features can\\neffectively capture temporal information and therefore improve the performance\\nof both video emotion classification and action recognition. Our approach has\\nachieved a state-of-the-art performance on the largest video emotion dataset\\n(VideoEmotion-8 dataset) and competitive results on UCF-101.\\n',\n",
       "  'title': u'Modelling Temporal Information Using Discrete Fourier Transform for\\n  Video Classification'},\n",
       " u'1603.04186': {'arxivid': u'1603.04186',\n",
       "  'authorsaffil': [[u'Amir Rosenfeld', None], [u'Shimon Ullman', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04186v2',\n",
       "  'published': u'2016-03-14T10:18:03Z',\n",
       "  'summary': u\"  Convolutional neural networks have been shown to develop internal\\nrepresentations, which correspond closely to semantically meaningful objects\\nand parts, although trained solely on class labels. Class Activation Mapping\\n(CAM) is a recent method that makes it possible to easily highlight the image\\nregions contributing to a network's classification decision. We build upon\\nthese two developments to enable a network to re-examine informative image\\nregions, which we term introspection. We propose a weakly-supervised iterative\\nscheme, which shifts its center of attention to increasingly discriminative\\nregions as it progresses, by alternating stages of classification and\\nintrospection. We evaluate our method and show its effectiveness over a range\\nof several datasets, where we obtain competitive or state-of-the-art results:\\non Stanford-40 Actions, we set a new state-of the art of 81.74%. On\\nFGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements\\nover baselines, some of which include significantly more supervision.\\n\",\n",
       "  'title': u'Visual Concept Recognition and Localization via Iterative Introspection'},\n",
       " u'1602.07637': {'arxivid': u'1602.07637',\n",
       "  'authorsaffil': [[u'Ivens Portugal', None],\n",
       "   [u'Paulo Alencar', None],\n",
       "   [u'Donald Cowan', None]],\n",
       "  'categoryterms': [u'cs.SE', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07637v2',\n",
       "  'published': u'2016-02-24T18:58:34Z',\n",
       "  'summary': u'  The amount of data generated in the modern society is increasing rapidly. New\\nproblems and novel approaches of data capture, storage, analysis and\\nvisualization are responsible for the emergence of the Big Data research field.\\nMachine Learning algorithms can be used in Big Data to make better and more\\naccurate inferences. However, because of the challenges Big Data imposes, these\\nalgorithms need to be adapted and optimized to specific applications. One\\nimportant decision made by software engineers is the choice of the language\\nthat is used in the implementation of these algorithms. Therefore, this\\nliterature survey identifies and describes domain-specific languages and\\nframeworks used for Machine Learning in Big Data. By doing this, software\\nengineers can then make more informed choices and beginners have an overview of\\nthe main languages used in this domain.\\n',\n",
       "  'title': u'A Survey on Domain-Specific Languages for Machine Learning in Big Data'},\n",
       " u'1512.05986': {'arxivid': u'1512.05986',\n",
       "  'authorsaffil': [[u'Vlado Menkovski', None],\n",
       "   [u'Zharko Aleksovski', None],\n",
       "   [u'Axel Saalbach', None],\n",
       "   [u'Hannes Nickisch', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.NE'],\n",
       "  'comment': u'NIPS 2015 Workshop on Machine Learning in Healthcare',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05986v1',\n",
       "  'published': u'2015-12-18T15:16:31Z',\n",
       "  'summary': u'  Convolutional neural networks demonstrated outstanding empirical results in\\ncomputer vision and speech recognition tasks where labeled training data is\\nabundant. In medical imaging, there is a huge variety of possible imaging\\nmodalities and contrasts, where annotated data is usually very scarce. We\\npresent two approaches to deal with this challenge. A network pretrained in a\\ndifferent domain with abundant data is used as a feature extractor, while a\\nsubsequent classifier is trained on a small target dataset; and a deep\\narchitecture trained with heavy augmentation and equipped with sophisticated\\nregularization methods. We test the approaches on a corpus of X-ray images to\\ndesign an anatomy detection system.\\n',\n",
       "  'title': u'Can Pretrained Neural Networks Detect Anatomy?'},\n",
       " u'1511.07122': {'arxivid': u'1511.07122',\n",
       "  'authorsaffil': [[u'Fisher Yu', None], [u'Vladlen Koltun', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Published as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07122v3',\n",
       "  'published': u'2015-11-23T07:32:14Z',\n",
       "  'summary': u'  State-of-the-art models for semantic segmentation are based on adaptations of\\nconvolutional networks that had originally been designed for image\\nclassification. However, dense prediction and image classification are\\nstructurally different. In this work, we develop a new convolutional network\\nmodule that is specifically designed for dense prediction. The presented module\\nuses dilated convolutions to systematically aggregate multi-scale contextual\\ninformation without losing resolution. The architecture is based on the fact\\nthat dilated convolutions support exponential expansion of the receptive field\\nwithout loss of resolution or coverage. We show that the presented context\\nmodule increases the accuracy of state-of-the-art semantic segmentation\\nsystems. In addition, we examine the adaptation of image classification\\nnetworks to dense prediction and show that simplifying the adapted network can\\nincrease accuracy.\\n',\n",
       "  'title': u'Multi-Scale Context Aggregation by Dilated Convolutions'},\n",
       " u'1601.04580': {'arxivid': u'1601.04580',\n",
       "  'authorsaffil': [[u'Vinodh Krishnan', None], [u'Jacob Eisenstein', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u\"This report is based on a rejected submission from the 2015\\n  Conference on Empirical Methods on Natural Language Processing, incorporating\\n  some of the reviewers' suggestions for improvement\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04580v1',\n",
       "  'published': u'2016-01-18T15:46:00Z',\n",
       "  'summary': u'  News events and social media are composed of evolving storylines, which\\ncapture public attention for a limited period of time. Identifying these\\nstorylines would enable many high-impact applications, such as tracking public\\ninterest and opinion in ongoing crisis events. However, this requires\\nintegrating temporal and linguistic information, and prior work takes a largely\\nheuristic approach. We present a novel online non-parametric Bayesian framework\\nfor storyline detection, using the distance-dependent Chinese Restaurant\\nProcess (dd-CRP). To ensure efficient linear-time inference, we employ a\\nfixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluate\\nour baseline and proposed models on the TREC Twitter Timeline Generation task\\nand show strong results.\\n',\n",
       "  'title': u'Nonparametric Bayesian Storyline Detection from Microtexts'},\n",
       " u'1602.06725': {'arxivid': u'1602.06725',\n",
       "  'authorsaffil': [[u'Andriy Mnih', None], [u'Danilo J. Rezende', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Appears in Proceedings of the 33rd International Conference on\\n  Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&CP volume 48',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06725v2',\n",
       "  'published': u'2016-02-22T11:06:06Z',\n",
       "  'summary': u'  Recent progress in deep latent variable models has largely been driven by the\\ndevelopment of flexible and scalable variational inference methods. Variational\\ntraining of this type involves maximizing a lower bound on the log-likelihood,\\nusing samples from the variational posterior to compute the required gradients.\\nRecently, Burda et al. (2016) have derived a tighter lower bound using a\\nmulti-sample importance sampling estimate of the likelihood and showed that\\noptimizing it yields models that use more of their capacity and achieve higher\\nlikelihoods. This development showed the importance of such multi-sample\\nobjectives and explained the success of several related approaches.\\n  We extend the multi-sample approach to discrete latent variables and analyze\\nthe difficulty encountered when estimating the gradients involved. We then\\ndevelop the first unbiased gradient estimator designed for importance-sampled\\nobjectives and evaluate it at training generative and structured output\\nprediction models. The resulting estimator, which is based on low-variance\\nper-sample learning signals, is both simpler and more effective than the NVIL\\nestimator proposed for the single-sample variational objective, and is\\ncompetitive with the currently used biased estimators.\\n',\n",
       "  'title': u'Variational inference for Monte Carlo objectives'},\n",
       " u'1602.06727': {'arxivid': u'1602.06727',\n",
       "  'authorsaffil': [[u'Zhizheng Wu', None], [u'Simon King', None]],\n",
       "  'categoryterms': [u'cs.SD', u'cs.CL', u'cs.NE'],\n",
       "  'comment': u'submitted to IEEE/ACM Transactions on Audio, Speech and Language\\n  Processing 2016 (AQ)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06727v3',\n",
       "  'published': u'2016-02-22T11:11:04Z',\n",
       "  'summary': u'  We propose two novel techniques --- stacking bottleneck features and minimum\\ngeneration error training criterion --- to improve the performance of deep\\nneural network (DNN)-based speech synthesis. The techniques address the related\\nissues of frame-by-frame independence and ignorance of the relationship between\\nstatic and dynamic features, within current typical DNN-based synthesis\\nframeworks. Stacking bottleneck features, which are an acoustically--informed\\nlinguistic representation, provides an efficient way to include more detailed\\nlinguistic context at the input. The minimum generation error training\\ncriterion minimises overall output trajectory error across an utterance, rather\\nthan minimising the error per frame independently, and thus takes into account\\nthe interaction between static and dynamic features. The two techniques can be\\neasily combined to further improve performance. We present both objective and\\nsubjective results that demonstrate the effectiveness of the proposed\\ntechniques. The subjective results show that combining the two techniques leads\\nto significantly more natural synthetic speech than from conventional DNN or\\nlong short-term memory (LSTM) recurrent neural network (RNN) systems.\\n',\n",
       "  'title': u'Improving Trajectory Modelling for DNN-based Speech Synthesis by using\\n  Stacked Bottleneck Features and Minimum Generation Error Training'},\n",
       " u'1601.04589': {'arxivid': u'1601.04589',\n",
       "  'authorsaffil': [[u'Chuan Li', None], [u'Michael Wand', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages, 9 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04589v1',\n",
       "  'published': u'2016-01-18T16:31:37Z',\n",
       "  'summary': u'  This paper studies a combination of generative Markov random field (MRF)\\nmodels and discriminatively trained deep convolutional neural networks (dCNNs)\\nfor synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN\\nfeature pyramid, controling the image layout at an abstract level. We apply the\\nmethod to both photographic and non-photo-realistic (artwork) synthesis tasks.\\nThe MRF regularizer prevents over-excitation artifacts and reduces implausible\\nfeature mixtures common to previous dCNN inversion approaches, permitting\\nsynthezing photographic content with increased visual plausibility. Unlike\\nstandard MRF-based texture synthesis, the combined system can both match and\\nadapt local features with considerable variability, yielding results far out of\\nreach of classic generative MRF methods.\\n',\n",
       "  'title': u'Combining Markov Random Fields and Convolutional Neural Networks for\\n  Image Synthesis'},\n",
       " u'1603.00912': {'arxivid': u'1603.00912',\n",
       "  'authorsaffil': [[u'Lei Wang', None], [u'Yongun Zhang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00912v1',\n",
       "  'published': u'2016-03-02T22:09:45Z',\n",
       "  'summary': u'  This paper addresses the task of separating ground points from airborne LiDAR\\npoint cloud data in urban areas. A novel ground filtering method using scan\\nline segmentation is proposed here, which we call SLSGF. It utilizes the scan\\nline information in LiDAR data to segment the LiDAR data. The similarity\\nmeasurements are designed to make it possible to segment complex roof\\nstructures into a single segment as much as possible so the topological\\nrelationships between the roof and the ground are simpler, which will benefit\\nthe labeling process. In the labeling process, the initial ground segments are\\ndetected and a coarse to fine labeling scheme is applied. Data from ISPRS 2011\\nare used to test the accuracy of SLSGF; and our analytical and experimental\\nresults show that this method is computationally-efficient and\\nnoise-insensitive, thereby making a denoising process unnecessary before\\nfiltering.\\n',\n",
       "  'title': u'LiDAR Ground Filtering Algorithm for Urban Areas Using Scan Line Based\\n  Segmentation'},\n",
       " u'1603.08182': {'arxivid': u'1603.08182',\n",
       "  'authorsaffil': [[u'Andy Zeng', None],\n",
       "   [u'Shuran Song', None],\n",
       "   [u'Matthias Nie\\xdfner', None],\n",
       "   [u'Matthew Fisher', None],\n",
       "   [u'Jianxiong Xiao', None]],\n",
       "  'categoryterms': [u'cs.CV', u'I.4.7; I.4.5; I.3.5; I.2.10; I.2.6'],\n",
       "  'comment': u'(13 pages, 13 figures) Project link: http://3dmatch.cs.princeton.edu/',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08182v1',\n",
       "  'published': u'2016-03-27T06:43:52Z',\n",
       "  'summary': u'  Establishing correspondences between 3D geometries is essential to a large\\nvariety of graphics and vision applications, including 3D reconstruction,\\nlocalization, and shape matching. Despite significant progress, geometric\\nmatching on real-world 3D data is still a challenging task due to the noisy,\\nlow-resolution, and incomplete nature of scanning data. These difficulties\\nlimit the performance of current state-of-art methods which are typically based\\non histograms over geometric properties. In this paper, we introduce 3DMatch, a\\ndata-driven local feature learner that jointly learns a geometric feature\\nrepresentation and an associated metric function from a large collection of\\nreal-world scanning data. We represent 3D geometry using accumulated distance\\nfields around key-point locations. This representation is suited to handle\\nnoisy and partial scanning data, and concurrently supports deep learning with\\nconvolutional neural networks directly in 3D. To train the networks, we propose\\na way to automatically generate correspondence labels for deep learning by\\nleveraging existing RGB-D reconstruction algorithms. In our results, we\\ndemonstrate that we are able to outperform state-of-the-art approaches by a\\nsignificant margin. In addition, we show the robustness of our descriptor in a\\npurely geometric sparse bundle adjustment pipeline for 3D reconstruction.\\n',\n",
       "  'title': u'3DMatch: Learning the Matching of Local 3D Geometry in Range Scans'},\n",
       " u'1603.03703': {'arxivid': u'1603.03703',\n",
       "  'authorsaffil': [[u'Kallol Roy', None],\n",
       "   [u'Anh Tong', None],\n",
       "   [u'Jaesik Choi', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03703v1',\n",
       "  'published': u'2016-03-11T17:47:00Z',\n",
       "  'summary': u'  Finding interesting symmetrical topological structures in high-dimensional\\nsystems is an important problem in statistical machine learning. Limited amount\\nof available high-dimensional data and its sensitivity to noise pose\\ncomputational challenges to find symmetry. Our paper presents a new method to\\nfind local symmetries in a low-dimensional 2-D grid structure which is embedded\\nin high-dimensional structure. To compute the symmetry in a grid structure, we\\nintroduce three legal grid moves (i) Commutation (ii) Cyclic Permutation (iii)\\nStabilization on sets of local grid squares, grid blocks. The three grid moves\\nare legal transformations as they preserve the statistical distribution of\\nhamming distances in each grid block. We propose and coin the term of grid\\nsymmetry of data on the 2-D data grid as the invariance of statistical\\ndistributions of hamming distance are preserved after a sequence of grid moves.\\nWe have computed and analyzed the grid symmetry of data on multivariate\\nGaussian distributions and Gamma distributions with noise.\\n',\n",
       "  'title': u'Searching for Topological Symmetry in Data Haystack'},\n",
       " u'1604.01360': {'arxivid': u'1604.01360',\n",
       "  'authorsaffil': [[u'Lerrel Pinto', None],\n",
       "   [u'Dhiraj Gandhi', None],\n",
       "   [u'Yuanfeng Han', None],\n",
       "   [u'Yong-Lae Park', None],\n",
       "   [u'Abhinav Gupta', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.RO'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01360v1',\n",
       "  'published': u'2016-04-05T18:47:15Z',\n",
       "  'summary': u'  What is the right supervisory signal to train visual representations? Current\\napproaches in computer vision use category labels from datasets such as\\nImageNet to train ConvNets. However, in case of biological agents, visual\\nrepresentation learning does not require semantic labels. In fact, we argue\\nthat biological agents use active exploration and physical interactions with\\nthe world to learn visual representations unlike current vision systems which\\njust use passive observations (images and videos downloaded from web). For\\nexample, babies push objects, poke them, put them in their mouth and throw them\\nto learn representations. Towards this goal, we build one of the first systems\\non a Baxter platform that pushes, pokes, grasps and actively observes objects\\nin a tabletop environment. It uses four different types of physical\\ninteractions to collect more than 130K datapoints, with each datapoint\\nproviding backprops to a shared ConvNet architecture allowing us to learn\\nvisual representations. We show the quality of learned representations by\\nobserving neuron activations and performing nearest neighbor retrieval on this\\nlearned representation. Finally, we evaluate our learned ConvNet on different\\nimage classification tasks and show improvements compared to learning without\\nexternal data.\\n',\n",
       "  'title': u'The Curious Robot: Learning Visual Representations via Physical\\n  Interactions'},\n",
       " u'1311.2901': {'arxivid': u'1311.2901',\n",
       "  'authorsaffil': [[u'Matthew D Zeiler', None], [u'Rob Fergus', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1311.2901v3',\n",
       "  'published': u'2013-11-12T20:02:22Z',\n",
       "  'summary': u'  Large Convolutional Network models have recently demonstrated impressive\\nclassification performance on the ImageNet benchmark. However there is no clear\\nunderstanding of why they perform so well, or how they might be improved. In\\nthis paper we address both issues. We introduce a novel visualization technique\\nthat gives insight into the function of intermediate feature layers and the\\noperation of the classifier. We also perform an ablation study to discover the\\nperformance contribution from different model layers. This enables us to find\\nmodel architectures that outperform Krizhevsky \\\\etal on the ImageNet\\nclassification benchmark. We show our ImageNet model generalizes well to other\\ndatasets: when the softmax classifier is retrained, it convincingly beats the\\ncurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.\\n',\n",
       "  'title': u'Visualizing and Understanding Convolutional Networks'},\n",
       " u'1511.06830': {'arxivid': u'1511.06830',\n",
       "  'authorsaffil': [[u'Xuan Dong', None],\n",
       "   [u'Boyan Bonev', None],\n",
       "   [u'Weixin Li', None],\n",
       "   [u'Weichao Qiu', None],\n",
       "   [u'Xianjie Chen', None],\n",
       "   [u'Alan Yuille', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'This paper has been withdrawn by the author due to some un-proper\\n  examples',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06830v2',\n",
       "  'published': u'2015-11-21T04:04:39Z',\n",
       "  'summary': u'  Base-detail separation is a fundamental computer vision problem consisting of\\nmodeling a smooth base layer with the coarse structures, and a detail layer\\ncontaining the texture-like structures. One of the challenges of estimating the\\nbase is to preserve sharp boundaries between objects or parts to avoid halo\\nartifacts. Many methods have been proposed to address this problem, but there\\nis no ground-truth dataset of real images for quantitative evaluation. We\\nproposed a procedure to construct such a dataset, and provide two datasets:\\nPascal Base-Detail and Fashionista Base-Detail, containing 1000 and 250 images,\\nrespectively. Our assumption is that the base is piecewise smooth and we label\\nthe appearance of each piece by a polynomial model. The pieces are objects and\\nparts of objects, obtained from human annotations. Finally, we proposed a way\\nto evaluate methods with our base-detail ground-truth and we compared the\\nperformances of seven state-of-the-art algorithms.\\n',\n",
       "  'title': u'Ground-truth dataset and baseline evaluations for image base-detail\\n  separation algorithms'},\n",
       " u'1603.08482': {'arxivid': u'1603.08482',\n",
       "  'authorsaffil': [[u'Sida I. Wang', None],\n",
       "   [u'Arun Tejasvi Chaganty', None],\n",
       "   [u'Percy Liang', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'NIPS 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08482v1',\n",
       "  'published': u'2016-03-28T18:55:02Z',\n",
       "  'summary': u'  Mixture modeling is a general technique for making any simple model more\\nexpressive through weighted combination. This generality and simplicity in part\\nexplains the success of the Expectation Maximization (EM) algorithm, in which\\nupdates are easy to derive for a wide class of mixture models. However, the\\nlikelihood of a mixture model is non-convex, so EM has no known global\\nconvergence guarantees. Recently, method of moments approaches offer global\\nguarantees for some mixture models, but they do not extend easily to the range\\nof mixture models that exist. In this work, we present Polymom, an unifying\\nframework based on method of moments in which estimation procedures are easily\\nderivable, just as in EM. Polymom is applicable when the moments of a single\\nmixture component are polynomials of the parameters. Our key observation is\\nthat the moments of the mixture model are a mixture of these polynomials, which\\nallows us to cast estimation as a Generalized Moment Problem. We solve its\\nrelaxations using semidefinite optimization, and then extract parameters using\\nideas from computer algebra. This framework allows us to draw insights and\\napply tools from convex optimization, computer algebra and the theory of\\nmoments to study problems in statistical estimation.\\n',\n",
       "  'title': u'Estimating Mixture Models via Mixtures of Polynomials'},\n",
       " u'1506.04557': {'arxivid': u'1506.04557',\n",
       "  'authorsaffil': [[u'Chao Du', None],\n",
       "   [u'Jun Zhu', None],\n",
       "   [u'Bo Zhang', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.04557v4',\n",
       "  'published': u'2015-06-15T11:37:09Z',\n",
       "  'summary': u'  We present doubly stochastic gradient MCMC, a simple and generic method for\\n(approximate) Bayesian inference of deep generative models (DGMs) in a\\ncollapsed continuous parameter space. At each MCMC sampling step, the algorithm\\nrandomly draws a mini-batch of data samples to estimate the gradient of\\nlog-posterior and further estimates the intractable expectation over hidden\\nvariables via a neural adaptive importance sampler, where the proposal\\ndistribution is parameterized by a deep neural network and learnt jointly. We\\ndemonstrate the effectiveness on learning various DGMs in a wide range of\\ntasks, including density estimation, data generation and missing data\\nimputation. Our method outperforms many state-of-the-art competitors.\\n',\n",
       "  'title': u'Learning Deep Generative Models with Doubly Stochastic MCMC'},\n",
       " u'1604.02509': {'arxivid': u'1604.02509',\n",
       "  'authorsaffil': [[u'Shiwali Mohan', None],\n",
       "   [u'Aaron Mininger', None],\n",
       "   [u'John Laird', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'Advances in Cognitive Systems 3 (2014)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02509v1',\n",
       "  'published': u'2016-04-09T01:57:13Z',\n",
       "  'summary': u'  We propose a computational model of situated language comprehension based on\\nthe Indexical Hypothesis that generates meaning representations by translating\\namodal linguistic symbols to modal representations of beliefs, knowledge, and\\nexperience external to the linguistic system. This Indexical Model incorporates\\nmultiple information sources, including perceptions, domain knowledge, and\\nshort-term and long-term experiences during comprehension. We show that\\nexploiting diverse information sources can alleviate ambiguities that arise\\nfrom contextual use of underspecific referring expressions and unexpressed\\nargument alternations of verbs. The model is being used to support linguistic\\ninteractions in Rosie, an agent implemented in Soar that learns from\\ninstruction.\\n',\n",
       "  'title': u'Towards an Indexical Model of Situated Language Comprehension for\\n  Cognitive Agents in Physical Worlds'},\n",
       " u'1603.02729': {'arxivid': u'1603.02729',\n",
       "  'authorsaffil': [[u'Ruzena Bajcsy', None],\n",
       "   [u'Yiannis Aloimonos', None],\n",
       "   [u'John K. Tsotsos', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.RO'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02729v2',\n",
       "  'published': u'2016-03-08T22:48:26Z',\n",
       "  'summary': u'  Despite the recent successes in robotics, artificial intelligence and\\ncomputer vision, a complete artificial agent necessarily must include active\\nperception. A multitude of ideas and methods for how to accomplish this have\\nalready appeared in the past, their broader utility perhaps impeded by\\ninsufficient computational power or costly hardware. The history of these\\nideas, perhaps selective due to our perspectives, is presented with the goal of\\norganizing the past literature and highlighting the seminal contributions. We\\nargue that those contributions are as relevant today as they were decades ago\\nand, with the state of modern computational tools, are poised to find new life\\nin the robotic perception systems of the next decade.\\n',\n",
       "  'title': u'Revisiting Active Perception'},\n",
       " u'1603.03381': {'arxivid': u'1603.03381',\n",
       "  'authorsaffil': [[u'Armin Mustafa', None],\n",
       "   [u'Hansung Kim', None],\n",
       "   [u'Jean-Yves Guillemaut', None],\n",
       "   [u'Adrian Hilton', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'To appear in The IEEE Conference on Computer Vision and Pattern\\n  Recognition (CVPR) 2016 . Video available at:\\n  https://www.youtube.com/watch?v=bm_P13_-DsQ',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03381v2',\n",
       "  'published': u'2016-03-10T19:16:43Z',\n",
       "  'summary': u'  This paper presents an approach for reconstruction of 4D temporally coherent\\nmodels of complex dynamic scenes. No prior knowledge is required of scene\\nstructure or camera calibration allowing reconstruction from multiple moving\\ncameras. Sparse-to-dense temporal correspondence is integrated with joint\\nmulti-view segmentation and reconstruction to obtain a complete 4D\\nrepresentation of static and dynamic objects. Temporal coherence is exploited\\nto overcome visual ambiguities resulting in improved reconstruction of complex\\nscenes. Robust joint segmentation and reconstruction of dynamic objects is\\nachieved by introducing a geodesic star convexity constraint. Comparative\\nevaluation is performed on a variety of unstructured indoor and outdoor dynamic\\nscenes with hand-held cameras and multiple people. This demonstrates\\nreconstruction of complete temporally coherent 4D scene models with improved\\nnonrigid object segmentation and shape reconstruction.\\n',\n",
       "  'title': u'Temporally coherent 4D reconstruction of complex dynamic scenes'},\n",
       " u'1506.07194': {'arxivid': u'1506.07194',\n",
       "  'authorsaffil': [[u'Giuseppe Boccignone', None]],\n",
       "  'categoryterms': [u'physics.data-an',\n",
       "   u'cs.CV',\n",
       "   u'q-bio.NC',\n",
       "   u'G.3; I.5; I.4'],\n",
       "  'comment': u'Extended draft of Chapter to appear in \"An introduction to the\\n  scientific foundations of eye movement research and its applications\"',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.07194v3',\n",
       "  'published': u'2015-06-23T20:38:15Z',\n",
       "  'summary': u\"  In this Chapter we show that by considering eye movements, and in particular,\\nthe resulting sequence of gaze shifts, a stochastic process, a wide variety of\\ntools become available for analyses and modelling beyond conventional\\nstatistical methods. Such tools encompass random walk analyses and more complex\\ntechniques borrowed from the pattern recognition and machine learning fields.\\n  After a brief, though critical, probabilistic tour of current computational\\nmodels of eye movements and visual attention, we lay down the basis for gaze\\nshift pattern analysis. To this end, the concepts of Markov Processes, the\\nWiener process and related random walks within the Gaussian framework of the\\nCentral Limit Theorem will be introduced. Then, we will deliberately violate\\nfundamental assumptions of the Central Limit Theorem to elicit a larger\\nperspective, rooted in statistical physics, for analysing and modelling eye\\nmovements in terms of anomalous, non-Gaussian, random walks and modern foraging\\ntheory.\\n  Eventually, by resorting to machine learning techniques, we discuss how the\\nanalyses of movement patterns can develop into the inference of hidden patterns\\nof the mind: inferring the observer's task, assessing cognitive impairments,\\nclassifying expertise.\\n\",\n",
       "  'title': u'Advanced statistical methods for eye movement analysis and modeling: a\\n  gentle introduction'},\n",
       " u'1604.01416': {'arxivid': u'1604.01416',\n",
       "  'authorsaffil': [[u'Steven Eliuk', None],\n",
       "   [u'Cameron Upright', None],\n",
       "   [u'Anthony Skjellum', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.DC', u'cs.MS'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01416v1',\n",
       "  'published': u'2016-04-05T20:28:26Z',\n",
       "  'summary': u'  A new scalable parallel math library, dMath, is presented in this paper that\\ndemonstrates leading scaling when using intranode, or internode,\\nhybrid-parallelism for deep-learning. dMath provides easy-to-use distributed\\nbase primitives and a variety of domain-specific algorithms. These include\\nmatrix multiplication, convolutions, and others allowing for rapid development\\nof highly scalable applications, including Deep Neural Networks (DNN), whereas\\npreviously one was restricted to libraries that provided effective primitives\\nfor only a single GPU, like Nvidia cublas and cudnn or DNN primitives from\\nNervana neon framework. Development of HPC software is difficult,\\nlabor-intensive work, requiring a unique skill set. dMath allows a wide range\\nof developers to utilize parallel and distributed hardware easily. One\\ncontribution of this approach is that data is stored persistently on the GPU\\nhardware, avoiding costly transfers between host and device. Advanced memory\\nmanagement techniques are utilized, including caching of transferred data and\\nmemory reuse through pooling. A key contribution of dMath is that it delivers\\nperformance, portability, and productivity to its specific domain of support.\\nIt enables algorithm and application programmers to quickly solve problems\\nwithout managing the significant complexity associated with multi-level\\nparallelism.\\n',\n",
       "  'title': u'dMath: A Scalable Linear Algebra and Math Library for Heterogeneous\\n  GP-GPU Architectures'},\n",
       " u'1604.02354': {'arxivid': u'1604.02354',\n",
       "  'authorsaffil': [[u'Dong Wang', None], [u'Xiaoyang Tan', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02354v1',\n",
       "  'published': u'2016-04-08T13:35:03Z',\n",
       "  'summary': u'  Learning a good distance metric in feature space potentially improves the\\nperformance of the KNN classifier and is useful in many real-world\\napplications. Many metric learning algorithms are however based on the point\\nestimation of a quadratic optimization problem, which is time-consuming,\\nsusceptible to overfitting, and lack a natural mechanism to reason with\\nparameter uncertainty, an important property useful especially when the\\ntraining set is small and/or noisy. To deal with these issues, we present a\\nnovel Bayesian metric learning method, called Bayesian NCA, based on the\\nwell-known Neighbourhood Component Analysis method, in which the metric\\nposterior is characterized by the local label consistency constraints of\\nobservations, encoded with a similarity graph instead of independent pairwise\\nconstraints. For efficient Bayesian optimization, we explore the variational\\nlower bound over the log-likelihood of the original NCA objective. Experiments\\non several publicly available datasets demonstrate that the proposed method is\\nable to learn robust metric measures from small size dataset and/or from\\nchallenging training set with labels contaminated by errors. The proposed\\nmethod is also shown to outperform a previous pairwise constrained Bayesian\\nmetric learning method.\\n',\n",
       "  'title': u'Bayesian Neighbourhood Component Analysis'},\n",
       " u'1602.00763': {'arxivid': u'1602.00763',\n",
       "  'authorsaffil': [[u'Alex Bewley', None],\n",
       "   [u'Zongyuan Ge', None],\n",
       "   [u'Lionel Ott', None],\n",
       "   [u'Fabio Ramos', None],\n",
       "   [u'Ben Upcroft', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'5 pages, 1 figure, short paper, formatted for the international\\n  conference on image processing ICIP',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00763v1',\n",
       "  'published': u'2016-02-02T01:39:28Z',\n",
       "  'summary': u'  This paper explores a pragmatic approach to multiple object tracking where\\nthe main focus is to associate objects efficiently for online and realtime\\napplications. To this end, detection quality is identified as a key factor\\ninfluencing tracking performance, where changing the detector can improve\\ntracking by up to 18.9%. Despite only using a rudimentary combination of\\nfamiliar techniques such as the Kalman Filter and Hungarian algorithm for the\\ntracking components, this approach achieves an accuracy comparable to\\nstate-of-the-art online trackers. Furthermore, due to the simplicity of our\\ntracking method, the tracker updates at a rate of 260 Hz which is over 20x\\nfaster than other state-of-the-art trackers.\\n',\n",
       "  'title': u'Simple Online and Realtime Tracking'},\n",
       " u'1604.03584': {'arxivid': u'1604.03584',\n",
       "  'authorsaffil': [[u'Zhouyuan Huo', None], [u'Heng Huang', None]],\n",
       "  'categoryterms': [u'cs.LG', u'math.OC'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03584v3',\n",
       "  'published': u'2016-04-12T21:02:38Z',\n",
       "  'summary': u'  We provide the first theoretical analysis on the convergence rate of the\\nasynchronous stochastic variance reduced gradient (SVRG) descent algorithm on\\nnon-convex optimization. Recent studies have shown that the asynchronous\\nstochastic gradient descent (SGD) based algorithms with variance reduction\\nconverge with a linear convergent rate on convex problems. However, there is no\\nwork to analyze asynchronous SGD with variance reduction technique on\\nnon-convex problem. In this paper, we study two asynchronous parallel\\nimplementations of SVRG: one is on a distributed memory system and the other is\\non a shared memory system. We provide the theoretical analysis that both\\nalgorithms can obtain a convergence rate of $O(1/T)$, and linear speed up is\\nachievable if the number of workers is upper bounded.\\n',\n",
       "  'title': u'Asynchronous Stochastic Gradient Descent with Variance Reduction for\\n  Non-Convex Optimization'},\n",
       " u'1604.01093': {'arxivid': u'1604.01093',\n",
       "  'authorsaffil': [[u'Angela Dai', None],\n",
       "   [u'Matthias Nie\\xdfner', None],\n",
       "   [u'Michael Zollh\\xf6fer', None],\n",
       "   [u'Shahram Izadi', None],\n",
       "   [u'Christian Theobalt', None]],\n",
       "  'categoryterms': [u'cs.GR', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01093v1',\n",
       "  'published': u'2016-04-05T00:06:39Z',\n",
       "  'summary': u'  Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed\\nreality and robotic applications. However, scalability brings challenges of\\ndrift in pose estimation, introducing significant errors in the accumulated\\nmodel. Approaches often require hours of offline processing to globally correct\\nmodel errors. Recent online methods demonstrate compelling results, but suffer\\nfrom: (1) needing minutes to perform online correction preventing true\\nreal-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation\\nresulting in many tracking failures; or (3) supporting only unstructured\\npoint-based representations, which limit scan quality and applicability. We\\nsystematically address these issues with a novel, real-time, end-to-end\\nreconstruction framework. At its core is a robust pose estimation strategy,\\noptimizing per frame for a global set of camera poses by considering the\\ncomplete history of RGB-D input with an efficient hierarchical approach. We\\nremove the heavy reliance on temporal tracking, and continually localize to the\\nglobally optimized frames instead. We contribute a parallelizable optimization\\nframework, which employs correspondences based on sparse features and dense\\ngeometric and photometric matching. Our approach estimates globally optimized\\n(i.e., bundle adjusted) poses in real-time, supports robust tracking with\\nrecovery from gross tracking failures (i.e., relocalization), and re-estimates\\nthe 3D model in real-time to ensure global consistency; all within a single\\nframework. Our approach outperforms state-of-the-art online systems with\\nquality on par to offline methods, but with unprecedented speed and scan\\ncompleteness. Our framework leads to a comprehensive online scanning solution\\nfor large indoor environments, enabling ease of use and high-quality results.\\n',\n",
       "  'title': u'BundleFusion: Real-time Globally Consistent 3D Reconstruction using\\n  On-the-fly Surface Re-integration'},\n",
       " u'1510.07712': {'arxivid': u'1510.07712',\n",
       "  'authorsaffil': [[u'Haonan Yu', None],\n",
       "   [u'Jiang Wang', None],\n",
       "   [u'Zhiheng Huang', None],\n",
       "   [u'Yi Yang', None],\n",
       "   [u'Wei Xu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'In CVPR2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.07712v2',\n",
       "  'published': u'2015-10-26T22:47:00Z',\n",
       "  'summary': u'  We present an approach that exploits hierarchical Recurrent Neural Networks\\n(RNNs) to tackle the video captioning problem, i.e., generating one or multiple\\nsentences to describe a realistic video. Our hierarchical framework contains a\\nsentence generator and a paragraph generator. The sentence generator produces\\none simple short sentence that describes a specific short video interval. It\\nexploits both temporal- and spatial-attention mechanisms to selectively focus\\non visual elements during generation. The paragraph generator captures the\\ninter-sentence dependency by taking as input the sentential embedding produced\\nby the sentence generator, combining it with the paragraph history, and\\noutputting the new initial state for the sentence generator. We evaluate our\\napproach on two large-scale benchmark datasets: YouTubeClips and\\nTACoS-MultiLevel. The experiments demonstrate that our approach significantly\\noutperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and\\n0.305 respectively.\\n',\n",
       "  'title': u'Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks'},\n",
       " u'1604.01335': {'arxivid': u'1604.01335',\n",
       "  'authorsaffil': [[u'Brendan Jou', None], [u'Shih-Fu Chang', None]],\n",
       "  'categoryterms': [u'cs.CV',\n",
       "   u'cs.AI',\n",
       "   u'cs.MM',\n",
       "   u'I.2.6; I.5.1; I.5.4; H.5.1'],\n",
       "  'comment': u'9 pages, 6 figures, ACM Multimedia',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01335v1',\n",
       "  'published': u'2016-04-05T17:08:14Z',\n",
       "  'summary': u'  Residual learning has recently surfaced as an effective means of constructing\\nvery deep neural networks for object recognition. However, current incarnations\\nof residual networks do not allow for the modeling and integration of complex\\nrelations between closely coupled recognition tasks or across domains. Such\\nproblems are often encountered in multimedia and vision applications involving\\nlarge-scale content recognition. We propose a novel extension of residual\\nlearning for deep networks that enables intuitive learning across multiple\\nrelated tasks using cross-connections called cross-residuals. These\\ncross-residuals connections can be viewed as a form of in-network\\nregularization and enables greater network generalization. We show how\\ncross-residual learning (CRL) can be integrated in multitask networks to\\njointly train and detect visual concepts across several tasks. We present a\\nsingle multitask cross-residual network with >40% less parameters that is able\\nto achieve competitive, or even better, detection performance on a visual\\nsentiment concept detection problem normally requiring multiple specialized\\nsingle-task networks. The resulting multitask cross-residual network also\\nachieves better detection performance by about 10.4% over a standard multitask\\nresidual network without cross-residuals with even a small amount of cross-task\\nweighting.\\n',\n",
       "  'title': u'Deep Cross Residual Learning for Multitask Visual Recognition'},\n",
       " u'1603.09064': {'arxivid': u'1603.09064',\n",
       "  'authorsaffil': [[u'Edith Cohen', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'21 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09064v2',\n",
       "  'published': u'2016-03-30T07:51:58Z',\n",
       "  'summary': u'  Semi-supervised learning algorithms are an indispensable tool when labeled\\nexamples are scarce and there are many unlabeled examples [Blum and Chawla\\n2001, Zhu et. al. 2003]. With graph-based methods, entities (examples)\\ncorrespond to nodes in a graph and edges correspond to related entities. The\\ngraph structure is used to infer implicit pairwise affinity values (kernel)\\nwhich are used to compute the learned labels.\\n  Two powerful techniques to define such a kernel are \"symmetric\" spectral\\nmethods and Personalized Page Rank (PPR). With spectral methods, labels can be\\nscalably learned using Jacobi iterations, but an inherent limiting issue is\\nthat they are applicable to {\\\\em symmetric} (undirected) graphs, whereas often,\\nsuch as with like, follow, or hyperlinks, relations between entities are\\ninherently asymmetric. PPR naturally works with directed graphs but even with\\nstate of the art techniques does not scale when we want to learn billions of\\nlabels.\\n  Aiming at both high scalability and handling of directed relations, we\\npropose here {\\\\em Reach Diffusion} and {\\\\em Distance Diffusion} kernels. Our\\ndesign is inspired by models for influence diffusion in social networks,\\nformalized and spawned from the seminal work of [Kempe, Kleinberg, and Tardos\\n2003]. We tailor these models to define a natural asymmetric \"kernel\" and\\ndesign highly scalable algorithms for parameter setting and label learning.\\n',\n",
       "  'title': u'Semi-Supervised Learning on Graphs through Reach and Distance Diffusion'},\n",
       " u'1603.08458': {'arxivid': u'1603.08458',\n",
       "  'authorsaffil': [[u'Shaodian Zhang', None],\n",
       "   [u'Edouard Grave', None],\n",
       "   [u'Elizabeth Sklar', None],\n",
       "   [u'Noemie Elhadad', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.CY', u'cs.SI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08458v3',\n",
       "  'published': u'2016-03-28T17:47:42Z',\n",
       "  'summary': u\"  Identifying topics of discussions in online health communities (OHC) is\\ncritical to various applications, but can be difficult because topics of OHC\\ncontent are usually heterogeneous and domain-dependent. In this paper, we\\nprovide a multi-class schema, an annotated dataset, and supervised classifiers\\nbased on convolutional neural network (CNN) and other models for the task of\\nclassifying discussion topics. We apply the CNN classifier to the most popular\\nbreast cancer online community, and carry out a longitudinal analysis to show\\ntopic distributions and topic changes throughout members' participation. Our\\nexperimental results suggest that CNN outperforms other classifiers in the task\\nof topic classification, and that certain trajectories can be detected with\\nrespect to topic changes.\\n\",\n",
       "  'title': u'Longitudinal Analysis of Discussion Topics in an Online Breast Cancer\\n  Community using Convolutional Neural Networks'},\n",
       " u'1603.04046': {'arxivid': u'1603.04046',\n",
       "  'authorsaffil': [[u'Mina Masoudifar', None], [u'Hamid Reza Pourreza', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'18 pages, 14 figures, submitted',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04046v1',\n",
       "  'published': u'2016-03-13T16:29:50Z',\n",
       "  'summary': u'  Depth from defocus and defocus deblurring from a single image are two\\nchallenging problems that are derived from the finite depth of field in\\nconventional cameras. Coded aperture imaging is one of the techniques that is\\nused for improving the results of these two problems. Up to now, different\\nmethods have been proposed for improving the results of either defocus\\ndeblurring or depth estimation. In this paper, a multi-objective function is\\nproposed for evaluating and designing aperture patterns with the aim of\\nimproving the results of both depth from defocus and defocus deblurring.\\nPattern evaluation is performed by considering the scene illumination condition\\nand camera system specification. Based on the proposed criteria, a single\\nasymmetric pattern is designed that is used for restoring a sharp image and a\\ndepth map from a single input. Since the designed pattern is asymmetric,\\ndefocus objects on the two sides of the focal plane can be distinguished. Depth\\nestimation is performed by using a new algorithm, which is based on image\\nquality assessment criteria and can distinguish between blurred objects lying\\nin front or behind the focal plane. Extensive simulations as well as\\nexperiments on a variety of real scenes are conducted to compare our aperture\\nwith previously proposed ones.\\n',\n",
       "  'title': u'Image and Depth from a Single Defocused Image Using Coded Aperture\\n  Photography'},\n",
       " u'1603.04042': {'arxivid': u'1603.04042',\n",
       "  'authorsaffil': [[u'Ning Xu', None],\n",
       "   [u'Brian Price', None],\n",
       "   [u'Scott Cohen', None],\n",
       "   [u'Jimei Yang', None],\n",
       "   [u'Thomas Huang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Computer Vision and Pattern Recognition',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04042v1',\n",
       "  'published': u'2016-03-13T15:42:34Z',\n",
       "  'summary': u'  Interactive object selection is a very important research problem and has\\nmany applications. Previous algorithms require substantial user interactions to\\nestimate the foreground and background distributions. In this paper, we present\\na novel deep learning based algorithm which has a much better understanding of\\nobjectness and thus can reduce user interactions to just a few clicks. Our\\nalgorithm transforms user provided positive and negative clicks into two\\nEuclidean distance maps which are then concatenated with the RGB channels of\\nimages to compose (image, user interactions) pairs. We generate many of such\\npairs by combining several random sampling strategies to model user click\\npatterns and use them to fine tune deep Fully Convolutional Networks (FCNs).\\nFinally the output probability maps of our FCN 8s model is integrated with\\ngraph cut optimization to refine the boundary segments. Our model is trained on\\nthe PASCAL segmentation dataset and evaluated on other datasets with different\\nobject classes. Experimental results on both seen and unseen objects clearly\\ndemonstrate that our algorithm has a good generalization ability and is\\nsuperior to all existing interactive object selection approaches.\\n',\n",
       "  'title': u'Deep Interactive Object Selection'},\n",
       " u'1603.08240': {'arxivid': u'1603.08240',\n",
       "  'authorsaffil': [[u'Stamatios Georgoulis', None],\n",
       "   [u'Konstantinos Rematas', None],\n",
       "   [u'Tobias Ritschel', None],\n",
       "   [u'Mario Fritz', None],\n",
       "   [u'Luc Van Gool', None],\n",
       "   [u'Tinne Tuytelaars', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Stamatios Georgoulis and Konstantinos Rematas contributed equally to\\n  this work',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08240v1',\n",
       "  'published': u'2016-03-27T18:03:28Z',\n",
       "  'summary': u'  In this paper we are extracting surface reflectance and natural environmental\\nillumination from a reflectance map, i.e. from a single 2D image of a sphere of\\none material under one illumination. This is a notoriously difficult problem,\\nyet key to various re-rendering applications. With the recent advances in\\nestimating reflectance maps from 2D images their further decomposition has\\nbecome increasingly relevant.\\n  To this end, we propose a Convolutional Neural Network (CNN) architecture to\\nreconstruct both material parameters (i.e. Phong) as well as illumination (i.e.\\nhigh-resolution spherical illumination maps), that is solely trained on\\nsynthetic data. We demonstrate that decomposition of synthetic as well as real\\nphotographs of reflectance maps, both in High Dynamic Range (HDR), and, for the\\nfirst time, on Low Dynamic Range (LDR) as well. Results are compared to\\nprevious approaches quantitatively as well as qualitatively in terms of\\nre-renderings where illumination, material, view or shape are changed.\\n',\n",
       "  'title': u'DeLight-Net: Decomposing Reflectance Maps into Specular Materials and\\n  Natural Illumination'},\n",
       " u'1604.02313': {'arxivid': u'1604.02313',\n",
       "  'authorsaffil': [[u'Artem Chernodub', None], [u'Dimitri Nowicki', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u\"Submitted to conference ICANN'2016\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02313v1',\n",
       "  'published': u'2016-04-08T11:39:31Z',\n",
       "  'summary': u'  We propose a novel activation function that implements piece-wise orthogonal\\nnon-linear mappings based on permutations. It is straightforward to implement,\\nand very computationally efficient, also it has little memory requirements. We\\ntested it on two toy problems for feedforward and recurrent networks, it shows\\nsimilar performance to tanh and ReLU. OPLU activation function ensures norm\\npreservance of the backpropagated gradients, therefore it is potentially good\\nfor the training of deep, extra deep, and recurrent neural networks.\\n',\n",
       "  'title': u'Norm-preserving Orthogonal Permutation Linear Unit Activation Functions\\n  (OPLU)'},\n",
       " u'1604.00546': {'arxivid': u'1604.00546',\n",
       "  'authorsaffil': [[u'Farida Memon', None],\n",
       "   [u'Mukhtiar Ali Unar', None],\n",
       "   [u'Sheeraz Memon', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'8 pages, Mehran University Research Journal of Engineering and\\n  Technology, Vol. 34, No. 4, 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00546v1',\n",
       "  'published': u'2016-04-02T19:28:01Z',\n",
       "  'summary': u'  This paper presents the performance evaluation of eight focus measure\\noperators namely Image CURV (Curvature), GRAE (Gradient Energy), HISE\\n(Histogram Entropy), LAPM (Modified Laplacian), LAPV (Variance of Laplacian),\\nLAPD (Diagonal Laplacian), LAP3 (Laplacian in 3D Window) and WAVS (Sum of\\nWavelet Coefficients). Statistical matrics such as MSE (Mean Squared Error),\\nPNSR (Peak Signal to Noise Ratio), SC (Structural Content), NCC (Normalized\\nCross Correlation), MD (Maximum Difference) and NAE (Normalized Absolute Error)\\nare used to evaluate stated focus measures in this research. . FR (Full\\nReference) method of the image quality assessment is utilized in this paper.\\nResults indicate that LAPD method is comparatively better than other seven\\nfocus operators at typical imaging conditions.\\n',\n",
       "  'title': u'Image Quality Assessment for Performance Evaluation of Focus Measure\\n  Operators'},\n",
       " u'1511.03240': {'arxivid': u'1511.03240',\n",
       "  'authorsaffil': [[u'Jun Xie', None],\n",
       "   [u'Martin Kiefel', None],\n",
       "   [u'Ming-Ting Sun', None],\n",
       "   [u'Andreas Geiger', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'10 pages in Conference on Computer Vision and Pattern Recognition\\n  (CVPR), 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03240v2',\n",
       "  'published': u'2015-11-10T19:56:01Z',\n",
       "  'summary': u'  Semantic annotations are vital for training models for object recognition,\\nsemantic segmentation or scene understanding. Unfortunately, pixelwise\\nannotation of images at very large scale is labor-intensive and only little\\nlabeled data is available, particularly at instance level and for street\\nscenes. In this paper, we propose to tackle this problem by lifting the\\nsemantic instance labeling task from 2D into 3D. Given reconstructions from\\nstereo or laser data, we annotate static 3D scene elements with rough bounding\\nprimitives and develop a model which transfers this information into the image\\ndomain. We leverage our method to obtain 2D labels for a novel suburban video\\ndataset which we have collected, resulting in 400k semantic and instance image\\nannotations. A comparison of our method to state-of-the-art label transfer\\nbaselines reveals that 3D information enables more efficient annotation while\\nat the same time resulting in improved accuracy and time-coherent labels.\\n',\n",
       "  'title': u'Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer'},\n",
       " u'1506.07990': {'arxivid': u'1506.07990',\n",
       "  'authorsaffil': [[u'Mikkel Birkegaard Andersen', None],\n",
       "   [u'Thomas Bolander', None],\n",
       "   [u'Hans van Ditmarsch', None],\n",
       "   [u'Martin Holm Jensen', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LO'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.07990v2',\n",
       "  'published': u'2015-06-26T08:17:32Z',\n",
       "  'summary': u'  Plausibility models are Kripke models that agents use to reason about\\nknowledge and belief, both of themselves and of each other. Such models are\\nused to interpret the notions of conditional belief, degrees of belief, and\\nsafe belief. The logic of conditional belief contains that modality and also\\nthe knowledge modality, and similarly for the logic of degrees of belief and\\nthe logic of safe belief. With respect to these logics, plausibility models may\\ncontain too much information. A proper notion of bisimulation is required that\\ncharacterises them. We define that notion of bisimulation and prove the\\nrequired characterisations: on the class of image-finite and preimage-finite\\nmodels (with respect to the plausibility relation), two pointed Kripke models\\nare modally equivalent in either of the three logics, if and only if they are\\nbisimilar. As a result, the information content of such a model can be\\nsimilarly expressed in the logic of conditional belief, or the logic of degrees\\nof belief, or that of safe belief. This, we found a surprising result. Still,\\nthat does not mean that the logics are equally expressive: the logics of\\nconditional and degrees of belief are incomparable, the logics of degrees of\\nbelief and safe belief are incomparable, while the logic of safe belief is more\\nexpressive than the logic of conditional belief. In view of the result on\\nbisimulation characterisation, this is an equally surprising result. We hope\\nour insights may contribute to the growing community of formal epistemology and\\non the relation between qualitative and quantitative modelling.\\n',\n",
       "  'title': u'Bisimulation and expressivity for conditional belief, degrees of belief,\\n  and safe belief'},\n",
       " u'1511.03719': {'arxivid': u'1511.03719',\n",
       "  'authorsaffil': [[u'Xiang Zhang', None], [u'Yann LeCun', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03719v6',\n",
       "  'published': u'2015-11-11T22:46:46Z',\n",
       "  'summary': u'  This paper shows that simply prescribing \"none of the above\" labels to\\nunlabeled data has a beneficial regularization effect to supervised learning.\\nWe call it universum prescription by the fact that the prescribed labels cannot\\nbe one of the supervised labels. In spite of its simplicity, universum\\nprescription obtained competitive results in training deep convolutional\\nnetworks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative\\njustification of these approaches using Rademacher complexity is presented. The\\neffect of a regularization parameter -- probability of sampling from unlabeled\\ndata -- is also studied empirically.\\n',\n",
       "  'title': u'Universum Prescription: Regularization using Unlabeled Data'},\n",
       " u'1603.00437': {'arxivid': u'1603.00437',\n",
       "  'authorsaffil': [[u'Tales Imbiriba',\n",
       "    u'Federal University of Santa Catarina, Florian\\xf3polis, SC, Brazil'],\n",
       "   [u'Jos\\xe9 Carlos Moreira Bermudez',\n",
       "    u'Federal University of Santa Catarina, Florian\\xf3polis, SC, Brazil'],\n",
       "   [u'C\\xe9dric Richard',\n",
       "    u'Universit\\xe9 de Nice Sophia-Antipolis, CNRS, Nice, France']],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00437v1',\n",
       "  'published': u'2016-03-01T20:08:51Z',\n",
       "  'summary': u'  Kernel-based nonlinear mixing models have been applied to unmix spectral\\ninformation of hyperspectral images when the type of mixing occurring in the\\nscene is too complex or unknown. Such methods, however, usually require the\\ninversion of matrices of sizes equal to the number of spectral bands. Reducing\\nthe computational load of these methods remains a challenge in large scale\\napplications. This paper proposes a centralized method for band selection (BS)\\nin the reproducing kernel Hilbert space (RKHS). It is based upon the coherence\\ncriterion, which sets the largest value allowed for correlations between the\\nbasis kernel functions characterizing the unmixing model. We show that the\\nproposed BS approach is equivalent to solving a maximum clique problem (MCP),\\nthat is, searching for the biggest complete subgraph in a graph. Furthermore,\\nwe devise a strategy for selecting the coherence threshold and the Gaussian\\nkernel bandwidth using coherence bounds for linearly independent bases.\\nSimulation results illustrate the efficiency of the proposed method.\\n',\n",
       "  'title': u'Technical Report: Band selection for nonlinear unmixing of hyperspectral\\n  images as a maximal click problem'},\n",
       " u'1406.7639': {'arxivid': u'1406.7639',\n",
       "  'authorsaffil': [[u'Jakub Marecek', None],\n",
       "   [u'Robert Shorten', None],\n",
       "   [u'Jia Yuan Yu', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.MA', u'math.DS'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1080/00207179.2015.1033758',\n",
       "  'journalref': u'International Journal of Control 88(10): 2086-2096, 2015',\n",
       "  'link': u'http://arxiv.org/abs/1406.7639v3',\n",
       "  'published': u'2014-06-30T09:15:59Z',\n",
       "  'summary': u'  We aim to reduce the social cost of congestion in many smart city\\napplications. In our model of congestion, agents interact over limited\\nresources after receiving signals from a central agent that observes the state\\nof congestion in real time. Under natural models of agent populations, we\\ndevelop new signalling schemes and show that by introducing a non-trivial\\namount of uncertainty in the signals, we reduce the social cost of congestion,\\ni.e., improve social welfare. The signalling schemes are efficient in terms of\\nboth communication and computation, and are consistent with past observations\\nof the congestion. Moreover, the resulting population dynamics converge under\\nreasonable assumptions.\\n',\n",
       "  'title': u'Signalling and obfuscation for congestion control'},\n",
       " u'1602.07726': {'arxivid': u'1602.07726',\n",
       "  'authorsaffil': [[u'Rachel Cummings', None],\n",
       "   [u'Katrina Ligett', None],\n",
       "   [u'Kobbi Nissim', None],\n",
       "   [u'Aaron Roth', None],\n",
       "   [u'Zhiwei Steven Wu', None]],\n",
       "  'categoryterms': [u'cs.DS', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07726v2',\n",
       "  'published': u'2016-02-24T21:59:30Z',\n",
       "  'summary': u'  The traditional notion of generalization---i.e., learning a hypothesis whose\\nempirical error is close to its true error---is surprisingly brittle. As has\\nrecently been noted in [DFH+15b], even if several algorithms have this\\nguarantee in isolation, the guarantee need not hold if the algorithms are\\ncomposed adaptively. In this paper, we study three notions of\\ngeneralization---increasing in strength---that are robust to postprocessing and\\namenable to adaptive composition, and examine the relationships between them.\\nWe call the weakest such notion Robust Generalization. A second, intermediate,\\nnotion is the stability guarantee known as differential privacy. The strongest\\nguarantee we consider we call Perfect Generalization. We prove that every\\nhypothesis class that is PAC learnable is also PAC learnable in a robustly\\ngeneralizing fashion, with almost the same sample complexity. It was previously\\nknown that differentially private algorithms satisfy robust generalization. In\\nthis paper, we show that robust generalization is a strictly weaker concept,\\nand that there is a learning task that can be carried out subject to robust\\ngeneralization guarantees, yet cannot be carried out subject to differential\\nprivacy. We also show that perfect generalization is a strictly stronger\\nguarantee than differential privacy, but that, nevertheless, many learning\\ntasks can be carried out subject to the guarantees of perfect generalization.\\n',\n",
       "  'title': u'Adaptive Learning with Robust Generalization Guarantees'},\n",
       " u'1501.00630': {'arxivid': u'1501.00630',\n",
       "  'authorsaffil': [[u'Yuehaw Khoo', None], [u'Ankur Kapoor', None]],\n",
       "  'categoryterms': [u'cs.CV', u'math.OC', u'90C22, 92C55', u'G.1.6; I.4.9'],\n",
       "  'comment': u'15 pages, 7 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1501.00630v3',\n",
       "  'published': u'2015-01-04T04:01:25Z',\n",
       "  'summary': u'  We describe a convex programming framework for pose estimation in 2D/3D\\npoint-set registration with unknown point correspondences. We give two\\nmixed-integer nonlinear program (MINP) formulations of the 2D/3D registration\\nproblem when there are multiple 2D images, and propose convex relaxations for\\nboth of the MINPs to semidefinite programs (SDP) that can be solved efficiently\\nby interior point methods. Our approach to the 2D/3D registration problem is\\nnon-iterative in nature as we jointly solve for pose and correspondence.\\nFurthermore, these convex programs can readily incorporate feature descriptors\\nof points to enhance registration results. We prove that the convex programs\\nexactly recover the solution to the original nonconvex 2D/3D registration\\nproblem under noiseless condition. We apply these formulations to the\\nregistration of 3D models of coronary vessels to their 2D projections obtained\\nfrom multiple intra-operative fluoroscopic images. For this application, we\\nexperimentally corroborate the exact recovery property in the absence of noise\\nand further demonstrate robustness of the convex programs in the presence of\\nnoise.\\n',\n",
       "  'title': u'Non-iterative rigid 2D/3D point-set registration using semidefinite\\n  programming'},\n",
       " u'1604.00906': {'arxivid': u'1604.00906',\n",
       "  'authorsaffil': [[u'Yu-Chuan Su', None], [u'Kristen Grauman', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00906v1',\n",
       "  'published': u'2016-04-04T15:21:16Z',\n",
       "  'summary': u'  In a wearable camera video, we see what the camera wearer sees. While this\\nmakes it easy to know roughly what he chose to look at, it does not immediately\\nreveal when he was engaged with the environment. Specifically, at what moments\\ndid his focus linger, as he paused to gather more information about something\\nhe saw? Knowing this answer would benefit various applications in video\\nsummarization and augmented reality, yet prior work focuses solely on the\\n\"what\" question (estimating saliency, gaze) without considering the \"when\"\\n(engagement). We propose a learning-based approach that uses long-term\\negomotion cues to detect engagement, specifically in browsing scenarios where\\none frequently takes in new visual information (e.g., shopping, touring). We\\nintroduce a large, richly annotated dataset for ego-engagement that is the\\nfirst of its kind. Our approach outperforms a wide array of existing methods.\\nWe show engagement can be detected well independent of both scene appearance\\nand the camera wearer\\'s identity.\\n',\n",
       "  'title': u'Detecting Engagement in Egocentric Video'},\n",
       " u'1602.08159': {'arxivid': u'1602.08159',\n",
       "  'authorsaffil': [[u'Keisuke Fujii', None], [u'Kohei Nakajima', None]],\n",
       "  'categoryterms': [u'quant-ph', u'cs.AI', u'cs.LG', u'cs.NE', u'nlin.CD'],\n",
       "  'comment': u'19 pages, 13 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08159v1',\n",
       "  'published': u'2016-02-26T00:57:59Z',\n",
       "  'summary': u'  Quantum computer has an amazing potential of fast information processing.\\nHowever, realisation of a digital quantum computer is still a challenging\\nproblem requiring highly accurate controls and key application strategies. Here\\nwe propose a novel platform, quantum reservoir computing, to solve these issues\\nsuccessfully by exploiting natural quantum dynamics, which is ubiquitous in\\nlaboratories nowadays, for machine learning. In this framework, nonlinear\\ndynamics including classical chaos can be universally emulated in quantum\\nsystems. A number of numerical experiments show that quantum systems consisting\\nof at most seven qubits possess computational capabilities comparable to\\nconventional recurrent neural networks of 500 nodes. This discovery opens up a\\nnew paradigm for information processing with artificial intelligence powered by\\nquantum physics.\\n',\n",
       "  'title': u'Harnessing disordered quantum dynamics for machine learning'},\n",
       " u'1602.08158': {'arxivid': u'1602.08158',\n",
       "  'authorsaffil': [[u'Gabriel J. Ferrer', None]],\n",
       "  'categoryterms': [u'cs.RO'],\n",
       "  'comment': u'Presented at \"2nd Workshop on Cognitive Architectures for Social\\n  Human-Robot Interaction 2016 (arXiv:1602.01868)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08158v1',\n",
       "  'published': u'2016-02-26T00:52:12Z',\n",
       "  'summary': u'  In this position paper, we discuss how the use of a cognitive architecture\\nbased on unsupervised clustering (the Kohonen Self-Organizing Map) enables us\\nto meet our goals of efficient action selection in a mobile robot. This\\narchitecture provides several opportunities for human-robot interaction, and we\\ndiscuss how its features facilitate these interactions.\\n',\n",
       "  'title': u'Associative Memories and Human-Robot Social Interaction'},\n",
       " u'1506.07285': {'arxivid': u'1506.07285',\n",
       "  'authorsaffil': [[u'Ankit Kumar', None],\n",
       "   [u'Ozan Irsoy', None],\n",
       "   [u'Peter Ondruska', None],\n",
       "   [u'Mohit Iyyer', None],\n",
       "   [u'James Bradbury', None],\n",
       "   [u'Ishaan Gulrajani', None],\n",
       "   [u'Victor Zhong', None],\n",
       "   [u'Romain Paulus', None],\n",
       "   [u'Richard Socher', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.07285v5',\n",
       "  'published': u'2015-06-24T08:27:02Z',\n",
       "  'summary': u\"  Most tasks in natural language processing can be cast into question answering\\n(QA) problems over language input. We introduce the dynamic memory network\\n(DMN), a neural network architecture which processes input sequences and\\nquestions, forms episodic memories, and generates relevant answers. Questions\\ntrigger an iterative attention process which allows the model to condition its\\nattention on the inputs and the result of previous iterations. These results\\nare then reasoned over in a hierarchical recurrent sequence model to generate\\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\\non several types of tasks and datasets: question answering (Facebook's bAbI\\ndataset), text classification for sentiment analysis (Stanford Sentiment\\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\\ntraining for these different tasks relies exclusively on trained word vector\\nrepresentations and input-question-answer triplets.\\n\",\n",
       "  'title': u'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'},\n",
       " u'1604.03635': {'arxivid': u'1604.03635',\n",
       "  'authorsaffil': [[u'Anton Milan', None],\n",
       "   [u'Seyed Hamid Rezatofighi', None],\n",
       "   [u'Anthony Dick', None],\n",
       "   [u'Konrad Schindler', None],\n",
       "   [u'Ian Reid', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03635v1',\n",
       "  'published': u'2016-04-13T02:41:43Z',\n",
       "  'summary': u'  We present a novel approach to online multi-target tracking based on\\nrecurrent neural networks (RNNs). Tracking multiple objects in real-world\\nscenes involves many challenges, including a) an a-priori unknown and\\ntime-varying number of targets, b) a continuous state estimation of all present\\ntargets, and c) a discrete combinatorial problem of data association. Most\\nprevious methods involve complex models that require tedious tuning of\\nparameters. Here, we propose for the first time, a full end-to-end learning\\napproach for online multi-target tracking based on deep learning. Existing deep\\nlearning methods are not designed for the above challenges and cannot be\\ntrivially applied to the task. Our solution addresses all of the above points\\nin a principled way. Experiments on both synthetic and real data show\\ncompetitive results obtained at 300 Hz on a standard CPU, and pave the way\\ntowards future research in this direction.\\n',\n",
       "  'title': u'Online Multi-target Tracking using Recurrent Neural Networks'},\n",
       " u'1602.00163': {'arxivid': u'1602.00163',\n",
       "  'authorsaffil': [[u'Manel Zoghlami', None],\n",
       "   [u'Sabeur Aridhi', None],\n",
       "   [u'Haitham Sghaier', None],\n",
       "   [u'Mondher Maddouri', None],\n",
       "   [u'Engelbert Mephu Nguifo', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Submitted to Data Mining and Knowledge Discovery Journal',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00163v1',\n",
       "  'published': u'2016-01-30T21:15:10Z',\n",
       "  'summary': u'  In Multiple Instance Learning (MIL) problem for sequence data, the learning\\ndata consist of a set of bags where each bag contains a set of\\ninstances/sequences. In many real world applications such as bioinformatics,\\nweb mining, and text mining, comparing a random couple of sequences makes no\\nsense. In fact, each instance of each bag may have structural and/or temporal\\nrelation with other instances in other bags. Thus, the classification task\\nshould take into account the relation between semantically related instances\\nacross bags. In this paper, we present two novel MIL approaches for sequence\\ndata classification: (1) ABClass and (2) ABSim. In ABClass, each sequence is\\nrepresented by one vector of attributes. For each sequence of the unknown bag,\\na discriminative classifier is applied in order to compute a partial\\nclassification result. Then, an aggregation method is applied to these partial\\nresults in order to generate the final result. In ABSim, we use a similarity\\nmeasure between each sequence of the unknown bag and the corresponding\\nsequences in the learning bags. An unknown bag is labeled with the bag that\\npresents more similar sequences. We applied both approaches to the problem of\\nbacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated and\\ndiscussed the proposed approaches on well known Ionizing Radiation Resistance\\nBacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented by\\nprimary structure of basal DNA repair proteins. The experimental results show\\nthat both ABClass and ABSim approaches are efficient.\\n',\n",
       "  'title': u'A multiple instance learning approach for sequence data with across bag\\n  dependencies'},\n",
       " u'1602.08151': {'arxivid': u'1602.08151',\n",
       "  'authorsaffil': [[u'Akshay Balsubramani', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08151v1',\n",
       "  'published': u'2016-02-25T23:46:57Z',\n",
       "  'summary': u'  We address how to learn a binary classifier capable of abstaining from making\\na label prediction. Such a classifier hopes to abstain where it would be most\\ninaccurate if forced to predict, so it has two goals in tension with each\\nother: minimizing errors, and avoiding abstaining unnecessarily often.\\n  In this work, we exactly characterize the best achievable tradeoff between\\nthese two goals in a general semi-supervised setting, given an ensemble of\\nclassifiers of varying competence as well as unlabeled data on which we wish to\\npredict or abstain. We give an algorithm for learning a classifier which trades\\noff its errors with abstentions in a minimax optimal manner. This algorithm is\\nas efficient as linear learning and prediction, and comes with strong and\\nrobust theoretical guarantees. Our analysis extends to a large class of loss\\nfunctions and other scenarios, including ensembles comprised of \"specialist\"\\nclassifiers that can themselves abstain.\\n',\n",
       "  'title': u'Learning to Abstain from Binary Prediction'},\n",
       " u'1602.00165': {'arxivid': u'1602.00165',\n",
       "  'authorsaffil': [[u'Amulya Yadav', None],\n",
       "   [u'Hau Chan', None],\n",
       "   [u'Albert Jiang', None],\n",
       "   [u'Haifeng Xu', None],\n",
       "   [u'Eric Rice', None],\n",
       "   [u'Milind Tambe', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CY', u'cs.SI'],\n",
       "  'comment': u'This is an extended version of our AAMAS 2016 paper (with the same\\n  name) with full proofs of all our theorems included',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00165v1',\n",
       "  'published': u'2016-01-30T21:59:27Z',\n",
       "  'summary': u\"  This paper presents HEALER, a software agent that recommends sequential\\nintervention plans for use by homeless shelters, who organize these\\ninterventions to raise awareness about HIV among homeless youth. HEALER's\\nsequential plans (built using knowledge of social networks of homeless youth)\\nchoose intervention participants strategically to maximize influence spread,\\nwhile reasoning about uncertainties in the network. While previous work\\npresents influence maximizing techniques to choose intervention participants,\\nthey do not address three real-world issues: (i) they completely fail to scale\\nup to real-world sizes; (ii) they do not handle deviations in execution of\\nintervention plans; (iii) constructing real-world social networks is an\\nexpensive process. HEALER handles these issues via four major contributions:\\n(i) HEALER casts this influence maximization problem as a POMDP and solves it\\nusing a novel planner which scales up to previously unsolvable real-world\\nsizes; (ii) HEALER allows shelter officials to modify its recommendations, and\\nupdates its future plans in a deviation-tolerant manner; (iii) HEALER\\nconstructs social networks of homeless youth at low cost, using a Facebook\\napplication. Finally, (iv) we show hardness results for the problem that HEALER\\nsolves. HEALER will be deployed in the real world in early Spring 2016 and is\\ncurrently undergoing testing at a homeless shelter.\\n\",\n",
       "  'title': u'Using Social Networks to Aid Homeless Shelters: Dynamic Influence\\n  Maximization under Uncertainty - An Extended Version'},\n",
       " u'1511.06051': {'arxivid': u'1511.06051',\n",
       "  'authorsaffil': [[u'Philipp Moritz', None],\n",
       "   [u'Robert Nishihara', None],\n",
       "   [u'Ion Stoica', None],\n",
       "   [u'Michael I. Jordan', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.DC', u'cs.LG', u'cs.NE', u'math.OC'],\n",
       "  'comment': u'12 pages, 7 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06051v4',\n",
       "  'published': u'2015-11-19T03:29:56Z',\n",
       "  'summary': u\"  Training deep networks is a time-consuming process, with networks for object\\nrecognition often requiring multiple days to train. For this reason, leveraging\\nthe resources of a cluster to speed up training is an important area of work.\\nHowever, widely-popular batch-processing computational frameworks like\\nMapReduce and Spark were not designed to support the asynchronous and\\ncommunication-intensive workloads of existing distributed deep learning\\nsystems. We introduce SparkNet, a framework for training deep networks in\\nSpark. Our implementation includes a convenient interface for reading data from\\nSpark RDDs, a Scala interface to the Caffe deep learning framework, and a\\nlightweight multi-dimensional tensor library. Using a simple parallelization\\nscheme for stochastic gradient descent, SparkNet scales well with the cluster\\nsize and tolerates very high-latency communication. Furthermore, it is easy to\\ndeploy and use with no parameter tuning, and it is compatible with existing\\nCaffe models. We quantify the dependence of the speedup obtained by SparkNet on\\nthe number of machines, the communication frequency, and the cluster's\\ncommunication overhead, and we benchmark our system's performance on the\\nImageNet dataset.\\n\",\n",
       "  'title': u'SparkNet: Training Deep Networks in Spark'},\n",
       " u'1601.04126': {'arxivid': u'1601.04126',\n",
       "  'authorsaffil': [[u'Kush R. Varshney', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.AI', u'cs.CY', u'cs.LG'],\n",
       "  'comment': u'2016 Information Theory and Applications Workshop, La Jolla,\\n  California',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04126v1',\n",
       "  'published': u'2016-01-16T05:46:57Z',\n",
       "  'summary': u'  Machine learning algorithms are increasingly influencing our decisions and\\ninteracting with us in all parts of our daily lives. Therefore, just like for\\npower plants, highways, and myriad other engineered sociotechnical systems, we\\nmust consider the safety of systems involving machine learning. In this paper,\\nwe first discuss the definition of safety in terms of risk, epistemic\\nuncertainty, and the harm incurred by unwanted outcomes. Then we examine\\ndimensions, such as the choice of cost function and the appropriateness of\\nminimizing the empirical average training cost, along which certain real-world\\napplications may not be completely amenable to the foundational principle of\\nmodern statistical machine learning: empirical risk minimization. In\\nparticular, we note an emerging dichotomy of applications: ones in which safety\\nis important and risk minimization is not the complete story (we name these\\nType A applications), and ones in which safety is not so critical and risk\\nminimization is sufficient (we name these Type B applications). Finally, we\\ndiscuss how four different strategies for achieving safety in engineering\\n(inherently safe design, safety reserves, safe fail, and procedural safeguards)\\ncan be mapped to the machine learning context through interpretability and\\ncausality of predictive models, objectives beyond expected prediction accuracy,\\nhuman involvement for labeling difficult or rare examples, and user experience\\ndesign of software.\\n',\n",
       "  'title': u'Engineering Safety in Machine Learning'},\n",
       " u'1511.08886': {'arxivid': u'1511.08886',\n",
       "  'authorsaffil': [[u'Roy Or - El', None],\n",
       "   [u'Rom Hershkovitz', None],\n",
       "   [u'Aaron Wetzler', None],\n",
       "   [u'Guy Rosman', None],\n",
       "   [u'Alfred M. Bruckstein', None],\n",
       "   [u'Ron Kimmel', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Camera-Ready version for CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.08886v2',\n",
       "  'published': u'2015-11-28T10:13:48Z',\n",
       "  'summary': u\"  The introduction of consumer RGB-D scanners set off a major boost in 3D\\ncomputer vision research. Yet, the precision of existing depth scanners is not\\naccurate enough to recover fine details of a scanned object. While modern\\nshading based depth refinement methods have been proven to work well with\\nLambertian objects, they break down in the presence of specularities. We\\npresent a novel shape from shading framework that addresses this issue and\\nenhances both diffuse and specular objects' depth profiles. We take advantage\\nof the built-in monochromatic IR projector and IR images of the RGB-D scanners\\nand present a lighting model that accounts for the specular regions in the\\ninput image. Using this model, we reconstruct the depth map in real-time. Both\\nquantitative tests and visual evaluations prove that the proposed method\\nproduces state of the art depth reconstruction results.\\n\",\n",
       "  'title': u'Real-Time Depth Refinement for Specular Objects'},\n",
       " u'1603.00124': {'arxivid': u'1603.00124',\n",
       "  'authorsaffil': [[u'Jiale Cao', None],\n",
       "   [u'Yanwei Pang', None],\n",
       "   [u'Xuelong Li', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00124v1',\n",
       "  'published': u'2016-03-01T03:25:45Z',\n",
       "  'summary': u\"  Pedestrian detection based on the combination of Convolutional Neural Network\\n(i.e., CNN) and traditional handcrafted features (i.e., HOG+LUV) has achieved\\ngreat success. Generally, HOG+LUV are used to generate the candidate proposals\\nand then CNN classifies these proposals. Despite its success, there is still\\nroom for improvement. For example, CNN classifies these proposals by the\\nfull-connected layer features while proposal scores and the features in the\\ninner-layers of CNN are ignored. In this paper, we propose a unifying framework\\ncalled Multilayer Channel Features (MCF) to overcome the drawback. It firstly\\nintegrates HOG+LUV with each layer of CNN into a multi-layer image channels.\\nBased on the multi-layer image channels, a multi-stage cascade AdaBoost is then\\nlearned. The weak classifiers in each stage of the multi-stage cascade is\\nlearned from the image channels of corresponding layer. With more abundant\\nfeatures, MCF achieves the state-of-the-art on Caltech pedestrian dataset\\n(i.e., 10.40% miss rate). Using new and accurate annotations, MCF achieves\\n7.98% miss rate. As many non-pedestrian detection windows can be quickly\\nrejected by the first few stages, it accelerates detection speed by 1.43 times.\\nBy eliminating the highly overlapped detection windows with lower scores after\\nthe first stage, it's 4.07 times faster with negligible performance loss.\\n\",\n",
       "  'title': u'Learning Multilayer Channel Features for Pedestrian Detection'},\n",
       " u'1602.00367': {'arxivid': u'1602.00367',\n",
       "  'authorsaffil': [[u'Yijun Xiao', None], [u'Kyunghyun Cho', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00367v1',\n",
       "  'published': u'2016-02-01T02:53:41Z',\n",
       "  'summary': u'  Document classification tasks were primarily tackled at word level. Recent\\nresearch that works with character-level inputs shows several benefits over\\nword-level approaches such as natural incorporation of morphemes and better\\nhandling of rare words. We propose a neural network architecture that utilizes\\nboth convolution and recurrent layers to efficiently encode character inputs.\\nWe validate the proposed model on eight large scale document classification\\ntasks and compare with character-level convolution-only models. It achieves\\ncomparable performances with much less parameters.\\n',\n",
       "  'title': u'Efficient Character-level Document Classification by Combining\\n  Convolution and Recurrent Layers'},\n",
       " u'1603.04283': {'arxivid': u'1603.04283',\n",
       "  'authorsaffil': [[u'Vladimir Vovk', None], [u'Dusko Pavlovic', None]],\n",
       "  'categoryterms': [u'cs.LG', u'68T05', u'I.2.6'],\n",
       "  'comment': u'8 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04283v1',\n",
       "  'published': u'2016-03-14T14:43:48Z',\n",
       "  'summary': u\"  We construct a universal prediction system in the spirit of Popper's\\nfalsifiability and Kolmogorov complexity. This prediction system does not\\ndepend on any statistical assumptions, but under the IID assumption it\\ndominates, although in a rather weak sense, conformal prediction.\\n\",\n",
       "  'title': u'Universal probability-free conformal prediction'},\n",
       " u'1602.03468': {'arxivid': u'1602.03468',\n",
       "  'authorsaffil': [[u'Abdolrahim Kadkhodamohammadi', None],\n",
       "   [u'Afshin Gangi', None],\n",
       "   [u'Michel de Mathelin', None],\n",
       "   [u'Nicolas Padoy', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'The supplementary video is available at https://youtu.be/iabbGSqRSgE',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03468v2',\n",
       "  'published': u'2016-02-10T17:56:47Z',\n",
       "  'summary': u'  Reliable human pose estimation (HPE) is essential to many clinical\\napplications, such as surgical workflow analysis, radiation safety monitoring\\nand human-robot cooperation. Proposed methods for the operating room (OR) rely\\neither on foreground estimation using a multi-camera system, which is a\\nchallenge in real ORs due to color similarities and frequent illumination\\nchanges, or on wearable sensors or markers, which are invasive and therefore\\ndifficult to introduce in the room. Instead, we propose a novel approach based\\non Pictorial Structures (PS) and on RGB-D data, which can be easily deployed in\\nreal ORs. We extend the PS framework in two ways. First, we build robust and\\ndiscriminative part detectors using both color and depth images. We also\\npresent a novel descriptor for depth images, called histogram of depth\\ndifferences (HDD). Second, we extend PS to 3D by proposing 3D pairwise\\nconstraints and a new method for exact and tractable inference. Our approach is\\nevaluated for pose estimation and clinician detection on a challenging RGB-D\\ndataset recorded in a busy operating room during live surgeries. We conduct\\nseries of experiments to study the different part detectors in conjunction with\\nthe various 2D or 3D pairwise constraints. Our comparisons demonstrate that 3D\\nPS with RGB-D part detectors significantly improves the results in a visually\\nchallenging operating environment.\\n',\n",
       "  'title': u'3D Pictorial Structures on RGB-D Data for Articulated Human Detection in\\n  Operating Rooms'},\n",
       " u'1511.02821': {'arxivid': u'1511.02821',\n",
       "  'authorsaffil': [[u'Chao Chen', None],\n",
       "   [u'Alina Zare', None],\n",
       "   [u'J. Tory Cobb', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CV'],\n",
       "  'comment': u'cut to 6 pages, add sunset results',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.02821v2',\n",
       "  'published': u'2015-11-09T20:04:56Z',\n",
       "  'summary': u'  Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting\\nimagery. These models are confined to crisp segmentation. Yet, there are many\\nimages in which some regions cannot be assigned a crisp label (e.g., transition\\nregions between a foggy sky and the ground or between sand and water at a\\nbeach). In these cases, a visual word is best represented with partial\\nmemberships across multiple topics. To address this, we present a partial\\nmembership latent Dirichlet allocation (PM-LDA) model and associated parameter\\nestimation algorithms. Experimental results on two natural image datasets and\\none SONAR image dataset show that PM-LDA can produce both crisp and soft\\nsemantic image segmentations; a capability existing methods do not have.\\n',\n",
       "  'title': u'Partial Membership Latent Dirichlet Allocation'},\n",
       " u'1509.02634': {'arxivid': u'1509.02634',\n",
       "  'authorsaffil': [[u'Ziwei Liu', None],\n",
       "   [u'Xiaoxiao Li', None],\n",
       "   [u'Ping Luo', None],\n",
       "   [u'Chen Change Loy', None],\n",
       "   [u'Xiaoou Tang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'To appear in International Conference on Computer Vision (ICCV) 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.02634v2',\n",
       "  'published': u'2015-09-09T04:39:34Z',\n",
       "  'summary': u'  This paper addresses semantic image segmentation by incorporating rich\\ninformation into Markov Random Field (MRF), including high-order relations and\\nmixture of label contexts. Unlike previous works that optimized MRFs using\\niterative algorithm, we solve MRF by proposing a Convolutional Neural Network\\n(CNN), namely Deep Parsing Network (DPN), which enables deterministic\\nend-to-end computation in a single forward pass. Specifically, DPN extends a\\ncontemporary CNN architecture to model unary terms and additional layers are\\ncarefully devised to approximate the mean field algorithm (MF) for pairwise\\nterms. It has several appealing properties. First, different from the recent\\nworks that combined CNN and MRF, where many iterations of MF were required for\\neach training image during back-propagation, DPN is able to achieve high\\nperformance by approximating one iteration of MF. Second, DPN represents\\nvarious types of pairwise terms, making many existing works as its special\\ncases. Third, DPN makes MF easier to be parallelized and speeded up in\\nGraphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC\\n2012 dataset, where a single DPN model yields a new state-of-the-art\\nsegmentation accuracy.\\n',\n",
       "  'title': u'Semantic Image Segmentation via Deep Parsing Network'},\n",
       " u'1506.03059': {'arxivid': u'1506.03059',\n",
       "  'authorsaffil': [[u'Nadav Cohen', None],\n",
       "   [u'Or Sharir', None],\n",
       "   [u'Amnon Shashua', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.03059v2',\n",
       "  'published': u'2015-06-09T19:40:05Z',\n",
       "  'summary': u'  We present a deep layered architecture that generalizes convolutional neural\\nnetworks (ConvNets). The architecture, called SimNets, is driven by two\\noperators: (i) a similarity function that generalizes inner-product, and (ii) a\\nlog-mean-exp function called MEX that generalizes maximum and average. The two\\noperators applied in succession give rise to a standard neuron but in \"feature\\nspace\". The feature spaces realized by SimNets depend on the choice of the\\nsimilarity operator. The simplest setting, which corresponds to a convolution,\\nrealizes the feature space of the Exponential kernel, while other settings\\nrealize feature spaces of more powerful kernels (Generalized Gaussian, which\\nincludes as special cases RBF and Laplacian), or even dynamically learned\\nfeature spaces (Generalized Multiple Kernel Learning). As a result, the SimNet\\ncontains a higher abstraction level compared to a traditional ConvNet. We argue\\nthat enhanced expressiveness is important when the networks are small due to\\nrun-time constraints (such as those imposed by mobile applications). Empirical\\nevaluation validates the superior expressiveness of SimNets, showing a\\nsignificant gain in accuracy over ConvNets when computational resources at\\nrun-time are limited. We also show that in large-scale settings, where\\ncomputational complexity is less of a concern, the additional capacity of\\nSimNets can be controlled with proper regularization, yielding accuracies\\ncomparable to state of the art ConvNets.\\n',\n",
       "  'title': u'Deep SimNets'},\n",
       " u'1601.05991': {'arxivid': u'1601.05991',\n",
       "  'authorsaffil': [[u'Milos Cernak', None],\n",
       "   [u'Stefan Benus', None],\n",
       "   [u'Alexandros Lazaridis', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.SD'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05991v2',\n",
       "  'published': u'2016-01-22T13:22:10Z',\n",
       "  'summary': u'  Using phonological speech vocoding, we propose a platform for exploring\\nrelations between phonology and speech processing, and in broader terms, for\\nexploring relations between the abstract and physical structures of a speech\\nsignal. Our goal is to make a step towards bridging phonology and speech\\nprocessing and to contribute to the program of Laboratory Phonology. We show\\nthree application examples for laboratory phonology: compositional phonological\\nspeech modelling, a comparison of phonological systems and an experimental\\nphonological parametric text-to-speech (TTS) system. The featural\\nrepresentations of the following three phonological systems are considered in\\nthis work: (i) Government Phonology (GP), (ii) the Sound Pattern of English\\n(SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded\\nspeech, we conclude that the latter achieves slightly better results than the\\nformer. However, GP - the most compact phonological speech representation -\\nperforms comparably to the systems with a higher number of phonological\\nfeatures. The parametric TTS based on phonological speech representation, and\\ntrained from an unlabelled audiobook in an unsupervised manner, achieves\\nintelligibility of 85% of the state-of-the-art parametric speech synthesis. We\\nenvision that the presented approach paves the way for researchers in both\\nfields to form meaningful hypotheses that are explicitly testable using the\\nconcepts developed and exemplified in this paper. On the one hand, laboratory\\nphonologists might test the applied concepts of their theoretical models, and\\non the other hand, the speech processing community may utilize the concepts\\ndeveloped for the theoretical phonological models for improvements of the\\ncurrent state-of-the-art applications.\\n',\n",
       "  'title': u'Speech vocoding for laboratory phonology'},\n",
       " u'1511.02825': {'arxivid': u'1511.02825',\n",
       "  'authorsaffil': [[u'Changzhe Jiao', None], [u'Alina Zare', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Cut down to 6 pages, Included Appendix to explain relationship\\n  between different discriminative terms',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.02825v2',\n",
       "  'published': u'2015-11-09T20:12:19Z',\n",
       "  'summary': u'  A multiple instance dictionary learning method using functions of multiple\\ninstances (DL-FUMI) is proposed to address target detection and two-class\\nclassification problems with inaccurate training labels. Given inaccurate\\ntraining labels, DL-FUMI learns a set of target dictionary atoms that describe\\nthe most distinctive and representative features of the true positive class as\\nwell as a set of nontarget dictionary atoms that account for the shared\\ninformation found in both the positive and negative instances. Experimental\\nresults show that the estimated target dictionary atoms found by DL-FUMI are\\nmore representative prototypes and identify better discriminative features of\\nthe true positive class than existing methods in the literature. DL-FUMI is\\nshown to have significantly better performance on several target detection and\\nclassification problems as compared to other multiple instance learning (MIL)\\ndictionary learning algorithms on a variety of MIL problems.\\n',\n",
       "  'title': u'Multiple Instance Dictionary Learning using Functions of Multiple\\n  Instances'},\n",
       " u'1507.04928': {'arxivid': u'1507.04928',\n",
       "  'authorsaffil': [[u'Kieran Greer', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.04928v2',\n",
       "  'published': u'2015-07-17T11:24:07Z',\n",
       "  'summary': u\"  This paper describes a detailed cognitive structure and related processes. It\\nalso suggests some specific methods for carrying out those processes, where the\\nmethods interact well with each other. The main purpose of this paper is to\\nreaffirm earlier research on different knowledge-based and experience-based\\nclustering techniques, by describing in more detail how they can work as part\\nof the same system. While an earlier paper defined three levels of functional\\nrequirement, this paper re-defines the levels in a more human vernacular. The\\nintended higher-level goals are made clearer, in terms of action-result pairs.\\nThe system is therefore modelled on the human brain, where pattern creation and\\nactivation would be automatic and distributed, and the information can flow\\nseamlessly between the main constructs. The overall architecture has stayed\\nessentially the same and so it is the localised processes or smaller details\\nthat have been updated. For example, a counting mechanism is used slightly\\ndifferently, to measure a level of 'cohesion' instead of a 'correct'\\nclassification, over pattern instances. The Concept Trees structure can perform\\nmore than one type of role and the introduction of features has enhanced the\\narchitecture again. There is also some new information and theory.\\n\",\n",
       "  'title': u'A Brain-like Cognitive Process with Shared Methods'},\n",
       " u'1602.01410': {'arxivid': u'1602.01410',\n",
       "  'authorsaffil': [[u'Miguel Sim\\xf5es', None],\n",
       "   [u'Luis B. Almeida', None],\n",
       "   [u'Jos\\xe9 Bioucas-Dias', None],\n",
       "   [u'Jocelyn Chanussot', None]],\n",
       "  'categoryterms': [u'cs.CV', u'stat.ML'],\n",
       "  'comment': u'13 pages, 12 figures. Submitted. MATLAB code to be made available\\n  soon',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.01410v1',\n",
       "  'published': u'2016-02-03T18:57:02Z',\n",
       "  'summary': u'  In image deconvolution problems, the diagonalization of the underlying\\noperators by means of the FFT usually yields very large speedups. When there\\nare incomplete observations (e.g., in the case of unknown boundaries), standard\\ndeconvolution techniques normally involve non-diagonalizable\\noperators---resulting in rather slow methods---or, otherwise, use inexact\\nconvolution models, resulting in the occurrence of artifacts in the enhanced\\nimages. In this paper, we propose a new deconvolution framework for images with\\nincomplete observations that allows us to work with diagonalized convolution\\noperators, and therefore is very fast. We iteratively alternate the estimation\\nof the unknown pixels and of the deconvolved image, using, e.g., a FFT-based\\ndeconvolution method. In principle, any fast deconvolution method can be used.\\nWe give an example in which a published method that assumes periodic boundary\\nconditions is extended, through the use of this framework, to unknown boundary\\nconditions. Furthermore, we propose an implementation of this framework, based\\non the alternating direction method of multipliers (ADMM). We provide a proof\\nof convergence for the resulting algorithm, which can be seen as a \"partial\"\\nADMM, in which not all variables are dualized. We report experimental\\ncomparisons with other primal-dual methods, in which the proposed one performed\\nat the level of the state of the art. Four different kinds of applications were\\ntested in the experiments: deconvolution, deconvolution with inpainting,\\nsuperresolution, and demosaicing, all with unknown boundaries.\\n',\n",
       "  'title': u'A General Framework for Fast Image Deconvolution with Incomplete\\n  Observations. Applications to Unknown Boundaries, Inpainting,\\n  Superresolution, and Demosaicing'},\n",
       " u'1506.02059': {'arxivid': u'1506.02059',\n",
       "  'authorsaffil': [[u'Haonan Yu', None], [u'Jeffrey Mark Siskind', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.02059v2',\n",
       "  'published': u'2015-06-05T20:34:12Z',\n",
       "  'summary': u'  We tackle the problem of video object codetection by leveraging the weak\\nsemantic constraint implied by sentences that describe the video content.\\nUnlike most existing work that focuses on codetecting large objects which are\\nusually salient both in size and appearance, we can codetect objects that are\\nsmall or medium sized. Our method assumes no human pose or depth information\\nsuch as is required by the most recent state-of-the-art method. We employ weak\\nsemantic constraint on the codetection process by pairing the video with\\nsentences. Although the semantic information is usually simple and weak, it can\\ngreatly boost the performance of our codetection framework by reducing the\\nsearch space of the hypothesized object detections. Our experiment demonstrates\\nan average IoU score of 0.423 on a new challenging dataset which contains 15\\nobject classes and 150 videos with 12,509 frames in total, and an average IoU\\nscore of 0.373 on a subset of an existing dataset, originally intended for\\nactivity recognition, which contains 5 object classes and 75 videos with 8,854\\nframes in total.\\n',\n",
       "  'title': u'Sentence Directed Video Object Codetection'},\n",
       " u'1603.07195': {'arxivid': u'1603.07195',\n",
       "  'authorsaffil': [[u'Mark Eisen', None],\n",
       "   [u'Aryan Mokhtari', None],\n",
       "   [u'Alejandro Ribeiro', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.DC', u'cs.LG'],\n",
       "  'comment': u'8 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07195v1',\n",
       "  'published': u'2016-03-23T14:24:39Z',\n",
       "  'summary': u'  This paper considers consensus optimization problems where each node of a\\nnetwork has access to a different summand of an aggregate cost function. Nodes\\ntry to minimize the aggregate cost function, while they exchange information\\nonly with their neighbors. We modify the dual decomposition method to\\nincorporate a curvature correction inspired by the\\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dual\\nD-BFGS method is a fully decentralized algorithm in which nodes approximate\\ncurvature information of themselves and their neighbors through the\\nsatisfaction of a secant condition. Dual D-BFGS is of interest in consensus\\noptimization problems that are not well conditioned, making first order\\ndecentralized methods ineffective, and in which second order information is not\\nreadily available, making decentralized second order methods infeasible.\\nAsynchronous implementation is discussed and convergence of D-BFGS is\\nestablished formally for both synchronous and asynchronous implementations.\\nPerformance advantages relative to alternative decentralized algorithms are\\nshown numerically.\\n',\n",
       "  'title': u'A Decentralized Quasi-Newton Method for Dual Formulations of Consensus\\n  Optimization'},\n",
       " u'1509.01851': {'arxivid': u'1509.01851',\n",
       "  'authorsaffil': [[u'David Balduzzi', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.GT', u'cs.NE'],\n",
       "  'comment': u'Rendered obsolete by arXiv:1604.01952. The new version contains the\\n  same basic results, with major changes to exposition and minor changes to\\n  terminology',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.01851v2',\n",
       "  'published': u'2015-09-06T20:25:32Z',\n",
       "  'summary': u'  Methods from convex optimization such as accelerated gradient descent are\\nwidely used as building blocks for deep learning algorithms. However, the\\nreasons for their empirical success are unclear, since neural networks are not\\nconvex and standard guarantees do not apply. This paper develops the first\\nrigorous link between online convex optimization and error backpropagation on\\nconvolutional networks. The first step is to introduce circadian games, a mild\\ngeneralization of convex games with similar convergence properties. The main\\nresult is that error backpropagation on a convolutional network is equivalent\\nto playing out a circadian game. It follows immediately that the waking-regret\\nof players in the game (the units in the neural network) controls the overall\\nrate of convergence of the network. Finally, we explore some implications of\\nthe results: (i) we describe the representations learned by a neural network\\ngame-theoretically, (ii) propose a learning setting at the level of individual\\nunits that can be plugged into deep architectures, and (iii) propose a new\\napproach to adaptive model selection by applying bandit algorithms to choose\\nwhich players to wake on each round.\\n',\n",
       "  'title': u'Deep Online Convex Optimization by Putting Forecaster to Sleep'},\n",
       " u'1511.02283': {'arxivid': u'1511.02283',\n",
       "  'authorsaffil': [[u'Junhua Mao', None],\n",
       "   [u'Jonathan Huang', None],\n",
       "   [u'Alexander Toshev', None],\n",
       "   [u'Oana Camburu', None],\n",
       "   [u'Alan Yuille', None],\n",
       "   [u'Kevin Murphy', None]],\n",
       "  'categoryterms': [u'cs.CV',\n",
       "   u'cs.CL',\n",
       "   u'cs.LG',\n",
       "   u'cs.RO',\n",
       "   u'I.2.6; I.2.7; I.2.10'],\n",
       "  'comment': u'We have released the Google Refexp dataset together with a toolbox\\n  for visualization and evaluation, see\\n  https://github.com/mjhucla/Google_Refexp_toolbox. Camera ready version for\\n  CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.02283v3',\n",
       "  'published': u'2015-11-07T02:17:36Z',\n",
       "  'summary': u'  We propose a method that can generate an unambiguous description (known as a\\nreferring expression) of a specific object or region in an image, and which can\\nalso comprehend or interpret such an expression to infer which object is being\\ndescribed. We show that our method outperforms previous methods that generate\\ndescriptions of objects without taking into account other potentially ambiguous\\nobjects in the scene. Our model is inspired by recent successes of deep\\nlearning methods for image captioning, but while image captioning is difficult\\nto evaluate, our task allows for easy objective evaluation. We also present a\\nnew large-scale dataset for referring expressions, based on MS-COCO. We have\\nreleased the dataset and a toolbox for visualization and evaluation, see\\nhttps://github.com/mjhucla/Google_Refexp_toolbox\\n',\n",
       "  'title': u'Generation and Comprehension of Unambiguous Object Descriptions'},\n",
       " u'1511.04747': {'arxivid': u'1511.04747',\n",
       "  'authorsaffil': [[u'Sayan Ghosh', None],\n",
       "   [u'Eugene Laksana', None],\n",
       "   [u'Louis-Philippe Morency', None],\n",
       "   [u'Stefan Scherer', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'This is a submission for the ICLR (International Conference on\\n  Learning Representations) Workshop 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04747v6',\n",
       "  'published': u'2015-11-15T18:16:20Z',\n",
       "  'summary': u'  There has been a lot of prior work on representation learning for speech\\nrecognition applications, but not much emphasis has been given to an\\ninvestigation of effective representations of affect from speech, where the\\nparalinguistic elements of speech are separated out from the verbal content. In\\nthis paper, we explore denoising autoencoders for learning paralinguistic\\nattributes i.e. categorical and dimensional affective traits from speech. We\\nshow that the representations learnt by the bottleneck layer of the autoencoder\\nare highly discriminative of activation intensity and at separating out\\nnegative valence (sadness and anger) from positive valence (happiness). We\\nexperiment with different input speech features (such as FFT and log-mel\\nspectrograms with temporal context windows), and different autoencoder\\narchitectures (such as stacked and deep autoencoders). We also learn utterance\\nspecific representations by a combination of denoising autoencoders and BLSTM\\nbased recurrent autoencoders. Emotion classification is performed with the\\nlearnt temporal/dynamic representations to evaluate the quality of the\\nrepresentations. Experiments on a well-established real-life speech dataset\\n(IEMOCAP) show that the learnt representations are comparable to state of the\\nart feature extractors (such as voice quality features and MFCCs) and are\\ncompetitive with state-of-the-art approaches at emotion and dimensional affect\\nrecognition.\\n',\n",
       "  'title': u'Learning Representations of Affect from Speech'},\n",
       " u'1604.02469': {'arxivid': u'1604.02469',\n",
       "  'authorsaffil': [[u'Artem Lenskiy', None]],\n",
       "  'categoryterms': [u'cs.CV', u'68T10', u'I.4.7; I.4.8; I.5.1'],\n",
       "  'comment': u'Corrected version of the chapter published in Advances in Robotics\\n  and Virtual Reality, Volume 26 of the series Intelligent Systems Reference\\n  Library pp 227-247',\n",
       "  'doi': u'10.1007/978-3-642-23363-0_10',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02469v1',\n",
       "  'published': u'2016-04-08T20:14:46Z',\n",
       "  'summary': u\"  Computer vision has become a major source of information for autonomous\\nnavigation of robots of various types, self-driving cars, military robots and\\nmars/lunar rovers are some examples. Nevertheless, the majority of methods\\nfocus on analysing images captured in visible spectrum. In this manuscript we\\nelaborate on the problem of segmenting cross-country scenes captured in IR\\nspectrum. For this purpose we proposed employing salient features. Salient\\nfeatures are robust to variations in scale, brightness and view angle. We\\nsuggest the Speeded-Up Robust Features as a basis for our salient features for\\na number of reasons discussed in the paper. We also provide a comparison of two\\nSURF implementations. The SURF features are extracted from images of different\\nterrain types. For every feature we estimate a terrain class membership\\nfunction. The membership values are obtained by means of either the multi-layer\\nperceptron or nearest neighbours. The features' class membership values and\\ntheir spatial positions are then applied to estimate class membership values\\nfor all pixels in the image. To decrease the effect of segmentation blinking\\nthat is caused by rapid switching between different terrain types and to speed\\nup segmentation, we are tracking camera position and predict features'\\npositions. The comparison of the multi-layer perception and the nearest\\nneighbour classifiers is presented in the paper. The error rate of the terrain\\nsegmentation using the nearest neighbours obtained on the testing set is\\n16.6+-9.17%.\\n\",\n",
       "  'title': u'Image segmentation of cross-country scenes captured in IR spectrum'},\n",
       " u'1602.00287': {'arxivid': u'1602.00287',\n",
       "  'authorsaffil': [[u'Kirthevasan Kandasamy', None], [u'Yaoliang Yu', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'International Conference on Machine Learning (ICML) 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00287v3',\n",
       "  'published': u'2016-01-31T17:32:51Z',\n",
       "  'summary': u'  High dimensional nonparametric regression is an inherently difficult problem\\nwith known lower bounds depending exponentially in dimension. A popular\\nstrategy to alleviate this curse of dimensionality has been to use additive\\nmodels of \\\\emph{first order}, which model the regression function as a sum of\\nindependent functions on each dimension. Though useful in controlling the\\nvariance of the estimate, such models are often too restrictive in practical\\nsettings. Between non-additive models which often have large variance and first\\norder additive models which have large bias, there has been little work to\\nexploit the trade-off in the middle via additive models of intermediate order.\\nIn this work, we propose SALSA, which bridges this gap by allowing interactions\\nbetween variables, but controls model capacity by limiting the order of\\ninteractions. SALSA minimises the residual sum of squares with squared RKHS\\nnorm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression\\nwith an additive kernel. When the regression function is additive, the excess\\nrisk is only polynomial in dimension. Using the Girard-Newton formulae, we\\nefficiently sum over a combinatorial number of terms in the additive expansion.\\nVia a comparison on $15$ real datasets, we show that our method is competitive\\nagainst $21$ other alternatives.\\n',\n",
       "  'title': u'Additive Approximations in High Dimensional Nonparametric Regression via\\n  the SALSA'},\n",
       " u'1509.01692': {'arxivid': u'1509.01692',\n",
       "  'authorsaffil': [[u'Ekaterina Vylomova', None],\n",
       "   [u'Laura Rimell', None],\n",
       "   [u'Trevor Cohn', None],\n",
       "   [u'Timothy Baldwin', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.01692v3',\n",
       "  'published': u'2015-09-05T11:23:44Z',\n",
       "  'summary': u'  Recent work on word embeddings has shown that simple vector subtraction over\\npre-trained embeddings is surprisingly effective at capturing different lexical\\nrelations, despite lacking explicit supervision. Prior work has evaluated this\\nintriguing result using a word analogy prediction formulation and hand-selected\\nrelations, but the generality of the finding over a broader range of lexical\\nrelation types and different learning settings has not been evaluated. In this\\npaper, we carry out such an evaluation in two learning settings: (1) spectral\\nclustering to induce word relations, and (2) supervised learning to classify\\nvector differences into relation types. We find that word embeddings capture a\\nsurprising amount of information, and that, under suitable supervised training,\\nvector subtraction generalises well to a broad range of relations, including\\nover unseen lexical items.\\n',\n",
       "  'title': u'Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility\\n  of Vector Differences for Lexical Relation Learning'},\n",
       " u'1512.00103': {'arxivid': u'1512.00103',\n",
       "  'authorsaffil': [[u'Dan Gillick', None],\n",
       "   [u'Cliff Brunk', None],\n",
       "   [u'Oriol Vinyals', None],\n",
       "   [u'Amarnag Subramanya', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.00103v2',\n",
       "  'published': u'2015-12-01T00:23:44Z',\n",
       "  'summary': u'  We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads\\ntext as bytes and outputs span annotations of the form [start, length, label]\\nwhere start positions, lengths, and labels are separate entries in our\\nvocabulary. Because we operate directly on unicode bytes rather than\\nlanguage-specific words or characters, we can analyze text in many languages\\nwith a single model. Due to the small vocabulary size, these multilingual\\nmodels are very compact, but produce results similar to or better than the\\nstate-of- the-art in Part-of-Speech tagging and Named Entity Recognition that\\nuse only the provided training datasets (no external data sources). Our models\\nare learning \"from scratch\" in that they do not rely on any elements of the\\nstandard pipeline in Natural Language Processing (including tokenization), and\\nthus can run in standalone fashion on raw text.\\n',\n",
       "  'title': u'Multilingual Language Processing From Bytes'},\n",
       " u'1601.05511': {'arxivid': u'1601.05511',\n",
       "  'authorsaffil': [[u'Jing Zhang', None],\n",
       "   [u'Wanqing Li', None],\n",
       "   [u'Philip O. Ogunbona', None],\n",
       "   [u'Pichao Wang', None],\n",
       "   [u'Chang Tang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05511v1',\n",
       "  'published': u'2016-01-21T04:58:04Z',\n",
       "  'summary': u\"  Human action recognition from RGB-D (Red, Green, Blue and Depth) data has\\nattracted increasing attention since the first work reported in 2010. Over this\\nperiod, many benchmark datasets have been created to facilitate the development\\nand evaluation of new algorithms. This raises the question of which dataset to\\nselect and how to use it in providing a fair and objective comparative\\nevaluation against state-of-the-art methods. To address this issue, this paper\\nprovides a comprehensive review of the most commonly used action recognition\\nrelated RGB-D video datasets, including 27 single-view datasets, 10 multi-view\\ndatasets, and 7 multi-person datasets. The detailed information and analysis of\\nthese datasets is a useful resource in guiding insightful selection of datasets\\nfor future research. In addition, the issues with current algorithm evaluation\\nvis-\\\\'{a}-vis limitations of the available datasets and evaluation protocols\\nare also highlighted; resulting in a number of recommendations for collection\\nof new datasets and use of evaluation protocols.\\n\",\n",
       "  'title': u'RGB-D-based Action Recognition Datasets: A Survey'},\n",
       " u'1311.0989': {'arxivid': u'1311.0989',\n",
       "  'authorsaffil': [[u'Teng Zhang', None], [u'Zhi-Hua Zhou', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u\"In: Proceedings of the 20th ACM SIGKDD Conference on Knowledge\\n  Discovery and Data Mining (KDD'14), New York, NY, 2014\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1311.0989v2',\n",
       "  'published': u'2013-11-05T08:46:26Z',\n",
       "  'summary': u'  Support vector machine (SVM) has been one of the most popular learning\\nalgorithms, with the central idea of maximizing the minimum margin, i.e., the\\nsmallest distance from the instances to the classification boundary. Recent\\ntheoretical results, however, disclosed that maximizing the minimum margin does\\nnot necessarily lead to better generalization performances, and instead, the\\nmargin distribution has been proven to be more crucial. In this paper, we\\npropose the Large margin Distribution Machine (LDM), which tries to achieve a\\nbetter generalization performance by optimizing the margin distribution. We\\ncharacterize the margin distribution by the first- and second-order statistics,\\ni.e., the margin mean and variance. The LDM is a general learning approach\\nwhich can be used in any place where SVM can be applied, and its superiority is\\nverified both theoretically and empirically in this paper.\\n',\n",
       "  'title': u'Large Margin Distribution Machine'},\n",
       " u'1604.03343': {'arxivid': u'1604.03343',\n",
       "  'authorsaffil': [[u'Daniel Filan', None],\n",
       "   [u'Marcus Hutter', None],\n",
       "   [u'Jan Leike', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'AISTATS 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03343v1',\n",
       "  'published': u'2016-04-12T11:26:12Z',\n",
       "  'summary': u\"  This paper establishes for the first time the predictive performance of speed\\npriors and their computational complexity. A speed prior is essentially a\\nprobability distribution that puts low probability on strings that are not\\nefficiently computable. We propose a variant to the original speed prior\\n(Schmidhuber, 2002), and show that our prior can predict sequences drawn from\\nprobability measures that are estimable in polynomial time. Our speed prior is\\ncomputable in doubly-exponential time, but not in polynomial time. On a\\npolynomial time computable sequence our speed prior is computable in\\nexponential time. We show better upper complexity bounds for Schmidhuber's\\nspeed prior under the same conditions, and that it predicts deterministic\\nsequences that are computable in polynomial time; however, we also show that it\\nis not computable in polynomial time, and the question of its predictive\\nproperties for stochastic sequences remains open.\\n\",\n",
       "  'title': u'Loss Bounds and Time Complexity for Speed Priors'},\n",
       " u'1512.04114': {'arxivid': u'1512.04114',\n",
       "  'authorsaffil': [[u'Luca Melis', None],\n",
       "   [u'Apostolos Pyrgelis', None],\n",
       "   [u'Emiliano De Cristofaro', None]],\n",
       "  'categoryterms': [u'cs.CR', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.04114v3',\n",
       "  'published': u'2015-12-13T20:05:53Z',\n",
       "  'summary': u'  Collaborative approaches to network defense are increasingly used to predict\\nattacks as well as to speed up their detection. For instance, with highly\\npredictive blacklisting, one aims to forecast attack sources based on alerts\\ncontributed by multiple organizations. While collaboration helps discover\\ngroups of correlated attacks targeting similar victims, it also raises\\nimportant privacy concerns.\\n  To address this challenge, we introduce a novel privacy-friendly system\\nwhereby organizations are clustered together based on the similarity of their\\nlogs, without disclosing them in the clear. Entities in the same cluster only\\nshare relevant logs and build more accurate blacklists. At the same time, we\\ninvestigate how to measure the effect of collaboration on prediction and find\\nthat the state-of-the-art (non privacy-preserving) system actually achieves\\nlower accuracy than if organizations predicted based on local alerts only. Our\\nexperiments shed light on how to improve the quality of predictions by\\noptimizing information shared across organizations, showing that our\\nprivacy-friendly methods markedly outperform non private tools both in terms of\\nprecision and recall.\\n',\n",
       "  'title': u'Building and Measuring Privacy-Preserving Predictive Blacklists'},\n",
       " u'1601.00741': {'arxivid': u'1601.00741',\n",
       "  'authorsaffil': [[u'Ashesh Jain', None],\n",
       "   [u'Shikhar Sharma', None],\n",
       "   [u'Thorsten Joachims', None],\n",
       "   [u'Ashutosh Saxena', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'IJRR accepted (Learning preferences over trajectories from coactive\\n  feedback)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00741v1',\n",
       "  'published': u'2016-01-05T05:47:09Z',\n",
       "  'summary': u'  We consider the problem of learning preferences over trajectories for mobile\\nmanipulators such as personal robots and assembly line robots. The preferences\\nwe learn are more intricate than simple geometric constraints on trajectories;\\nthey are rather governed by the surrounding context of various objects and\\nhuman interactions in the environment. We propose a coactive online learning\\nframework for teaching preferences in contextually rich environments. The key\\nnovelty of our approach lies in the type of feedback expected from the user:\\nthe human user does not need to demonstrate optimal trajectories as training\\ndata, but merely needs to iteratively provide trajectories that slightly\\nimprove over the trajectory currently proposed by the system. We argue that\\nthis coactive preference feedback can be more easily elicited than\\ndemonstrations of optimal trajectories. Nevertheless, theoretical regret bounds\\nof our algorithm match the asymptotic rates of optimal trajectory algorithms.\\n  We implement our algorithm on two high degree-of-freedom robots, PR2 and\\nBaxter, and present three intuitive mechanisms for providing such incremental\\nfeedback. In our experimental evaluation we consider two context rich settings\\n-- household chores and grocery store checkout -- and show that users are able\\nto train the robot with just a few feedbacks (taking only a few\\nminutes).\\\\footnote{Parts of this work has been published at NIPS and ISRR\\nconferences~\\\\citep{Jain13,Jain13b}. This journal submission presents a\\nconsistent full paper, and also includes the proof of regret bounds, more\\ndetails of the robotic system, and a thorough related work.}\\n',\n",
       "  'title': u'Learning Preferences for Manipulation Tasks from Online Coactive\\n  Feedback'},\n",
       " u'1601.00740': {'arxivid': u'1601.00740',\n",
       "  'authorsaffil': [[u'Ashesh Jain', None],\n",
       "   [u'Hema S Koppula', None],\n",
       "   [u'Shane Soh', None],\n",
       "   [u'Bharad Raghavan', None],\n",
       "   [u'Avi Singh', None],\n",
       "   [u'Ashutosh Saxena', None]],\n",
       "  'categoryterms': [u'cs.RO', u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'Journal Version (ICCV and ICRA combination with more system details)\\n  http://brain4cars.com',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.00740v1',\n",
       "  'published': u'2016-01-05T05:25:14Z',\n",
       "  'summary': u'  Advanced Driver Assistance Systems (ADAS) have made driving safer over the\\nlast decade. They prepare vehicles for unsafe road conditions and alert drivers\\nif they perform a dangerous maneuver. However, many accidents are unavoidable\\nbecause by the time drivers are alerted, it is already too late. Anticipating\\nmaneuvers beforehand can alert drivers before they perform the maneuver and\\nalso give ADAS more time to avoid or prepare for the danger.\\n  In this work we propose a vehicular sensor-rich platform and learning\\nalgorithms for maneuver anticipation. For this purpose we equip a car with\\ncameras, Global Positioning System (GPS), and a computing device to capture the\\ndriving context from both inside and outside of the car. In order to anticipate\\nmaneuvers, we propose a sensory-fusion deep learning architecture which jointly\\nlearns to anticipate and fuse multiple sensory streams. Our architecture\\nconsists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory\\n(LSTM) units to capture long temporal dependencies. We propose a novel training\\nprocedure which allows the network to predict the future given only a partial\\ntemporal context. We introduce a diverse data set with 1180 miles of natural\\nfreeway and city driving, and show that we can anticipate maneuvers 3.5 seconds\\nbefore they occur in real-time with a precision and recall of 90.5\\\\% and 87.4\\\\%\\nrespectively.\\n',\n",
       "  'title': u'Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep\\n  Learning Architecture'},\n",
       " u'1509.05173': {'arxivid': u'1509.05173',\n",
       "  'authorsaffil': [[u'Andrew J. R. Simpson', None]],\n",
       "  'categoryterms': [u'cs.LG', u'68Txx'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.05173v1',\n",
       "  'published': u'2015-09-17T09:04:30Z',\n",
       "  'summary': u\"  Rectified Linear Units (ReLU) seem to have displaced traditional 'smooth'\\nnonlinearities as activation-function-du-jour in many - but not all - deep\\nneural network (DNN) applications. However, nobody seems to know why. In this\\narticle, we argue that ReLU are useful because they are ideal demodulators -\\nthis helps them perform fast abstract learning. However, this fast learning\\ncomes at the expense of serious nonlinear distortion products - decoy features.\\nWe show that Parallel Dither acts to suppress the decoy features, preventing\\noverfitting and leaving the true features cleanly demodulated for rapid,\\nreliable learning.\\n\",\n",
       "  'title': u'Taming the ReLU with Parallel Dither in a Deep Neural Network'},\n",
       " u'1603.06170': {'arxivid': u'1603.06170',\n",
       "  'authorsaffil': [[u'Haotian Xu', None], [u'Zhijian Ou', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'8 pages, 2 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06170v1',\n",
       "  'published': u'2016-03-20T00:55:06Z',\n",
       "  'summary': u\"  Though with progress, model learning and performing posterior inference still\\nremains a common challenge for using deep generative models, especially for\\nhandling discrete hidden variables. This paper is mainly concerned with\\nalgorithms for learning Helmholz machines, which is characterized by pairing\\nthe generative model with an auxiliary inference model. A common drawback of\\nprevious learning algorithms is that they indirectly optimize some bounds of\\nthe targeted marginal log-likelihood. In contrast, we successfully develop a\\nnew class of algorithms, based on stochastic approximation (SA) theory of the\\nRobbins-Monro type, to directly optimize the marginal log-likelihood and\\nsimultaneously minimize the inclusive KL-divergence. The resulting learning\\nalgorithm is thus called joint SA (JSA). Moreover, we construct an effective\\nMCMC operator for JSA. Our results on the MNIST datasets demonstrate that the\\nJSA's performance is consistently superior to that of competing algorithms like\\nRWS, for learning a range of difficult models.\\n\",\n",
       "  'title': u'Joint Stochastic Approximation learning of Helmholtz Machines'},\n",
       " u'1603.00128': {'arxivid': u'1603.00128',\n",
       "  'authorsaffil': [[u'Xiaoheng Jiang', None],\n",
       "   [u'Yanwei Pang', None],\n",
       "   [u'Manli Sun', None],\n",
       "   [u'Xuelong Li', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00128v1',\n",
       "  'published': u'2016-03-01T03:44:49Z',\n",
       "  'summary': u'  Conventional Convolutional Neural Networks (CNNs) use either a linear or\\nnon-linear filter to extract features from an image patch (region) of spatial\\nsize $ H\\\\times W $ (Typically, $ H $ is small and is equal to $ W$, e.g., $ H $\\nis 5 or 7). Generally, the size of the filter is equal to the size $ H\\\\times W\\n$ of the input patch. We argue that the representation ability of equal-size\\nstrategy is not strong enough. To overcome the drawback, we propose to use\\nsubpatch filter whose spatial size $ h\\\\times w $ is smaller than $ H\\\\times W $.\\nThe proposed subpatch filter consists of two subsequent filters. The first one\\nis a linear filter of spatial size $ h\\\\times w $ and is aimed at extracting\\nfeatures from spatial domain. The second one is of spatial size $ 1\\\\times 1 $\\nand is used for strengthening the connection between different input feature\\nchannels and for reducing the number of parameters. The subpatch filter\\nconvolves with the input patch and the resulting network is called a subpatch\\nnetwork. Taking the output of one subpatch network as input, we further repeat\\nconstructing subpatch networks until the output contains only one neuron in\\nspatial domain. These subpatch networks form a new network called Cascaded\\nSubpatch Network (CSNet). The feature layer generated by CSNet is called csconv\\nlayer. For the whole input image, we construct a deep neural network by\\nstacking a sequence of csconv layers. Experimental results on four benchmark\\ndatasets demonstrate the effectiveness and compactness of the proposed CSNet.\\nFor example, our CSNet reaches a test error of $ 5.68\\\\% $ on the CIFAR10\\ndataset without model averaging. To the best of our knowledge, this is the best\\nresult ever obtained on the CIFAR10 dataset.\\n',\n",
       "  'title': u'Cascaded Subpatch Networks for Effective CNNs'},\n",
       " u'1601.05861': {'arxivid': u'1601.05861',\n",
       "  'authorsaffil': [[u'Amr Bakry', None], [u'Ahmed Elgammal', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05861v1',\n",
       "  'published': u'2016-01-22T01:59:26Z',\n",
       "  'summary': u'  Speech recognition is a challenging problem. Due to the acoustic limitations,\\nusing visual information is essential for improving the recognition accuracy in\\nreal-life unconstraint situations. One common approach is to model the visual\\nrecognition as nonlinear optimization problem. Measuring the distances between\\nvisual units is essential for solving this problem. Embedding the visual units\\non a manifold and using manifold kernels is one way to measure these distances.\\nThis work is intended to evaluate the performance of several manifold kernels\\nfor solving the problem of visual speech recognition. We show the theory behind\\neach kernel. We apply manifold kernel partial least squares framework to OuluVs\\nand AvLetters databases, and show empirical comparison between all kernels.\\nThis framework provides convenient way to explore different kernels.\\n',\n",
       "  'title': u'Manifold-Kernels Comparison in MKPLS for Visual Speech Recognition'},\n",
       " u'1511.03483': {'arxivid': u'1511.03483',\n",
       "  'authorsaffil': [[u'Jun He', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.03483v2',\n",
       "  'published': u'2015-11-11T13:02:36Z',\n",
       "  'summary': u'  An important theoretical question in evolutionary computation is how good\\nsolutions evolutionary algorithms can produce. This paper aims to provide an\\nanalytic analysis of solution quality of evolutionary algorithms in terms of\\nthe performance rate, which is defined by the difference between 1 and the\\napproximation ratio of the best solution found in each generation. The\\nperformance rate can be represented by a function of time. With the help of\\nmatrix analysis, it is possible to obtain an exact expression of such a\\nfunction. For the first time, an analytic expression for calculating the\\nperformance rate is presented in this paper for a class of evolutionary\\nalgorithms, that is, (1+1) strictly elitist evolution algorithms. Furthermore,\\nanalytic expressions for calculate the fitness value and the average\\nconvergence rate in each generation are also derived for this class of\\nevolutionary algorithms. The approach is promising, and it can be extended to\\nnon-elitist or population-based algorithms too.\\n',\n",
       "  'title': u'An Analytic Expression of Performance Rate, Fitness Value and Average\\n  Convergence Rate for a Class of Evolutionary Algorithms'},\n",
       " u'1412.7580': {'arxivid': u'1412.7580',\n",
       "  'authorsaffil': [[u'Nicolas Vasilache', None],\n",
       "   [u'Jeff Johnson', None],\n",
       "   [u'Michael Mathieu', None],\n",
       "   [u'Soumith Chintala', None],\n",
       "   [u'Serkan Piantino', None],\n",
       "   [u'Yann LeCun', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DC', u'cs.NE'],\n",
       "  'comment': u'Camera ready for ICLR2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1412.7580v3',\n",
       "  'published': u'2014-12-24T01:31:36Z',\n",
       "  'summary': u\"  We examine the performance profile of Convolutional Neural Network training\\non the current generation of NVIDIA Graphics Processing Units. We introduce two\\nnew Fast Fourier Transform convolution implementations: one based on NVIDIA's\\ncuFFT library, and another based on a Facebook authored FFT implementation,\\nfbfft, that provides significant speedups over cuFFT (over 1.5x) for whole\\nCNNs. Both of these convolution implementations are available in open source,\\nand are faster than NVIDIA's cuDNN implementation for many common convolutional\\nlayers (up to 23.5x for some synthetic kernel configurations). We discuss\\ndifferent performance regimes of convolutions, comparing areas where\\nstraightforward time domain convolutions outperform Fourier frequency domain\\nconvolutions. Details on algorithmic applications of NVIDIA GPU hardware\\nspecifics in the implementation of fbfft are also provided.\\n\",\n",
       "  'title': u'Fast Convolutional Nets With fbfft: A GPU Performance Evaluation'},\n",
       " u'1604.00470': {'arxivid': u'1604.00470',\n",
       "  'authorsaffil': [[u'Raghvendra Kannao', None], [u'Prithwijit Guha', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Published in INDICON 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00470v1',\n",
       "  'published': u'2016-04-02T07:28:23Z',\n",
       "  'summary': u'  The text data present in overlaid bands convey brief descriptions of news\\nevents in broadcast videos. The process of text extraction becomes challenging\\nas overlay text is presented in widely varying formats and often with animation\\neffects. We note that existing edge density based methods are well suited for\\nour application on account of their simplicity and speed of operation. However,\\nthese methods are sensitive to thresholds and have high false positive rates.\\nIn this paper, we present a contrast enhancement based preprocessing stage for\\noverlay text detection and a parameter free edge density based scheme for\\nefficient text band detection. The second contribution of this paper is a novel\\napproach for multiple text region tracking with a formal identification of all\\npossible detection failure cases. The tracking stage enables us to establish\\nthe temporal presence of text bands and their linking over time. The third\\ncontribution is the adoption of Tesseract OCR for the specific task of overlay\\ntext recognition using web news articles. The proposed approach is tested and\\nfound superior on news videos acquired from three Indian English television\\nnews channels along with benchmark datasets.\\n',\n",
       "  'title': u'Overlay Text Extraction From TV News Broadcast'},\n",
       " u'1604.00834': {'arxivid': u'1604.00834',\n",
       "  'authorsaffil': [[u'Andrzej Kulig', None],\n",
       "   [u'Jaroslaw Kwapien', None],\n",
       "   [u'Tomasz Stanisz', None],\n",
       "   [u'Stanislaw Drozdz', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00834v1',\n",
       "  'published': u'2016-04-04T12:29:00Z',\n",
       "  'summary': u\"  From a grammar point of view, the role of punctuation marks in a sentence is\\nformally defined and well understood. In semantic analysis punctuation plays\\nalso a crucial role as a method of avoiding ambiguity of the meaning. A\\ndifferent situation can be observed in the statistical analyses of language\\nsamples, where the decision on whether the punctuation marks should be\\nconsidered or should be neglected is seen rather as arbitrary and at present it\\nbelongs to a researcher's preference. An objective of this work is to shed some\\nlight onto this problem by providing us with an answer to the question whether\\nthe punctuation marks may be treated as ordinary words and whether they should\\nbe included in any analysis of the word co-occurences. We already know from our\\nprevious study \\\\cite{drozdz2016} that full stops that determine the length of\\nsentences are the main carrier of long-range correlations. Now we extend that\\nstudy and analyze statistical properties of the most common punctuation marks\\nin a few Indo-European languages, investigate their frequencies, and locate\\nthem accordingly in the Zipf rank-frequency plots as well as study their role\\nin the word-adjacency networks. We show that, from a statistical viewpoint, the\\npunctuation marks reveal properties that are qualitatively similar to the\\nproperties of the most frequent words like articles, conjunctions, pronouns,\\nand prepositions. This refers to both the Zipfian analysis and the network\\nanalysis. Our results can be exploited in the computer-based analyses of large\\ntext corpora and be incorporated in the related automated systems. As a side\\nresult, we propose an efficient method of sampling the language corpora for a\\nword-adjacency network analysis.\\n\",\n",
       "  'title': u'In narrative texts punctuation marks obey the same statistics as words'},\n",
       " u'1502.05689': {'arxivid': u'1502.05689',\n",
       "  'authorsaffil': [[u'Ming-Yu Liu', None],\n",
       "   [u'Arun Mallya', None],\n",
       "   [u'Oncel C. Tuzel', None],\n",
       "   [u'Xi Chen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'9 pages, 11 figures, WACV 2016: IEEE Conference on Applications of\\n  Computer Vision',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.05689v2',\n",
       "  'published': u'2015-02-19T20:26:35Z',\n",
       "  'summary': u'  Over the years, computer vision researchers have spent an immense amount of\\neffort on designing image features for the visual object recognition task. We\\npropose to incorporate this valuable experience to guide the task of training\\ndeep neural networks. Our idea is to pretrain the network through the task of\\nreplicating the process of hand-designed feature extraction. By learning to\\nreplicate the process, the neural network integrates previous research\\nknowledge and learns to model visual objects in a way similar to the\\nhand-designed features. In the succeeding finetuning step, it further learns\\nobject-specific representations from labeled data and this boosts its\\nclassification power. We pretrain two convolutional neural networks where one\\nreplicates the process of histogram of oriented gradients feature extraction,\\nand the other replicates the process of region covariance feature extraction.\\nAfter finetuning, we achieve substantially better performance than the baseline\\nmethods.\\n',\n",
       "  'title': u'Unsupervised Network Pretraining via Encoding Human Design'},\n",
       " u'1604.02993': {'arxivid': u'1604.02993',\n",
       "  'authorsaffil': [[u'Karl Pichotta', None], [u'Raymond J. Mooney', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'To appear in Proceedings of the 54th Annual Meeting of the\\n  Association for Computational Linguistics (ACL-16)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02993v2',\n",
       "  'published': u'2016-04-11T15:21:05Z',\n",
       "  'summary': u'  There is a small but growing body of research on statistical scripts, models\\nof event sequences that allow probabilistic inference of implicit events from\\ndocuments. These systems operate on structured verb-argument events produced by\\nan NLP pipeline. We compare these systems with recent Recurrent Neural Net\\nmodels that directly operate on raw tokens to predict sentences, finding the\\nlatter to be roughly comparable to the former in terms of predicting missing\\nevents in documents.\\n',\n",
       "  'title': u'Using Sentence-Level LSTM Language Models for Script Inference'},\n",
       " u'1501.00092': {'arxivid': u'1501.00092',\n",
       "  'authorsaffil': [[u'Chao Dong', None],\n",
       "   [u'Chen Change Loy', None],\n",
       "   [u'Kaiming He', None],\n",
       "   [u'Xiaoou Tang', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.NE', u'I.4.5; I.2.6'],\n",
       "  'comment': u'14 pages, 14 figures, journal',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1501.00092v3',\n",
       "  'published': u'2014-12-31T08:35:09Z',\n",
       "  'summary': u'  We propose a deep learning method for single image super-resolution (SR). Our\\nmethod directly learns an end-to-end mapping between the low/high-resolution\\nimages. The mapping is represented as a deep convolutional neural network (CNN)\\nthat takes the low-resolution image as the input and outputs the\\nhigh-resolution one. We further show that traditional sparse-coding-based SR\\nmethods can also be viewed as a deep convolutional network. But unlike\\ntraditional methods that handle each component separately, our method jointly\\noptimizes all layers. Our deep CNN has a lightweight structure, yet\\ndemonstrates state-of-the-art restoration quality, and achieves fast speed for\\npractical on-line usage. We explore different network structures and parameter\\nsettings to achieve trade-offs between performance and speed. Moreover, we\\nextend our network to cope with three color channels simultaneously, and show\\nbetter overall reconstruction quality.\\n',\n",
       "  'title': u'Image Super-Resolution Using Deep Convolutional Networks'},\n",
       " u'1506.02465': {'arxivid': u'1506.02465',\n",
       "  'authorsaffil': [[u'Bernd Bischl', None],\n",
       "   [u'Pascal Kerschke', None],\n",
       "   [u'Lars Kotthoff', None],\n",
       "   [u'Marius Lindauer', None],\n",
       "   [u'Yuri Malitsky', None],\n",
       "   [u'Alexandre Frechette', None],\n",
       "   [u'Holger Hoos', None],\n",
       "   [u'Frank Hutter', None],\n",
       "   [u'Kevin Leyton-Brown', None],\n",
       "   [u'Kevin Tierney', None],\n",
       "   [u'Joaquin Vanschoren', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'Accepted to be published in Artificial Intelligence Journal',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.02465v3',\n",
       "  'published': u'2015-06-08T12:35:04Z',\n",
       "  'summary': u'  The task of algorithm selection involves choosing an algorithm from a set of\\nalgorithms on a per-instance basis in order to exploit the varying performance\\nof algorithms over a set of instances. The algorithm selection problem is\\nattracting increasing attention from researchers and practitioners in AI. Years\\nof fruitful applications in a number of domains have resulted in a large amount\\nof data, but the community lacks a standard format or repository for this data.\\nThis situation makes it difficult to share and compare different approaches\\neffectively, as is done in other, more established fields. It also\\nunnecessarily hinders new researchers who want to work in this area. To address\\nthis problem, we introduce a standardized format for representing algorithm\\nselection scenarios and a repository that contains a growing number of data\\nsets from the literature. Our format has been designed to be able to express a\\nwide variety of different scenarios. Demonstrating the breadth and power of our\\nplatform, we describe a set of example experiments that build and evaluate\\nalgorithm selection models through a common interface. The results display the\\npotential of algorithm selection to achieve significant performance\\nimprovements across a broad range of problems and algorithms.\\n',\n",
       "  'title': u'ASlib: A Benchmark Library for Algorithm Selection'},\n",
       " u'1603.07396': {'arxivid': u'1603.07396',\n",
       "  'authorsaffil': [[u'Aniruddha Kembhavi', None],\n",
       "   [u'Mike Salvato', None],\n",
       "   [u'Eric Kolve', None],\n",
       "   [u'Minjoon Seo', None],\n",
       "   [u'Hannaneh Hajishirzi', None],\n",
       "   [u'Ali Farhadi', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07396v1',\n",
       "  'published': u'2016-03-24T00:02:58Z',\n",
       "  'summary': u'  Diagrams are common tools for representing complex concepts, relationships\\nand events, often when it would be difficult to portray the same information\\nwith natural images. Understanding natural images has been extensively studied\\nin computer vision, while diagram understanding has received little attention.\\nIn this paper, we study the problem of diagram interpretation and reasoning,\\nthe challenging task of identifying the structure of a diagram and the\\nsemantics of its constituents and their relationships. We introduce Diagram\\nParse Graphs (DPG) as our representation to model the structure of diagrams. We\\ndefine syntactic parsing of diagrams as learning to infer DPGs for diagrams and\\nstudy semantic interpretation and reasoning of diagrams in the context of\\ndiagram question answering. We devise an LSTM-based method for syntactic\\nparsing of diagrams and introduce a DPG-based attention model for diagram\\nquestion answering. We compile a new dataset of diagrams with exhaustive\\nannotations of constituents and relationships for over 5,000 diagrams and\\n15,000 questions and answers. Our results show the significance of our models\\nfor syntactic parsing and question answering in diagrams using DPGs.\\n',\n",
       "  'title': u'A Diagram Is Worth A Dozen Images'},\n",
       " u'1604.03029': {'arxivid': u'1604.03029',\n",
       "  'authorsaffil': [[u'Semi Min', None], [u'Juyong Park', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.SI', u'physics.soc-ph'],\n",
       "  'comment': u'17 pages, 10 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03029v1',\n",
       "  'published': u'2016-03-24T10:59:28Z',\n",
       "  'summary': u'  Human communication is often executed in the form of a narrative, an account\\nof connected events composed of characters, actions, and settings. A coherent\\nnarrative structure is therefore a requisite for a well-formulated narrative --\\nbe it fictional or nonfictional -- for informative and effective communication,\\nopening up the possibility of a deeper understanding of a narrative by studying\\nits structural properties. In this paper we present a network-based framework\\nfor modeling and analyzing the structure of a narrative, which is further\\nexpanded by incorporating methods from computational linguistics to utilize the\\nnarrative text. Modeling a narrative as a dynamically unfolding system, we\\ncharacterize its progression via the growth patterns of the character network,\\nand use sentiment analysis and topic modeling to represent the actual content\\nof the narrative in the form of interaction maps between characters with\\nassociated sentiment values and keywords. This is a network framework advanced\\nbeyond the simple occurrence-based one most often used until now, allowing one\\nto utilize the unique characteristics of a given narrative to a high degree.\\nGiven the ubiquity and importance of narratives, such advanced network-based\\nrepresentation and analysis framework may lead to a more systematic modeling\\nand understanding of narratives for social interactions, expression of human\\nsentiments, and communication.\\n',\n",
       "  'title': u'Mapping Out Narrative Structures and Dynamics Using Networks and Textual\\n  Information'},\n",
       " u'1603.05800': {'arxivid': u'1603.05800',\n",
       "  'authorsaffil': [[u'Zhiyun Lu', None],\n",
       "   [u'Dong Guo', None],\n",
       "   [u'Alireza Bagheri Garakani', None],\n",
       "   [u'Kuan Liu', None],\n",
       "   [u'Avner May', None],\n",
       "   [u'Aurelien Bellet', None],\n",
       "   [u'Linxi Fan', None],\n",
       "   [u'Michael Collins', None],\n",
       "   [u'Brian Kingsbury', None],\n",
       "   [u'Michael Picheny', None],\n",
       "   [u'Fei Sha', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1411.4000',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.05800v1',\n",
       "  'published': u'2016-03-18T09:16:01Z',\n",
       "  'summary': u\"  We study large-scale kernel methods for acoustic modeling and compare to DNNs\\non performance metrics related to both acoustic modeling and recognition.\\nMeasuring perplexity and frame-level classification accuracy, kernel-based\\nacoustic models are as effective as their DNN counterparts. However, on\\ntoken-error-rates DNN models can be significantly better. We have discovered\\nthat this might be attributed to DNN's unique strength in reducing both the\\nperplexity and the entropy of the predicted posterior probabilities. Motivated\\nby our findings, we propose a new technique, entropy regularized perplexity,\\nfor model selection. This technique can noticeably improve the recognition\\nperformance of both types of models, and reduces the gap between them. While\\neffective on Broadcast News, this technique could be also applicable to other\\ntasks.\\n\",\n",
       "  'title': u'A Comparison between Deep Neural Nets and Kernel Acoustic Models for\\n  Speech Recognition'},\n",
       " u'1502.08029': {'arxivid': u'1502.08029',\n",
       "  'authorsaffil': [[u'Li Yao', None],\n",
       "   [u'Atousa Torabi', None],\n",
       "   [u'Kyunghyun Cho', None],\n",
       "   [u'Nicolas Ballas', None],\n",
       "   [u'Christopher Pal', None],\n",
       "   [u'Hugo Larochelle', None],\n",
       "   [u'Aaron Courville', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.AI', u'cs.CL', u'cs.CV', u'cs.LG'],\n",
       "  'comment': u'Accepted to ICCV15. This version comes with code release and\\n  supplementary material',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.08029v5',\n",
       "  'published': u'2015-02-27T19:30:40Z',\n",
       "  'summary': u'  Recent progress in using recurrent neural networks (RNNs) for image\\ndescription has motivated the exploration of their application for video\\ndescription. However, while images are static, working with videos requires\\nmodeling their dynamic temporal structure and then properly integrating that\\ninformation into a natural language description. In this context, we propose an\\napproach that successfully takes into account both the local and global\\ntemporal structure of videos to produce descriptions. First, our approach\\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\\ntrained on video action recognition tasks, so as to produce a representation\\nthat is tuned to human motion and behavior. Second we propose a temporal\\nattention mechanism that allows to go beyond local temporal modeling and learns\\nto automatically select the most relevant temporal segments given the\\ntext-generating RNN. Our approach exceeds the current state-of-art for both\\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\\na new, larger and more challenging dataset of paired video and natural language\\ndescriptions.\\n',\n",
       "  'title': u'Describing Videos by Exploiting Temporal Structure'},\n",
       " u'1603.02078': {'arxivid': u'1603.02078',\n",
       "  'authorsaffil': [[u'Jiang Liu', None],\n",
       "   [u'Chenqiang Gao', None],\n",
       "   [u'Lan Wang', None],\n",
       "   [u'Deyu Meng', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'submitted to IEEE ICIP 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.02078v1',\n",
       "  'published': u'2016-03-07T14:15:55Z',\n",
       "  'summary': u'  Detecting complex events in a large video collection crawled from video\\nwebsites is a challenging task. When applying directly good image-based feature\\nrepresentation, e.g., HOG, SIFT, to videos, we have to face the problem of how\\nto pool multiple frame feature representations into one feature representation.\\nIn this paper, we propose a novel learning-based frame pooling method. We\\nformulate the pooling weight learning as an optimization problem and thus our\\nmethod can automatically learn the best pooling weight configuration for each\\nspecific event category. Experimental results conducted on TRECVID MED 2011\\nreveal that our method outperforms the commonly used average pooling and max\\npooling strategies on both high-level and low-level 2D image features.\\n',\n",
       "  'title': u'A Learning-based Frame Pooling Model For Event Detection'},\n",
       " u'1502.04623': {'arxivid': u'1502.04623',\n",
       "  'authorsaffil': [[u'Karol Gregor', None],\n",
       "   [u'Ivo Danihelka', None],\n",
       "   [u'Alex Graves', None],\n",
       "   [u'Danilo Jimenez Rezende', None],\n",
       "   [u'Daan Wierstra', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.04623v2',\n",
       "  'published': u'2015-02-16T16:48:56Z',\n",
       "  'summary': u'  This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\\nnetwork architecture for image generation. DRAW networks combine a novel\\nspatial attention mechanism that mimics the foveation of the human eye, with a\\nsequential variational auto-encoding framework that allows for the iterative\\nconstruction of complex images. The system substantially improves on the state\\nof the art for generative models on MNIST, and, when trained on the Street View\\nHouse Numbers dataset, it generates images that cannot be distinguished from\\nreal data with the naked eye.\\n',\n",
       "  'title': u'DRAW: A Recurrent Neural Network For Image Generation'},\n",
       " u'1512.05227': {'arxivid': u'1512.05227',\n",
       "  'authorsaffil': [[u'Yin Cui', None],\n",
       "   [u'Feng Zhou', None],\n",
       "   [u'Yuanqing Lin', None],\n",
       "   [u'Serge Belongie', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'10 pages, 9 figures, CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.05227v2',\n",
       "  'published': u'2015-12-16T16:14:22Z',\n",
       "  'summary': u'  Existing fine-grained visual categorization methods often suffer from three\\nchallenges: lack of training data, large number of fine-grained categories, and\\nhigh intraclass vs. low inter-class variance. In this work we propose a generic\\niterative framework for fine-grained categorization and dataset bootstrapping\\nthat handles these three challenges. Using deep metric learning with humans in\\nthe loop, we learn a low dimensional feature embedding with anchor points on\\nmanifolds for each category. These anchor points capture intra-class variances\\nand remain discriminative between classes. In each round, images with high\\nconfidence scores from our model are sent to humans for labeling. By comparing\\nwith exemplar images, labelers mark each candidate image as either a \"true\\npositive\" or a \"false positive\". True positives are added into our current\\ndataset and false positives are regarded as \"hard negatives\" for our metric\\nlearning model. Then the model is retrained with an expanded dataset and hard\\nnegatives for the next round. To demonstrate the effectiveness of the proposed\\nframework, we bootstrap a fine-grained flower dataset with 620 categories from\\nInstagram images. The proposed deep metric learning scheme is evaluated on both\\nour dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show\\nsignificant performance gain using dataset bootstrapping and demonstrate\\nstate-of-the-art results achieved by the proposed deep metric learning methods.\\n',\n",
       "  'title': u'Fine-grained Categorization and Dataset Bootstrapping using Deep Metric\\n  Learning with Humans in the Loop'},\n",
       " u'1603.09170': {'arxivid': u'1603.09170',\n",
       "  'authorsaffil': [[u'Bin Wang', None],\n",
       "   [u'Zhijian Ou', None],\n",
       "   [u'Yong He', None],\n",
       "   [u'Akinori Kawamura', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u\"The author has to withdraw the paper, as I didn't get the\\n  authorization of the third author Zhiqiang Tan\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09170v2',\n",
       "  'published': u'2016-03-30T13:09:20Z',\n",
       "  'summary': u'  The dominant language models (LMs) such as n-gram and neural network (NN)\\nmodels represent sentence probabilities in terms of conditionals. In contrast,\\na new trans-dimensional random field (TRF) LM has been recently introduced to\\nshow superior performances, where the whole sentence is modeled as a random\\nfield. In this paper, we further develop the TDF LMs with two technical\\nimprovements, which are a new method of exploiting Hessian information in\\nparameter optimization to further enhance the convergence of the training\\nalgorithm and an enabling method for training the TRF LMs on large corpus which\\nmay contain rare very long sentences. Experiments show that the TRF LMs can\\nscale to using training data of up to 32 million words, consistently achieve\\n10% relative perplexity reductions over 5-gram LMs, and perform as good as NN\\nLMs but with much faster speed in calculating sentence probabilities.\\n',\n",
       "  'title': u'Improving and Scaling Trans-dimensional Random Field Language Models'},\n",
       " u'1602.08886': {'arxivid': u'1602.08886',\n",
       "  'authorsaffil': [[u'Ravi Kumar Kolla', None],\n",
       "   [u'Krishna Jagannathan', None],\n",
       "   [u'Aditya Gopalan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'16 Pages, 4 Figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08886v1',\n",
       "  'published': u'2016-02-29T09:53:28Z',\n",
       "  'summary': u'  We consider a collaborative online learning paradigm, wherein a group of\\nagents connected through a social network are engaged in learning a Multi-Armed\\nBandit problem. Each time an agent takes an action, the corresponding reward is\\ninstantaneously observed by the agent, as well as its neighbours in the social\\nnetwork. We perform a regret analysis of various policies in this collaborative\\nlearning setting. A key finding of this paper is that appropriate network\\nextensions of widely-studied single agent learning policies do not perform well\\nin terms of regret. In particular, we identify a class of non-altruistic and\\nindividually consistent policies, which could suffer a large regret. We also\\nshow that the regret performance can be substantially improved by exploiting\\nthe network structure. Specifically, we consider a star network, which is a\\ncommon motif in hierarchical social networks, and show that the hub agent can\\nbe used as an information sink, to aid the learning rates of the entire\\nnetwork. We also present numerical experiments to corroborate our analytical\\nresults.\\n',\n",
       "  'title': u'Stochastic bandits on a social network: Collaborative learning with\\n  local information sharing'},\n",
       " u'1602.02172': {'arxivid': u'1602.02172',\n",
       "  'authorsaffil': [[u'Weiran Wang', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02172v1',\n",
       "  'published': u'2016-02-05T21:51:41Z',\n",
       "  'summary': u'  We study the problem of column selection in large-scale kernel canonical\\ncorrelation analysis (KCCA) using the Nystr\\\\\"om approximation, where one\\napproximates two positive semi-definite kernel matrices using \"landmark\" points\\nfrom the training set. When building low-rank kernel approximations in KCCA,\\nprevious work mostly samples the landmarks uniformly at random from the\\ntraining set. We propose novel strategies for sampling the landmarks\\nnon-uniformly based on a version of statistical leverage scores recently\\ndeveloped for kernel ridge regression. We study the approximation accuracy of\\nthe proposed non-uniform sampling strategy, develop an incremental algorithm\\nthat explores the path of approximation ranks and facilitates efficient model\\nselection, and derive the kernel stability of out-of-sample mapping for our\\nmethod. Experimental results on both synthetic and real-world datasets\\ndemonstrate the promise of our method.\\n',\n",
       "  'title': u'On Column Selection in Approximate Kernel Canonical Correlation Analysis'},\n",
       " u'1512.08571': {'arxivid': u'1512.08571',\n",
       "  'authorsaffil': [[u'Sajid Anwar', None],\n",
       "   [u'Kyuyeon Hwang', None],\n",
       "   [u'Wonyong Sung', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'11 pages, 8 figures, 1 table',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.08571v1',\n",
       "  'published': u'2015-12-29T01:21:08Z',\n",
       "  'summary': u'  Real time application of deep learning algorithms is often hindered by high\\ncomputational complexity and frequent memory accesses. Network pruning is a\\npromising technique to solve this problem. However, pruning usually results in\\nirregular network connections that not only demand extra representation efforts\\nbut also do not fit well on parallel computation. We introduce structured\\nsparsity at various scales for convolutional neural networks, which are channel\\nwise, kernel wise and intra kernel strided sparsity. This structured sparsity\\nis very advantageous for direct computational resource savings on embedded\\ncomputers, parallel computing environments and hardware based systems. To\\ndecide the importance of network connections and paths, the proposed method\\nuses a particle filtering approach. The importance weight of each particle is\\nassigned by computing the misclassification rate with corresponding\\nconnectivity pattern. The pruned network is re-trained to compensate for the\\nlosses due to pruning. While implementing convolutions as matrix products, we\\nparticularly show that intra kernel strided sparsity with a simple constraint\\ncan significantly reduce the size of kernel and feature map matrices. The\\npruned network is finally fixed point optimized with reduced word length\\nprecision. This results in significant reduction in the total storage size\\nproviding advantages for on-chip memory based implementations of deep neural\\nnetworks.\\n',\n",
       "  'title': u'Structured Pruning of Deep Convolutional Neural Networks'},\n",
       " u'1509.06791': {'arxivid': u'1509.06791',\n",
       "  'authorsaffil': [[u'Tianhao Zhang', None],\n",
       "   [u'Gregory Kahn', None],\n",
       "   [u'Sergey Levine', None],\n",
       "   [u'Pieter Abbeel', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.RO'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.06791v2',\n",
       "  'published': u'2015-09-22T21:27:27Z',\n",
       "  'summary': u\"  Model predictive control (MPC) is an effective method for controlling robotic\\nsystems, particularly autonomous aerial vehicles such as quadcopters. However,\\napplication of MPC can be computationally demanding, and typically requires\\nestimating the state of the system, which can be challenging in complex,\\nunstructured environments. Reinforcement learning can in principle forego the\\nneed for explicit state estimation and acquire a policy that directly maps\\nsensor readings to actions, but is difficult to apply to unstable systems that\\nare liable to fail catastrophically during training before an effective policy\\nhas been found. We propose to combine MPC with reinforcement learning in the\\nframework of guided policy search, where MPC is used to generate data at\\ntraining time, under full state observations provided by an instrumented\\ntraining environment. This data is used to train a deep neural network policy,\\nwhich is allowed to access only the raw observations from the vehicle's onboard\\nsensors. After training, the neural network policy can successfully control the\\nrobot without knowledge of the full state, and at a fraction of the\\ncomputational cost of MPC. We evaluate our method by learning obstacle\\navoidance policies for a simulated quadrotor, using simulated onboard sensors\\nand no explicit state estimation at test time.\\n\",\n",
       "  'title': u'Learning Deep Control Policies for Autonomous Aerial Vehicles with\\n  MPC-Guided Policy Search'},\n",
       " u'1603.01076': {'arxivid': u'1603.01076',\n",
       "  'authorsaffil': [[u'Gabriela Csurka', None],\n",
       "   [u'Diane Larlus', None],\n",
       "   [u'Albert Gordo', None],\n",
       "   [u'Jon Almazan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Preprint of our Pattern Recognition submission',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01076v2',\n",
       "  'published': u'2016-03-03T12:46:51Z',\n",
       "  'summary': u'  In this article we study the problem of document image representation based\\non visual features. We propose a comprehensive experimental study that compares\\nthree types of visual document image representations: (1) traditional so-called\\nshallow features, such as the RunLength and the Fisher-Vector descriptors, (2)\\ndeep features based on Convolutional Neural Networks, and (3) features\\nextracted from hybrid architectures that take inspiration from the two previous\\nones.\\n  We evaluate these features in several tasks (i.e. classification, clustering,\\nand retrieval) and in different setups (e.g. domain transfer) using several\\npublic and in-house datasets. Our results show that deep features generally\\noutperform other types of features when there is no domain shift and the new\\ntask is closely related to the one used to train the model. However, when a\\nlarge domain or task shift is present, the Fisher-Vector shallow features\\ngeneralize better and often obtain the best results.\\n',\n",
       "  'title': u'What is the right way to represent document images?'},\n",
       " u'1508.00945': {'arxivid': u'1508.00945',\n",
       "  'authorsaffil': [[u'Jean Honorio', None], [u'Tommi Jaakkola', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'Uncertainty in Artificial Intelligence (UAI) 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.00945v4',\n",
       "  'published': u'2015-08-05T00:22:39Z',\n",
       "  'summary': u'  Margin-based structured prediction commonly uses a maximum loss over all\\npossible structured outputs \\\\cite{Altun03,Collins04b,Taskar03}. In natural\\nlanguage processing, recent work \\\\cite{Zhang14,Zhang15} has proposed the use of\\nthe maximum loss over random structured outputs sampled independently from some\\nproposal distribution. This method is linear-time in the number of random\\nstructured outputs and trivially parallelizable. We study this family of loss\\nfunctions in the PAC-Bayes framework under Gaussian perturbations\\n\\\\cite{McAllester07}. Under some technical conditions and up to statistical\\naccuracy, we show that this family of loss functions produces a tighter upper\\nbound of the Gibbs decoder distortion than commonly used methods. Thus, using\\nthe maximum loss over random structured outputs is a principled way of learning\\nthe parameter of structured prediction models. Besides explaining the\\nexperimental success of \\\\cite{Zhang14,Zhang15}, our theoretical results show\\nthat more general techniques are possible.\\n',\n",
       "  'title': u'Structured Prediction: From Gaussian Perturbations to Linear-Time\\n  Principled Algorithms'},\n",
       " u'1407.4420': {'arxivid': u'1407.4420',\n",
       "  'authorsaffil': [[u'Fei Zhu', None],\n",
       "   [u'Paul Honeine', None],\n",
       "   [u'Maya Kallas', None]],\n",
       "  'categoryterms': [u'cs.CV',\n",
       "   u'cs.IT',\n",
       "   u'cs.LG',\n",
       "   u'cs.NE',\n",
       "   u'math.IT',\n",
       "   u'stat.ML'],\n",
       "  'comment': u'13 pages, 12 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1407.4420v2',\n",
       "  'published': u'2014-07-16T18:46:41Z',\n",
       "  'summary': u'  The nonnegative matrix factorization (NMF) is widely used in signal and image\\nprocessing, including bio-informatics, blind source separation and\\nhyperspectral image analysis in remote sensing. A great challenge arises when\\ndealing with a nonlinear formulation of the NMF. Within the framework of kernel\\nmachines, the models suggested in the literature do not allow the\\nrepresentation of the factorization matrices, which is a fallout of the curse\\nof the pre-image. In this paper, we propose a novel kernel-based model for the\\nNMF that does not suffer from the pre-image problem, by investigating the\\nestimation of the factorization matrices directly in the input space. For\\ndifferent kernel functions, we describe two schemes for iterative algorithms:\\nan additive update rule based on a gradient descent scheme and a multiplicative\\nupdate rule in the same spirit as in the Lee and Seung algorithm. Within the\\nproposed framework, we develop several extensions to incorporate constraints,\\nincluding sparseness, smoothness, and spatial regularization with a\\ntotal-variation-like penalty. The effectiveness of the proposed method is\\ndemonstrated with the problem of unmixing hyperspectral images, using\\nwell-known real images and results with state-of-the-art techniques.\\n',\n",
       "  'title': u'Kernel Nonnegative Matrix Factorization Without the Curse of the\\n  Pre-image - Application to Unmixing Hyperspectral Images'},\n",
       " u'1603.04733': {'arxivid': u'1603.04733',\n",
       "  'authorsaffil': [[u'Christos Louizos', None], [u'Max Welling', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': u'To appear in the International Conference on Machine Learning (ICML)\\n  2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04733v4',\n",
       "  'published': u'2016-03-15T16:01:14Z',\n",
       "  'summary': u'  We introduce a variational Bayesian neural network where the parameters are\\ngoverned via a probability distribution on random matrices. Specifically, we\\nemploy a matrix variate Gaussian \\\\cite{gupta1999matrix} parameter posterior\\ndistribution where we explicitly model the covariance among the input and\\noutput dimensions of each layer. Furthermore, with approximate covariance\\nmatrices we can achieve a more efficient way to represent those correlations\\nthat is also cheaper than fully factorized parameter posteriors. We further\\nshow that with the \"local reprarametrization trick\"\\n\\\\cite{kingma2015variational} on this posterior distribution we arrive at a\\nGaussian Process \\\\cite{rasmussen2006gaussian} interpretation of the hidden\\nunits in each layer and we, similarly with \\\\cite{gal2015dropout}, provide\\nconnections with deep Gaussian processes. We continue in taking advantage of\\nthis duality and incorporate \"pseudo-data\" \\\\cite{snelson2005sparse} in our\\nmodel, which in turn allows for more efficient sampling while maintaining the\\nproperties of the original model. The validity of the proposed approach is\\nverified through extensive experiments.\\n',\n",
       "  'title': u'Structured and Efficient Variational Deep Learning with Matrix Gaussian\\n  Posteriors'},\n",
       " u'1603.06541': {'arxivid': u'1603.06541',\n",
       "  'authorsaffil': [[u'Ping Li', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06541v1',\n",
       "  'published': u'2016-03-21T19:11:50Z',\n",
       "  'summary': u'  In this paper, we compare 5 different nonlinear kernels: min-max, RBF, fRBF\\n(folded RBF), acos, and acos-$\\\\chi^2$, on a wide range of publicly available\\ndatasets. The proposed fRBF kernel performs very similarly to the RBF kernel.\\nBoth RBF and fRBF kernels require an important tuning parameter ($\\\\gamma$).\\nInterestingly, for a significant portion of the datasets, the min-max kernel\\noutperforms the best-tuned RBF/fRBF kernels. The acos kernel and acos-$\\\\chi^2$\\nkernel also perform well in general and in some datasets achieve the best\\naccuracies.\\n  One crucial issue with the use of nonlinear kernels is the excessive\\ncomputational and memory cost. These days, one increasingly popular strategy is\\nto linearize the kernels through various randomization algorithms. In our\\nstudy, the randomization method for the min-max kernel demonstrates excellent\\nperformance compared to the randomization methods for other types of nonlinear\\nkernels, measured in terms of the number of nonzero terms in the transformed\\ndataset.\\n  Our study provides evidence for supporting the use of the min-max kernel and\\nthe corresponding randomized linearization method (i.e., the so-called \"0-bit\\nCWS\"). Furthermore, the results motivate at least two directions for future\\nresearch: (i) To develop new (and linearizable) nonlinear kernels for better\\naccuracies; and (ii) To develop better linearization algorithms for improving\\nthe current linearization methods for the RBF kernel, the acos kernel, and the\\nacos-$\\\\chi^2$ kernel. One attempt is to combine the min-max kernel with the\\nacos kernel or the acos-$\\\\chi^2$ kernel. The advantages of these two new and\\ntuning-free nonlinear kernels are demonstrated vias our extensive experiments.\\n',\n",
       "  'title': u'A Comparison Study of Nonlinear Kernels'},\n",
       " u'1602.02499': {'arxivid': u'1602.02499',\n",
       "  'authorsaffil': [[u'David A. van Leeuwen', None], [u'Rosemary Orr', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CL'],\n",
       "  'comment': u'Accepted to Speaker and Language Recognition Odyssey 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02499v2',\n",
       "  'published': u'2016-02-08T09:23:19Z',\n",
       "  'summary': u'  This paper describes the data collection effort that is part of the project\\nSprekend Nederland (The Netherlands Talking), and discusses its potential use\\nin Automatic Accent Location. We define Automatic Accent Location as the task\\nto describe the accent of a speaker in terms of the location of the speaker and\\nits history. We discuss possible ways of describing accent location, the\\nconsequence these have for the task of automatic accent location, and potential\\nevaluation metrics.\\n',\n",
       "  'title': u'The \"Sprekend Nederland\" project and its application to accent location'},\n",
       " u'1602.08124': {'arxivid': u'1602.08124',\n",
       "  'authorsaffil': [[u'Minsoo Rhu', None],\n",
       "   [u'Natalia Gimelshein', None],\n",
       "   [u'Jason Clemons', None],\n",
       "   [u'Arslan Zulfiqar', None],\n",
       "   [u'Stephen W. Keckler', None]],\n",
       "  'categoryterms': [u'cs.DC', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08124v2',\n",
       "  'published': u'2016-02-25T21:31:55Z',\n",
       "  'summary': u\"  The most widely used machine learning frameworks require users to carefully\\ntune their memory usage so that the deep neural network (DNN) fits into the\\nDRAM capacity of a GPU. This restriction hampers a researcher's flexibility to\\nstudy different machine learning algorithms, forcing them to either use a less\\ndesirable network architecture or parallelize the processing across multiple\\nGPUs. We propose a runtime memory manager that virtualizes the memory usage of\\nDNNs such that both GPU and CPU memory can simultaneously be utilized for\\ntraining larger DNNs. Our virtualized DNN (vDNN) reduces the average memory\\nusage of AlexNet by 61% and OverFeat by 83%, a significant reduction in memory\\nrequirements of DNNs. Similar experiments on VGG-16, one of the deepest and\\nmemory hungry DNNs to date, demonstrate the memory-efficiency of our proposal.\\nvDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be\\ntrained on a single NVIDIA K40 GPU card containing 12 GB of memory, with 22%\\nperformance loss compared to a hypothetical GPU with enough memory to hold the\\nentire DNN.\\n\",\n",
       "  'title': u'Virtualizing Deep Neural Networks for Memory-Efficient Neural Network\\n  Design'},\n",
       " u'1604.03334': {'arxivid': u'1604.03334',\n",
       "  'authorsaffil': [[u'Qi Ye', None],\n",
       "   [u'Shanxin Yuan', None],\n",
       "   [u'Tae-Kyun Kim', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Demo video: https://youtu.be/2Hg0c88rHkk',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03334v1',\n",
       "  'published': u'2016-04-12T10:47:12Z',\n",
       "  'summary': u'  Discriminative methods often generate hand poses kinematically implausible,\\nthen generative methods are used to correct (or verify) these results in a\\nhybrid method. Estimating 3D hand pose in a hierarchy, where the\\nhigh-dimensional output space is decomposed into smaller ones, has been shown\\neffective. Existing hierarchical methods mainly focus on the decomposition of\\nthe output space while the input space remains almost the same along the\\nhierarchy. In this paper, a hybrid hand pose estimation method is proposed by\\napplying the kinematic hierarchy strategy to the input space (as well as the\\noutput space) of the discriminative method by a spatial attention mechanism and\\nto the optimization of the generative method by hierarchical Particle Swarm\\nOptimization (PSO). The spatial attention mechanism integrates cascaded and\\nhierarchical regression into a CNN framework by transforming both the input(and\\nfeature space) and the output space, which greatly reduces the viewpoint and\\narticulation variations. Between the levels in the hierarchy, the hierarchical\\nPSO forces the kinematic constraints to the results of the CNNs. The\\nexperimental results show that our method significantly outperforms four\\nstate-of-the-art methods and three baselines on three public benchmarks.\\n',\n",
       "  'title': u'Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand\\n  Pose Estimation'},\n",
       " u'1603.07893': {'arxivid': u'1603.07893',\n",
       "  'authorsaffil': [[u'Hengjian Jia', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG'],\n",
       "  'comment': u'7 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07893v2',\n",
       "  'published': u'2016-03-25T12:28:02Z',\n",
       "  'summary': u'  The effectiveness of long short term memory networks trained by\\nbackpropagation through time for stock price prediction is explored in this\\npaper. A range of different architecture LSTM networks are constructed trained\\nand tested.\\n',\n",
       "  'title': u'Investigation Into The Effectiveness Of Long Short Term Memory Networks\\n  For Stock Price Prediction'},\n",
       " u'1505.02074': {'arxivid': u'1505.02074',\n",
       "  'authorsaffil': [[u'Mengye Ren', None],\n",
       "   [u'Ryan Kiros', None],\n",
       "   [u'Richard Zemel', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'cs.CL', u'cs.CV'],\n",
       "  'comment': u'12 pages. Conference paper at NIPS 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.02074v4',\n",
       "  'published': u'2015-05-08T15:59:44Z',\n",
       "  'summary': u'  This work aims to address the problem of image-based question-answering (QA)\\nwith new models and datasets. In our work, we propose to use neural networks\\nand visual semantic embeddings, without intermediate stages such as object\\ndetection and image segmentation, to predict answers to simple questions about\\nimages. Our model performs 1.8 times better than the only published results on\\nan existing image QA dataset. We also present a question generation algorithm\\nthat converts image descriptions, which are widely available, into QA form. We\\nused this algorithm to produce an order-of-magnitude larger dataset, with more\\nevenly distributed answers. A suite of baseline results on this new dataset are\\nalso presented.\\n',\n",
       "  'title': u'Exploring Models and Data for Image Question Answering'},\n",
       " u'1601.07224': {'arxivid': u'1601.07224',\n",
       "  'authorsaffil': [[u'Yura N Perov', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.PL'],\n",
       "  'comment': u'49 pages, in Russian',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07224v1',\n",
       "  'published': u'2016-01-26T23:31:05Z',\n",
       "  'summary': u\"  This Bachelor's thesis, written in Russian, is devoted to a relatively new\\ndirection in the field of machine learning and artificial intelligence, namely\\nprobabilistic programming. The thesis gives a brief overview to the already\\nexisting probabilistic programming languages: Church, Venture, and Anglican. It\\nalso describes the results of the first experiments on the automatic induction\\nof probabilistic programs. The thesis was submitted, in June 2014, in partial\\nfulfilment of the requirements for the degree of Bachelor of Science in\\nMathematics in the Department of Mathematics and Computer Science, Siberian\\nFederal University, Krasnoyarsk, Russia. The work, which is described in this\\nthesis, has been performing in 2012-2014 in the Massachusetts Institute of\\nTechnology and in the University of Oxford by the colleagues of the author and\\nby himself.\\n\",\n",
       "  'title': u\"Bachelor's thesis on generative probabilistic programming (in Russian\\n  language, June 2014)\"},\n",
       " u'1601.07227': {'arxivid': u'1601.07227',\n",
       "  'authorsaffil': [[u'Veit Elser', None]],\n",
       "  'categoryterms': [u'math.NA', u'cs.NE'],\n",
       "  'comment': u'15 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07227v1',\n",
       "  'published': u'2016-01-26T23:45:56Z',\n",
       "  'summary': u'  We study neural networks whose only non-linear components are multipliers, to\\ntest a new training rule in a context where the precise representation of data\\nis paramount. These networks are challenged to discover the rules of matrix\\nmultiplication, given many examples. By limiting the number of multipliers, the\\nnetwork is forced to discover the Strassen multiplication rules. This is the\\nmathematical equivalent of finding low rank decompositions of the $n\\\\times n$\\nmatrix multiplication tensor, $M_n$. We train these networks with the\\nconservative learning rule, which makes minimal changes to the weights so as to\\ngive the correct output for each input at the time the input-output pair is\\nreceived. Conservative learning needs a few thousand examples to find the rank\\n7 decomposition of $M_2$, and $10^5$ for the rank 23 decomposition of $M_3$\\n(the lowest known). High precision is critical, especially for $M_3$, to\\ndiscriminate between true decompositions and \"border approximations\".\\n',\n",
       "  'title': u'A network that learns Strassen multiplication'},\n",
       " u'1604.01962': {'arxivid': u'1604.01962',\n",
       "  'authorsaffil': [[u'Akshay Gadi Patil', None],\n",
       "   [u'Shanmuganathan Raman', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'6 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01962v4',\n",
       "  'published': u'2016-04-07T11:31:03Z',\n",
       "  'summary': u'  Non-photorealistic rendering techniques work on image features and often\\nmanipulate a set of characteristics such as edges and texture to achieve a\\ndesired depiction of the scene. Most computational photography methods\\ndecompose an image using edge preserving filters and work on the resulting base\\nand detail layers independently to achieve desired visual effects. We propose a\\nnew approach for content-aware non-photorealistic rendering of images where we\\nmanipulate the visually salient and the non-salient regions separately. We\\npropose a novel content-aware framework in order to render an image for\\napplications such as detail exaggeration, artificial blurring and image\\nabstraction. The processed regions of the image are blended seamlessly for all\\nthese applications. We demonstrate that content awareness of the proposed\\nmethod leads to automatic generation of non-photorealistic rendering of the\\nsame image for the different applications mentioned above.\\n',\n",
       "  'title': u'Automatic Content-aware Non-Photorealistic Rendering of Images'},\n",
       " u'1603.07625': {'arxivid': u'1603.07625',\n",
       "  'authorsaffil': [[u'Stephen Yu', None], [u'Mike Wu', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07625v1',\n",
       "  'published': u'2016-03-24T15:28:26Z',\n",
       "  'summary': u\"  The proposed method uses live image footage which, based on calculations of\\npixel motion, decides whether or not an object is in the blind-spot. If found,\\nthe driver is notified by a sensory light or noise built into the vehicle's\\nCPU. The new technology incorporates optical vectors and flow fields rather\\nthan expensive radar-waves, creating cheaper detection systems that retain the\\nneeded accuracy while adapting to the current processor speeds.\\n\",\n",
       "  'title': u'Position and Vector Detection of Blind Spot motion with the Horn-Schunck\\n  Optical Flow'},\n",
       " u'1506.06129': {'arxivid': u'1506.06129',\n",
       "  'authorsaffil': [[u'Paul A. Wiggins', None]],\n",
       "  'categoryterms': [u'physics.data-an', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'7 Pages, 1 figure, & Appendix. arXiv admin note: text overlap with\\n  arXiv:1506.05855',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.06129v1',\n",
       "  'published': u'2015-06-19T00:39:43Z',\n",
       "  'summary': u'  We have recently proposed a new information-based approach to model\\nselection, the Frequentist Information Criterion (FIC), that reconciles\\ninformation-based and frequentist inference. The purpose of this current paper\\nis to provide a simple example of the application of this criterion and a\\ndemonstration of the natural emergence of model complexities with both AIC-like\\n($N^0$) and BIC-like ($\\\\log N$) scaling with observation number $N$. The\\napplication developed is deliberately simplified to make the analysis\\nanalytically tractable.\\n',\n",
       "  'title': u'A simple application of FIC to model selection'},\n",
       " u'1602.02543': {'arxivid': u'1602.02543',\n",
       "  'authorsaffil': [[u'Brijnesh J. Jain', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02543v1',\n",
       "  'published': u'2016-02-08T12:28:57Z',\n",
       "  'summary': u'  The expectation and the mean of partitions generated by a cluster ensemble\\nare not unique in general. This issue poses challenges in statistical inference\\nand cluster stability. In this contribution, we state sufficient conditions for\\nuniqueness of expectation and mean. The proposed conditions show that a unique\\nmean is neither exceptional nor generic. To cope with this issue, we introduce\\nhomogeneity as a measure of how likely is a unique mean for a sample of\\npartitions. We show that homogeneity is related to cluster stability. This\\nresult points to a possible conflict between cluster stability and diversity in\\nconsensus clustering. To assess homogeneity in a practical setting, we propose\\nan efficient way to compute a lower bound of homogeneity. Empirical results\\nusing the k-means algorithm suggest that uniqueness of the mean partition is\\nnot exceptional for real-world data. Moreover, for samples of high homogeneity,\\nuniqueness can be enforced by increasing the number of data points or by\\nremoving outlier partitions. In a broader context, this contribution can be\\nplaced as a further step towards a statistical theory of partitions.\\n',\n",
       "  'title': u'Homogeneity of Cluster Ensembles'},\n",
       " u'1512.09354': {'arxivid': u'1512.09354',\n",
       "  'authorsaffil': [[u\"Fabio D'Andreagiovanni\", None],\n",
       "   [u'Fabian Mett', None],\n",
       "   [u'Jonad Pulaj', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.AI', u'cs.NI'],\n",
       "  'comment': u'Accepted for publication in Proc. of EvoStar - EvoApplications 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.09354v2',\n",
       "  'published': u'2015-12-31T19:53:16Z',\n",
       "  'summary': u'  We investigate the 3-architecture Connected Facility Location Problem arising\\nin the design of urban telecommunication access networks. We propose an\\noriginal optimization model for the problem that includes additional variables\\nand constraints to take into account wireless signal coverage. Since the\\nproblem can prove challenging even for modern state-of-the art optimization\\nsolvers, we propose to solve it by an original primal heuristic which combines\\na probabilistic fixing procedure, guided by peculiar Linear Programming\\nrelaxations, with an exact MIP heuristic, based on a very large neighborhood\\nsearch. Computational experiments on a set of realistic instances show that our\\nheuristic can find solutions associated with much lower optimality gaps than a\\nstate-of-the-art solver.\\n',\n",
       "  'title': u'An (MI)LP-based Primal Heuristic for 3-Architecture Connected Facility\\n  Location in Urban Access Network Design'},\n",
       " u'1602.08977': {'arxivid': u'1602.08977',\n",
       "  'authorsaffil': [[u'Crist\\xf3bal Mackenzie', None],\n",
       "   [u'Karim Pichara', None],\n",
       "   [u'Pavlos Protopapas', None]],\n",
       "  'categoryterms': [u'astro-ph.SR', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.3847/0004-637X/820/2/138',\n",
       "  'journalref': u'ApJ 820 (2016) 138',\n",
       "  'link': u'http://arxiv.org/abs/1602.08977v1',\n",
       "  'published': u'2016-02-29T14:26:17Z',\n",
       "  'summary': u'  The success of automatic classification of variable stars strongly depends on\\nthe lightcurve representation. Usually, lightcurves are represented as a vector\\nof many statistical descriptors designed by astronomers called features. These\\ndescriptors commonly demand significant computational power to calculate,\\nrequire substantial research effort to develop and do not guarantee good\\nperformance on the final classification task. Today, lightcurve representation\\nis not entirely automatic; algorithms that extract lightcurve features are\\ndesigned by humans and must be manually tuned up for every survey. The vast\\namounts of data that will be generated in future surveys like LSST mean\\nastronomers must develop analysis pipelines that are both scalable and\\nautomated. Recently, substantial efforts have been made in the machine learning\\ncommunity to develop methods that prescind from expert-designed and manually\\ntuned features for features that are automatically learned from data. In this\\nwork we present what is, to our knowledge, the first unsupervised feature\\nlearning algorithm designed for variable stars. Our method first extracts a\\nlarge number of lightcurve subsequences from a given set of photometric data,\\nwhich are then clustered to find common local patterns in the time series.\\nRepresentatives of these patterns, called exemplars, are then used to transform\\nlightcurves of a labeled set into a new representation that can then be used to\\ntrain an automatic classifier. The proposed algorithm learns the features from\\nboth labeled and unlabeled lightcurves, overcoming the bias generated when the\\nlearning process is done only with labeled data. We test our method on MACHO\\nand OGLE datasets; the results show that the classification performance we\\nachieve is as good and in some cases better than the performance achieved using\\ntraditional features, while the computational cost is significantly lower.\\n',\n",
       "  'title': u'Clustering Based Feature Learning on Variable Stars'},\n",
       " u'1602.09118': {'arxivid': u'1602.09118',\n",
       "  'authorsaffil': [[u'Joshua Achiam', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.09118v1',\n",
       "  'published': u'2016-02-29T19:59:16Z',\n",
       "  'summary': u'  A key problem in reinforcement learning for control with general function\\napproximators (such as deep neural networks and other nonlinear functions) is\\nthat, for many algorithms employed in practice, updates to the policy or\\n$Q$-function may fail to improve performance---or worse, actually cause the\\npolicy performance to degrade. Prior work has addressed this for policy\\niteration by deriving tight policy improvement bounds; by optimizing the lower\\nbound on policy improvement, a better policy is guaranteed. However, existing\\napproaches suffer from bounds that are hard to optimize in practice because\\nthey include sup norm terms which cannot be efficiently estimated or\\ndifferentiated. In this work, we derive a better policy improvement bound where\\nthe sup norm of the policy divergence has been replaced with an average\\ndivergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that\\ngenerates sequences of policies with guaranteed non-decreasing returns and is\\neasy to implement in a sample-based framework.\\n',\n",
       "  'title': u'Easy Monotonic Policy Iteration'},\n",
       " u'1602.02089': {'arxivid': u'1602.02089',\n",
       "  'authorsaffil': [[u'Martha Lewis', None], [u'Bob Coecke', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.CL'],\n",
       "  'comment': u'Abstract, Advances in Distributional Semantics, IWCS',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02089v1',\n",
       "  'published': u'2016-02-05T16:40:44Z',\n",
       "  'summary': u\"  The model of cognition developed in (Smolensky and Legendre, 2006) seeks to\\nunify two levels of description of the cognitive process: the connectionist and\\nthe symbolic. The theory developed brings together these two levels into the\\nIntegrated Connectionist/Symbolic Cognitive architecture (ICS). Clark and\\nPulman (2007) draw a parallel with semantics where meaning may be modelled on\\nboth distributional and symbolic levels, developed by Coecke et al, 2010 into\\nthe Distributional Compositional (DisCo) model of meaning. In the current work,\\nwe revisit Smolensky and Legendre (S&L)'s model. We describe the DisCo\\nframework, summarise the key ideas in S&L's architecture, and describe how\\ntheir description of harmony as a graded measure of grammaticality may be\\napplied in the DisCo model.\\n\",\n",
       "  'title': u'Harmonic Grammar in a DisCo Model of Meaning'},\n",
       " u'1602.03146': {'arxivid': u'1602.03146',\n",
       "  'authorsaffil': [[u'Sumeet Katariya', None],\n",
       "   [u'Branislav Kveton', None],\n",
       "   [u'Csaba Szepesv\\xe1ri', None],\n",
       "   [u'Zheng Wen', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'Proceedings of the 33rd International Conference on Machine Learning',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03146v2',\n",
       "  'published': u'2016-02-09T20:03:30Z',\n",
       "  'summary': u'  A search engine recommends to the user a list of web pages. The user examines\\nthis list, from the first page to the last, and clicks on all attractive pages\\nuntil the user is satisfied. This behavior of the user can be described by the\\ndependent click model (DCM). We propose DCM bandits, an online learning variant\\nof the DCM where the goal is to maximize the probability of recommending\\nsatisfactory items, such as web pages. The main challenge of our learning\\nproblem is that we do not observe which attractive item is satisfactory. We\\npropose a computationally-efficient learning algorithm for solving our problem,\\ndcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable\\nassumptions; and also prove a matching lower bound up to logarithmic factors.\\nWe evaluate our algorithm on synthetic and real-world problems, and show that\\nit performs well even when our model is misspecified. This work presents the\\nfirst practical and regret-optimal online algorithm for learning to rank with\\nmultiple clicks in a cascade-like click model.\\n',\n",
       "  'title': u'DCM Bandits: Learning to Rank with Multiple Clicks'},\n",
       " u'1602.03145': {'arxivid': u'1602.03145',\n",
       "  'authorsaffil': [[u'Guillaume Noyel', u'CMM'],\n",
       "   [u'Jesus Angulo', u'CMM'],\n",
       "   [u'Dominique Jeulin', u'CMM']],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1080/01431161.2010.512314',\n",
       "  'journalref': u'International Journal of Remote Sensing, Taylor \\\\& Francis, 2010,\\n  31 (22), pp.5895-5920',\n",
       "  'link': u'http://arxiv.org/abs/1602.03145v1',\n",
       "  'published': u'2016-02-09T20:02:00Z',\n",
       "  'summary': u'  A general framework of spatio-spectral segmentation for multi-spectral images\\nis introduced in this paper. The method is based on classification-driven\\nstochastic watershed (WS) by Monte Carlo simulations, and it gives more regular\\nand reliable contours than standard WS. The present approach is decomposed into\\nseveral sequential steps. First, a dimensionality-reduction stage is performed\\nusing the factor-correspondence analysis method. In this context, a new way to\\nselect the factor axes (eigenvectors) according to their spatial information is\\nintroduced. Then, a spectral classification produces a spectral\\npre-segmentation of the image. Subsequently, a probability density function\\n(pdf) of contours containing spatial and spectral information is estimated by\\nsimulation using a stochastic WS approach driven by the spectral\\nclassification. The pdf of the contours is finally segmented by a WS controlled\\nby markers from a regularization of the initial classification.\\n',\n",
       "  'title': u'A New Spatio-Spectral Morphological Segmentation For Multi-Spectral\\n  Remote-Sensing Images'},\n",
       " u'1604.02126': {'arxivid': u'1604.02126',\n",
       "  'authorsaffil': [[u'Gavin Rens', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'Presented at the Sixteenth International Workshop on Non-Monotonic\\n  Reasoning, 22-24 April 2016, Cape Town, South Africa. 10 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.02126v1',\n",
       "  'published': u'2016-04-07T19:28:00Z',\n",
       "  'summary': u'  I propose a framework for an agent to change its probabilistic beliefs when a\\nnew piece of propositional information $\\\\alpha$ is observed. Traditionally,\\nbelief change occurs by either a revision process or by an update process,\\ndepending on whether the agent is informed with $\\\\alpha$ in a static world or,\\nrespectively, whether $\\\\alpha$ is a \\'signal\\' from the environment due to an\\nevent occurring. Boutilier suggested a unified model of qualitative belief\\nchange, which \"combines aspects of revision and update, providing a more\\nrealistic characterization of belief change.\" In this paper, I propose a\\nunified model of quantitative belief change, where an agent\\'s beliefs are\\nrepresented as a probability distribution over possible worlds. As does\\nBoutilier, I take a dynamical systems perspective. The proposed approach is\\nevaluated against several rationality postulated, and some properties of the\\napproach are worked out.\\n',\n",
       "  'title': u'On Stochastic Belief Revision and Update and their Combination'},\n",
       " u'1602.06462': {'arxivid': u'1602.06462',\n",
       "  'authorsaffil': [[u'Toby Walsh', None]],\n",
       "  'categoryterms': [u'cs.AI', u'I.2.0'],\n",
       "  'comment': u'Under review',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06462v1',\n",
       "  'published': u'2016-02-20T21:09:07Z',\n",
       "  'summary': u'  There is both much optimism and pessimism around artificial intelligence (AI)\\ntoday. The optimists are investing millions of dollars, and even in some cases\\nbillions of dollars into AI. The pessimists, on the other hand, predict that AI\\nwill end many things: jobs, warfare, and even the human race. Both the\\noptimists and the pessimists often appeal to the idea of a technological\\nsingularity, a point in time where machine intelligence starts to run away, and\\na new, more intelligent species starts to inhabit the earth. If the optimists\\nare right, this will be a moment that fundamentally changes our economy and our\\nsociety. If the pessimists are right, this will be a moment that also\\nfundamentally changes our economy and our society. It is therefore very\\nworthwhile spending some time deciding if either of them might be right.\\n',\n",
       "  'title': u'The Singularity May Never Be Near'},\n",
       " u'1604.03225': {'arxivid': u'1604.03225',\n",
       "  'authorsaffil': [[u'Deepak Ghimire', None], [u'Joonwhoan Lee', None]],\n",
       "  'categoryterms': [u'cs.CV', u'68T01', u'I.4; I.5'],\n",
       "  'comment': u'21 pages, Sensors Journal, facial expression recognition',\n",
       "  'doi': u'10.3390/s130607714',\n",
       "  'journalref': u'Sensors 2013, 13, 7714-7734',\n",
       "  'link': u'http://arxiv.org/abs/1604.03225v1',\n",
       "  'published': u'2016-04-12T03:00:13Z',\n",
       "  'summary': u'  Facial expressions are widely used in the behavioral interpretation of\\nemotions, cognitive science, and social interactions. In this paper, we present\\na novel method for fully automatic facial expression recognition in facial\\nimage sequences. As the facial expression evolves over time facial landmarks\\nare automatically tracked in consecutive video frames, using displacements\\nbased on elastic bunch graph matching displacement estimation. Feature vectors\\nfrom individual landmarks, as well as pairs of landmarks tracking results are\\nextracted, and normalized, with respect to the first frame in the sequence. The\\nprototypical expression sequence for each class of facial expression is formed,\\nby taking the median of the landmark tracking results from the training facial\\nexpression sequences. Multi-class AdaBoost with dynamic time warping similarity\\ndistance between the feature vector of input facial expression and prototypical\\nfacial expression, is used as a weak classifier to select the subset of\\ndiscriminative feature vectors. Finally, two methods for facial expression\\nrecognition are presented, either by using multi-class AdaBoost with dynamic\\ntime warping, or by using support vector machine on the boosted feature\\nvectors. The results on the Cohn-Kanade (CK+) facial expression database show a\\nrecognition accuracy of 95.17% and 97.35% using multi-class AdaBoost and\\nsupport vector machines, respectively.\\n',\n",
       "  'title': u'Geometric Feature-Based Facial Expression Recognition in Image Sequences\\n  Using Multi-Class AdaBoost and Support Vector Machines'},\n",
       " u'1602.05112': {'arxivid': u'1602.05112',\n",
       "  'authorsaffil': [[u'Hongteng Xu', None],\n",
       "   [u'Weichang Wu', None],\n",
       "   [u'Shamim Nemati', None],\n",
       "   [u'Hongyuan Zha', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05112v1',\n",
       "  'published': u'2016-02-14T21:29:29Z',\n",
       "  'summary': u\"  Over the past decade the rate of intensive care unit (ICU) use in the United\\nStates has been increasing, with a recent study reporting almost one in three\\nMedicare beneficiaries experiencing an ICU visit during the last month of their\\nlives. With an aging population and ever-growing demand for critical care,\\neffective management of patient flow and transition among different care\\nfacilities will prove indispensible for shortening lengths of hospital stays,\\nimproving patient outcomes, allocating critical resources, and reducing\\npreventable re-admissions. In this paper, we focus on a new problem of\\npredicting the so-called ICU patient flow from longitudinal electronic health\\nrecords (EHRs), which is not explored via existing machine learning techniques.\\nBy treating a sequence of transition events as a point process, we develop a\\nnovel framework for modeling patient flow through various ICU care units and\\npredict patients' destination ICUs and duration days jointly. Instead of\\nlearning a generative point process model via maximum likelihood estimation, we\\npropose a novel discriminative learning algorithm aiming at improving the\\nprediction of transition events. By parameterizing the proposed model as a\\nmutually-correcting process, we formulate the problem as a generalized linear\\nmodel, i.e., multinomial logistic regression, which yields itself to efficient\\nlearning via alternating direction method of multipliers (ADMM). Furthermore,\\nwe achieve simultaneous feature selection and learning by adding a group-lasso\\nregularizer to the ADMM algorithm. Using real-world data of ICU patients, we\\nshow that our method obtains superior performance in terms of accuracy of\\npredicting the destination ICU transition and duration of each ICU occupancy.\\n\",\n",
       "  'title': u'ICU Patient Flow Prediction via Discriminative Learning of\\n  Mutually-Correcting Processes'},\n",
       " u'1602.02722': {'arxivid': u'1602.02722',\n",
       "  'authorsaffil': [[u'Akshay Krishnamurthy', None],\n",
       "   [u'Alekh Agarwal', None],\n",
       "   [u'John Langford', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02722v3',\n",
       "  'published': u'2016-02-08T20:12:50Z',\n",
       "  'summary': u'  We propose and study a new tractable model for reinforcement learning with\\nrich observations called Contextual-MDPs, generalizing contextual bandits to\\nsequential decision making. These models require an agent to take actions based\\non observations (features) with the goal of achieving long-term performance\\ncompetitive with a large set of policies. To avoid barriers to sample-efficient\\nlearning associated with large observation spaces and general POMDPs,\\nContextual-MDPs can be summarized by a small number of hidden states and\\nlong-term rewards are predictable by a reactive function class. In this\\nsetting, we design a new reinforcement learning algorithm that engages in\\nglobal exploration and analyze its sample complexity. We prove that the\\nalgorithm learns near optimal behavior after a number of episodes that is\\npolynomial in all relevant parameters, logarithmic in the number of policies,\\nand independent of the size of the observation space. This represents an\\nexponential improvement over all existing alternative approaches and provides\\ntheoretical justification for reinforcement learning with function\\napproximation.\\n',\n",
       "  'title': u'Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations'},\n",
       " u'1602.02726': {'arxivid': u'1602.02726',\n",
       "  'authorsaffil': [[u'Patrick R. Johnstone', None], [u'Pierre Moulin', None]],\n",
       "  'categoryterms': [u'math.OC', u'cs.LG', u'math.NA'],\n",
       "  'comment': u'33 pages 1 figure',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.02726v1',\n",
       "  'published': u'2016-02-08T20:27:19Z',\n",
       "  'summary': u'  This paper is concerned with convex composite minimization problems in a\\nHilbert space. In these problems, the objective is the sum of two closed,\\nproper, and convex functions where one is smooth and the other admits a\\ncomputationally inexpensive proximal operator. We analyze a general family of\\ninertial proximal splitting algorithms (GIPSA) for solving such problems. We\\nestablish finiteness of the sum of squared increments of the iterates and\\noptimality of the accumulation points. Weak convergence of the entire sequence\\nthen follows if the minimum is attained. Our analysis unifies and extends\\nseveral previous results.\\n  We then focus on $\\\\ell_1$-regularized optimization, which is the ubiquitous\\nspecial case where the nonsmooth term is the $\\\\ell_1$-norm. For certain\\nparameter choices, GIPSA is amenable to a local analysis for this problem. For\\nthese choices we show that GIPSA achieves finite \"active manifold\\nidentification\", i.e. convergence in a finite number of iterations to the\\noptimal support and sign, after which GIPSA reduces to minimizing a local\\nsmooth function. Local linear convergence then holds under certain conditions.\\nWe determine the rate in terms of the inertia, stepsize, and local curvature.\\nOur local analysis is applicable to certain recent variants of the Fast\\nIterative Shrinkage-Thresholding Algorithm (FISTA), for which we establish\\nactive manifold identification and local linear convergence. Our analysis\\nmotivates the use of a momentum restart scheme in these FISTA variants to\\nobtain the optimal local linear convergence rate.\\n',\n",
       "  'title': u'Local and Global Convergence of a General Inertial Proximal Splitting\\n  Scheme'},\n",
       " u'1502.03473': {'arxivid': u'1502.03473',\n",
       "  'authorsaffil': [[u'Shuai Li', None],\n",
       "   [u'Alexandros Karatzoglou', None],\n",
       "   [u'Claudio Gentile', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'stat.ML'],\n",
       "  'comment': u'The 39th SIGIR (SIGIR 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.03473v7',\n",
       "  'published': u'2015-02-11T22:28:14Z',\n",
       "  'summary': u'  Classical collaborative filtering, and content-based filtering methods try to\\nlearn a static recommendation model given training data. These approaches are\\nfar from ideal in highly dynamic recommendation domains such as news\\nrecommendation and computational advertisement, where the set of items and\\nusers is very fluid. In this work, we investigate an adaptive clustering\\ntechnique for content recommendation based on exploration-exploitation\\nstrategies in contextual multi-armed bandit settings. Our algorithm takes into\\naccount the collaborative effects that arise due to the interaction of the\\nusers with the items, by dynamically grouping users based on the items under\\nconsideration and, at the same time, grouping items based on the similarity of\\nthe clusterings induced over the users. The resulting algorithm thus takes\\nadvantage of preference patterns in the data in a way akin to collaborative\\nfiltering methods. We provide an empirical analysis on medium-size real-world\\ndatasets, showing scalability and increased prediction performance (as measured\\nby click-through rate) over state-of-the-art methods for clustering bandits. We\\nalso provide a regret analysis within a standard linear stochastic noise\\nsetting.\\n',\n",
       "  'title': u'Collaborative Filtering Bandits'},\n",
       " u'1604.03227': {'arxivid': u'1604.03227',\n",
       "  'authorsaffil': [[u'Jason Kuen', None],\n",
       "   [u'Zhenhua Wang', None],\n",
       "   [u'Gang Wang', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03227v1',\n",
       "  'published': u'2016-04-12T03:03:04Z',\n",
       "  'summary': u'  Convolutional-deconvolution networks can be adopted to perform end-to-end\\nsaliency detection. But, they do not work well with objects of multiple scales.\\nTo overcome such a limitation, in this work, we propose a recurrent attentional\\nconvolutional-deconvolution network (RACDNN). Using spatial transformer and\\nrecurrent network units, RACDNN is able to iteratively attend to selected image\\nsub-regions to perform saliency refinement progressively. Besides tackling the\\nscale problem, RACDNN can also learn context-aware features from past\\niterations to enhance saliency refinement in future iterations. Experiments on\\nseveral challenging saliency detection datasets validate the effectiveness of\\nRACDNN, and show that RACDNN outperforms state-of-the-art saliency detection\\nmethods.\\n',\n",
       "  'title': u'Recurrent Attentional Networks for Saliency Detection'},\n",
       " u'1511.08400': {'arxivid': u'1511.08400',\n",
       "  'authorsaffil': [[u'David Krueger', None], [u'Roland Memisevic', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.CL', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.08400v7',\n",
       "  'published': u'2015-11-26T14:35:27Z',\n",
       "  'summary': u\"  We stabilize the activations of Recurrent Neural Networks (RNNs) by\\npenalizing the squared distance between successive hidden states' norms.\\n  This penalty term is an effective regularizer for RNNs including LSTMs and\\nIRNNs, improving performance on character-level language modeling and phoneme\\nrecognition, and outperforming weight noise and dropout.\\n  We achieve competitive performance (18.6\\\\% PER) on the TIMIT phoneme\\nrecognition task for RNNs evaluated without beam search or an RNN transducer.\\n  With this penalty term, IRNN can achieve similar performance to LSTM on\\nlanguage modeling, although adding the penalty term to the LSTM results in\\nsuperior performance.\\n  Our penalty term also prevents the exponential growth of IRNN's activations\\noutside of their training horizon, allowing them to generalize to much longer\\nsequences.\\n\",\n",
       "  'title': u'Regularizing RNNs by Stabilizing Activations'},\n",
       " u'1511.04377': {'arxivid': u'1511.04377',\n",
       "  'authorsaffil': [[u'Adam W. Harley', None],\n",
       "   [u'Konstantinos G. Derpanis', None],\n",
       "   [u'Iasonas Kokkinos', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04377v3',\n",
       "  'published': u'2015-11-13T17:32:11Z',\n",
       "  'summary': u'  This paper proposes a new deep convolutional neural network (DCNN)\\narchitecture that learns pixel embeddings, such that pairwise distances between\\nthe embeddings can be used to infer whether or not the pixels lie on the same\\nregion. That is, for any two pixels on the same object, the embeddings are\\ntrained to be similar; for any pair that straddles an object boundary, the\\nembeddings are trained to be dissimilar. Experimental results show that when\\nthis embedding network is used in conjunction with a DCNN trained on semantic\\nsegmentation, there is a systematic improvement in per-pixel classification\\naccuracy. Our contributions are integrated in the popular Caffe deep learning\\nframework, and consist in straightforward modifications to convolution\\nroutines. As such, they can be exploited for any task involving convolution\\nlayers.\\n',\n",
       "  'title': u'Learning Dense Convolutional Embeddings for Semantic Segmentation'},\n",
       " u'1503.08248': {'arxivid': u'1503.08248',\n",
       "  'authorsaffil': [[u'Xirong Li', None],\n",
       "   [u'Tiberio Uricchio', None],\n",
       "   [u'Lamberto Ballan', None],\n",
       "   [u'Marco Bertini', None],\n",
       "   [u'Cees G. M. Snoek', None],\n",
       "   [u'Alberto Del Bimbo', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.CV', u'cs.MM', u'cs.SI', u'H.3.1; H.3.3'],\n",
       "  'comment': u'to appear in ACM Computing Surveys',\n",
       "  'doi': u'10.1145/2906152',\n",
       "  'journalref': u'ACM Computing Surveys, Volume 49 Issue 1, 14:1-14:39, June 2016',\n",
       "  'link': u'http://arxiv.org/abs/1503.08248v3',\n",
       "  'published': u'2015-03-28T00:10:16Z',\n",
       "  'summary': u'  Where previous reviews on content-based image retrieval emphasize on what can\\nbe seen in an image to bridge the semantic gap, this survey considers what\\npeople tag about an image. A comprehensive treatise of three closely linked\\nproblems, i.e., image tag assignment, refinement, and tag-based image retrieval\\nis presented. While existing works vary in terms of their targeted tasks and\\nmethodology, they rely on the key functionality of tag relevance, i.e.\\nestimating the relevance of a specific tag with respect to the visual content\\nof a given image and its social context. By analyzing what information a\\nspecific method exploits to construct its tag relevance function and how such\\ninformation is exploited, this paper introduces a taxonomy to structure the\\ngrowing literature, understand the ingredients of the main works, clarify their\\nconnections and difference, and recognize their merits and limitations. For a\\nhead-to-head comparison between the state-of-the-art, a new experimental\\nprotocol is presented, with training sets containing 10k, 100k and 1m images\\nand an evaluation on three test sets, contributed by various research groups.\\nEleven representative works are implemented and evaluated. Putting all this\\ntogether, the survey aims to provide an overview of the past and foster\\nprogress for the near future.\\n',\n",
       "  'title': u'Socializing the Semantic Gap: A Comparative Survey on Image Tag\\n  Assignment, Refinement and Retrieval'},\n",
       " u'1601.02745': {'arxivid': u'1601.02745',\n",
       "  'authorsaffil': [[u'Paul Smolensky', None],\n",
       "   [u'Moontae Lee', None],\n",
       "   [u'Xiaodong He', None],\n",
       "   [u'Wen-tau Yih', None],\n",
       "   [u'Jianfeng Gao', None],\n",
       "   [u'Li Deng', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02745v1',\n",
       "  'published': u'2016-01-12T06:44:54Z',\n",
       "  'summary': u\"  In this paper we present the initial development of a general theory for\\nmapping inference in predicate logic to computation over Tensor Product\\nRepresentations (TPRs; Smolensky (1990), Smolensky & Legendre (2006)). After an\\ninitial brief synopsis of TPRs (Section 0), we begin with particular examples\\nof inference with TPRs in the 'bAbI' question-answering task of Weston et al.\\n(2015) (Section 1). We then present a simplification of the general analysis\\nthat suffices for the bAbI task (Section 2). Finally, we lay out the general\\ntreatment of inference over TPRs (Section 3). We also show the simplification\\nin Section 2 derives the inference methods described in Lee et al. (2016); this\\nshows how the simple methods of Lee et al. (2016) can be formally extended to\\nmore general reasoning tasks.\\n\",\n",
       "  'title': u'Basic Reasoning with Tensor Product Representations'},\n",
       " u'1602.05285': {'arxivid': u'1602.05285',\n",
       "  'authorsaffil': [[u'Truyen Tran', None],\n",
       "   [u'Dinh Phung', None],\n",
       "   [u'Svetha Venkatesh', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.IR', u'cs.LG'],\n",
       "  'comment': u\"PAKDD workshop on Biologically Inspired Techniques for Data Mining\\n  (BDM'16)\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05285v1',\n",
       "  'published': u'2016-02-17T03:17:10Z',\n",
       "  'summary': u'  We introduce Neural Choice by Elimination, a new framework that integrates\\ndeep neural networks into probabilistic sequential choice models for learning\\nto rank. Given a set of items to chose from, the elimination strategy starts\\nwith the whole item set and iteratively eliminates the least worthy item in the\\nremaining subset. We prove that the choice by elimination is equivalent to\\nmarginalizing out the random Gompertz latent utilities. Coupled with the choice\\nmodel is the recently introduced Neural Highway Networks for approximating\\narbitrarily complex rank functions. We evaluate the proposed framework on a\\nlarge-scale public dataset with over 425K items, drawn from the Yahoo! learning\\nto rank challenge. It is demonstrated that the proposed method is competitive\\nagainst state-of-the-art learning to rank methods.\\n',\n",
       "  'title': u'Choice by Elimination via Deep Neural Networks'},\n",
       " u'1601.07596': {'arxivid': u'1601.07596',\n",
       "  'authorsaffil': [[u'Francisco Chicano', None],\n",
       "   [u'Darrell Whitley', None],\n",
       "   [u'Renato Tinos', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.NE', u'I.2.8'],\n",
       "  'comment': u'Paper accepted for publication in the 16th European Conference on\\n  Evolutionary Computation for Combinatorial Optimisation (EvoCOP 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07596v1',\n",
       "  'published': u'2016-01-27T23:35:05Z',\n",
       "  'summary': u'  Local search algorithms and iterated local search algorithms are a basic\\ntechnique. Local search can be a stand along search methods, but it can also be\\nhybridized with evolutionary algorithms. Recently, it has been shown that it is\\npossible to identify improving moves in Hamming neighborhoods for k-bounded\\npseudo-Boolean optimization problems in constant time. This means that local\\nsearch does not need to enumerate neighborhoods to find improving moves. It\\nalso means that evolutionary algorithms do not need to use random mutation as a\\noperator, except perhaps as a way to escape local optima. In this paper, we\\nshow how improving moves can be identified in constant time for multiobjective\\nproblems that are expressed as k-bounded pseudo-Boolean functions. In\\nparticular, multiobjective forms of NK Landscapes and Mk Landscapes are\\nconsidered.\\n',\n",
       "  'title': u'Efficient Hill-Climber for Multi-Objective Pseudo-Boolean Optimization'},\n",
       " u'1603.09446': {'arxivid': u'1603.09446',\n",
       "  'authorsaffil': [[u'Wei Shen', None],\n",
       "   [u'Kai Zhao', None],\n",
       "   [u'Yuan Jiang', None],\n",
       "   [u'Yan Wang', None],\n",
       "   [u'Zhijiang Zhang', None],\n",
       "   [u'Xiang Bai', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted by CVPR2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09446v2',\n",
       "  'published': u'2016-03-31T03:21:33Z',\n",
       "  'summary': u'  Object skeleton is a useful cue for object detection, complementary to the\\nobject contour, as it provides a structural representation to describe the\\nrelationship among object parts. While object skeleton extraction in natural\\nimages is a very challenging problem, as it requires the extractor to be able\\nto capture both local and global image context to determine the intrinsic scale\\nof each skeleton pixel. Existing methods rely on per-pixel based multi-scale\\nfeature computation, which results in difficult modeling and high time\\nconsumption. In this paper, we present a fully convolutional network with\\nmultiple scale-associated side outputs to address this problem. By observing\\nthe relationship between the receptive field sizes of the sequential stages in\\nthe network and the skeleton scales they can capture, we introduce a\\nscale-associated side output to each stage. We impose supervision to different\\nstages by guiding the scale-associated side outputs toward groundtruth\\nskeletons of different scales. The responses of the multiple scale-associated\\nside outputs are then fused in a scale-specific way to localize skeleton pixels\\nwith multiple scales effectively. Our method achieves promising results on two\\nskeleton extraction datasets, and significantly outperforms other competitors.\\n',\n",
       "  'title': u'Object Skeleton Extraction in Natural Images by Fusing Scale-associated\\n  Deep Side Outputs'},\n",
       " u'1602.05264': {'arxivid': u'1602.05264',\n",
       "  'authorsaffil': [[u'Puneet S Chhabra', None],\n",
       "   [u'Andrew M Wallace', None],\n",
       "   [u'James R Hopgood', None]],\n",
       "  'categoryterms': [u'physics.optics',\n",
       "   u'cs.LG',\n",
       "   u'physics.ins-det',\n",
       "   u'stat.AP',\n",
       "   u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05264v1',\n",
       "  'published': u'2016-02-17T01:39:29Z',\n",
       "  'summary': u\"  Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide a\\nseries of echoes that reflect from objects in a scene. These can be first, last\\nor multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measure\\nthe intensity of light reflected from objects continuously over a period of\\ntime. In a camouflaged scenario, e.g., objects hidden behind dense foliage, a\\nFW-Ladar penetrates such foliage and returns a sequence of echoes including\\nburied faint echoes. The aim of this paper is to learn local-patterns of\\nco-occurring echoes characterised by their measured spectra. A deviation from\\nsuch patterns defines an abnormal event in a forest/tree depth profile. As far\\nas the authors know, neither DR or FW-Ladar, along with several spectral\\nmeasurements, has not been applied to anomaly detection. This work presents an\\nalgorithm that allows detection of spectral and temporal anomalies in FW-Multi\\nSpectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveform\\ntemporal and spectral signature that does not conform to a prior expectation,\\nrepresented using a learnt subspace (dictionary) and set of coefficients that\\ncapture co-occurring local-patterns using an overlapping temporal window. A\\nmodified optimization scheme is proposed for subspace learning based on\\nstochastic approximations. The objective function is augmented with a\\ndiscriminative term that represents the subspace's separability properties and\\nsupports anomaly characterisation. The algorithm detects several man-made\\nobjects and anomalous spectra hidden in a dense clutter of vegetation and also\\nallows tree species classification.\\n\",\n",
       "  'title': u'Anomaly Detection in Clutter using Spectrally Enhanced Ladar'},\n",
       " u'1603.06042': {'arxivid': u'1603.06042',\n",
       "  'authorsaffil': [[u'Daniel Andor', None],\n",
       "   [u'Chris Alberti', None],\n",
       "   [u'David Weiss', None],\n",
       "   [u'Aliaksei Severyn', None],\n",
       "   [u'Alessandro Presta', None],\n",
       "   [u'Kuzman Ganchev', None],\n",
       "   [u'Slav Petrov', None],\n",
       "   [u'Michael Collins', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.06042v2',\n",
       "  'published': u'2016-03-19T03:56:03Z',\n",
       "  'summary': u'  We introduce a globally normalized transition-based neural network model that\\nachieves state-of-the-art part-of-speech tagging, dependency parsing and\\nsentence compression results. Our model is a simple feed-forward neural network\\nthat operates on a task-specific transition system, yet achieves comparable or\\nbetter accuracies than recurrent models. We discuss the importance of global as\\nopposed to local normalization: a key insight is that the label bias problem\\nimplies that globally normalized models can be strictly more expressive than\\nlocally normalized models.\\n',\n",
       "  'title': u'Globally Normalized Transition-Based Neural Networks'},\n",
       " u'1506.06825': {'arxivid': u'1506.06825',\n",
       "  'authorsaffil': [[u'John Flynn', None],\n",
       "   [u'Ivan Neulander', None],\n",
       "   [u'James Philbin', None],\n",
       "   [u'Noah Snavely', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Video showing additional results available at\\n  http://youtu.be/cizgVZ8rjKA',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.06825v1',\n",
       "  'published': u'2015-06-22T23:48:21Z',\n",
       "  'summary': u'  Deep networks have recently enjoyed enormous success when applied to\\nrecognition and classification problems in computer vision, but their use in\\ngraphics problems has been limited. In this work, we present a novel deep\\narchitecture that performs new view synthesis directly from pixels, trained\\nfrom a large number of posed image sets. In contrast to traditional approaches\\nwhich consist of multiple complex stages of processing, each of which require\\ncareful tuning and can fail in unexpected ways, our system is trained\\nend-to-end. The pixels from neighboring views of a scene are presented to the\\nnetwork which then directly produces the pixels of the unseen view. The\\nbenefits of our approach include generality (we only require posed image sets\\nand can easily apply our method to different domains), and high quality results\\non traditionally difficult scenes. We believe this is due to the end-to-end\\nnature of our system which is able to plausibly generate pixels according to\\ncolor, depth, and texture priors learnt automatically from the training data.\\nTo verify our method we show that it can convincingly reproduce known test\\nviews from nearby imagery. Additionally we show images rendered from novel\\nviewpoints. To our knowledge, our work is the first to apply deep learning to\\nthe problem of new view synthesis from sets of real-world, natural imagery.\\n',\n",
       "  'title': u\"DeepStereo: Learning to Predict New Views from the World's Imagery\"},\n",
       " u'1601.03073': {'arxivid': u'1601.03073',\n",
       "  'authorsaffil': [[u'Gautam Reddy', None],\n",
       "   [u'Antonio Celani', None],\n",
       "   [u'Massimo Vergassola', None]],\n",
       "  'categoryterms': [u'cs.LG',\n",
       "   u'cs.IT',\n",
       "   u'math.IT',\n",
       "   u'physics.data-an',\n",
       "   u'q-bio.PE',\n",
       "   u'stat.ML'],\n",
       "  'comment': u'16 pages, 5 figures',\n",
       "  'doi': u'10.1007/s10955-016-1521-0',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03073v1',\n",
       "  'published': u'2016-01-12T21:50:03Z',\n",
       "  'summary': u'  Proper balance between exploitation and exploration is what makes good\\ndecisions, which achieve high rewards like payoff or evolutionary fitness. The\\nInfomax principle postulates that maximization of information directs the\\nfunction of diverse systems, from living systems to artificial neural networks.\\nWhile specific applications are successful, the validity of information as a\\nproxy for reward remains unclear. Here, we consider the multi-armed bandit\\ndecision problem, which features arms (slot-machines) of unknown probabilities\\nof success and a player trying to maximize cumulative payoff by choosing the\\nsequence of arms to play. We show that an Infomax strategy (Info-p) which\\noptimally gathers information on the highest mean reward among the arms\\nsaturates known optimal bounds and compares favorably to existing policies. The\\nhighest mean reward considered by Info-p is not the quantity actually needed\\nfor the choice of the arm to play, yet it allows for optimal tradeoffs between\\nexploration and exploitation.\\n',\n",
       "  'title': u'Infomax strategies for an optimal balance between exploration and\\n  exploitation'},\n",
       " u'1603.07965': {'arxivid': u'1603.07965',\n",
       "  'authorsaffil': [[u'Xiaosong Wang', None],\n",
       "   [u'Le Lu', None],\n",
       "   [u'Hoo-chang Shin', None],\n",
       "   [u'Lauren Kim', None],\n",
       "   [u'Isabella Nogues', None],\n",
       "   [u'Jianhua Yao', None],\n",
       "   [u'Ronald Summers', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07965v1',\n",
       "  'published': u'2016-03-25T17:16:00Z',\n",
       "  'summary': u'  Obtaining semantic labels on a large scale radiology image database (215,786\\nkey images from 61,845 unique patients) is a prerequisite yet bottleneck to\\ntrain highly effective deep convolutional neural network (CNN) models for image\\nrecognition. Nevertheless, conventional methods for collecting image labels\\n(e.g., Google search followed by crowd-sourcing) are not applicable due to the\\nformidable difficulties of medical annotation tasks for those who are not\\nclinically trained. This type of image labeling task remains non-trivial even\\nfor radiologists due to uncertainty and possible drastic inter-observer\\nvariation or inconsistency.\\n  In this paper, we present a looped deep pseudo-task optimization procedure\\nfor automatic category discovery of visually coherent and clinically semantic\\n(concept) clusters. Our system can be initialized by domain-specific (CNN\\ntrained on radiology images and text report derived labels) or generic\\n(ImageNet based) CNN models. Afterwards, a sequence of pseudo-tasks are\\nexploited by the looped deep image feature clustering (to refine image labels)\\nand deep CNN training/classification using new labels (to obtain more task\\nrepresentative deep features). Our method is conceptually simple and based on\\nthe hypothesized \"convergence\" of better labels leading to better trained CNN\\nmodels which in turn feed more effective deep image features to facilitate more\\nmeaningful clustering/labels. We have empirically validated the convergence and\\ndemonstrated promising quantitative and qualitative results. Category labels of\\nsignificantly higher quality than those in previous work are discovered. This\\nallows for further investigation of the hierarchical semantic nature of the\\ngiven large-scale radiology image database.\\n',\n",
       "  'title': u'Unsupervised Category Discovery via Looped Deep Pseudo-Task Optimization\\n  Using a Large Scale Radiology Image Database'},\n",
       " u'1601.03679': {'arxivid': u'1601.03679',\n",
       "  'authorsaffil': [[u'Xiaojun Chang', None],\n",
       "   [u'Yi Yang', None],\n",
       "   [u'Guodong Long', None],\n",
       "   [u'Chengqi Zhang', None],\n",
       "   [u'Alexander G. Hauptmann', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'7 pages, AAAI 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.03679v1',\n",
       "  'published': u'2016-01-14T17:40:09Z',\n",
       "  'summary': u'  In this paper, we focus on automatically detecting events in unconstrained\\nvideos without the use of any visual training exemplars. In principle,\\nzero-shot learning makes it possible to train an event detection model based on\\nthe assumption that events (e.g. \\\\emph{birthday party}) can be described by\\nmultiple mid-level semantic concepts (e.g. \"blowing candle\", \"birthday cake\").\\nTowards this goal, we first pre-train a bundle of concept classifiers using\\ndata from other sources. Then we evaluate the semantic correlation of each\\nconcept \\\\wrt the event of interest and pick up the relevant concept\\nclassifiers, which are applied on all test videos to get multiple prediction\\nscore vectors. While most existing systems combine the predictions of the\\nconcept classifiers with fixed weights, we propose to learn the optimal weights\\nof the concept classifiers for each testing video by exploring a set of online\\navailable videos with free-form text descriptions of their content. To validate\\nthe effectiveness of the proposed approach, we have conducted extensive\\nexperiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset.\\nThe experimental results confirm the superiority of the proposed approach.\\n',\n",
       "  'title': u'Dynamic Concept Composition for Zero-Example Event Detection'},\n",
       " u'1604.03930': {'arxivid': u'1604.03930',\n",
       "  'authorsaffil': [[u'Rong Ge', None],\n",
       "   [u'Chi Jin', None],\n",
       "   [u'Sham M. Kakade', None],\n",
       "   [u'Praneeth Netrapalli', None],\n",
       "   [u'Aaron Sidford', None]],\n",
       "  'categoryterms': [u'cs.LG', u'math.OC', u'stat.ML'],\n",
       "  'comment': u'International Conference on Machine Learning (ICML) 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03930v2',\n",
       "  'published': u'2016-04-13T19:57:46Z',\n",
       "  'summary': u'  This paper considers the problem of canonical-correlation analysis (CCA)\\n(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a\\npair of symmetric matrices. These are two fundamental problems in data analysis\\nand scientific computing with numerous applications in machine learning and\\nstatistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).\\n  We provide simple iterative algorithms, with improved runtimes, for solving\\nthese problems that are globally linearly convergent with moderate dependencies\\non the condition numbers and eigenvalue gaps of the matrices involved.\\n  We obtain our results by reducing CCA to the top-$k$ generalized eigenvector\\nproblem. We solve this problem through a general framework that simply requires\\nblack box access to an approximate linear system solver. Instantiating this\\nframework with accelerated gradient descent we obtain a running time of\\n$O(\\\\frac{z k \\\\sqrt{\\\\kappa}}{\\\\rho} \\\\log(1/\\\\epsilon) \\\\log\\n\\\\left(k\\\\kappa/\\\\rho\\\\right))$ where $z$ is the total number of nonzero entries,\\n$\\\\kappa$ is the condition number and $\\\\rho$ is the relative eigenvalue gap of\\nthe appropriate matrices.\\n  Our algorithm is linear in the input size and the number of components $k$ up\\nto a $\\\\log(k)$ factor. This is essential for handling large-scale matrices that\\nappear in practice. To the best of our knowledge this is the first such\\nalgorithm with global linear convergence. We hope that our results prompt\\nfurther research and ultimately improve the practical running time for\\nperforming these important data analysis procedures on large data sets.\\n',\n",
       "  'title': u'Efficient Algorithms for Large-scale Generalized Eigenvector Computation\\n  and Canonical Correlation Analysis'},\n",
       " u'1512.06757': {'arxivid': u'1512.06757',\n",
       "  'authorsaffil': [[u'Jiaji Huang', None],\n",
       "   [u'Qiang Qiu', None],\n",
       "   [u'Robert Calderbank', None],\n",
       "   [u'Guillermo Sapiro', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'Theorems need more validation',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.06757v2',\n",
       "  'published': u'2015-12-21T18:42:45Z',\n",
       "  'summary': u'  Deep neural networks have proved very successful in domains where large\\ntraining sets are available, but when the number of training samples is small,\\ntheir performance suffers from overfitting. Prior methods of reducing\\noverfitting such as weight decay, Dropout and DropConnect are data-independent.\\nThis paper proposes a new method, GraphConnect, that is data-dependent, and is\\nmotivated by the observation that data of interest lie close to a manifold. The\\nnew method encourages the relationships between the learned decisions to\\nresemble a graph representing the manifold structure. Essentially GraphConnect\\nis designed to learn attributes that are present in data samples in contrast to\\nweight decay, Dropout and DropConnect which are simply designed to make it more\\ndifficult to fit to random error or noise. Empirical Rademacher complexity is\\nused to connect the generalization error of the neural network to spectral\\nproperties of the graph learned from the input data. This framework is used to\\nshow that GraphConnect is superior to weight decay. Experimental results on\\nseveral benchmark datasets validate the theoretical analysis, and show that\\nwhen the number of training samples is small, GraphConnect is able to\\nsignificantly improve performance over weight decay.\\n',\n",
       "  'title': u'GraphConnect: A Regularization Framework for Neural Networks'},\n",
       " u'1601.04800': {'arxivid': u'1601.04800',\n",
       "  'authorsaffil': [[u'Zhao Kang', None],\n",
       "   [u'Chong Peng', None],\n",
       "   [u'Qiang Cheng', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.AI', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'AAAI 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04800v1',\n",
       "  'published': u'2016-01-19T04:48:42Z',\n",
       "  'summary': u'  Top-N recommender systems have been investigated widely both in industry and\\nacademia. However, the recommendation quality is far from satisfactory. In this\\npaper, we propose a simple yet promising algorithm. We fill the user-item\\nmatrix based on a low-rank assumption and simultaneously keep the original\\ninformation. To do that, a nonconvex rank relaxation rather than the nuclear\\nnorm is adopted to provide a better rank approximation and an efficient\\noptimization strategy is designed. A comprehensive set of experiments on real\\ndatasets demonstrates that our method pushes the accuracy of Top-N\\nrecommendation to a new level.\\n',\n",
       "  'title': u'Top-N Recommender System via Matrix Completion'},\n",
       " u'1601.07358': {'arxivid': u'1601.07358',\n",
       "  'authorsaffil': [[u'Jens Clausen', None], [u'Hans J. Briegel', None]],\n",
       "  'categoryterms': [u'quant-ph', u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'20 pages, 14 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07358v1',\n",
       "  'published': u'2016-01-27T13:31:38Z',\n",
       "  'summary': u\"  We consider a general class of models, where a reinforcement learning (RL)\\nagent learns from cyclic interactions with an external environment via\\nclassical signals. Perceptual inputs are encoded as quantum states, which are\\nsubsequently transformed by a quantum channel representing the agent's memory,\\nwhile the outcomes of measurements performed at the channel's output determine\\nthe agent's actions. The learning takes place via stepwise modifications of the\\nchannel properties. They are described by an update rule that is inspired by\\nthe projective simulation (PS) model and equipped with a glow mechanism that\\nallows for a backpropagation of policy changes, analogous to the eligibility\\ntraces in RL and edge glow in PS. In this way, the model combines features of\\nPS with the ability for generalization, offered by its physical embodiment as a\\nquantum system. We apply the agent to various setups of an invasion game and a\\ngrid world, which serve as elementary model tasks allowing a direct comparison\\nwith a basic classical PS agent.\\n\",\n",
       "  'title': u'Quantum machine learning with glow for episodic tasks and decision games'},\n",
       " u'1510.00726': {'arxivid': u'1510.00726',\n",
       "  'authorsaffil': [[u'Yoav Goldberg', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.00726v1',\n",
       "  'published': u'2015-10-02T20:17:33Z',\n",
       "  'summary': u'  Over the past few years, neural networks have re-emerged as powerful\\nmachine-learning models, yielding state-of-the-art results in fields such as\\nimage recognition and speech processing. More recently, neural network models\\nstarted to be applied also to textual natural language signals, again with very\\npromising results. This tutorial surveys neural network models from the\\nperspective of natural language processing research, in an attempt to bring\\nnatural-language researchers up to speed with the neural techniques. The\\ntutorial covers input encoding for natural language tasks, feed-forward\\nnetworks, convolutional networks, recurrent networks and recursive networks, as\\nwell as the computation graph abstraction for automatic gradient computation.\\n',\n",
       "  'title': u'A Primer on Neural Network Models for Natural Language Processing'},\n",
       " u'1601.03855': {'arxivid': u'1601.03855',\n",
       "  'authorsaffil': [[u'Pratik Gajane', u'FT R and D'],\n",
       "   [u'Tanguy Urvoy', u'FT R and D'],\n",
       "   [u'Fabrice Cl\\xe9rot', u'FT R and D']],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': u'The 32nd International Conference on Machine Learning, Jul 2015,\\n  Lille, France. 37, pp.218-227, Proceedings of The 32nd International\\n  Conference on Machine Learning',\n",
       "  'link': u'http://arxiv.org/abs/1601.03855v1',\n",
       "  'published': u'2016-01-15T09:50:07Z',\n",
       "  'summary': u'  We study the K-armed dueling bandit problem which is a variation of the\\nclassical Multi-Armed Bandit (MAB) problem in which the learner receives only\\nrelative feedback about the selected pairs of arms. We propose a new algorithm\\ncalled Relative Exponential-weight algorithm for Exploration and Exploitation\\n(REX3) to handle the adversarial utility-based formulation of this problem.\\nThis algorithm is a non-trivial extension of the Exponential-weight algorithm\\nfor Exploration and Exploitation (EXP3) algorithm. We prove a finite time\\nexpected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and a\\ngeneral lower bound of order omega(sqrt(KT)). At the end, we provide\\nexperimental results using real data from information retrieval applications.\\n',\n",
       "  'title': u'A Relative Exponential Weighing Algorithm for Adversarial Utility-based\\n  Dueling Bandits'},\n",
       " u'1211.5063': {'arxivid': u'1211.5063',\n",
       "  'authorsaffil': [[u'Razvan Pascanu', None],\n",
       "   [u'Tomas Mikolov', None],\n",
       "   [u'Yoshua Bengio', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'Improved description of the exploding gradient problem and\\n  description and analysis of the vanishing gradient problem',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1211.5063v2',\n",
       "  'published': u'2012-11-21T15:40:11Z',\n",
       "  'summary': u'  There are two widely known issues with properly training Recurrent Neural\\nNetworks, the vanishing and the exploding gradient problems detailed in Bengio\\net al. (1994). In this paper we attempt to improve the understanding of the\\nunderlying issues by exploring these problems from an analytical, a geometric\\nand a dynamical systems perspective. Our analysis is used to justify a simple\\nyet effective solution. We propose a gradient norm clipping strategy to deal\\nwith exploding gradients and a soft constraint for the vanishing gradients\\nproblem. We validate empirically our hypothesis and proposed solutions in the\\nexperimental section.\\n',\n",
       "  'title': u'On the difficulty of training Recurrent Neural Networks'},\n",
       " u'1601.04805': {'arxivid': u'1601.04805',\n",
       "  'authorsaffil': [[u'Anh Cat Le Ngo', None],\n",
       "   [u'John See', None],\n",
       "   [u'Raphael Chung-Wei Phan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'IEEE Transaction of Affective Computing (2016)',\n",
       "  'doi': u'10.1109/TAFFC.2016.2523996',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04805v2',\n",
       "  'published': u'2016-01-19T06:13:49Z',\n",
       "  'summary': u'  Spontaneous subtle emotions are expressed through micro-expressions, which\\nare tiny, sudden and short-lived dynamics of facial muscles; thus poses a great\\nchallenge for visual recognition. The abrupt but significant dynamics for the\\nrecognition task are temporally sparse while the rest, irrelevant dynamics, are\\ntemporally redundant. In this work, we analyze and enforce sparsity constrains\\nto learn significant temporal and spectral structures while eliminate\\nirrelevant facial dynamics of micro-expressions, which would ease the challenge\\nin the visual recognition of spontaneous subtle emotions. The hypothesis is\\nconfirmed through experimental results of automatic spontaneous subtle emotion\\nrecognition with several sparsity levels on CASME II and SMIC, the only two\\npublicly available spontaneous subtle emotion databases. The overall\\nperformances of the automatic subtle emotion recognition are boosted when only\\nsignificant dynamics are preserved from the original sequences.\\n',\n",
       "  'title': u'Sparsity in Dynamics of Spontaneous Subtle Emotions: Analysis \\\\&\\n  Application'},\n",
       " u'1601.06087': {'arxivid': u'1601.06087',\n",
       "  'authorsaffil': [[u'Aria Ahmadi', None], [u'Ioannis Patras', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Submitted to ICIP 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06087v1',\n",
       "  'published': u'2016-01-22T17:57:07Z',\n",
       "  'summary': u'  Traditional methods for motion estimation estimate the motion field F between\\na pair of images as the one that minimizes a predesigned cost function. In this\\npaper, we propose a direct method and train a Convolutional Neural Network\\n(CNN) that when, at test time, is given a pair of images as input it produces a\\ndense motion field F at its output layer. In the absence of large datasets with\\nground truth motion that would allow classical supervised training, we propose\\nto train the network in an unsupervised manner. The proposed cost function that\\nis optimized during training, is based on the classical optical flow\\nconstraint. The latter is differentiable with respect to the motion field and,\\ntherefore, allows backpropagation of the error to previous layers of the\\nnetwork. Our method is tested on both synthetic and real image sequences and\\nperforms similarly to the state-of-the-art methods.\\n',\n",
       "  'title': u'Unsupervised convolutional neural networks for motion estimation'},\n",
       " u'1602.07024': {'arxivid': u'1602.07024',\n",
       "  'authorsaffil': [[u'Satya Gautam Vadlamudi', None],\n",
       "   [u'Sailik Sengupta', None],\n",
       "   [u'Subbarao Kambhampati', None],\n",
       "   [u'Marthony Taguinod', None],\n",
       "   [u'Ziming Zhao', None],\n",
       "   [u'Adam Doup\\xe9', None],\n",
       "   [u'Gail-Joon Ahn', None]],\n",
       "  'categoryterms': [u'cs.CR', u'cs.AI', u'cs.GT', u'cs.MA'],\n",
       "  'comment': u'10 pages, 2 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07024v1',\n",
       "  'published': u'2016-02-23T03:44:16Z',\n",
       "  'summary': u'  Web applications form a critical component of cyber security systems as they\\nact as a gateway for many institutions. Vulnerabilities in web applications\\nallow malicious actors to access and/or modify restricted data. Here the\\nhackers have the opportunity to perform reconnaissance so as to gain knowledge\\nabout the web application layout before launching an attack, whereas the\\ndefender (administrator of the web application) must secure the application\\neven with its potential vulnerabilities. In order to mask such vulnerabilities\\nwhich are primarily associated with different individual configurations, Moving\\nTarget Defense systems were proposed wherein the defender switches between\\nvarious configurations thereby making it difficult to attack with success,\\nwhile maintaining a seamless experience for the genuine users. However, the\\ndesign of good quality switching strategies is still an open problem which is\\ncrucial for the effectiveness of the Moving Target Defense approach. In this\\npaper, we present a way to find effective switching strategies by modeling this\\necosystem as a Bayesian Stackelberg game with the administrator as the leader\\nand the hackers as the followers, which as we show succinctly captures various\\naspects of the Moving Target Defense systems. Furthermore, we show how to\\ndetermine which vulnerability areas should be addressed first once the system\\nis deployed and which attacker type uncertainties should be calibrated with\\nhigh precision, for increasing the security of the web application. We present\\nexperimental results on a representative web application system demonstrating\\nthe utility of switching strategies obtained using the proposed method, and we\\ndiscuss various future directions that are unique to the web application\\ndomain.\\n',\n",
       "  'title': u'Moving Target Defense for Web Applications using Bayesian Stackelberg\\n  Games'},\n",
       " u'1601.06081': {'arxivid': u'1601.06081',\n",
       "  'authorsaffil': [[u'Marco Guerini', None], [u'Carlo Strapparava', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.CY', u'cs.SI'],\n",
       "  'comment': u'Preprint of paper in Journal of Information Processing and Management\\n  Volume 52, Issue 1, January 2016, Pages 163-172',\n",
       "  'doi': u'10.1016/j.ipm.2015.05.003',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.06081v1',\n",
       "  'published': u'2016-01-22T17:33:28Z',\n",
       "  'summary': u'  Urban legends are a genre of modern folklore, consisting of stories about\\nrare and exceptional events, just plausible enough to be believed, which tend\\nto propagate inexorably across communities. In our view, while urban legends\\nrepresent a form of \"sticky\" deceptive text, they are marked by a tension\\nbetween the credible and incredible. They should be credible like a news\\narticle and incredible like a fairy tale to go viral. In particular we will\\nfocus on the idea that urban legends should mimic the details of news (who,\\nwhere, when) to be credible, while they should be emotional and readable like a\\nfairy tale to be catchy and memorable. Using NLP tools we will provide a\\nquantitative analysis of these prototypical characteristics. We also lay out\\nsome machine learning experiments showing that it is possible to recognize an\\nurban legend using just these simple features.\\n',\n",
       "  'title': u'Why Do Urban Legends Go Viral?'},\n",
       " u'1602.03032': {'arxivid': u'1602.03032',\n",
       "  'authorsaffil': [[u'Ivo Danihelka', None],\n",
       "   [u'Greg Wayne', None],\n",
       "   [u'Benigno Uria', None],\n",
       "   [u'Nal Kalchbrenner', None],\n",
       "   [u'Alex Graves', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'ICML-2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03032v2',\n",
       "  'published': u'2016-02-09T15:26:26Z',\n",
       "  'summary': u'  We investigate a new method to augment recurrent neural networks with extra\\nmemory without increasing the number of network parameters. The system has an\\nassociative memory based on complex-valued vectors and is closely related to\\nHolographic Reduced Representations and Long Short-Term Memory networks.\\nHolographic Reduced Representations have limited capacity: as they store more\\ninformation, each retrieval becomes noisier due to interference. Our system in\\ncontrast creates redundant copies of stored information, which enables\\nretrieval with reduced noise. Experiments demonstrate faster learning on\\nmultiple memorization tasks.\\n',\n",
       "  'title': u'Associative Long Short-Term Memory'},\n",
       " u'1601.02093': {'arxivid': u'1601.02093',\n",
       "  'authorsaffil': [[u'Olivier Mor\\xe8re', None],\n",
       "   [u'Antoine Veillard', None],\n",
       "   [u'Jie Lin', None],\n",
       "   [u'Julie Petta', None],\n",
       "   [u'Vijay Chandrasekhar', None],\n",
       "   [u'Tomaso Poggio', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.IR'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02093v2',\n",
       "  'published': u'2016-01-09T10:42:35Z',\n",
       "  'summary': u'  Most image instance retrieval pipelines are based on comparison of vectors\\nknown as global image descriptors between a query image and the database\\nimages. Due to their success in large scale image classification,\\nrepresentations extracted from Convolutional Neural Networks (CNN) are quickly\\ngaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors\\nfor image instance retrieval. While CNN-based descriptors are generally\\nremarked for good retrieval performance at lower bitrates, they nevertheless\\npresent a number of drawbacks including the lack of robustness to common object\\ntransformations such as rotations compared with their interest point based FV\\ncounterparts.\\n  In this paper, we propose a method for computing invariant global descriptors\\nfrom CNNs. Our method implements a recently proposed mathematical theory for\\ninvariance in a sensory cortex modeled as a feedforward neural network. The\\nresulting global descriptors can be made invariant to multiple arbitrary\\ntransformation groups while retaining good discriminativeness.\\n  Based on a thorough empirical evaluation using several publicly available\\ndatasets, we show that our method is able to significantly and consistently\\nimprove retrieval results every time a new type of invariance is incorporated.\\nWe also show that our method which has few parameters is not prone to\\noverfitting: improvements generalize well across datasets with different\\nproperties with regard to invariances. Finally, we show that our descriptors\\nare able to compare favourably to other state-of-the-art compact descriptors in\\nsimilar bitranges, exceeding the highest retrieval results reported in the\\nliterature on some datasets. A dedicated dimensionality reduction step\\n--quantization or hashing-- may be able to further improve the competitiveness\\nof the descriptors.\\n',\n",
       "  'title': u'Group Invariant Deep Representations for Image Instance Retrieval'},\n",
       " u'1510.00878': {'arxivid': u'1510.00878',\n",
       "  'authorsaffil': [[u'Claudio Alexandre', None], [u'Jo\\xe3o Balsa', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.AI', u'stat.ML'],\n",
       "  'comment': u'7 pages, 15 figures, 3 tables',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.00878v2',\n",
       "  'published': u'2015-10-03T22:31:58Z',\n",
       "  'summary': u'  We present a data mining approach for profiling bank clients in order to\\nsupport the process of detection of anti-money laundering operations. We first\\npresent the overall system architecture, and then focus on the relevant\\ncomponent for this paper. We detail the experiments performed on real world\\ndata from a financial institution, which allowed us to group clients in\\nclusters and then generate a set of classification rules. We discuss the\\nrelevance of the founded client profiles and of the generated classification\\nrules. According to the defined overall agent-based architecture, these rules\\nwill be incorporated in the knowledge base of the intelligent agents\\nresponsible for the signaling of suspicious transactions.\\n',\n",
       "  'title': u'Client Profiling for an Anti-Money Laundering System'},\n",
       " u'1602.08715': {'arxivid': u'1602.08715',\n",
       "  'authorsaffil': [[u'Avi Shmidman', None],\n",
       "   [u'Moshe Koppel', None],\n",
       "   [u'Ely Porat', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': u'Submission to the Journal of Data Mining and Digital Humanities\\n  (Special Issue on Computer-Aided Processing of Intertextuality in Ancient\\n  Languages)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08715v1',\n",
       "  'published': u'2016-02-28T13:43:33Z',\n",
       "  'summary': u'  We propose a method for efficiently finding all parallel passages in a large\\ncorpus, even if the passages are not quite identical due to rephrasing and\\northographic variation. The key ideas are the representation of each word in\\nthe corpus by its two most infrequent letters, finding matched pairs of strings\\nof four or five words that differ by at most one word and then identifying\\nclusters of such matched pairs. Using this method, over 4600 parallel pairs of\\npassages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of\\nover 1.8 million words, in just over 30 seconds. Empirical comparisons on\\nsample data indicate that the coverage obtained by our method is essentially\\nthe same as that obtained using slow exhaustive methods.\\n',\n",
       "  'title': u'Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus'},\n",
       " u'1601.07576': {'arxivid': u'1601.07576',\n",
       "  'authorsaffil': [[u'Sheng Guo', None],\n",
       "   [u'Weilin Huang', None],\n",
       "   [u'Yu Qiao', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07576v1',\n",
       "  'published': u'2016-01-27T21:32:15Z',\n",
       "  'summary': u'  Convolutional neural networks (CNN) have recently achieved remarkable\\nsuccesses in various image classification and understanding tasks. The deep\\nfeatures obtained at the top fully-connected layer of the CNN (FC-features)\\nexhibit rich global semantic information and are extremely effective in image\\nclassification. On the other hand, the convolutional features in the middle\\nlayers of the CNN also contain meaningful local information, but are not fully\\nexplored for image representation. In this paper, we propose a novel\\nLocally-Supervised Deep Hybrid Model (LS-DHM) that effectively enhances and\\nexplores the convolutional features for scene recognition. Firstly, we notice\\nthat the convolutional features capture local objects and fine structures of\\nscene images, which yield important cues for discriminating ambiguous scenes,\\nwhereas these features are significantly eliminated in the highly-compressed FC\\nrepresentation. Secondly, we propose a new Local Convolutional Supervision\\n(LCS) layer to enhance the local structure of the image by directly propagating\\nthe label information to the convolutional layers. Thirdly, we propose an\\nefficient Fisher Convolutional Vector (FCV) that successfully rescues the\\norderless mid-level semantic information (e.g. objects and textures) of scene\\nimage. The FCV encodes the large-sized convolutional maps into a fixed-length\\nmid-level representation, and is demonstrated to be strongly complementary to\\nthe high-level FC-features. Finally, both the FCV and FC-features are\\ncollaboratively employed in the LSDHM representation, which achieves\\noutstanding performance in our experiments. It obtains 83.75% and 67.56%\\naccuracies respectively on the heavily benchmarked MIT Indoor67 and SUN397\\ndatasets, advancing the stat-of-the-art substantially.\\n',\n",
       "  'title': u'Locally-Supervised Deep Hybrid Model for Scene Recognition'},\n",
       " u'1603.04549': {'arxivid': u'1603.04549',\n",
       "  'authorsaffil': [[u'Ramesh Johari', None],\n",
       "   [u'Vijay Kamble', None],\n",
       "   [u'Yash Kanoria', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DS', u'stat.ME', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.04549v1',\n",
       "  'published': u'2016-03-15T04:29:31Z',\n",
       "  'summary': u'  A wide range of resource allocation and platform operation settings exhibit\\nthe following two simultaneous challenges: (1) service resources are capacity\\nconstrained; and (2) clients\\' preferences are not perfectly known. To study\\nthis pair of challenges, we consider a service system with heterogeneous\\nservers and clients. Server types are known and there is fixed capacity of\\nservers of each type. Clients arrive over time, with types initially unknown\\nand drawn from some distribution. Each client sequentially brings $N$ jobs\\nbefore leaving. The system operator assigns each job to some server type,\\nresulting in a payoff whose distribution depends on the client and server\\ntypes.\\n  Our main contribution is a complete characterization of the structure of the\\noptimal policy for maximization of the rate of payoff accumulation. Such a\\npolicy must balance three goals: (i) earning immediate payoffs; (ii) learning\\nclient types to increase future payoffs; and (iii) satisfying the capacity\\nconstraints. We construct a policy that has provably optimal regret (to leading\\norder as $N$ grows large). Our policy has an appealingly simple three-phase\\nstructure: a short type-\"guessing\" phase, a type-\"confirmation\" phase that\\nbalances payoffs with learning, and finally an \"exploitation\" phase that\\nfocuses on payoffs. Crucially, our approach employs the shadow prices of the\\ncapacity constraints in the assignment problem with known types as \"externality\\nprices\" on the servers\\' capacity.\\n',\n",
       "  'title': u'Know Your Customer: Multi-armed Bandits with Capacity Constraints'},\n",
       " u'1502.03273': {'arxivid': u'1502.03273',\n",
       "  'authorsaffil': [[u'Dai-Qiang Chen', None]],\n",
       "  'categoryterms': [u'cs.CV', u'68U10, 90C90, 65T60'],\n",
       "  'comment': u'19 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.03273v2',\n",
       "  'published': u'2015-02-11T11:57:53Z',\n",
       "  'summary': u'  Sparse representation of images under certain transform domain has been\\nplaying a fundamental role in image restoration tasks. One such representative\\nmethod is the widely used wavelet tight frame systems. Instead of adopting\\nfixed filters for constructing a tight frame to sparsely model any input image,\\na data-driven tight frame was proposed for the sparse representation of images,\\nand shown to be very efficient for image denoising very recently. However, in\\nthis method the number of framelet filters used for constructing a tight frame\\nis the same as the length of filters. In fact, through further investigation it\\nis found that part of these filters are unnecessary and even harmful to the\\nrecovery effect due to the influence of noise. Therefore, an improved\\ndata-driven sparse representation systems constructed with much less number of\\nfilters are proposed. Numerical results on denoising experiments demonstrate\\nthat the proposed algorithm overall outperforms the original data-driven tight\\nframe construction scheme on both the recovery quality and computational time.\\n',\n",
       "  'title': u'Image denoising based on improved data-driven sparse representation'},\n",
       " u'1505.04630': {'arxivid': u'1505.04630',\n",
       "  'authorsaffil': [[u'Zhiyuan Tang', None],\n",
       "   [u'Dong Wang', None],\n",
       "   [u'Zhiyong Zhang', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'ICASSP 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.04630v5',\n",
       "  'published': u'2015-05-18T13:26:02Z',\n",
       "  'summary': u'  Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),\\nhave gained much attention in automatic speech recognition (ASR). Although some\\nsuccessful stories have been reported, training RNNs remains highly\\nchallenging, especially with limited training data. Recent research found that\\na well-trained model can be used as a teacher to train other child models, by\\nusing the predictions generated by the teacher model as supervision. This\\nknowledge transfer learning has been employed to train simple neural nets with\\na complex one, so that the final performance can reach a level that is\\ninfeasible to obtain by regular training. In this paper, we employ the\\nknowledge transfer learning approach to train RNNs (precisely LSTM) using a\\ndeep neural network (DNN) model as the teacher. This is different from most of\\nthe existing research on knowledge transfer learning, since the teacher (DNN)\\nis assumed to be weaker than the child (RNN); however, our experiments on an\\nASR task showed that it works fairly well: without applying any tricks on the\\nlearning scheme, this approach can train RNNs successfully even with limited\\ntraining data.\\n',\n",
       "  'title': u'Recurrent Neural Network Training with Dark Knowledge Transfer'},\n",
       " u'1603.01250': {'arxivid': u'1603.01250',\n",
       "  'authorsaffil': [[u'Yani Ioannou', None],\n",
       "   [u'Duncan Robertson', None],\n",
       "   [u'Darko Zikic', None],\n",
       "   [u'Peter Kontschieder', None],\n",
       "   [u'Jamie Shotton', None],\n",
       "   [u'Matthew Brown', None],\n",
       "   [u'Antonio Criminisi', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI'],\n",
       "  'comment': u'Microsoft Research Technical Report',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.01250v1',\n",
       "  'published': u'2016-03-03T20:41:47Z',\n",
       "  'summary': u'  This paper investigates the connections between two state of the art\\nclassifiers: decision forests (DFs, including decision jungles) and\\nconvolutional neural networks (CNNs). Decision forests are computationally\\nefficient thanks to their conditional computation property (computation is\\nconfined to only a small region of the tree, the nodes along a single branch).\\nCNNs achieve state of the art accuracy, thanks to their representation learning\\ncapabilities. We present a systematic analysis of how to fuse conditional\\ncomputation with representation learning and achieve a continuum of hybrid\\nmodels with different ratios of accuracy vs. efficiency. We call this new\\nfamily of hybrid models conditional networks. Conditional networks can be\\nthought of as: i) decision trees augmented with data transformation operators,\\nor ii) CNNs, with block-diagonal sparse weight matrices, and explicit data\\nrouting functions. Experimental validation is performed on the common task of\\nimage classification on both the CIFAR and Imagenet datasets. Compared to state\\nof the art CNNs, our hybrid models yield the same accuracy with a fraction of\\nthe compute cost and much smaller number of parameters.\\n',\n",
       "  'title': u'Decision Forests, Convolutional Networks and the Models in-Between'},\n",
       " u'1511.04119': {'arxivid': u'1511.04119',\n",
       "  'authorsaffil': [[u'Shikhar Sharma', None],\n",
       "   [u'Ryan Kiros', None],\n",
       "   [u'Ruslan Salakhutdinov', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04119v3',\n",
       "  'published': u'2015-11-12T23:06:42Z',\n",
       "  'summary': u'  We propose a soft attention based model for the task of action recognition in\\nvideos. We use multi-layered Recurrent Neural Networks (RNNs) with Long\\nShort-Term Memory (LSTM) units which are deep both spatially and temporally.\\nOur model learns to focus selectively on parts of the video frames and\\nclassifies videos after taking a few glimpses. The model essentially learns\\nwhich parts in the frames are relevant for the task at hand and attaches higher\\nimportance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51\\nand Hollywood2 datasets and analyze how the model focuses its attention\\ndepending on the scene and the action being performed.\\n',\n",
       "  'title': u'Action Recognition using Visual Attention'},\n",
       " u'1212.3023': {'arxivid': u'1212.3023',\n",
       "  'authorsaffil': [[u'Mahyuddin K. M. Nasution', None],\n",
       "   [u'Shahrul Azman Mohd Noah', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.CL'],\n",
       "  'comment': u'7 pages, nothing, draft to ICOCSIM 2012',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1212.3023v1',\n",
       "  'published': u'2012-12-13T00:34:23Z',\n",
       "  'summary': u'  Identifying the social actor has become one of tasks in Artificial\\nIntelligence, whereby extracting keyword from Web snippets depend on the use of\\nweb is steadily gaining ground in this research. We develop therefore an\\napproach based on overlap principle for utilizing a collection of features in\\nweb snippets, where use of keyword will eliminate the un-relevant web pages.\\n',\n",
       "  'title': u'Keyword Extraction for Identifying Social Actors'},\n",
       " u'1511.06359': {'arxivid': u'1511.06359',\n",
       "  'authorsaffil': [[u'Bihan Wen', None],\n",
       "   [u'Saiprasad Ravishankar', None],\n",
       "   [u'Yoram Bresler', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': u'25 pages (including the references and supplementary material)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06359v3',\n",
       "  'published': u'2015-11-19T20:55:49Z',\n",
       "  'summary': u'  Features based on sparse representation, especially using the synthesis\\ndictionary model, have been heavily exploited in signal processing and computer\\nvision. However, synthesis dictionary learning typically involves NP-hard\\nsparse coding and expensive learning steps. Recently, sparsifying transform\\nlearning received interest for its cheap computation and its optimal updates in\\nthe alternating algorithms. In this work, we develop a methodology for learning\\nof Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to\\nbetter represent natural images that contain textures with various geometrical\\ndirections. The proposed alternating learning algorithm involves efficient\\noptimal updates. We provide a convergence guarantee, and demonstrate the\\nempirical convergence behavior of the proposed FRIST learning algorithm.\\nPreliminary experiments show the usefulness of adaptive sparse representation\\nby FRIST for image sparse representation, segmentation, denoising, robust\\ninpainting, and MRI reconstruction with promising performances.\\n',\n",
       "  'title': u'FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning\\n  and Applications to Inverse Problems'},\n",
       " u'1503.03488': {'arxivid': u'1503.03488',\n",
       "  'authorsaffil': [[u'Robert A. Murphy', None]],\n",
       "  'categoryterms': [u'cs.LG', u'60D05'],\n",
       "  'comment': u'These writings are part of a longer writing which has been submitted\\n  for publication. I plan to replace this writing (and the other 2 writings)\\n  with the single writing that has been submitted for publication. The other\\n  writings to be withdraw are 1501.07227 and 1412.4178',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.03488v2',\n",
       "  'published': u'2015-03-07T22:45:54Z',\n",
       "  'summary': u'  Utilizing the sample size of a dataset, the random cluster model is employed\\nin order to derive an estimate of the mean number of K-Means clusters to form\\nduring classification of a dataset.\\n',\n",
       "  'title': u'Estimating the Mean Number of K-Means Clusters to Form'},\n",
       " u'1512.01250': {'arxivid': u'1512.01250',\n",
       "  'authorsaffil': [[u'Timber Kerkvliet', None], [u'Ronald Meester', None]],\n",
       "  'categoryterms': [u'math.PR', u'cs.AI'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1512.01249. Accepted for\\n  publication in Law, Probability and Risk',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.01250v2',\n",
       "  'published': u'2015-12-02T13:37:40Z',\n",
       "  'summary': u\"  We first discuss certain problems with the classical probabilistic approach\\nfor assessing forensic evidence, in particular its inability to distinguish\\nbetween lack of belief and disbelief, and its inability to model complete\\nignorance within a given population. We then discuss Shafer belief functions, a\\ngeneralization of probability distributions, which can deal with both these\\nobjections. We use a calculus of belief functions which does not use the much\\ncriticized Dempster rule of combination, but only the very natural\\nDempster-Shafer conditioning. We then apply this calculus to some classical\\nforensic problems like the various island problems and the problem of parental\\nidentification. If we impose no prior knowledge apart from assuming that the\\nculprit or parent belongs to a given population (something which is possible in\\nour setting), then our answers differ from the classical ones when uniform or\\nother priors are imposed. We can actually retrieve the classical answers by\\nimposing the relevant priors, so our setup can and should be interpreted as a\\ngeneralization of the classical methodology, allowing more flexibility. We show\\nhow our calculus can be used to develop an analogue of Bayes' rule, with belief\\nfunctions instead of classical probabilities. We also discuss consequences of\\nour theory for legal practice.\\n\",\n",
       "  'title': u'Assessing forensic evidence by computing belief functions'},\n",
       " u'1604.00359': {'arxivid': u'1604.00359',\n",
       "  'authorsaffil': [[u'Tea Tusar', u'Inria'],\n",
       "   [u'Dimo Brockhoff', u'Inria'],\n",
       "   [u'Nikolaus Hansen', u'Inria'],\n",
       "   [u'Anne Auger', u'Inria']],\n",
       "  'categoryterms': [u'cs.AI', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00359v2',\n",
       "  'published': u'2016-04-01T18:55:05Z',\n",
       "  'summary': u'  The bbob-biobj test suite contains 55 bi-objective functions in continuous\\ndomain which are derived from combining functions of the well-known\\nsingle-objective noiseless bbob test suite. Besides giving the actual function\\ndefinitions and presenting their (known) properties, this documentation also\\naims at giving the rationale behind our approach in terms of function groups,\\ninstances, and potential objective space normalization.\\n',\n",
       "  'title': u'COCO: The Bi-objective Black Box Optimization Benchmarking (bbob-biobj)\\n  Test Suite'},\n",
       " u'1508.03040': {'arxivid': u'1508.03040',\n",
       "  'authorsaffil': [[u'Ram\\xf3n Casares', None]],\n",
       "  'categoryterms': [u'cs.CL', u'91F20, 68T20, 68T50, 92D15', u'I.2.7; I.2.8'],\n",
       "  'comment': u'34 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1508.03040v4',\n",
       "  'published': u'2015-08-12T09:04:01Z',\n",
       "  'summary': u'  We are Turing complete, and natural language parsing is decidable, so our\\nsyntactic abilities are in excess to those needed to speak a natural language.\\nThis is an anomaly, because evolution would not select and keep an\\noverqualified feature for long. We solve this anomaly by using a coincidence,\\nboth syntax and problem solving are computing, and a difference, Turing\\ncompleteness is not a requirement of syntax, but of problem solving. Then\\ncomputing should have been shaped by evolutionary requirements coming from both\\nsyntax and problem solving, but the last one, Turing completeness, only from\\nproblem solving. So we propose and analyze a hypothesis: syntax and problem\\nsolving co-evolved in humans towards Turing completeness. Finally, we argue\\nthat Turing completeness, also known as recursion, is our most singular\\nfeature.\\n',\n",
       "  'title': u'Syntax Evolution: Problems and Recursion'},\n",
       " u'1502.02476': {'arxivid': u'1502.02476',\n",
       "  'authorsaffil': [[u'Marc-Alexandre C\\xf4t\\xe9', None],\n",
       "   [u'Hugo Larochelle', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'25 pages, 8 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1502.02476v4',\n",
       "  'published': u'2015-02-09T13:18:24Z',\n",
       "  'summary': u\"  We present a mathematical construction for the restricted Boltzmann machine\\n(RBM) that doesn't require specifying the number of hidden units. In fact, the\\nhidden layer size is adaptive and can grow during training. This is obtained by\\nfirst extending the RBM to be sensitive to the ordering of its hidden units.\\nThen, thanks to a carefully chosen definition of the energy function, we show\\nthat the limit of infinitely many hidden units is well defined. As with RBM,\\napproximate maximum likelihood training can be performed, resulting in an\\nalgorithm that naturally and adaptively adds trained hidden units during\\nlearning. We empirically study the behaviour of this infinite RBM, showing that\\nits performance is competitive to that of the RBM, while not requiring the\\ntuning of a hidden layer size.\\n\",\n",
       "  'title': u'An Infinite Restricted Boltzmann Machine'},\n",
       " u'1603.07704': {'arxivid': u'1603.07704',\n",
       "  'authorsaffil': [[u'Quan Liu', None],\n",
       "   [u'Hui Jiang', None],\n",
       "   [u'Zhen-Hua Ling', None],\n",
       "   [u'Si Wei', None],\n",
       "   [u'Yu Hu', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'Probabilistic reasoning, Deep learning',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07704v1',\n",
       "  'published': u'2016-03-24T18:54:18Z',\n",
       "  'summary': u'  In this paper, we propose a new deep learning approach, called neural\\nassociation model (NAM), for probabilistic reasoning in artificial\\nintelligence. We propose to use neural networks to model association between\\nany two events in a domain. Neural networks take one event as input and compute\\na conditional probability of the other event to model how likely these two\\nevents are associated. The actual meaning of the conditional probabilities\\nvaries between applications and depends on how the models are trained. In this\\nwork, as two case studies, we have investigated two NAM structures, namely deep\\nneural networks (DNNs) and relation modulated neural nets (RMNNs), on several\\nprobabilistic reasoning tasks in AI, including recognizing textual entailment,\\ntriple classification in multirelational knowledge bases and common-sense\\nreasoning. Experimental results on several popular data sets derived from\\nWordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs\\nperform equally well and they can significantly outperform the conventional\\nmethods available for these reasoning tasks. Moreover, comparing with DNNs,\\nRMNNs are superior in knowledge transfer, where a pre-trained model can be\\nquickly extended to an unseen relation after observing only a few training\\nsamples.\\n',\n",
       "  'title': u'Probabilistic Reasoning via Deep Learning: Neural Association Models'},\n",
       " u'1602.00133': {'arxivid': u'1602.00133',\n",
       "  'authorsaffil': [[u'Shen-Yi Zhao', None],\n",
       "   [u'Ru Xiang', None],\n",
       "   [u'Ying-Hao Shi', None],\n",
       "   [u'Peng Gao', None],\n",
       "   [u'Wu-Jun Li', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00133v4',\n",
       "  'published': u'2016-01-30T16:11:53Z',\n",
       "  'summary': u'  Many machine learning models, such as logistic regression~(LR) and support\\nvector machine~(SVM), can be formulated as composite optimization problems.\\nRecently, many distributed stochastic optimization~(DSO) methods have been\\nproposed to solve the large-scale composite optimization problems, which have\\nshown better performance than traditional batch methods. However, most of these\\nDSO methods are not scalable enough. In this paper, we propose a novel DSO\\nmethod, called \\\\underline{s}calable \\\\underline{c}omposite\\n\\\\underline{op}timization for l\\\\underline{e}arning~({SCOPE}), and implement it\\non the fault-tolerant distributed platform \\\\mbox{Spark}. SCOPE is both\\ncomputation-efficient and communication-efficient. Theoretical analysis shows\\nthat SCOPE is convergent with linear convergence rate when the objective\\nfunction is convex. Furthermore, empirical results on real datasets show that\\nSCOPE can outperform other state-of-the-art distributed learning methods on\\nSpark, including both batch learning methods and DSO methods.\\n',\n",
       "  'title': u'SCOPE: Scalable Composite Optimization for Learning on Spark'},\n",
       " u'1602.07280': {'arxivid': u'1602.07280',\n",
       "  'authorsaffil': [[u'Abhishek Sengupta', None],\n",
       "   [u'Vaibhav Rajan', None],\n",
       "   [u'Sakyajit Bhattacharya', None],\n",
       "   [u'G R K Sarma', None]],\n",
       "  'categoryterms': [u'stat.AP', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07280v1',\n",
       "  'published': u'2016-02-22T12:51:39Z',\n",
       "  'summary': u'  Stroke is a major cause of mortality and long--term disability in the world.\\nPredictive outcome models in stroke are valuable for personalized treatment,\\nrehabilitation planning and in controlled clinical trials. In this paper we\\ndesign a new model to predict outcome in the short-term, the putative\\ntherapeutic window for several treatments. Our regression-based model has a\\nparametric form that is designed to address many challenges common in medical\\ndatasets like highly correlated variables and class imbalance. Empirically our\\nmodel outperforms the best--known previous models in predicting short--term\\noutcomes and in inferring the most effective treatments that improve outcome.\\n',\n",
       "  'title': u'A Statistical Model for Stroke Outcome Prediction and Treatment Planning'},\n",
       " u'1604.01999': {'arxivid': u'1604.01999',\n",
       "  'authorsaffil': [[u'Vincent Cohen-Addad', None], [u'Varun Kanade', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01999v2',\n",
       "  'published': u'2016-04-07T13:52:47Z',\n",
       "  'summary': u'  We study online optimization of smoothed piecewise constant functions over\\nthe domain [0, 1). This is motivated by the problem of adaptively picking\\nparameters of learning algorithms as in the recently introduced framework by\\nGupta and Roughgarden (2016). Majority of the machine learning literature has\\nfocused on Lipschitz-continuous functions or functions with bounded gradients.\\n1 This is with good reason---any learning algorithm suffers linear regret even\\nagainst piecewise constant functions that are chosen adversarially, arguably\\nthe simplest of non-Lipschitz continuous functions. The smoothed setting we\\nconsider is inspired by the seminal work of Spielman and Teng (2004) and the\\nrecent work of Gupta and Roughgarden---in this setting, the sequence of\\nfunctions may be chosen by an adversary, however, with some uncertainty in the\\nlocation of discontinuities. We give algorithms that achieve sublinear regret\\nin the full information and bandit settings.\\n',\n",
       "  'title': u'Online Optimization of Smoothed Piecewise Constant Functions'},\n",
       " u'1512.07392': {'arxivid': u'1512.07392',\n",
       "  'authorsaffil': [[u'Lester Mackey', None], [u'Jackson Gorham', None]],\n",
       "  'categoryterms': [u'math.PR'],\n",
       "  'comment': u'12 pages. arXiv admin note: substantial text overlap with\\n  arXiv:1506.03039',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07392v3',\n",
       "  'published': u'2015-12-23T08:58:10Z',\n",
       "  'summary': u'  We establish uniform bounds on the low-order derivatives of Stein equation\\nsolutions for a broad class of multivariate, strongly log-concave target\\ndistributions. These \"Stein factor\" bounds deliver control over Wasserstein and\\nrelated smooth function distances and are well-suited to analyzing the\\ncomputable Stein discrepancy measures of Gorham and Mackey. Our arguments of\\nproof are probabilistic and feature the synchronous coupling of multiple\\noverdamped Langevin diffusions.\\n',\n",
       "  'title': u'Multivariate Stein Factors for a Class of Strongly Log-concave\\n  Distributions'},\n",
       " u'1601.05194': {'arxivid': u'1601.05194',\n",
       "  'authorsaffil': [[u'Kuan-Yu Chen', None],\n",
       "   [u'Shih-Hung Liu', None],\n",
       "   [u'Berlin Chen', None],\n",
       "   [u'Hsin-Min Wang', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.IR'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1506.04365',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.05194v1',\n",
       "  'published': u'2016-01-20T08:26:07Z',\n",
       "  'summary': u'  Extractive summarization aims at selecting a set of indicative sentences from\\na source document as a summary that can express the major theme of the\\ndocument. A general consensus on extractive summarization is that both\\nrelevance and coverage are critical issues to address. The existing methods\\ndesigned to model coverage can be characterized by either reducing redundancy\\nor increasing diversity in the summary. Maximal margin relevance (MMR) is a\\nwidely-cited method since it takes both relevance and redundancy into account\\nwhen generating a summary for a given document. In addition to MMR, there is\\nonly a dearth of research concentrating on reducing redundancy or increasing\\ndiversity for the spoken document summarization task, as far as we are aware.\\nMotivated by these observations, two major contributions are presented in this\\npaper. First, in contrast to MMR, which considers coverage by reducing\\nredundancy, we propose two novel coverage-based methods, which directly\\nincrease diversity. With the proposed methods, a set of representative\\nsentences, which not only are relevant to the given document but also cover\\nmost of the important sub-themes of the document, can be selected\\nautomatically. Second, we make a step forward to plug in several\\ndocument/sentence representation methods into the proposed framework to further\\nenhance the summarization performance. A series of empirical evaluations\\ndemonstrate the effectiveness of our proposed methods.\\n',\n",
       "  'title': u'Improved Spoken Document Summarization with Coverage Modeling Techniques'},\n",
       " u'1602.00426': {'arxivid': u'1602.00426',\n",
       "  'authorsaffil': [[u'Cheng-Tao Chung', None],\n",
       "   [u'Cheng-Yu Tsai', None],\n",
       "   [u'Hsiang-Hung Lu', None],\n",
       "   [u'Chia-Hsiang Liu', None],\n",
       "   [u'Hung-yi Lee', None],\n",
       "   [u'Lin-shan Lee', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1506.02327',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00426v1',\n",
       "  'published': u'2016-02-01T08:37:56Z',\n",
       "  'summary': u'  In this work we aim to discover high quality speech features and linguistic\\nunits directly from unlabeled speech data in a zero resource scenario. The\\nresults are evaluated using the metrics and corpora proposed in the Zero\\nResource Speech Challenge organized at Interspeech 2015. A Multi-layered\\nAcoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets\\nof acoustic tokens from the given corpus. Each acoustic token set is specified\\nby a set of hyperparameters that describe the model configuration. These sets\\nof acoustic tokens carry different characteristics fof the given corpus and the\\nlanguage behind, thus can be mutually reinforced. The multiple sets of token\\nlabels are then used as the targets of a Multi-target Deep Neural Network\\n(MDNN) trained on low-level acoustic features. Bottleneck features extracted\\nfrom the MDNN are then used as the feedback input to the MAT and the MDNN\\nitself in the next iteration. We call this iterative deep learning framework\\nthe Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which\\ngenerates both high quality speech features for the Track 1 of the Challenge\\nand acoustic tokens for the Track 2 of the Challenge. In addition, we performed\\nextra experiments on the same corpora on the application of query-by-example\\nspoken term detection. The experimental results showed the iterative deep\\nlearning framework of MAT-DNN improved the detection performance due to better\\nunderlying speech features and acoustic tokens.\\n',\n",
       "  'title': u'An Iterative Deep Learning Framework for Unsupervised Discovery of\\n  Speech Features and Linguistic Units with Applications on Spoken Term\\n  Detection'},\n",
       " u'1511.06530': {'arxivid': u'1511.06530',\n",
       "  'authorsaffil': [[u'Yong-Deok Kim', None],\n",
       "   [u'Eunhyeok Park', None],\n",
       "   [u'Sungjoo Yoo', None],\n",
       "   [u'Taelim Choi', None],\n",
       "   [u'Lu Yang', None],\n",
       "   [u'Dongjun Shin', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06530v2',\n",
       "  'published': u'2015-11-20T09:20:08Z',\n",
       "  'summary': u'  Although the latest high-end smartphone has powerful CPU and GPU, running\\ndeeper convolutional neural networks (CNNs) for complex tasks such as ImageNet\\nclassification on mobile devices is challenging. To deploy deep CNNs on mobile\\ndevices, we present a simple and effective scheme to compress the entire CNN,\\nwhich we call one-shot whole network compression. The proposed scheme consists\\nof three steps: (1) rank selection with variational Bayesian matrix\\nfactorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning\\nto recover accumulated loss of accuracy, and each step can be easily\\nimplemented using publicly available tools. We demonstrate the effectiveness of\\nthe proposed scheme by testing the performance of various compressed CNNs\\n(AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant\\nreductions in model size, runtime, and energy consumption are obtained, at the\\ncost of small loss in accuracy. In addition, we address the important\\nimplementation level issue on 1?1 convolution, which is a key operation of\\ninception module of GoogLeNet as well as CNNs compressed by our proposed\\nscheme.\\n',\n",
       "  'title': u'Compression of Deep Convolutional Neural Networks for Fast and Low Power\\n  Mobile Applications'},\n",
       " u'1510.05711': {'arxivid': u'1510.05711',\n",
       "  'authorsaffil': [[u'Andrew J. R. Simpson', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'68Txx'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1510.05711v2',\n",
       "  'published': u'2015-10-19T22:38:09Z',\n",
       "  'summary': u'  Deep neural networks (DNN) abstract by demodulating the output of linear\\nfilters. In this article, we refine this definition of abstraction to show that\\nthe inputs of a DNN are abstracted with respect to the filters. Or, to restate,\\nthe abstraction is qualified by the filters. This leads us to introduce the\\nnotion of qualitative projection. We use qualitative projection to abstract\\nMNIST hand-written digits with respect to the various dogs, horses, planes and\\ncars of the CIFAR dataset. We then classify the MNIST digits according to the\\nmagnitude of their dogness, horseness, planeness and carness qualities,\\nillustrating the generality of qualitative projection.\\n',\n",
       "  'title': u'Qualitative Projection Using Deep Neural Networks'},\n",
       " u'1503.01640': {'arxivid': u'1503.01640',\n",
       "  'authorsaffil': [[u'Jifeng Dai', None],\n",
       "   [u'Kaiming He', None],\n",
       "   [u'Jian Sun', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.01640v2',\n",
       "  'published': u'2015-03-05T14:06:53Z',\n",
       "  'summary': u'  Recent leading approaches to semantic segmentation rely on deep convolutional\\nnetworks trained with human-annotated, pixel-level segmentation masks. Such\\npixel-accurate supervision demands expensive labeling effort and limits the\\nperformance of deep networks that usually benefit from more training data. In\\nthis paper, we propose a method that achieves competitive accuracy but only\\nrequires easily obtained bounding box annotations. The basic idea is to iterate\\nbetween automatically generating region proposals and training convolutional\\nnetworks. These two steps gradually recover segmentation masks for improving\\nthe networks, and vise versa. Our method, called BoxSup, produces competitive\\nresults supervised by boxes only, on par with strong baselines fully supervised\\nby masks under the same setting. By leveraging a large amount of bounding\\nboxes, BoxSup further unleashes the power of deep convolutional networks and\\nyields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.\\n',\n",
       "  'title': u'BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks\\n  for Semantic Segmentation'},\n",
       " u'1602.03458': {'arxivid': u'1602.03458',\n",
       "  'authorsaffil': [[u'Thomas K\\xf6hler', None],\n",
       "   [u'Axel Heinrich', None],\n",
       "   [u'Andreas Maier', None],\n",
       "   [u'Joachim Hornegger', None],\n",
       "   [u'Ralf P. Tornow', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'accepted for 2016 IEEE 13th International Symposium on Biomedical\\n  Imaging (ISBI 2016)',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03458v1',\n",
       "  'published': u'2016-02-10T17:30:27Z',\n",
       "  'summary': u'  The acquisition of high-resolution retinal fundus images with a large field\\nof view (FOV) is challenging due to technological, physiological and economic\\nreasons. This paper proposes a fully automatic framework to reconstruct retinal\\nimages of high spatial resolution and increased FOV from multiple\\nlow-resolution images captured with non-mydriatic, mobile and video-capable but\\nlow-cost cameras. Within the scope of one examination, we scan different\\nregions on the retina by exploiting eye motion conducted by a patient guidance.\\nAppropriate views for our mosaicing method are selected based on optic disk\\ntracking to trace eye movements. For each view, one super-resolved image is\\nreconstructed by fusion of multiple video frames. Finally, all super-resolved\\nviews are registered to a common reference using a novel polynomial\\nregistration scheme and combined by means of image mosaicing. We evaluated our\\nframework for a mobile and low-cost video fundus camera. In our experiments, we\\nreconstructed retinal images of up to 30{\\\\deg} FOV from 10 complementary views\\nof 15{\\\\deg} FOV. An evaluation of the mosaics by human experts as well as a\\nquantitative comparison to conventional color fundus images encourage the\\nclinical usability of our framework.\\n',\n",
       "  'title': u'Super-Resolved Retinal Image Mosaicing'},\n",
       " u'1604.03605': {'arxivid': u'1604.03605',\n",
       "  'authorsaffil': [[u'Zoya Bylinskii', None],\n",
       "   [u'Tilke Judd', None],\n",
       "   [u'Aude Oliva', None],\n",
       "   [u'Antonio Torralba', None],\n",
       "   [u'Fr\\xe9do Durand', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03605v1',\n",
       "  'published': u'2016-04-12T22:16:20Z',\n",
       "  'summary': u\"  How best to evaluate a saliency model's ability to predict where humans look\\nin images is an open research question. The choice of evaluation metric depends\\non how saliency is defined and how the ground truth is represented. Metrics\\ndiffer in how they rank saliency models, and this results from how false\\npositives and false negatives are treated, whether viewing biases are accounted\\nfor, whether spatial deviations are factored in, and how the saliency maps are\\npre-processed. In this paper, we provide an analysis of 8 different evaluation\\nmetrics and their properties. With the help of systematic experiments and\\nvisualizations of metric computations, we add interpretability to saliency\\nscores and more transparency to the evaluation of saliency models. Building off\\nthe differences in metric properties and behaviors, we make recommendations for\\nmetric selections under specific assumptions and for specific applications.\\n\",\n",
       "  'title': u'What do different evaluation metrics tell us about saliency models?'},\n",
       " u'1412.4210': {'arxivid': u'1412.4210',\n",
       "  'authorsaffil': [[u'Arunava Banerjee', None]],\n",
       "  'categoryterms': [u'cs.NE'],\n",
       "  'comment': u'31 pages, 5 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1412.4210v2',\n",
       "  'published': u'2014-12-13T09:30:57Z',\n",
       "  'summary': u'  We derive a synaptic weight update rule for learning temporally precise spike\\ntrain to spike train transformations in multilayer feedforward networks of\\nspiking neurons. The framework, aimed at seamlessly generalizing error\\nbackpropagation to the deterministic spiking neuron setting, is based strictly\\non spike timing and avoids invoking concepts pertaining to spike rates or\\nprobabilistic models of spiking. The derivation is founded on two innovations.\\nFirst, an error functional is proposed that compares the spike train emitted by\\nthe output neuron of the network to the desired spike train by way of their\\nputative impact on a virtual postsynaptic neuron. This formulation sidesteps\\nthe need for spike alignment and leads to closed form solutions for all\\nquantities of interest. Second, virtual assignment of weights to spikes rather\\nthan synapses enables a perturbation analysis of individual spike times and\\nsynaptic weights of the output as well as all intermediate neurons in the\\nnetwork, which yields the gradients of the error functional with respect to the\\nsaid entities. Learning proceeds via a gradient descent mechanism that\\nleverages these quantities. Simulation experiments demonstrate the efficacy of\\nthe proposed learning framework. The experiments also highlight asymmetries\\nbetween synapses on excitatory and inhibitory neurons.\\n',\n",
       "  'title': u'Learning Precise Spike Train to Spike Train Transformations in\\n  Multilayer Feedforward Neuronal Networks'},\n",
       " u'1602.06647': {'arxivid': u'1602.06647',\n",
       "  'authorsaffil': [[u'Song Liu', None],\n",
       "   [u'Wanqing Li', None],\n",
       "   [u'Stephen Davis', None],\n",
       "   [u'Christian Ritz', None],\n",
       "   [u'Hongda Tian', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted by MM (IEEE Multimedia Magazine) 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06647v1',\n",
       "  'published': u'2016-02-22T04:49:14Z',\n",
       "  'summary': u'  In this paper, a novel method for automatic planogram compliance checking in\\nretail chains is proposed without requiring product template images for\\ntraining. Product layout is extracted from an input image by means of\\nunsupervised recurring pattern detection and matched via graph matching with\\nthe expected product layout specified by a planogram to measure the level of\\ncompliance. A divide and conquer strategy is employed to improve the speed.\\nSpecifically, the input image is divided into several regions based on the\\nplanogram. Recurring patterns are detected in each region respectively and then\\nmerged together to estimate the product layout. Experimental results on real\\ndata have verified the efficacy of the proposed method. Compared with a\\ntemplate-based method, higher accuracies are achieved by the proposed method\\nover a wide range of products.\\n',\n",
       "  'title': u'Planogram Compliance Checking Based on Detection of Recurring Patterns'},\n",
       " u'1602.06645': {'arxivid': u'1602.06645',\n",
       "  'authorsaffil': [[u'Song Liu', None],\n",
       "   [u'Wanqing Li', None],\n",
       "   [u'Philip Ogunbona', None],\n",
       "   [u'Yang-Wai Chow', None]],\n",
       "  'categoryterms': [u'cs.GR', u'cs.CV'],\n",
       "  'comment': u'2015 International Conference on Digital Image Computing: Techniques\\n  and Applications (DICTA), Page 1 - 8',\n",
       "  'doi': u'10.1109/DICTA.2015.7371249',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06645v1',\n",
       "  'published': u'2016-02-22T04:45:43Z',\n",
       "  'summary': u'  This paper presents an extension to the KinectFusion algorithm which allows\\ncreating simplified 3D models with high quality RGB textures. This is achieved\\nthrough (i) creating model textures using images from an HD RGB camera that is\\ncalibrated with Kinect depth camera, (ii) using a modified scheme to update\\nmodel textures in an asymmetrical colour volume that contains a higher number\\nof voxels than that of the geometry volume, (iii) simplifying dense polygon\\nmesh model using quadric-based mesh decimation algorithm, and (iv) creating and\\nmapping 2D textures to every polygon in the output 3D model. The proposed\\nmethod is implemented in real-time by means of GPU parallel processing.\\nVisualization via ray casting of both geometry and colour volumes provides\\nusers with a real-time feedback of the currently scanned 3D model. Experimental\\nresults show that the proposed method is capable of keeping the model texture\\nquality even for a heavily decimated model and that, when reconstructing small\\nobjects, photorealistic RGB textures can still be reconstructed.\\n',\n",
       "  'title': u'Creating Simplified 3D Models with High Quality Textures'},\n",
       " u'1602.05705': {'arxivid': u'1602.05705',\n",
       "  'authorsaffil': [[u'Jonathan Darren Nix', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'14 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.05705v1',\n",
       "  'published': u'2016-02-18T07:42:00Z',\n",
       "  'summary': u\"  In this paper we explore the application of some notable Boolean methods,\\nnamely the Disjunctive Normal Form representation of logic table expansions,\\nand apply them to a real-valued logic model which utilizes quantities on the\\nrange [0,1] to produce a probabilistic programming of a game character's logic\\nin mathematical form.\\n\",\n",
       "  'title': u'Applying Boolean discrete methods in the production of a real-valued\\n  probabilistic programming model'},\n",
       " u'1602.06136': {'arxivid': u'1602.06136',\n",
       "  'authorsaffil': [[u'Mazen Alsarem', u'DRIM'],\n",
       "   [u'Pierre-Edouard Portier', u'DRIM'],\n",
       "   [u'Sylvie Calabretto', u'DRIM'],\n",
       "   [u'Harald Kosch', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.AI'],\n",
       "  'comment': u\"in French, Revue des Sciences et Technologies de l'Information -\\n  S{\\\\'e}rie Document Num\\\\'erique, Lavoisier, 2015, Nouvelles approches en\\n  recherche d'information, 18 (2-3/2015 ), pp.123-154\",\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06136v1',\n",
       "  'published': u'2016-02-19T13:05:42Z',\n",
       "  'summary': u'  The advances of the Linked Open Data (LOD) initiative are giving rise to a\\nmore structured web of data. Indeed, a few datasets act as hubs (e.g., DBpedia)\\nconnecting many other datasets. They also made possible new web services for\\nentity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for\\nnew applications that will benefit from a combination of the web of documents\\nand the web of data. To ease the emergence of these new use-cases, we propose a\\nquery-biased algorithm for the ranking of entities detected inside a web page.\\nOur algorithm combine link analysis with dimensionality reduction. We use\\ncrowdsourcing for building a publicly available and reusable dataset on which\\nwe compare our algorithm to the state of the art. Finally, we use this\\nalgorithm for the construction of semantic snippets for which we evaluate the\\nusability and the usefulness with a crowdsourcing-based approach.\\n',\n",
       "  'title': u\"Ordonnancement d'entit\\xe9s pour la rencontre du web des documents et du\\n  web des donn\\xe9es\"},\n",
       " u'1602.06489': {'arxivid': u'1602.06489',\n",
       "  'authorsaffil': [[u'Chencheng Li', None],\n",
       "   [u'Pan Zhou', None],\n",
       "   [u'Yingxue Zhou', None],\n",
       "   [u'Kaigui Bian', None],\n",
       "   [u'Tao Jiang', None],\n",
       "   [u'Susanto Rahardja', None]],\n",
       "  'categoryterms': [u'cs.DC', u'cs.LG', u'cs.SI'],\n",
       "  'comment': u'ICC2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06489v1',\n",
       "  'published': u'2016-02-21T02:32:25Z',\n",
       "  'summary': u'  With the rapid growth of Internet technologies, cloud computing and social\\nnetworks have become ubiquitous. An increasing number of people participate in\\nsocial networks and massive online social data are obtained. In order to\\nexploit knowledge from copious amounts of data obtained and predict social\\nbehavior of users, we urge to realize data mining in social networks. Almost\\nall online websites use cloud services to effectively process the large scale\\nof social data, which are gathered from distributed data centers. These data\\nare so large-scale, high-dimension and widely distributed that we propose a\\ndistributed sparse online algorithm to handle them. Additionally,\\nprivacy-protection is an important point in social networks. We should not\\ncompromise the privacy of individuals in networks, while these social data are\\nbeing learned for data mining. Thus we also consider the privacy problem in\\nthis article. Our simulations shows that the appropriate sparsity of data would\\nenhance the performance of our algorithm and the privacy-preserving method does\\nnot significantly hurt the performance of the proposed algorithm.\\n',\n",
       "  'title': u'Distributed Private Online Learning for Social Big Data Computing over\\n  Data Center Networks'},\n",
       " u'1511.04590': {'arxivid': u'1511.04590',\n",
       "  'authorsaffil': [[u'Li Yao', None],\n",
       "   [u'Nicolas Ballas', None],\n",
       "   [u'Kyunghyun Cho', None],\n",
       "   [u'John R. Smith', None],\n",
       "   [u'Yoshua Bengio', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.CL', u'stat.ML'],\n",
       "  'comment': u'ICLR 2016 submission, updated on 6th Jan 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.04590v4',\n",
       "  'published': u'2015-11-14T18:02:39Z',\n",
       "  'summary': u'  The task of associating images and videos with a natural language description\\nhas attracted a great amount of attention recently. Rapid progress has been\\nmade in terms of both developing novel algorithms and releasing new datasets.\\nIndeed, the state-of-the-art results on some of the standard datasets have been\\npushed into the regime where it has become more and more difficult to make\\nsignificant improvements. Instead of proposing new models, this work\\ninvestigates the possibility of empirically establishing performance upper\\nbounds on various visual captioning datasets without extra data labelling\\neffort or human evaluation. In particular, it is assumed that visual captioning\\nis decomposed into two steps: from visual inputs to visual concepts, and from\\nvisual concepts to natural language descriptions. One would be able to obtain\\nan upper bound when assuming the first step is perfect and only requiring\\ntraining a conditional language model for the second step. We demonstrate the\\nconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination\\nof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used\\nfor visual concept extraction in the first step and the simplicity of the\\nlanguage model for the second step, we show that current state-of-the-art\\nmodels fall short when being compared with the learned upper bounds.\\nFurthermore, with such a bound, we quantify several important factors\\nconcerning image and video captioning: the number of visual concepts captured\\nby different models, the trade-off between the amount of visual elements\\ncaptured and their accuracy, and the intrinsic difficulty and blessing of\\ndifferent datasets.\\n',\n",
       "  'title': u'Empirical performance upper bounds for image and video captioning'},\n",
       " u'1306.5860': {'arxivid': u'1306.5860',\n",
       "  'authorsaffil': [[u'Berk Ustun', None],\n",
       "   [u'Stefano Traca', None],\n",
       "   [u'Cynthia Rudin', None]],\n",
       "  'categoryterms': [u'stat.ML'],\n",
       "  'comment': u'Short version',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1306.5860v1',\n",
       "  'published': u'2013-06-25T07:02:20Z',\n",
       "  'summary': u'  We introduce Supersparse Linear Integer Models (SLIM) as a tool to create\\nscoring systems for binary classification. We derive theoretical bounds on the\\ntrue risk of SLIM scoring systems, and present experimental results to show\\nthat SLIM scoring systems are accurate, sparse, and interpretable\\nclassification models.\\n',\n",
       "  'title': u'Supersparse Linear Integer Models for Predictive Scoring Systems'},\n",
       " u'1602.06484': {'arxivid': u'1602.06484',\n",
       "  'authorsaffil': [[u'Mark O. Riedl', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'5 pages, published in the CHI 2016 Workshop on Human-Centered Machine\\n  Learning',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.06484v1',\n",
       "  'published': u'2016-02-21T01:59:09Z',\n",
       "  'summary': u'  Narrative intelligence is the ability to craft, tell, understand, and respond\\naffectively to stories. We argue that instilling artificial intelligences with\\ncomputational narrative intelligence affords a number of applications\\nbeneficial to humans. We lay out some of the machine learning challenges\\nnecessary to solve to achieve computational narrative intelligence. Finally, we\\nargue that computational narrative is a practical step towards machine\\nenculturation, the teaching of sociocultural values to machines.\\n',\n",
       "  'title': u'Computational Narrative Intelligence: A Human-Centered Goal for\\n  Artificial Intelligence'},\n",
       " u'1604.05417': {'arxivid': u'1604.05417',\n",
       "  'authorsaffil': [[u'Swami Sankaranarayanan', None],\n",
       "   [u'Azadeh Alavi', None],\n",
       "   [u'Carlos Castillo', None],\n",
       "   [u'Rama Chellappa', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.LG', u'stat.ML'],\n",
       "  'comment': u'arXiv admin note: text overlap with arXiv:1602.03418',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.05417v2',\n",
       "  'published': u'2016-04-19T03:29:56Z',\n",
       "  'summary': u'  Despite significant progress made over the past twenty five years,\\nunconstrained face verification remains a challenging problem. This paper\\nproposes an approach that couples a deep CNN-based approach with a\\nlow-dimensional discriminative embedding learned using triplet probability\\nconstraints to solve the unconstrained face verification problem. Aside from\\nyielding performance improvements, this embedding provides significant\\nadvantages in terms of memory and for post-processing operations like subject\\nspecific clustering. Experiments on the challenging IJB-A dataset show that the\\nproposed algorithm performs comparably or better than the state of the art\\nmethods in verification and identification metrics, while requiring much less\\ntraining data and training time. The superior performance of the proposed\\nmethod on the CFP dataset shows that the representation learned by our deep CNN\\nis robust to extreme pose variation. Furthermore, we demonstrate the robustness\\nof the deep features to challenges including age, pose, blur and clutter by\\nperforming simple clustering experiments on both IJB-A and LFW datasets.\\n',\n",
       "  'title': u'Triplet Probabilistic Embedding for Face Verification and Clustering'},\n",
       " u'1505.06389': {'arxivid': u'1505.06389',\n",
       "  'authorsaffil': [[u'Ting Liu', None],\n",
       "   [u'Mojtaba Seyedhosseini', None],\n",
       "   [u'Tolga Tasdizen', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.06389v2',\n",
       "  'published': u'2015-05-24T00:22:09Z',\n",
       "  'summary': u'  This paper investigates one of the most fundamental computer vision problems:\\nimage segmentation. We propose a supervised hierarchical approach to\\nobject-independent image segmentation. Starting with over-segmenting\\nsuperpixels, we use a tree structure to represent the hierarchy of region\\nmerging, by which we reduce the problem of segmenting image regions to finding\\na set of label assignment to tree nodes. We formulate the tree structure as a\\nconstrained conditional model to associate region merging with likelihoods\\npredicted using an ensemble boundary classifier. Final segmentations can then\\nbe inferred by finding globally optimal solutions to the model efficiently. We\\nalso present an iterative training and testing algorithm that generates various\\ntree structures and combines them to emphasize accurate boundaries by\\nsegmentation accumulation. Experiment results and comparisons with other very\\nrecent methods on six public data sets demonstrate that our approach achieves\\nthe state-of-the-art region accuracy and is very competitive in image\\nsegmentation without semantic priors.\\n',\n",
       "  'title': u'Image Segmentation Using Hierarchical Merge Tree'},\n",
       " u'1512.07711': {'arxivid': u'1512.07711',\n",
       "  'authorsaffil': [[u'Yongxi Lu', None],\n",
       "   [u'Tara Javidi', None],\n",
       "   [u'Svetlana Lazebnik', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Accepted to CVPR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07711v2',\n",
       "  'published': u'2015-12-24T04:20:16Z',\n",
       "  'summary': u'  State-of-the-art object detection systems rely on an accurate set of region\\nproposals. Several recent methods use a neural network architecture to\\nhypothesize promising object locations. While these approaches are\\ncomputationally efficient, they rely on fixed image regions as anchors for\\npredictions. In this paper we propose to use a search strategy that adaptively\\ndirects computational resources to sub-regions likely to contain objects.\\nCompared to methods based on fixed anchor locations, our approach naturally\\nadapts to cases where object instances are sparse and small. Our approach is\\ncomparable in terms of accuracy to the state-of-the-art Faster R-CNN approach\\nwhile using two orders of magnitude fewer anchors on average. Code is publicly\\navailable.\\n',\n",
       "  'title': u'Adaptive Object Detection Using Adjacency and Zoom Prediction'},\n",
       " u'1601.02300': {'arxivid': u'1601.02300',\n",
       "  'authorsaffil': [[u'Young-Min Kim', None],\n",
       "   [u'Julien Velcin', None],\n",
       "   [u'St\\xe9phane Bonnevay', None],\n",
       "   [u'Marian-Andrei Rizoiu', None]],\n",
       "  'categoryterms': [u'cs.IR', u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1007/978-3-319-16354-3_66',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.02300v1',\n",
       "  'published': u'2016-01-11T02:06:36Z',\n",
       "  'summary': u'  Evolutionary clustering aims at capturing the temporal evolution of clusters.\\nThis issue is particularly important in the context of social media data that\\nare naturally temporally driven. In this paper, we propose a new probabilistic\\nmodel-based evolutionary clustering technique. The Temporal Multinomial Mixture\\n(TMM) is an extension of classical mixture model that optimizes feature\\nco-occurrences in the trade-off with temporal smoothness. Our model is\\nevaluated for two recent case studies on opinion aggregation over time. We\\ncompare four different probabilistic clustering models and we show the\\nsuperiority of our proposal in the task of instance-oriented clustering.\\n',\n",
       "  'title': u'Temporal Multinomial Mixture for Instance-Oriented Evolutionary\\n  Clustering'},\n",
       " u'1603.09454': {'arxivid': u'1603.09454',\n",
       "  'authorsaffil': [[u'Wenxi Liu', None],\n",
       "   [u'Rynson W. H. Lau', None],\n",
       "   [u'Xiaogang Wang', None],\n",
       "   [u'Dinesh Manocha', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.09454v1',\n",
       "  'published': u'2016-03-31T04:58:25Z',\n",
       "  'summary': u'  In this paper, we present a novel method to recognize the types of crowd\\nmovement from crowd trajectories using agent-based motion models (AMMs). Our\\nidea is to apply a number of AMMs, referred to as exemplar-AMMs, to describe\\nthe crowd movement. Specifically, we propose an optimization framework that\\nfilters out the unknown noise in the crowd trajectories and measures their\\nsimilarity to the exemplar-AMMs to produce a crowd motion feature. We then\\naddress our real-world crowd movement recognition problem as a multi-label\\nclassification problem. Our experiments show that the proposed feature\\noutperforms the state-of-the-art methods in recognizing both simulated and\\nreal-world crowd movements from their trajectories. Finally, we have created a\\nsynthetic dataset, SynCrowd, which contains 2D crowd trajectories in various\\nscenarios, generated by various crowd simulators. This dataset can serve as a\\ntraining set or benchmark for crowd analysis work.\\n',\n",
       "  'title': u'Exemplar-AMMs: Recognizing Crowd Movements from Pedestrian Trajectories'},\n",
       " u'1505.01861': {'arxivid': u'1505.01861',\n",
       "  'authorsaffil': [[u'Yingwei Pan', None],\n",
       "   [u'Tao Mei', None],\n",
       "   [u'Ting Yao', None],\n",
       "   [u'Houqiang Li', None],\n",
       "   [u'Yong Rui', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.MM'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.01861v3',\n",
       "  'published': u'2015-05-07T20:13:33Z',\n",
       "  'summary': u'  Automatically describing video content with natural language is a fundamental\\nchallenge of multimedia. Recurrent Neural Networks (RNN), which models sequence\\ndynamics, has attracted increasing attention on visual interpretation. However,\\nmost existing approaches generate a word locally with given previous words and\\nthe visual content, while the relationship between sentence semantics and\\nvisual content is not holistically exploited. As a result, the generated\\nsentences may be contextually correct but the semantics (e.g., subjects, verbs\\nor objects) are not true.\\n  This paper presents a novel unified framework, named Long Short-Term Memory\\nwith visual-semantic Embedding (LSTM-E), which can simultaneously explore the\\nlearning of LSTM and visual-semantic embedding. The former aims to locally\\nmaximize the probability of generating the next word given previous words and\\nvisual content, while the latter is to create a visual-semantic embedding space\\nfor enforcing the relationship between the semantics of the entire sentence and\\nvisual content. Our proposed LSTM-E consists of three components: a 2-D and/or\\n3-D deep convolutional neural networks for learning powerful video\\nrepresentation, a deep RNN for generating sentences, and a joint embedding\\nmodel for exploring the relationships between visual content and sentence\\nsemantics. The experiments on YouTube2Text dataset show that our proposed\\nLSTM-E achieves to-date the best reported performance in generating natural\\nsentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also\\ndemonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO)\\ntriplets to several state-of-the-art techniques.\\n',\n",
       "  'title': u'Jointly Modeling Embedding and Translation to Bridge Video and Language'},\n",
       " u'1603.08109': {'arxivid': u'1603.08109',\n",
       "  'authorsaffil': [[u'Kunal N. Chaudhury', None],\n",
       "   [u'Swapnil D. Dabhade', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'To appear in IEEE Transactions on Image Processing (10 pages, 10\\n  figures, 4 tables)',\n",
       "  'doi': u'10.1109/TIP.2016.2548363',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08109v1',\n",
       "  'published': u'2016-03-26T14:05:34Z',\n",
       "  'summary': u'  The bilateral filter is a non-linear filter that uses a range filter along\\nwith a spatial filter to perform edge-preserving smoothing of images. A direct\\ncomputation of the bilateral filter requires $O(S)$ operations per pixel, where\\n$S$ is the size of the support of the spatial filter. In this paper, we present\\na fast and provably accurate algorithm for approximating the bilateral filter\\nwhen the range kernel is Gaussian. In particular, for box and Gaussian spatial\\nfilters, the proposed algorithm can cut down the complexity to $O(1)$ per pixel\\nfor any arbitrary $S$. The algorithm has a simple implementation involving\\n$N+1$ spatial filterings, where $N$ is the approximation order. We give a\\ndetailed analysis of the filtering accuracy that can be achieved by the\\nproposed approximation in relation to the target bilateral filter. This allows\\nus to to estimate the order $N$ required to obtain a given accuracy. We also\\npresent comprehensive numerical results to demonstrate that the proposed\\nalgorithm is competitive with state-of-the-art methods in terms of speed and\\naccuracy.\\n',\n",
       "  'title': u'Fast and Provably Accurate Bilateral Filtering'},\n",
       " u'1603.08108': {'arxivid': u'1603.08108',\n",
       "  'authorsaffil': [[u'Liangtian He', None],\n",
       "   [u'Yilun Wang', None],\n",
       "   [u'Zhaoyin Xiang', None]],\n",
       "  'categoryterms': [u'cs.CV', u'90C26', u'I.4.3'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08108v1',\n",
       "  'published': u'2016-03-26T13:45:01Z',\n",
       "  'summary': u'  The wavelet frame systems have been playing an active role in image\\nrestoration and many other image processing fields over the past decades, owing\\nto the good capability of sparsely approximating piece-wise smooth functions\\nsuch as images. In this paper, we propose a novel wavelet frame based sparse\\nrecovery model called \\\\textit{Support Driven Sparse Regularization} (SDSR) for\\nimage deblurring, where the partial support information of frame coefficients\\nis attained via a self-learning strategy and exploited via the proposed\\ntruncated $\\\\ell_0$ regularization. Moreover, the state-of-the-art image\\nrestoration methods can be naturally incorporated into our proposed wavelet\\nframe based sparse recovery framework. In particular, in order to achieve\\nreliable support estimation of the frame coefficients, we make use of the\\nstate-of-the-art image restoration result such as that from the IDD-BM3D method\\nas the initial reference image for support estimation. Our extensive\\nexperimental results have shown convincing improvements over existing\\nstate-of-the-art deblurring methods.\\n',\n",
       "  'title': u'Support Driven Wavelet Frame-based Image Deblurring'},\n",
       " u'1511.06281': {'arxivid': u'1511.06281',\n",
       "  'authorsaffil': [[u'Johannes Ball\\xe9', None],\n",
       "   [u'Valero Laparra', None],\n",
       "   [u'Eero P. Simoncelli', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': u'published as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06281v4',\n",
       "  'published': u'2015-11-19T17:52:01Z',\n",
       "  'summary': u'  We introduce a parametric nonlinear transformation that is well-suited for\\nGaussianizing data from natural images. The data are linearly transformed, and\\neach component is then normalized by a pooled activity measure, computed by\\nexponentiating a weighted sum of rectified and exponentiated components and a\\nconstant. We optimize the parameters of the full transformation (linear\\ntransform, exponents, weights, constant) over a database of natural images,\\ndirectly minimizing the negentropy of the responses. The optimized\\ntransformation substantially Gaussianizes the data, achieving a significantly\\nsmaller mutual information between transformed components than alternative\\nmethods including ICA and radial Gaussianization. The transformation is\\ndifferentiable and can be efficiently inverted, and thus induces a density\\nmodel on images. We show that samples of this model are visually similar to\\nsamples of natural image patches. We demonstrate the use of the model as a\\nprior probability density that can be used to remove additive noise. Finally,\\nwe show that the transformation can be cascaded, with each layer optimized\\nusing the same Gaussianization objective, thus offering an unsupervised method\\nof optimizing a deep network architecture.\\n',\n",
       "  'title': u'Density Modeling of Images using a Generalized Normalization\\n  Transformation'},\n",
       " u'1603.07012': {'arxivid': u'1603.07012',\n",
       "  'authorsaffil': [[u'Dayu Yuan', None],\n",
       "   [u'Ryan Doherty', None],\n",
       "   [u'Julian Richardson', None],\n",
       "   [u'Colin Evans', None],\n",
       "   [u'Eric Altendorf', None]],\n",
       "  'categoryterms': [u'cs.CL'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07012v1',\n",
       "  'published': u'2016-03-22T22:15:10Z',\n",
       "  'summary': u'  Determining the intended sense of words in text -- word sense disambiguation\\n(WSD) -- is a long-standing problem in natural language processing. In this\\npaper, we present WSD algorithms which use neural network language models to\\nachieve state-of-the-art precision. Each of these methods learns to\\ndisambiguate word senses using only a set of word senses, a few example\\nsentences for each sense taken from a licensed lexicon, and a large unlabeled\\ntext corpus. We classify based on cosine similarity of vectors derived from the\\ncontexts in unlabeled query and labeled example sentences. We demonstrate\\nstate-of-the-art results when using the WordNet sense inventory, and\\nsignificantly better than baseline performance using the New Oxford American\\nDictionary inventory. The best performance was achieved by combining an LSTM\\nlanguage model with graph label propagation.\\n',\n",
       "  'title': u'Word Sense Disambiguation with Neural Language Models'},\n",
       " u'1603.03130': {'arxivid': u'1603.03130',\n",
       "  'authorsaffil': [[u'Gang Niu', None],\n",
       "   [u'Marthinus Christoffel du Plessis', None],\n",
       "   [u'Tomoya Sakai', None],\n",
       "   [u'Yao Ma', None],\n",
       "   [u'Masashi Sugiyama', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03130v2',\n",
       "  'published': u'2016-03-10T02:53:52Z',\n",
       "  'summary': u'  In PU learning, a binary classifier is trained from positive (P) and\\nunlabeled (U) data without negative (N) data. Although N data is missing, it\\nsometimes outperforms PN learning (i.e., ordinary supervised learning).\\nHitherto, neither theoretical nor experimental analysis has been given to\\nexplain this phenomenon. In this paper, we theoretically compare PU (and NU)\\nlearning against PN learning based on the upper bounds of estimation errors. We\\nfind simple conditions when PU and NU learning are likely to outperform PN\\nlearning, and we prove that, in terms of the upper bounds, either PU or NU\\nlearning (depending on the class-prior probability and the sizes of P and N\\ndata) given infinite U data will improve on PN learning. Our theoretical\\nfindings well agree with the experimental results on artificial and benchmark\\ndata even when the experimental setup does not match the theoretical\\nassumptions exactly.\\n',\n",
       "  'title': u'Theoretical Comparisons of Positive-Unlabeled Learning against\\n  Positive-Negative Learning'},\n",
       " u'1604.01792': {'arxivid': u'1604.01792',\n",
       "  'authorsaffil': [[u'Tom Sercu', None], [u'Vaibhava Goel', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01792v1',\n",
       "  'published': u'2016-04-06T20:07:52Z',\n",
       "  'summary': u'  Very deep CNNs with small 3x3 kernels have recently been shown to achieve\\nvery strong performance as acoustic models in hybrid NN-HMM speech recognition\\nsystems. In this paper, we demonstrate that the accuracy gains of these deep\\nCNNs are retained both on larger scale data, and after sequence training. We\\nshow this by carrying out sequence training on both the 300h switchboard-1 and\\nthe 2000h switchboard dataset. Furthermore, we investigate how pooling and\\npadding in time influences performance, both in terms of word error rate and\\ncomputational cost. We argue that designing CNNs without timepadding and\\nwithout timepooling, though slightly suboptimal for accuracy, has two\\nsignificant consequences. Firstly, the proposed design allows for efficient\\nevaluation at sequence training and test (deployment) time. Secondly, this\\ndesign principle allows for batch normalization to be adopted to CNNs on\\nsequence data. Our very deep CNN model sequence trained on the 2000h\\nswitchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matching\\nwith a single model the performance of 2015 IBM system combination, which was\\nthe previous best published result.\\n',\n",
       "  'title': u'Advances in Very Deep Convolutional Neural Networks for LVCSR'},\n",
       " u'1602.07261': {'arxivid': u'1602.07261',\n",
       "  'authorsaffil': [[u'Christian Szegedy', None],\n",
       "   [u'Sergey Ioffe', None],\n",
       "   [u'Vincent Vanhoucke', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.07261v1',\n",
       "  'published': u'2016-02-23T18:44:39Z',\n",
       "  'summary': u'  Very deep convolutional networks have been central to the largest advances in\\nimage recognition performance in recent years. One example is the Inception\\narchitecture that has been shown to achieve very good performance at relatively\\nlow computational cost. Recently, the introduction of residual connections in\\nconjunction with a more traditional architecture has yielded state-of-the-art\\nperformance in the 2015 ILSVRC challenge; its performance was similar to the\\nlatest generation Inception-v3 network. This raises the question of whether\\nthere are any benefit in combining the Inception architecture with residual\\nconnections. Here we give clear empirical evidence that training with residual\\nconnections accelerates the training of Inception networks significantly. There\\nis also some evidence of residual Inception networks outperforming similarly\\nexpensive Inception networks without residual connections by a thin margin. We\\nalso present several new streamlined architectures for both residual and\\nnon-residual Inception networks. These variations improve the single-frame\\nrecognition performance on the ILSVRC 2012 classification task significantly.\\nWe further demonstrate how proper activation scaling stabilizes the training of\\nvery wide residual Inception networks. With an ensemble of three residual and\\none Inception-v4, we achieve 3.08 percent top-5 error on the test set of the\\nImageNet classification (CLS) challenge\\n',\n",
       "  'title': u'Inception-v4, Inception-ResNet and the Impact of Residual Connections on\\n  Learning'},\n",
       " u'1603.07415': {'arxivid': u'1603.07415',\n",
       "  'authorsaffil': [[u'Jianan Li', None],\n",
       "   [u'Yunchao Wei', None],\n",
       "   [u'Xiaodan Liang', None],\n",
       "   [u'Jian Dong', None],\n",
       "   [u'Tingfa Xu', None],\n",
       "   [u'Jiashi Feng', None],\n",
       "   [u'Shuicheng Yan', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07415v1',\n",
       "  'published': u'2016-03-24T02:18:37Z',\n",
       "  'summary': u'  Modern deep neural network based object detection methods typically classify\\ncandidate proposals using their interior features. However, global and local\\nsurrounding contexts that are believed to be valuable for object detection are\\nnot fully exploited by existing methods yet. In this work, we take a step\\ntowards understanding what is a robust practice to extract and utilize\\ncontextual information to facilitate object detection in practice.\\nSpecifically, we consider the following two questions: \"how to identify useful\\nglobal contextual information for detecting a certain object?\" and \"how to\\nexploit local context surrounding a proposal for better inferring its\\ncontents?\". We provide preliminary answers to these questions through\\ndeveloping a novel Attention to Context Convolution Neural Network (AC-CNN)\\nbased object detection model. AC-CNN effectively incorporates global and local\\ncontextual information into the region-based CNN (e.g. Fast RCNN) detection\\nmodel and provides better object detection performance. It consists of one\\nattention-based global contextualized (AGC) sub-network and one multi-scale\\nlocal contextualized (MLC) sub-network. To capture global context, the AGC\\nsub-network recurrently generates an attention map for an input image to\\nhighlight useful global contextual locations, through multiple stacked Long\\nShort-Term Memory (LSTM) layers. For capturing surrounding local context, the\\nMLC sub-network exploits both the inside and outside contextual information of\\neach specific proposal at multiple scales. The global and local context are\\nthen fused together for making the final decision for detection. Extensive\\nexperiments on PASCAL VOC 2007 and VOC 2012 well demonstrate the superiority of\\nthe proposed AC-CNN over well-established baselines. In particular, AC-CNN\\noutperforms the popular Fast-RCNN by 2.0% and 2.2% on VOC 2007 and VOC 2012 in\\nterms of mAP, respectively.\\n',\n",
       "  'title': u'Attentive Contexts for Object Detection'},\n",
       " u'1402.0240': {'arxivid': u'1402.0240',\n",
       "  'authorsaffil': [[u'Stefanie Jegelka', u'MIT'],\n",
       "   [u'Jeff Bilmes', u'University of Washington']],\n",
       "  'categoryterms': [u'cs.DS', u'cs.CV', u'cs.DM', u'math.OC'],\n",
       "  'comment': u'46 pages',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1402.0240v4',\n",
       "  'published': u'2014-02-02T20:03:19Z',\n",
       "  'summary': u'  We study an extension of the classical graph cut problem, wherein we replace\\nthe modular (sum of edge weights) cost function by a submodular set function\\ndefined over graph edges. Special cases of this problem have appeared in\\ndifferent applications in signal processing, machine learning, and computer\\nvision. In this paper, we connect these applications via the generic\\nformulation of \"cooperative graph cuts\", for which we study complexity,\\nalgorithms, and connections to polymatroidal network flows. Finally, we compare\\nthe proposed algorithms empirically.\\n',\n",
       "  'title': u'Graph Cuts with Interacting Edge Costs - Examples, Approximations, and\\n  Algorithms'},\n",
       " u'1603.07141': {'arxivid': u'1603.07141',\n",
       "  'authorsaffil': [[u'Arnau Ramisa', None],\n",
       "   [u'Fei Yan', None],\n",
       "   [u'Francesc Moreno-Noguer', None],\n",
       "   [u'Krystian Mikolajczyk', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07141v1',\n",
       "  'published': u'2016-03-23T11:30:24Z',\n",
       "  'summary': u'  Building upon recent Deep Neural Network architectures, current approaches\\nlying in the intersection of computer vision and natural language processing\\nhave achieved unprecedented breakthroughs in tasks like automatic captioning or\\nimage retrieval. Most of these learning methods, though, rely on large training\\nsets of images associated with human annotations that specifically describe the\\nvisual content. In this paper we propose to go a step further and explore the\\nmore complex cases where textual descriptions are loosely related to the\\nimages. We focus on the particular domain of News articles in which the textual\\ncontent often expresses connotative and ambiguous relations that are only\\nsuggested but not directly inferred from images. We introduce new deep learning\\nmethods that address source detection, popularity prediction, article\\nillustration and geolocation of articles. An adaptive CNN architecture is\\nproposed, that shares most of the structure for all the tasks, and is suitable\\nfor multitask and transfer learning. Deep Canonical Correlation Analysis is\\ndeployed for article illustration, and a new loss function based on Great\\nCircle Distance is proposed for geolocation. Furthermore, we present\\nBreakingNews, a novel dataset with approximately 100K news articles including\\nimages, text and captions, and enriched with heterogeneous meta-data (such as\\nGPS coordinates and popularity metrics). We show this dataset to be appropriate\\nto explore all aforementioned problems, for which we provide a baseline\\nperformance using various Deep Learning architectures, and different\\nrepresentations of the textual and visual features. We report very promising\\nresults and bring to light several limitations of current state-of-the-art in\\nthis kind of domain, which we hope will help spur progress in the field.\\n',\n",
       "  'title': u'BreakingNews: Article Annotation by Image and Text Processing'},\n",
       " u'1511.06442': {'arxivid': u'1511.06442',\n",
       "  'authorsaffil': [[u'Henry Gouk', None],\n",
       "   [u'Bernhard Pfahringer', None],\n",
       "   [u'Michael Cree', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06442v5',\n",
       "  'published': u'2015-11-19T23:10:00Z',\n",
       "  'summary': u'  Similarity metrics are a core component of many information retrieval and\\nmachine learning systems. In this work we propose a method capable of learning\\na similarity metric from data equipped with a binary relation. By considering\\nonly the similarity constraints, and initially ignoring the features, we are\\nable to learn target vectors for each instance using one of several\\nappropriately designed loss functions. A regression model can then be\\nconstructed that maps novel feature vectors to the same target vector space,\\nresulting in a feature extractor that computes vectors for which a predefined\\nmetric is a meaningful measure of similarity. We present results on both\\nmulticlass and multi-label classification datasets that demonstrate\\nconsiderably faster convergence, as well as higher accuracy on the majority of\\nthe intrinsic evaluation tasks and all extrinsic evaluation tasks.\\n',\n",
       "  'title': u'Fast Metric Learning For Deep Neural Networks'},\n",
       " u'1512.04280': {'arxivid': u'1512.04280',\n",
       "  'authorsaffil': [[u'Liang Lu', None], [u'Steve Renals', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.LG', u'cs.NE'],\n",
       "  'comment': u'5 pages, 3 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.04280v2',\n",
       "  'published': u'2015-12-14T12:29:32Z',\n",
       "  'summary': u'  For speech recognition, deep neural networks (DNNs) have significantly\\nimproved the recognition accuracy in most of benchmark datasets and application\\ndomains. However, compared to the conventional Gaussian mixture models,\\nDNN-based acoustic models usually have much larger number of model parameters,\\nmaking it challenging for their applications in resource constrained platforms,\\ne.g., mobile devices. In this paper, we study the application of the recently\\nproposed highway network to train small-footprint DNNs, which are {\\\\it thinner}\\nand {\\\\it deeper}, and have significantly smaller number of model parameters\\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\\nspeech transcription corpus which has around 70 hours of audio data. The\\nhighway neural networks constantly outperformed their plain DNN counterparts,\\nand the number of model parameters can be reduced significantly without\\nsacrificing the recognition accuracy.\\n',\n",
       "  'title': u'Small-footprint Deep Neural Networks with Highway Connections for Speech\\n  Recognition'},\n",
       " u'1602.00554': {'arxivid': u'1602.00554',\n",
       "  'authorsaffil': [[u'Bj\\xf6rn Weghenkel', None],\n",
       "   [u'Asja Fischer', None],\n",
       "   [u'Laurenz Wiskott', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00554v1',\n",
       "  'published': u'2016-02-01T15:11:48Z',\n",
       "  'summary': u'  We propose a new method for the unsupervised extraction of predictable\\nfeatures from high-dimensional time-series, where high predictability is\\nunderstood very generically as low variance in the distribution of the next\\ndata point given the current one. We show how this objective can be understood\\nin terms of graph embedding as well as how it corresponds to the\\ninformation-theoretic measure of excess entropy in special cases.\\nExperimentally, we compare the approach to two other algorithms for the\\nextraction of predictable features, namely ForeCA and PFA, and show how it is\\nable to outperform them in certain settings.\\n',\n",
       "  'title': u'Graph-based Predictable Feature Analysis'},\n",
       " u'1511.06444': {'arxivid': u'1511.06444',\n",
       "  'authorsaffil': [[u'Levent Sagun', None],\n",
       "   [u'Thomas Trogdon', None],\n",
       "   [u'Yann LeCun', None]],\n",
       "  'categoryterms': [u'cs.LG', u'math.NA', u'math.PR', u'65K10, 82D30, 37E20'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06444v2',\n",
       "  'published': u'2015-11-19T23:14:25Z',\n",
       "  'summary': u'  The authors present empirical universal distributions for the halting time\\n(measured by the number of iterations to reach a given accuracy) of\\noptimization algorithms applied to two random systems: spin glasses and deep\\nlearning. Given an algorithm, which we take to be both the optimization routine\\nand the form of the random landscape, the fluctuations of the halting time\\nfollow a distribution that remains unchanged even when the input is changed\\ndrastically. We observe two main universality classes, a Gumbel-like\\ndistribution that appears in Google searches, human decision times, QR\\nfactorization and spin glasses, and a Gaussian-like distribution that appears\\nin conjugate gradient method, deep network with MNIST input data and deep\\nnetwork with random input data.\\n',\n",
       "  'title': u'Universality in halting time and its applications in optimization'},\n",
       " u'1603.00050': {'arxivid': u'1603.00050',\n",
       "  'authorsaffil': [[u'Glenn Healey', None]],\n",
       "  'categoryterms': [u'stat.AP', u'cs.LG', u'I.2.6'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.00050v1',\n",
       "  'published': u'2016-02-21T06:27:40Z',\n",
       "  'summary': u'  We present an algorithm for learning the intrinsic value of a batted ball in\\nbaseball. This work addresses the fundamental problem of separating the value\\nof a batted ball at contact from factors such as the defense, weather, and\\nballpark that can affect its observed outcome. The algorithm uses a Bayesian\\nmodel to construct a continuous mapping from a vector of batted ball parameters\\nto an intrinsic measure defined as the expected value of a linear weights\\nrepresentation for run value. A kernel method is used to build nonparametric\\nestimates for the component probability density functions in Bayes theorem from\\na set of over one hundred thousand batted ball measurements recorded by the\\nHITf/x system during the 2014 major league baseball (MLB) season.\\nCross-validation is used to determine the optimal vector of smoothing\\nparameters for the density estimates. Properties of the mapping are visualized\\nby considering reduced-dimension subsets of the batted ball parameter space. We\\nuse the mapping to derive statistics for intrinsic quality of contact for\\nbatters and pitchers which have the potential to improve the accuracy of player\\nmodels and forecasting systems. We also show that the new approach leads to a\\nsimple automated measure of contact-adjusted defense and provides insight into\\nthe impact of environmental variables on batted balls.\\n',\n",
       "  'title': u'Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of\\n  a Batted Ball'},\n",
       " u'1511.06448': {'arxivid': u'1511.06448',\n",
       "  'authorsaffil': [[u'Pouya Bashivan', None],\n",
       "   [u'Irina Rish', None],\n",
       "   [u'Mohammed Yeasin', None],\n",
       "   [u'Noel Codella', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': u'To be published as a conference paper at ICLR 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06448v3',\n",
       "  'published': u'2015-11-19T23:29:55Z',\n",
       "  'summary': u'  One of the challenges in modeling cognitive events from electroencephalogram\\n(EEG) data is finding representations that are invariant to inter- and\\nintra-subject differences, as well as to inherent noise associated with such\\ndata. Herein, we propose a novel approach for learning such representations\\nfrom multi-channel EEG time-series, and demonstrate its advantages in the\\ncontext of mental load classification task. First, we transform EEG activities\\ninto a sequence of topology-preserving multi-spectral images, as opposed to\\nstandard EEG analysis techniques that ignore such spatial information. Next, we\\ntrain a deep recurrent-convolutional network inspired by state-of-the-art video\\nclassification to learn robust representations from the sequence of images. The\\nproposed approach is designed to preserve the spatial, spectral, and temporal\\nstructure of EEG which leads to finding features that are less sensitive to\\nvariations and distortions within each dimension. Empirical evaluation on the\\ncognitive load classification task demonstrated significant improvements in\\nclassification accuracy over current state-of-the-art approaches in this field.\\n',\n",
       "  'title': u'Learning Representations from EEG with Deep Recurrent-Convolutional\\n  Neural Networks'},\n",
       " u'1601.04115': {'arxivid': u'1601.04115',\n",
       "  'authorsaffil': [[u'Chuyang Ye', None],\n",
       "   [u'Jiachen Zhuo', None],\n",
       "   [u'Rao P. Gullapalli', None],\n",
       "   [u'Jerry L. Prince', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'Journal paper accepted in Medical Image Analysis. 35 pages and 16\\n  figures',\n",
       "  'doi': u'10.1016/j.media.2016.05.008',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.04115v2',\n",
       "  'published': u'2016-01-16T02:49:28Z',\n",
       "  'summary': u'  Data from diffusion magnetic resonance imaging (dMRI) can be used to\\nreconstruct fiber tracts, for example, in muscle and white matter. Estimation\\nof fiber orientations (FOs) is a crucial step in the reconstruction process and\\nthese estimates can be corrupted by noise. In this paper, a new method called\\nFiber Orientation Reconstruction using Neighborhood Information (FORNI) is\\ndescribed and shown to reduce the effects of noise and improve FO estimation\\nperformance by incorporating spatial consistency. FORNI uses a fixed tensor\\nbasis to model the diffusion weighted signals, which has the advantage of\\nproviding an explicit relationship between the basis vectors and the FOs. FO\\nspatial coherence is encouraged using weighted l1-norm regularization terms,\\nwhich contain the interaction of directional information between neighbor\\nvoxels. Data fidelity is encouraged using a squared error between the observed\\nand reconstructed diffusion weighted signals. After appropriate weighting of\\nthese competing objectives, the resulting objective function is minimized using\\na block coordinate descent algorithm, and a straightforward parallelization\\nstrategy is used to speed up processing. Experiments were performed on a\\ndigital crossing phantom, ex vivo tongue dMRI data, and in vivo brain dMRI data\\nfor both qualitative and quantitative evaluation. The results demonstrate that\\nFORNI improves the quality of FO estimation over other state of the art\\nalgorithms.\\n',\n",
       "  'title': u'Estimation of Fiber Orientations Using Neighborhood Information'},\n",
       " u'1505.00289': {'arxivid': u'1505.00289',\n",
       "  'authorsaffil': [[u'Andrew J. R Simpson', None],\n",
       "   [u'Gerard Roma', None],\n",
       "   [u'Mark D. Plumbley', None]],\n",
       "  'categoryterms': [u'cs.SD', u'68Txx'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1505.00289v1',\n",
       "  'published': u'2015-05-01T22:10:58Z',\n",
       "  'summary': u\"  Audio source separation is a difficult machine learning problem and\\nperformance is measured by comparing extracted signals with the component\\nsource signals. However, if separation is motivated by the ultimate goal of\\nre-mixing then complete separation is not necessary and hence separation\\ndifficulty and separation quality are dependent on the nature of the re-mix.\\nHere, we use a convolutional deep neural network (DNN), trained to estimate\\n'ideal' binary masks for separating voice from music, to perform re-mixing of\\nthe vocal balance by operating directly on the individual magnitude components\\nof the musical mixture spectrogram. Our results demonstrate that small changes\\nin vocal gain may be applied with very little distortion to the ultimate\\nre-mix. Our method may be useful for re-mixing existing mixes.\\n\",\n",
       "  'title': u'Deep Remix: Remixing Musical Mixtures Using a Convolutional Deep Neural\\n  Network'},\n",
       " u'1602.08571': {'arxivid': u'1602.08571',\n",
       "  'authorsaffil': [[u'Haoxi Zhang', None],\n",
       "   [u'Cesar Sanin', None],\n",
       "   [u'Edward Szczerbicki', None]],\n",
       "  'categoryterms': [u'cs.AI', u'cs.NE'],\n",
       "  'comment': u'8 pages, 3 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08571v1',\n",
       "  'published': u'2016-02-27T08:45:35Z',\n",
       "  'summary': u'  In this paper, we propose the Neural Knowledge DNA, a framework that tailors\\nthe ideas underlying the success of neural networks to the scope of knowledge\\nrepresentation. Knowledge representation is a fundamental field that dedicate\\nto representing information about the world in a form that computer systems can\\nutilize to solve complex tasks. The proposed Neural Knowledge DNA is designed\\nto support discovering, storing, reusing, improving, and sharing knowledge\\namong machines and organisation. It is constructed in a similar fashion of how\\nDNA formed: built up by four essential elements. As the DNA produces\\nphenotypes, the Neural Knowledge DNA carries information and knowledge via its\\nfour essential elements, namely, Networks, Experiences, States, and Actions.\\n',\n",
       "  'title': u'Towards Neural Knowledge DNA'},\n",
       " u'1511.07131': {'arxivid': u'1511.07131',\n",
       "  'authorsaffil': [[u'Jun Zhu', None],\n",
       "   [u'Xianjie Chen', None],\n",
       "   [u'Alan L. Yuille', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'the final revision to ICLR 2016, in which some color errors in the\\n  figures are fixed',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.07131v3',\n",
       "  'published': u'2015-11-23T08:24:18Z',\n",
       "  'summary': u'  In this paper, we propose a deep part-based model (DeePM) for symbiotic\\nobject detection and semantic part localization. For this purpose, we annotate\\nsemantic parts for all 20 object categories on the PASCAL VOC 2012 dataset,\\nwhich provides information on object pose, occlusion, viewpoint and\\nfunctionality. DeePM is a latent graphical model based on the state-of-the-art\\nR-CNN framework, which learns an explicit representation of the object-part\\nconfiguration with flexible type sharing (e.g., a sideview horse head can be\\nshared by a fully-visible sideview horse and a highly truncated sideview horse\\nwith head and neck only). For comparison, we also present an end-to-end\\nObject-Part (OP) R-CNN which learns an implicit feature representation for\\njointly mapping an image ROI to the object and part bounding boxes. We evaluate\\nthe proposed methods for both the object and part detection performance on\\nPASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in\\ndetecting objects and parts. In addition, it obtains superior performance to\\nFast and Faster R-CNNs in object detection.\\n',\n",
       "  'title': u'DeePM: A Deep Part-Based Model for Object Detection and Semantic Part\\n  Localization'},\n",
       " u'1602.08575': {'arxivid': u'1602.08575',\n",
       "  'authorsaffil': [[u'Wojciech Czaja', None],\n",
       "   [u'James M. Murphy', None],\n",
       "   [u'Daniel Weinberg', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08575v1',\n",
       "  'published': u'2016-02-27T09:33:07Z',\n",
       "  'summary': u'  We develop a mathematically-motivated algorithm for image superresolution,\\nbased on the discrete shearlet transform. The shearlet transform is strongly\\ndirectional, and is known to provide near-optimally sparse representations for\\na broad class of images. This often leads to superior performance in edge\\ndetection and image representation, when compared to other isotropic frames. We\\njustify the use of shearlet frames for superresolution mathematically before\\npresenting a superresolution algorithm that combines the shearlet transform\\nwith the sparse mixing estimators (SME) approach pioneered by Mallat and Yu.\\nOur algorithm is compared with an isotropic superresolution method, a previous\\nprototype of a shearlet superresolution algorithm, and SME superresolution with\\na discrete wavelet frame. Our numerical results on a variety of image types\\nshow strong performance in terms of PSNR.\\n',\n",
       "  'title': u'Single-Image Superresolution Through Directional Representations'},\n",
       " u'1602.08574': {'arxivid': u'1602.08574',\n",
       "  'authorsaffil': [[u'Luca Calatroni', None],\n",
       "   [u'Yves van Gennip', None],\n",
       "   [u'Carola-Bibiane Sch\\xf6nlieb', None],\n",
       "   [u'Hannah Rowland', None],\n",
       "   [u'Arjuna Flenner', None]],\n",
       "  'categoryterms': [u'math.AP', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.08574v1',\n",
       "  'published': u'2016-02-27T09:27:01Z',\n",
       "  'summary': u'  We consider the problem of scale detection in images where a region of\\ninterest is present together with a measurement tool (e.g. a ruler). For the\\nsegmentation part, we focus on the graph based method by Flenner and Bertozzi\\nwhich reinterprets classical continuous Ginzburg-Landau minimisation models in\\na totally discrete framework. To overcome the numerical difficulties due to the\\nlarge size of the images considered we use matrix completion and splitting\\ntechniques. The scale on the measurement tool is detected via a Hough transform\\nbased algorithm. The method is then applied to some measurement tasks arising\\nin real-world applications such as zoology, medicine and archaeology.\\n',\n",
       "  'title': u'Graph clustering, variational image segmentation methods and Hough\\n  transform scale detection for object measurement in images'},\n",
       " u'1306.4793': {'arxivid': u'1306.4793',\n",
       "  'authorsaffil': [[u'Larry Bull', None]],\n",
       "  'categoryterms': [u'cs.NE', u'q-bio.MN'],\n",
       "  'comment': u'18 pages, 10 figures. arXiv admin note: substantial text overlap with\\n  arXiv:1303.7220',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1306.4793v1',\n",
       "  'published': u'2013-06-20T09:00:40Z',\n",
       "  'summary': u'  The significant role of epigenetic mechanisms within natural systems has\\nbecome increasingly clear. This paper uses a recently presented abstract,\\ntunable Boolean genetic regulatory network model to explore aspects of\\nepigenetics. It is shown how dynamically controlling transcription via a DNA\\nmethylation-inspired mechanism can be selected for by simulated evolution under\\nvarious single and multiple cell scenarios. Further, it is shown that the\\neffects of such control can be inherited without detriment to fitness.\\n',\n",
       "  'title': u'Evolving Boolean Regulatory Networks with Epigenetic Control'},\n",
       " u'1512.07158': {'arxivid': u'1512.07158',\n",
       "  'authorsaffil': [[u'Baichuan Zhang', None],\n",
       "   [u'Vachik Dave', None],\n",
       "   [u'Noman Mohammed', None],\n",
       "   [u'Mohammad Al Hasan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CR'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07158v4',\n",
       "  'published': u'2015-12-22T17:06:01Z',\n",
       "  'summary': u'  Over the last decade, proliferation of various online platforms and their\\nincreasing adoption by billions of users have heightened the privacy risk of a\\nuser enormously. In fact, security researchers have shown that sparse microdata\\ncontaining information about online activities of a user although anonymous,\\ncan still be used to disclose the identity of the user by cross-referencing the\\ndata with other data sources. To preserve the privacy of a user, in existing\\nworks several methods (k-anonymity, l-diversity, differential privacy) are\\nproposed that ensure a dataset which is meant to share or publish bears small\\nidentity disclosure risk. However, the majority of these methods modify the\\ndata in isolation, without considering their utility in subsequent knowledge\\ndiscovery tasks, which makes these datasets less informative. In this work, we\\nconsider labeled data that are generally used for classification, and propose\\ntwo methods for feature selection considering two goals: first, on the reduced\\nfeature set the data has small disclosure risk, and second, the utility of the\\ndata is preserved for performing a classification task. Experimental results on\\nvarious real-world datasets show that the method is effective and useful in\\npractice.\\n',\n",
       "  'title': u'Feature Selection for Classification under Anonymity Constraint'},\n",
       " u'1603.08079': {'arxivid': u'1603.08079',\n",
       "  'authorsaffil': [[u'Yevgeni Berzak', None],\n",
       "   [u'Andrei Barbu', None],\n",
       "   [u'Daniel Harari', None],\n",
       "   [u'Boris Katz', None],\n",
       "   [u'Shimon Ullman', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.CL'],\n",
       "  'comment': u'EMNLP 2015',\n",
       "  'doi': None,\n",
       "  'journalref': u'Conference on Empirical Methods in Natural Language Processing\\n  (EMNLP), 2015, pages 1477--1487',\n",
       "  'link': u'http://arxiv.org/abs/1603.08079v1',\n",
       "  'published': u'2016-03-26T06:49:33Z',\n",
       "  'summary': u'  Understanding language goes hand in hand with the ability to integrate\\ncomplex contextual information obtained via perception. In this work, we\\npresent a novel task for grounded language understanding: disambiguating a\\nsentence given a visual scene which depicts one of the possible interpretations\\nof that sentence. To this end, we introduce a new multimodal corpus containing\\nambiguous sentences, representing a wide range of syntactic, semantic and\\ndiscourse ambiguities, coupled with videos that visualize the different\\ninterpretations for each sentence. We address this task by extending a vision\\nmodel which determines if a sentence is depicted by a video. We demonstrate how\\nsuch a model can be adjusted to recognize different interpretations of the same\\nunderlying sentence, allowing to disambiguate sentences in a unified fashion\\nacross the different ambiguity types.\\n',\n",
       "  'title': u'Do You See What I Mean? Visual Resolution of Linguistic Ambiguities'},\n",
       " u'1507.01193': {'arxivid': u'1507.01193',\n",
       "  'authorsaffil': [[u'Piotr Mirowski', None], [u'Andreas Vlachos', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.AI', u'cs.LG'],\n",
       "  'comment': u'Accepted for publication at ACL 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.01193v1',\n",
       "  'published': u'2015-07-05T11:10:24Z',\n",
       "  'summary': u'  Recent work on language modelling has shifted focus from count-based models\\nto neural models. In these works, the words in each sentence are always\\nconsidered in a left-to-right order. In this paper we show how we can improve\\nthe performance of the recurrent neural network (RNN) language model by\\nincorporating the syntactic dependencies of a sentence, which have the effect\\nof bringing relevant contexts closer to the word being predicted. We evaluate\\nour approach on the Microsoft Research Sentence Completion Challenge and show\\nthat the dependency RNN proposed improves over the RNN by about 10 points in\\naccuracy. Furthermore, we achieve results comparable with the state-of-the-art\\nmodels on this task.\\n',\n",
       "  'title': u'Dependency Recurrent Neural Language Models for Sentence Completion'},\n",
       " u'1604.03010': {'arxivid': u'1604.03010',\n",
       "  'authorsaffil': [[u'Xin Du', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.03010v1',\n",
       "  'published': u'2016-04-11T15:53:04Z',\n",
       "  'summary': u'  In this paper, we study the problem of semi-supervised structured output\\nprediction, which aims to learn predictors for structured outputs, such as\\nsequences, tree nodes, vectors, etc., from a set of data points of both\\ninput-output pairs and single inputs without outputs. The traditional methods\\nto solve this problem usually learns one single predictor for all the data\\npoints, and ignores the variety of the different data points. Different parts\\nof the data set may have different local distributions, and requires different\\noptimal local predictors. To overcome this disadvantage of existing methods, we\\npropose to learn different local predictors for neighborhoods of different data\\npoints, and the missing structured outputs simultaneously. In the neighborhood\\nof each data point, we proposed to learn a linear predictor by minimizing both\\nthe complexity of the predictor and the upper bound of the structured\\nprediction loss. The minimization is conducted by gradient descent algorithms.\\nExperiments over four benchmark data sets, including DDSM mammography medical\\nimages, SUN natural image data set, Cora research paper data set, and Spanish\\nnews wire article sentence data set, show the advantages of the proposed\\nmethod.\\n',\n",
       "  'title': u'Semi-supervised learning of local structured output predictors'},\n",
       " u'1503.00848': {'arxivid': u'1503.00848',\n",
       "  'authorsaffil': [[u'Jordi Pont-Tuset', None],\n",
       "   [u'Pablo Arbelaez', None],\n",
       "   [u'Jonathan T. Barron', None],\n",
       "   [u'Ferran Marques', None],\n",
       "   [u'Jitendra Malik', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': u'10.1109/TPAMI.2016.2537320',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1503.00848v4',\n",
       "  'published': u'2015-03-03T07:58:22Z',\n",
       "  'summary': u'  We propose a unified approach for bottom-up hierarchical image segmentation\\nand object proposal generation for recognition, called Multiscale Combinatorial\\nGrouping (MCG). For this purpose, we first develop a fast normalized cuts\\nalgorithm. We then propose a high-performance hierarchical segmenter that makes\\neffective use of multiscale information. Finally, we propose a grouping\\nstrategy that combines our multiscale regions into highly-accurate object\\nproposals by exploring efficiently their combinatorial space. We also present\\nSingle-scale Combinatorial Grouping (SCG), a faster version of MCG that\\nproduces competitive proposals in under five second per image. We conduct an\\nextensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD,\\nand COCO datasets, showing that MCG produces state-of-the-art contours,\\nhierarchical regions, and object proposals.\\n',\n",
       "  'title': u'Multiscale Combinatorial Grouping for Image Segmentation and Object\\n  Proposal Generation'},\n",
       " u'1512.02017': {'arxivid': u'1512.02017',\n",
       "  'authorsaffil': [[u'Aravindh Mahendran', None], [u'Andrea Vedaldi', None]],\n",
       "  'categoryterms': [u'cs.CV', u'68T45'],\n",
       "  'comment': u'A substantially extended version of\\n  http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf.\\n  arXiv admin note: text overlap with arXiv:1412.0035',\n",
       "  'doi': u'10.1007/s11263-016-0911-8',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.02017v3',\n",
       "  'published': u'2015-12-07T12:38:13Z',\n",
       "  'summary': u'  Image representations, from SIFT and bag of visual words to Convolutional\\nNeural Networks (CNNs) are a crucial component of almost all computer vision\\nsystems. However, our understanding of them remains limited. In this paper we\\nstudy several landmark representations, both shallow and deep, by a number of\\ncomplementary visualization techniques. These visualizations are based on the\\nconcept of \"natural pre-image\", namely a natural-looking image whose\\nrepresentation has some notable property. We study in particular three such\\nvisualizations: inversion, in which the aim is to reconstruct an image from its\\nrepresentation, activation maximization, in which we search for patterns that\\nmaximally stimulate a representation component, and caricaturization, in which\\nthe visual patterns that a representation detects in an image are exaggerated.\\nWe pose these as a regularized energy-minimization framework and demonstrate\\nits generality and effectiveness. In particular, we show that this method can\\ninvert representations such as HOG more accurately than recent alternatives\\nwhile being applicable to CNNs too. Among our findings, we show that several\\nlayers in CNNs retain photographically accurate information about the image,\\nwith different degrees of geometric and photometric invariance.\\n',\n",
       "  'title': u'Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images'},\n",
       " u'1602.00955': {'arxivid': u'1602.00955',\n",
       "  'authorsaffil': [[u'Dengxin Dai', None], [u'Luc Van Gool', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'22 pages, 8 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.00955v2',\n",
       "  'published': u'2016-02-02T14:53:36Z',\n",
       "  'summary': u'  This paper investigates the problem of image classification with limited or\\nno annotations, but abundant unlabeled data. The setting exists in many tasks\\nsuch as semi-supervised image classification, image clustering, and image\\nretrieval. Unlike previous methods, which develop or learn sophisticated\\nregularizers for classifiers, our method learns a new image representation by\\nexploiting the distribution patterns of all available data for the task at\\nhand. Particularly, a rich set of visual prototypes are sampled from all\\navailable data, and are taken as surrogate classes to train discriminative\\nclassifiers; images are projected via the classifiers; the projected values,\\nsimilarities to the prototypes, are stacked to build the new feature vector.\\nThe training set is noisy. Hence, in the spirit of ensemble learning we create\\na set of such training sets which are all diverse, leading to diverse\\nclassifiers. The method is dubbed Ensemble Projection (EP). EP captures not\\nonly the characteristics of individual images, but also the relationships among\\nimages. It is conceptually simple and computationally efficient, yet effective\\nand flexible. Experiments on eight standard datasets show that: (1) EP\\noutperforms previous methods for semi-supervised image classification; (2) EP\\nproduces promising results for self-taught image classification, where\\nunlabeled samples are a random collection of images rather than being from the\\nsame distribution as the labeled ones; and (3) EP improves over the original\\nfeatures for image clustering. The code of the method is available on the\\nproject page.\\n',\n",
       "  'title': u'Unsupervised High-level Feature Learning by Ensemble Projection for\\n  Semi-supervised Image Classification and Image Clustering'},\n",
       " u'1506.01972': {'arxivid': u'1506.01972',\n",
       "  'authorsaffil': [[u'Zeyuan Allen-Zhu', None], [u'Yang Yuan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.DS', u'math.OC', u'stat.ML'],\n",
       "  'comment': u'improved writing and included more experiments in this version',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.01972v3',\n",
       "  'published': u'2015-06-05T17:00:43Z',\n",
       "  'summary': u'  Many classical algorithms are found until several years later to outlive the\\nconfines in which they were conceived, and continue to be relevant in\\nunforeseen settings. In this paper, we show that SVRG is one such method: being\\noriginally designed for strongly convex objectives, it is also very robust in\\nnon-strongly convex or sum-of-non-convex settings.\\n  More precisely, we provide new analysis to improve the state-of-the-art\\nrunning times in both settings by either applying SVRG or its novel variant.\\nSince non-strongly convex objectives include important examples such as Lasso\\nor logistic regression, and sum-of-non-convex objectives include famous\\nexamples such as stochastic PCA and is even believed to be related to training\\ndeep neural nets, our results also imply better performances in these\\napplications.\\n',\n",
       "  'title': u'Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives'},\n",
       " u'1603.08071': {'arxivid': u'1603.08071',\n",
       "  'authorsaffil': [[u'Sohini Roychowdhury', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'4 pages, 6 figures, [Submitted], 38th Annual International Conference\\n  of the IEEE Engineering in Medicine and Biology Society 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.08071v1',\n",
       "  'published': u'2016-03-26T04:07:30Z',\n",
       "  'summary': u'  Large medical image data sets with high dimensionality require substantial\\namount of computation time for data creation and data processing. This paper\\npresents a novel generalized method that finds optimal image-based feature sets\\nthat reduce computational time complexity while maximizing overall\\nclassification accuracy for detection of diabetic retinopathy (DR). First,\\nregion-based and pixel-based features are extracted from fundus images for\\nclassification of DR lesions and vessel-like structures. Next, feature ranking\\nstrategies are used to distinguish the optimal classification feature sets. DR\\nlesion and vessel classification accuracies are computed using the boosted\\ndecision tree and decision forest classifiers in the Microsoft Azure Machine\\nLearning Studio platform, respectively. For images from the DIARETDB1 data set,\\n40 of its highest-ranked features are used to classify four DR lesion types\\nwith an average classification accuracy of 90.1% in 792 seconds. Also, for\\nclassification of red lesion regions and hemorrhages from microaneurysms,\\naccuracies of 85% and 72% are observed, respectively. For images from STARE\\ndata set, 40 high-ranked features can classify minor blood vessels with an\\naccuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysis\\nsystems can significantly enhance the borderline classification performances in\\nautomated screening systems.\\n',\n",
       "  'title': u'Classification of Large-Scale Fundus Image Data Sets: A Cloud-Computing\\n  Framework'},\n",
       " u'1603.08070': {'arxivid': u'1603.08070',\n",
       "  'authorsaffil': [[u'Matthew Bihis', None], [u'Sohini Roychowdhury', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'10 pages, 7 figures, Conference',\n",
       "  'doi': u'10.1109/BigData.2015.7363944',\n",
       "  'journalref': u'Big Data (Big Data), 2015 IEEE International Conference on, Santa\\n  Clara, CA, 2015, pp. 1728-1737',\n",
       "  'link': u'http://arxiv.org/abs/1603.08070v1',\n",
       "  'published': u'2016-03-26T03:55:53Z',\n",
       "  'summary': u'  The constant growth in the present day real-world databases pose\\ncomputational challenges for a single computer. Cloud-based platforms, on the\\nother hand, are capable of handling large volumes of information manipulation\\ntasks, thereby necessitating their use for large real-world data set\\ncomputations. This work focuses on creating a novel Generalized Flow within the\\ncloud-based computing platform: Microsoft Azure Machine Learning Studio (MAMLS)\\nthat accepts multi-class and binary classification data sets alike and\\nprocesses them to maximize the overall classification accuracy. First, each\\ndata set is split into training and testing data sets, respectively. Then,\\nlinear and nonlinear classification model parameters are estimated using the\\ntraining data set. Data dimensionality reduction is then performed to maximize\\nclassification accuracy. For multi-class data sets, data centric information is\\nused to further improve overall classification accuracy by reducing the\\nmulti-class classification to a series of hierarchical binary classification\\ntasks. Finally, the performance of optimized classification model thus achieved\\nis evaluated and scored on the testing data set. The classification\\ncharacteristics of the proposed flow are comparatively evaluated on 3 public\\ndata sets and a local data set with respect to existing state-of-the-art\\nmethods. On the 3 public data sets, the proposed flow achieves 78-97.5%\\nclassification accuracy. Also, the local data set, created using the\\ninformation regarding presence of Diabetic Retinopathy lesions in fundus\\nimages, results in 85.3-95.7% average classification accuracy, which is higher\\nthan the existing methods. Thus, the proposed generalized flow can be useful\\nfor a wide range of application-oriented \"big data sets\".\\n',\n",
       "  'title': u'A generalized flow for multi-class and binary classification tasks: An\\n  Azure ML approach'},\n",
       " u'1512.07155': {'arxivid': u'1512.07155',\n",
       "  'authorsaffil': [[u'Shugao Ma', None],\n",
       "   [u'Sarah Adel Bargal', None],\n",
       "   [u'Jianming Zhang', None],\n",
       "   [u'Leonid Sigal', None],\n",
       "   [u'Stan Sclaroff', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1512.07155v1',\n",
       "  'published': u'2015-12-22T16:52:19Z',\n",
       "  'summary': u\"  Recently, attempts have been made to collect millions of videos to train CNN\\nmodels for action recognition in videos. However, curating such large-scale\\nvideo datasets requires immense human labor, and training CNNs on millions of\\nvideos demands huge computational resources. In contrast, collecting action\\nimages from the Web is much easier and training on images requires much less\\ncomputation. In addition, labeled web images tend to contain discriminative\\naction poses, which highlight discriminative portions of a video's temporal\\nprogression. We explore the question of whether we can utilize web action\\nimages to train better CNN models for action recognition in videos. We collect\\n23.8K manually filtered images from the Web that depict the 101 actions in the\\nUCF101 action video dataset. We show that by utilizing web action images along\\nwith videos in training, significant performance boosts of CNN models can be\\nachieved. We then investigate the scalability of the process by leveraging\\ncrawled web images (unfiltered) for UCF101 and ActivityNet. We replace 16.2M\\nvideo frames by 393K unfiltered images and get comparable performance.\\n\",\n",
       "  'title': u'Do Less and Achieve More: Training CNNs for Action Recognition Utilizing\\n  Action Images from the Web'},\n",
       " u'1604.01319': {'arxivid': u'1604.01319',\n",
       "  'authorsaffil': [[u'Ke Ye', None], [u'Lek-Heng Lim', None]],\n",
       "  'categoryterms': [u'cs.CV', u'math.AT'],\n",
       "  'comment': u'21 pages, 3 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.01319v1',\n",
       "  'published': u'2016-04-05T16:26:47Z',\n",
       "  'summary': u'  The goal of cryo-electron microscopy (EM) is to reconstruct the 3-dimensional\\nstructure of a molecule from a collection of its 2-dimensional projected\\nimages. In this article, we show that the basic premise of cryo-EM --- patching\\ntogether 2-dimensional projections to reconstruct a 3-dimensional object --- is\\nnaturally one of Cech cohomology with SO(2)-coefficients. We deduce that every\\ncryo-EM reconstruction problem corresponds to an oriented circle bundle on a\\nsimplicial complex, allowing us to classify cryo-EM problems via principal\\nbundles. In practice, the 2-dimensional images are noisy and a main task in\\ncryo-EM is to denoise them. We will see how the aforementioned insights can be\\nused towards this end.\\n',\n",
       "  'title': u'Cohomology of Cryo-Electron Microscopy'},\n",
       " u'1604.00825': {'arxivid': u'1604.00825',\n",
       "  'authorsaffil': [[u'Alexander Binder', None],\n",
       "   [u'Gr\\xe9goire Montavon', None],\n",
       "   [u'Sebastian Bach', None],\n",
       "   [u'Klaus-Robert M\\xfcller', None],\n",
       "   [u'Wojciech Samek', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1604.00825v1',\n",
       "  'published': u'2016-04-04T11:52:07Z',\n",
       "  'summary': u'  Layer-wise relevance propagation is a framework which allows to decompose the\\nprediction of a deep neural network computed over a sample, e.g. an image, down\\nto relevance scores for the single input dimensions of the sample such as\\nsubpixels of an image. While this approach can be applied directly to\\ngeneralized linear mappings, product type non-linearities are not covered. This\\npaper proposes an approach to extend layer-wise relevance propagation to neural\\nnetworks with local renormalization layers, which is a very common product-type\\nnon-linearity in convolutional neural networks. We evaluate the proposed method\\nfor local renormalization layers on the CIFAR-10, Imagenet and MIT Places\\ndatasets.\\n',\n",
       "  'title': u'Layer-wise Relevance Propagation for Neural Networks with Local\\n  Renormalization Layers'},\n",
       " u'1409.2752': {'arxivid': u'1409.2752',\n",
       "  'authorsaffil': [[u'Alireza Makhzani', None], [u'Brendan Frey', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NE'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1409.2752v2',\n",
       "  'published': u'2014-09-09T14:38:43Z',\n",
       "  'summary': u'  In this paper, we propose a winner-take-all method for learning hierarchical\\nsparse representations in an unsupervised fashion. We first introduce\\nfully-connected winner-take-all autoencoders which use mini-batch statistics to\\ndirectly enforce a lifetime sparsity in the activations of the hidden units. We\\nthen propose the convolutional winner-take-all autoencoder which combines the\\nbenefits of convolutional architectures and autoencoders for learning\\nshift-invariant sparse representations. We describe a way to train\\nconvolutional autoencoders layer by layer, where in addition to lifetime\\nsparsity, a spatial sparsity within each feature map is achieved using\\nwinner-take-all activation functions. We will show that winner-take-all\\nautoencoders can be used to to learn deep sparse representations from the\\nMNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets,\\nand achieve competitive classification performance.\\n',\n",
       "  'title': u'Winner-Take-All Autoencoders'},\n",
       " u'1603.08497': {'arxivid': u'1603.08497',\n",
       "  'authorsaffil': [[u'Guillaume Noyel', u'CMM'],\n",
       "   [u'Jesus Angulo', u'CMM'],\n",
       "   [u'Dominique Jeulin', u'CMM']],\n",
       "  'categoryterms': [u'cs.CV', u'math.NA'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': u'Proceedings of the 8th International Symposium on Mathematical\\n  Morphology: Volume 1, pp.399-410, 2007, 978-85-17-00032-5',\n",
       "  'link': u'http://arxiv.org/abs/1603.08497v1',\n",
       "  'published': u'2016-02-02T09:17:06Z',\n",
       "  'summary': u'  The present paper introduces the $\\\\eta$ and {\\\\eta} connections in order to\\nadd regional information on $\\\\lambda$-flat zones, which only take into account\\na local information. A top-down approach is considered. First $\\\\lambda$-flat\\nzones are built in a way leading to a sub-segmentation. Then a finer\\nsegmentation is obtained by computing $\\\\eta$-bounded regions and $\\\\mu$-geodesic\\nballs inside the $\\\\lambda$-flat zones. The proposed algorithms for the\\nconstruction of new partitions are based on queues with an ordered selection of\\nseeds using the cumulative distance. $\\\\eta$-bounded regions offers a control on\\nthe variations of amplitude in the class from a point, called center, and\\n$\\\\mu$-geodesic balls controls the \"size\" of the class. These results are\\napplied to hyperspectral images.\\n',\n",
       "  'title': u'On distances, paths and connections for hyperspectral image segmentation'},\n",
       " u'1601.07913': {'arxivid': u'1601.07913',\n",
       "  'authorsaffil': [[u'Pierre Baldi', None],\n",
       "   [u'Kyle Cranmer', None],\n",
       "   [u'Taylor Faucett', None],\n",
       "   [u'Peter Sadowski', None],\n",
       "   [u'Daniel Whiteson', None]],\n",
       "  'categoryterms': [u'hep-ex', u'cs.LG', u'hep-ph'],\n",
       "  'comment': u'For submission to PRD',\n",
       "  'doi': u'10.1140/epjc/s10052-016-4099-4',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.07913v1',\n",
       "  'published': u'2016-01-28T21:01:05Z',\n",
       "  'summary': u'  We investigate a new structure for machine learning classifiers applied to\\nproblems in high-energy physics by expanding the inputs to include not only\\nmeasured features but also physics parameters. The physics parameters represent\\na smoothly varying learning task, and the resulting parameterized classifier\\ncan smoothly interpolate between them and replace sets of classifiers trained\\nat individual values. This simplifies the training process and gives improved\\nperformance at intermediate values, even for complex problems requiring deep\\nlearning. Applications include tools parameterized in terms of theoretical\\nmodel parameters, such as the mass of a particle, which allow for a single\\nnetwork to provide improved discrimination across a range of masses. This\\nconcept is simple to implement and allows for optimized interpolatable results.\\n',\n",
       "  'title': u'Parameterized Machine Learning for High-Energy Physics'},\n",
       " u'1410.1165': {'arxivid': u'1410.1165',\n",
       "  'authorsaffil': [[u'Rupesh Kumar Srivastava', None],\n",
       "   [u'Jonathan Masci', None],\n",
       "   [u'Faustino Gomez', None],\n",
       "   [u'J\\xfcrgen Schmidhuber', None]],\n",
       "  'categoryterms': [u'cs.NE', u'cs.LG', u'68T30, 68T10', u'I.2.6'],\n",
       "  'comment': u'9 pages + 2 supplementary, Accepted to ICLR 2015 Conference track',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1410.1165v3',\n",
       "  'published': u'2014-10-05T14:46:47Z',\n",
       "  'summary': u'  Recently proposed neural network activation functions such as rectified\\nlinear, maxout, and local winner-take-all have allowed for faster and more\\neffective training of deep neural architectures on large and complex datasets.\\nThe common trait among these functions is that they implement local competition\\nbetween small groups of computational units within a layer, so that only part\\nof the network is activated for any given input pattern. In this paper, we\\nattempt to visualize and understand this self-modularization, and suggest a\\nunified explanation for the beneficial properties of such networks. We also\\nshow how our insights can be directly useful for efficiently performing\\nretrieval over large datasets using neural networks.\\n',\n",
       "  'title': u'Understanding Locally Competitive Networks'},\n",
       " u'1603.03627': {'arxivid': u'1603.03627',\n",
       "  'authorsaffil': [[u'Roberto L. Shinmoto Torres', None],\n",
       "   [u'Damith C. Ranasinghe', None],\n",
       "   [u'Qinfeng Shi', None],\n",
       "   [u'Anton van den Hengel', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': u'28 pages, 8 figures, 1 table',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.03627v1',\n",
       "  'published': u'2016-03-11T13:51:37Z',\n",
       "  'summary': u'  The present study introduces a method for improving the classification\\nperformance of imbalanced multiclass data streams from wireless body worn\\nsensors. Data imbalance is an inherent problem in activity recognition caused\\nby the irregular time distribution of activities, which are sequential and\\ndependent on previous movements. We use conditional random fields (CRF), a\\ngraphical model for structured classification, to take advantage of\\ndependencies between activities in a sequence. However, CRFs do not consider\\nthe negative effects of class imbalance during training. We propose a\\nclass-wise dynamically weighted CRF (dWCRF) where weights are automatically\\ndetermined during training by maximizing the expected overall F-score. Our\\nresults based on three case studies from a healthcare application using a\\nbatteryless body worn sensor, demonstrate that our method, in general, improves\\noverall and minority class F-score when compared to other CRF based classifiers\\nand achieves similar or better overall and class-wise performance when compared\\nto SVM based classifiers under conditions of limited training data. We also\\nconfirm the performance of our approach using an additional battery powered\\nbody worn sensor dataset, achieving similar results in cases of high class\\nimbalance.\\n',\n",
       "  'title': u'Learning from Imbalanced Multiclass Sequential Data Streams Using\\n  Dynamically Weighted Conditional Random Fields'},\n",
       " u'1602.03256': {'arxivid': u'1602.03256',\n",
       "  'authorsaffil': [[u'Bappaditya Mandal', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'6 pages, 4 figures, ICIP 2015',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1602.03256v1',\n",
       "  'published': u'2016-02-10T03:52:11Z',\n",
       "  'summary': u'  In this work, we propose to divide each class (a person) into subclasses\\nusing spatial partition trees which helps in better capturing the\\nintra-personal variances arising from the appearances of the same individual.\\nWe perform a comprehensive analysis on within-class and within-subclass\\neigenspectrums of face images and propose a novel method of eigenspectrum\\nmodeling which extracts discriminative features of faces from both\\nwithin-subclass and total or between-subclass scatter matrices. Effective\\nlow-dimensional face discriminative features are extracted for face recognition\\n(FR) after performing discriminant evaluation in the entire eigenspace.\\nExperimental results on popular face databases (AR, FERET) and the challenging\\nunconstrained YouTube Face database show the superiority of our proposed\\napproach on all three databases.\\n',\n",
       "  'title': u'Improved Eigenfeature Regularization for Face Identification'},\n",
       " u'1511.06660': {'arxivid': u'1511.06660',\n",
       "  'authorsaffil': [[u'Bjarke Felbo', None],\n",
       "   [u'P\\xe5l Sunds\\xf8y', None],\n",
       "   [u\"Alex 'Sandy' Pentland\", None],\n",
       "   [u'Sune Lehmann', None],\n",
       "   [u'Yves-Alexandre de Montjoye', None]],\n",
       "  'categoryterms': [u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06660v4',\n",
       "  'published': u'2015-11-20T16:07:21Z',\n",
       "  'summary': u'  Mobile phone metadata are increasingly used to study human behavior at\\nlarge-scale. There has recently been a growing interest in predicting\\ndemographic information from metadata. Previous approaches relied on\\nhand-engineered features. We here apply, for the first time, deep learning\\nmethods to mobile phone metadata using a convolutional network. Our method\\nprovides high accuracy on both age and gender prediction. These results show\\ngreat potential for deep learning approaches for prediction tasks using\\nstandard mobile phone metadata.\\n',\n",
       "  'title': u'Using Deep Learning to Predict Demographics from Mobile Phone Metadata'},\n",
       " u'1511.08250': {'arxivid': u'1511.08250',\n",
       "  'authorsaffil': [[u'Bernardino Romera-Paredes', None],\n",
       "   [u'Philip H. S. Torr', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI'],\n",
       "  'comment': u'14 pages (main paper). 23 pages including references and appendix',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.08250v2',\n",
       "  'published': u'2015-11-25T23:28:14Z',\n",
       "  'summary': u'  Instance segmentation is the problem of detecting and delineating each\\ndistinct object of interest appearing in an image. Current instance\\nsegmentation approaches consist of ensembles of modules that are trained\\nindependently of each other, thus missing learning opportunities. Here we\\npropose a new instance segmentation paradigm consisting in an end-to-end method\\nthat learns how to segment instances sequentially. The model is based on a\\nrecurrent neural network that sequentially finds objects and their\\nsegmentations one at a time. This net is provided with a spatial memory that\\nkeeps track of what pixels have been explained and allows handling occlusion.\\nIn order to train the model we designed a new principled loss function that\\naccurately represents the properties of the instance segmentation problem. In\\nthe experiments carried out, we found that our method outperforms recent\\napproaches on multiple person segmentation, and all state of the art approaches\\non the Plant Phenotyping dataset for leaf counting.\\n',\n",
       "  'title': u'Recurrent Instance Segmentation'},\n",
       " u'1509.05789': {'arxivid': u'1509.05789',\n",
       "  'authorsaffil': [[u'Alessandro Checco', None],\n",
       "   [u'Giuseppe Bianchi', None],\n",
       "   [u'Doug Leith', None]],\n",
       "  'categoryterms': [u'cs.LG', u'stat.ML'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.05789v2',\n",
       "  'published': u'2015-09-18T20:21:43Z',\n",
       "  'summary': u'  We propose a privacy-enhanced matrix factorization recommender that exploits\\nthe fact that users can often be grouped together by interest. This allows a\\nform of \"hiding in the crowd\" privacy. We introduce a novel matrix\\nfactorization approach suited to making recommendations in a shared group (or\\nnym) setting and the BLC algorithm for carrying out this matrix factorization\\nin a privacy-enhanced manner. We demonstrate that the increased privacy does\\nnot come at the cost of reduced recommendation accuracy.\\n',\n",
       "  'title': u'BLC: Private Matrix Factorization Recommenders via Automatic Group\\n  Learning'},\n",
       " u'1506.04395': {'arxivid': u'1506.04395',\n",
       "  'authorsaffil': [[u'Pan He', None],\n",
       "   [u'Weilin Huang', None],\n",
       "   [u'Yu Qiao', None],\n",
       "   [u'Chen Change Loy', None],\n",
       "   [u'Xiaoou Tang', None]],\n",
       "  'categoryterms': [u'cs.CV'],\n",
       "  'comment': u'To appear in the 13th AAAI Conference on Artificial Intelligence\\n  (AAAI-16), 2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.04395v2',\n",
       "  'published': u'2015-06-14T13:34:10Z',\n",
       "  'summary': u'  We develop a Deep-Text Recurrent Network (DTRN) that regards scene text\\nreading as a sequence labelling problem. We leverage recent advances of deep\\nconvolutional neural networks to generate an ordered high-level sequence from a\\nwhole word image, avoiding the difficult character segmentation problem. Then a\\ndeep recurrent model, building on long short-term memory (LSTM), is developed\\nto robustly recognize the generated CNN sequences, departing from most existing\\napproaches recognising each character independently. Our model has a number of\\nappealing properties in comparison to existing scene text recognition methods:\\n(i) It can recognise highly ambiguous words by leveraging meaningful context\\ninformation, allowing it to work reliably without either pre- or\\npost-processing; (ii) the deep CNN feature is robust to various image\\ndistortions; (iii) it retains the explicit order information in word image,\\nwhich is essential to discriminate word strings; (iv) the model does not depend\\non pre-defined dictionary, and it can process unknown words and arbitrary\\nstrings. Codes for the DTRN will be available.\\n',\n",
       "  'title': u'Reading Scene Text in Deep Convolutional Sequences'},\n",
       " u'1509.05016': {'arxivid': u'1509.05016',\n",
       "  'authorsaffil': [[u'Ashesh Jain', None],\n",
       "   [u'Avi Singh', None],\n",
       "   [u'Hema S Koppula', None],\n",
       "   [u'Shane Soh', None],\n",
       "   [u'Ashutosh Saxena', None]],\n",
       "  'categoryterms': [u'cs.CV', u'cs.AI', u'cs.RO'],\n",
       "  'comment': u'Follow-up of ICCV 2015 Brain4Cars http://www.brain4cars.com',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1509.05016v1',\n",
       "  'published': u'2015-09-16T19:49:24Z',\n",
       "  'summary': u'  Anticipating the future actions of a human is a widely studied problem in\\nrobotics that requires spatio-temporal reasoning. In this work we propose a\\ndeep learning approach for anticipation in sensory-rich robotics applications.\\nWe introduce a sensory-fusion architecture which jointly learns to anticipate\\nand fuse information from multiple sensory streams. Our architecture consists\\nof Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM)\\nunits to capture long temporal dependencies. We train our architecture in a\\nsequence-to-sequence prediction manner, and it explicitly learns to predict the\\nfuture given only a partial temporal context. We further introduce a novel loss\\nlayer for anticipation which prevents over-fitting and encourages early\\nanticipation. We use our architecture to anticipate driving maneuvers several\\nseconds before they happen on a natural driving data set of 1180 miles. The\\ncontext for maneuver anticipation comes from multiple sensors installed on the\\nvehicle. Our approach shows significant improvement over the state-of-the-art\\nin maneuver anticipation by increasing the precision from 77.4% to 90.5% and\\nrecall from 71.2% to 87.4%.\\n',\n",
       "  'title': u'Recurrent Neural Networks for Driver Activity Anticipation via\\n  Sensory-Fusion Architecture'},\n",
       " u'1507.05122': {'arxivid': u'1507.05122',\n",
       "  'authorsaffil': [[u'Yingxiao Wu', None],\n",
       "   [u'Yan Zhuang', None],\n",
       "   [u'Xi Long', None],\n",
       "   [u'Feng Lin', None],\n",
       "   [u'Wenyao Xu', None]],\n",
       "  'categoryterms': [u'cs.AI'],\n",
       "  'comment': u'This paper has been withdrawn by the author due to several literature\\n  mistakes',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.05122v2',\n",
       "  'published': u'2015-07-17T21:58:01Z',\n",
       "  'summary': u'  Gender contains a wide range of information regarding to the characteristics\\ndifference between male and female. Successful gender recognition is essential\\nand critical for many applications in the commercial domains such as\\napplications of human-computer interaction and computer-aided physiological or\\npsychological analysis. Some have proposed various approaches for automatic\\ngender classification using the features derived from human bodies and/or\\nbehaviors. First, this paper introduces the challenge and application for\\ngender classification research. Then, the development and framework of gender\\nclassification are described. Besides, we compare these state-of-the-art\\napproaches, including vision-based methods, biological information-based\\nmethod, and social network information-based method, to provide a comprehensive\\nreview in the area of gender classification. In mean time, we highlight the\\nstrength and discuss the limitation of each method. Finally, this review also\\ndiscusses several promising applications for the future work.\\n',\n",
       "  'title': u'Human Gender Classification: A Review'},\n",
       " u'1603.07094': {'arxivid': u'1603.07094',\n",
       "  'authorsaffil': [[u'Motohide Higaki', None],\n",
       "   [u'Kai Morino', None],\n",
       "   [u'Hiroshi Murata', None],\n",
       "   [u'Ryo Asaoka', None],\n",
       "   [u'Kenji Yamanishi', None]],\n",
       "  'categoryterms': [u'stat.ML', u'cs.LG'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1603.07094v1',\n",
       "  'published': u'2016-03-23T09:06:19Z',\n",
       "  'summary': u'  This study addresses the issue of predicting the glaucomatous visual field\\nloss from patient disease datasets. Our goal is to accurately predict the\\nprogress of the disease in individual patients. As very few measurements are\\navailable for each patient, it is difficult to produce good predictors for\\nindividuals. A recently proposed clustering-based method enhances the power of\\nprediction using patient data with similar spatiotemporal patterns. Each\\npatient is categorized into a cluster of patients, and a predictive model is\\nconstructed using all of the data in the class. Predictions are highly\\ndependent on the quality of clustering, but it is difficult to identify the\\nbest clustering method. Thus, we propose a method for aggregating cluster-based\\npredictors to obtain better prediction accuracy than from a single\\ncluster-based prediction. Further, the method shows very high performances by\\nhierarchically aggregating experts generated from several cluster-based\\nmethods. We use real datasets to demonstrate that our method performs\\nsignificantly better than conventional clustering-based and patient-wise\\nregression methods, because the hierarchical aggregating strategy has a\\nmechanism whereby good predictors in a small community can thrive.\\n',\n",
       "  'title': u'Predicting Glaucoma Visual Field Loss by Hierarchically Aggregating\\n  Clustering-based Predictors'},\n",
       " u'1507.04201': {'arxivid': u'1507.04201',\n",
       "  'authorsaffil': [[u'Nicos Pavlidis', None],\n",
       "   [u'David Hofmeyr', None],\n",
       "   [u'Sotiris Tasoulis', None]],\n",
       "  'categoryterms': [u'stat.ML',\n",
       "   u'cs.LG',\n",
       "   u'62H30, 68T10',\n",
       "   u'I.5.0; I.5.3; G.3'],\n",
       "  'comment': None,\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1507.04201v2',\n",
       "  'published': u'2015-07-15T13:08:11Z',\n",
       "  'summary': u'  Associating distinct groups of objects (clusters) with contiguous regions of\\nhigh probability density (high-density clusters), is central to many\\nstatistical and machine learning approaches to the classification of unlabelled\\ndata. We propose a novel hyperplane classifier for clustering and\\nsemi-supervised classification which is motivated by this objective. The\\nproposed minimum density hyperplane minimises the integral of the empirical\\nprobability density function along it, thereby avoiding intersection with high\\ndensity clusters. We show that the minimum density and the maximum margin\\nhyperplanes are asymptotically equivalent, thus linking this approach to\\nmaximum margin clustering and semi-supervised support vector classifiers. We\\npropose a projection pursuit formulation of the associated optimisation problem\\nwhich allows us to find minimum density hyperplanes efficiently in practice,\\nand evaluate its performance on a range of benchmark datasets. The proposed\\napproach is found to be very competitive with state of the art methods for\\nclustering and semi-supervised classification.\\n',\n",
       "  'title': u'Minimum Density Hyperplanes'},\n",
       " ...}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for soup in lesoups:\n",
    "\n",
    "\n",
    "papers = {}\n",
    "for soup in lesoups:\n",
    "    for entry in soup.find_all(\"entry\"):\n",
    "        paper = {}\n",
    "        paper[\"link\"] = entry.id.string\n",
    "        paper[\"title\"] = entry.title.string\n",
    "        paper[\"summary\"] = entry.summary.string\n",
    "\n",
    "        paper[\"comment\"] = entry.comment.string if entry.comment else None\n",
    "        paper[\"doi\"] = entry.doi.string if entry.doi else None\n",
    "        paper[\"journalref\"] = entry.journal_ref.string if entry.journal_ref else None\n",
    "\n",
    "        res = re.findall(\"([019][0-9][0-1][0-9]\\.[0-9]+)\",entry.id.string)\n",
    "        if len(res) != 1:\n",
    "            print(\"ERROR wrong match in \", entry)\n",
    "            break\n",
    "        elif res in papers.keys():\n",
    "            print(\"ERROR RES ALREADY IN HERE\", entry)\n",
    "            break\n",
    "        paper[\"arxivid\"] = res[0]\n",
    "        authorsaffil = []\n",
    "        for author in entry.find_all(\"author\"):\n",
    "            authorsaffil.append([author.find(\"name\").string, author.affiliation.string if author.affiliation else None])\n",
    "        paper[\"authorsaffil\"] = authorsaffil\n",
    "\n",
    "        temp = entry.find_all([\"category\", \"primary_category\"])\n",
    "        if temp[0][\"term\"] == temp[1][\"term\"]:\n",
    "            paper[\"categoryterms\"] = [x[\"term\"] for x in temp[1:]]\n",
    "        else:\n",
    "            paper[\"categoryterms\"] = [x[\"term\"] for x in temp]\n",
    "\n",
    "        paper[\"published\"] = entry.published.string\n",
    "        papers[res[0]] = paper\n",
    "\n",
    "papers\n",
    "    #    for child in entry.descendants:\n",
    "#        if child.name not in (None, \"id\", \"updated\", \"published\", \"title\", \"summary\", \"author\",\n",
    "#                              \"name\", \"link\", \"category\", \"primary_category\", \"comment\"):\n",
    "#            print(child.parent.name, child.name)\n",
    "#            print(\"attrs\",child.attrs)\n",
    "\n",
    "# entry: doi\n",
    "# entry: journal_ref\n",
    "# author: affiliation\n",
    "# author: name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<journal_ref>IEEE Transactions on Information Theory 62(4):2092--2099, 2016</journal_ref>,\n",
       " <journal_ref>THEORIA 31/1 (2016): 7-25</journal_ref>,\n",
       " <journal_ref>JLCL - Journal for Language Technology and Computational\\n  Linguistics, 2015, 30 (1)</journal_ref>,\n",
       " <journal_ref>Annals of Probability 2016, Vol. 44, No. 2, 1107-1133</journal_ref>,\n",
       " <journal_ref>Bernoulli 2016, Vol. 22, No. 3, 1535-1571</journal_ref>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test = {\"a\":papers[\"1509.08535\"], \"b\":papers[\"1602.03742\"], \"c\":papers[\"1602.03742\"]}\n",
    "# tuple(test.values())\n",
    "\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"login_info.json\",\"r\") as f:\n",
    "    login_obj = json.load(f)[\"ML\"]\n",
    "\n",
    "myConnection = psycopg2.connect(host=login_obj['hostname'],\n",
    "                                    user=login_obj['username'],\n",
    "                                    password=login_obj['password'],\n",
    "                                    dbname=login_obj['database'])\n",
    "\n",
    "\n",
    "# cur = conn.cursor()\n",
    "# cur.executemany(\"\"\"INSERT INTO bar(first_name,last_name) VALUES (%(first_name)s, %(last_name)s)\"\"\", namedict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with myConnection.cursor() as cur:\n",
    "    cur.executemany((\"INSERT INTO paper (title, summary, comment, arxivid, doi, journalref,\" +\n",
    "                                        \"link, authorsaffil, categoryterms, published) \" +\n",
    "                    \"VALUES (%(title)s, %(summary)s, %(comment)s, %(arxivid)s, %(doi)s, \" +\n",
    "                            \"%(journalref)s, %(link)s, %(authorsaffil)s, %(categoryterms)s, \" +\n",
    "                            \"%(published)s )\"), tuple(papers.values()))\n",
    "\n",
    "                    #(%(first_name)s, %(last_name)s)\"\"\", namedict)\n",
    "myConnection.commit()\n",
    "myConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix a dumb duplicate\n",
    "for item in papers.values():\n",
    "    if item[\"doi\"] == \"10.1145/1235\":\n",
    "        papers[item[\"arxivid\"]][\"doi\"] = None\n",
    "        print(\"woop\")\n",
    "        \n",
    "\n",
    "\n",
    "#    print((\"INSERT INTO paper (title, summary, comment, arxivid, doi, journalref,\" +\n",
    "#                                        \"link, authorsaffil, categoryterms, published) \" +\n",
    "#                    \"VALUES (%(title)s, %(summary)s, %(comment)s, %(arxivid)s, %(doi)s, \" +\n",
    "#                            \"%(journalref)s, %(link)s, %(authorsaffil)s, %(categoryterms)s, \" +\n",
    "#                            \"%(published)s )\") % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extraurl = \"http://export.arxiv.org/api/query?search_query=id:1510.00331 OR id:1512.01124 OR id:1502.02590 OR id:1505.05007 OR id:1601.00909 OR id:1511.06085 OR id:1502.06922 OR id:1602.02697 OR id:1602.03822 OR id:1509.01277 OR id:1602.07362 OR id:1508.00330 OR id:1112.5309 OR id:1406.3284 OR id:1406.4729 OR id:1411.4389 OR id:1411.4952 OR id:1505.05612 OR id:1511.06881 OR id:1502.05698 OR id:1602.03822 OR id:1602.03822 OR id:1602.03822&start=0&max_results=30\"\n",
    "\n",
    "with open(\"newquery.xml\", \"r\") as f:\n",
    "    extrasoup = BeautifulSoup(\"\".join(f.readlines()), \"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = extrasoup.find_all(\"entry\")\n",
    "len(entries)\n",
    "\n",
    "\n",
    "# so, the only special addition here is doi... good to know\n",
    "for entry in entries:\n",
    "    for child in entry.descendants:\n",
    "        if child.name not in (None, \"id\", \"updated\", \"published\", \"title\", \"summary\", \"author\",\n",
    "                     \"name\", \"link\", \"category\", \"primary_category\", \"comment\"):\n",
    "            print(child.parent.name, child.name)\n",
    "            print(\"attrs\",child.attrs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "papers = {}\n",
    "\n",
    "\n",
    "for entry in entries:\n",
    "    paper = {}\n",
    "    paper[\"link\"] = entry.id.string\n",
    "    paper[\"title\"] = entry.title.string\n",
    "    paper[\"summary\"] = entry.summary.string\n",
    "\n",
    "    paper[\"comment\"] = entry.comment.string if entry.comment else None\n",
    "    paper[\"doi\"] = entry.doi.string if entry.doi else None\n",
    "    \n",
    "    res = re.findall(\"([019][0-9][0-1][0-9]\\.[0-9]+)\",entry.id.string)\n",
    "    if len(res) != 1:\n",
    "        print(\"ERROR wrong match in \", entry)\n",
    "        break\n",
    "    elif res in papers.keys():\n",
    "        print(\"ERROR RES ALREADY IN HERE\", entry)\n",
    "        break\n",
    "    paper[\"arxivid\"] = res[0]\n",
    "    authorsaffil = []\n",
    "    for author in entry.find_all(\"author\"):\n",
    "        authorsaffil.append([author.find(\"name\").string, author.affiliation.string if author.affiliation else None])\n",
    "    paper[\"authorsaffil\"] = authorsaffil\n",
    "\n",
    "    temp = entry.find_all([\"category\", \"primary_category\"])\n",
    "    if temp[0][\"term\"] == temp[1][\"term\"]:\n",
    "        paper[\"categoryterms\"] = [x[\"term\"] for x in temp[1:]]\n",
    "    else:\n",
    "        paper[\"categoryterms\"] = [x[\"term\"] for x in temp]\n",
    "\n",
    "    paper[\"published\"] = entry.published.string\n",
    "    papers[res[0]] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test = {\"a\":papers[\"1509.08535\"], \"b\":papers[\"1602.03742\"], \"c\":papers[\"1602.03742\"]}\n",
    "# tuple(test.values())\n",
    "\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "with open(\"login_info.json\",\"r\") as f:\n",
    "    login_obj = json.load(f)[\"ML\"]\n",
    "\n",
    "myConnection = psycopg2.connect(host=login_obj['hostname'],\n",
    "                                    user=login_obj['username'],\n",
    "                                    password=login_obj['password'],\n",
    "                                    dbname=login_obj['database'])\n",
    "\n",
    "# journalrefs removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with myConnection.cursor() as cur:\n",
    "    cur.executemany((\"INSERT INTO paper (title, summary, comment, arxivid, doi,\" +\n",
    "                                        \"link, authorsaffil, categoryterms, published) \" +\n",
    "                    \"VALUES (%(title)s, %(summary)s, %(comment)s, %(arxivid)s, %(doi)s, \" +\n",
    "                            \"%(link)s, %(authorsaffil)s, %(categoryterms)s, \" +\n",
    "                            \"%(published)s )\"), tuple(papers.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myConnection.commit()\n",
    "myConnection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
