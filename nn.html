<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=Lx1xfUTR4qFjwg0Z_pb901bqibjAC9TcrSzAoXtxa2E');ul.lst-kix_6yojqqjec0st-5{list-style-type:none}ul.lst-kix_6yojqqjec0st-4{list-style-type:none}ul.lst-kix_6yojqqjec0st-3{list-style-type:none}ul.lst-kix_6yojqqjec0st-2{list-style-type:none}.lst-kix_qp3zt329w8t5-0>li:before{content:"\0025cf  "}.lst-kix_qp3zt329w8t5-4>li:before{content:"\0025cb  "}ul.lst-kix_6yojqqjec0st-8{list-style-type:none}ul.lst-kix_6yojqqjec0st-7{list-style-type:none}ul.lst-kix_6yojqqjec0st-6{list-style-type:none}.lst-kix_ltwr0s349me7-3>li:before{content:"\0025cf  "}ul.lst-kix_6yojqqjec0st-1{list-style-type:none}ul.lst-kix_6yojqqjec0st-0{list-style-type:none}.lst-kix_dowwk88w0dao-6>li:before{content:"\0025cf  "}.lst-kix_hl5oa0f6tf4f-5>li:before{content:"\0025a0  "}.lst-kix_ltwr0s349me7-7>li:before{content:"\0025cb  "}.lst-kix_qp3zt329w8t5-8>li:before{content:"\0025a0  "}.lst-kix_dowwk88w0dao-2>li:before{content:"\0025a0  "}.lst-kix_1qjopwyk3n75-1>li:before{content:"\0025cb  "}.lst-kix_gsefpdxx2m31-5>li:before{content:"\0025a0  "}.lst-kix_kk7o9gi3ij0l-2>li:before{content:"\0025a0  "}.lst-kix_axhbb6oyblrk-6>li:before{content:"\0025cf  "}.lst-kix_hl5oa0f6tf4f-1>li:before{content:"\0025cb  "}.lst-kix_acuhah6xdzsp-3>li:before{content:"\0025cf  "}.lst-kix_gsefpdxx2m31-1>li:before{content:"\0025cb  "}.lst-kix_acuhah6xdzsp-7>li:before{content:"\0025cb  "}.lst-kix_bj2se7l3xxv-3>li:before{content:"\0025cf  "}.lst-kix_bj2se7l3xxv-7>li:before{content:"\0025cb  "}.lst-kix_7qzrf8aug8gu-6>li:before{content:"\0025cf  "}.lst-kix_2mto5juffctz-3>li:before{content:"\0025cf  "}.lst-kix_3fnpvvr0i8os-0>li:before{content:"\0025cf  "}.lst-kix_pfssn45qm13m-1>li:before{content:"\0025cb  "}.lst-kix_uest4hng52tf-7>li:before{content:"\0025cb  "}.lst-kix_axhbb6oyblrk-2>li:before{content:"\0025a0  "}.lst-kix_qwp9c5bklrce-1>li:before{content:"\0025cb  "}.lst-kix_h4ky6k6cvoue-2>li:before{content:"\0025a0  "}.lst-kix_h4ky6k6cvoue-6>li:before{content:"\0025cf  "}.lst-kix_qwp9c5bklrce-5>li:before{content:"\0025a0  "}.lst-kix_gg06s1qsfn23-0>li:before{content:"\0025cf  "}.lst-kix_pfssn45qm13m-5>li:before{content:"\0025a0  "}.lst-kix_3xkj88gevxle-5>li:before{content:"\0025a0  "}ul.lst-kix_rcp8sez4svjd-8{list-style-type:none}ul.lst-kix_rcp8sez4svjd-7{list-style-type:none}ul.lst-kix_rcp8sez4svjd-4{list-style-type:none}ul.lst-kix_rcp8sez4svjd-3{list-style-type:none}.lst-kix_rbhmpzgzmhdz-5>li:before{content:"\0025a0  "}ul.lst-kix_rcp8sez4svjd-6{list-style-type:none}.lst-kix_3fnpvvr0i8os-8>li:before{content:"\0025a0  "}ul.lst-kix_rcp8sez4svjd-5{list-style-type:none}.lst-kix_3xkj88gevxle-1>li:before{content:"\0025cb  "}.lst-kix_tlb25jkk5mkd-0>li:before{content:"\0025cf  "}ul.lst-kix_rcp8sez4svjd-0{list-style-type:none}.lst-kix_gg06s1qsfn23-4>li:before{content:"\0025cb  "}.lst-kix_qhrkxnqxxds4-1>li:before{content:"\0025cb  "}.lst-kix_uz5smpj76010-7>li:before{content:"\0025cb  "}ul.lst-kix_rcp8sez4svjd-2{list-style-type:none}ul.lst-kix_rcp8sez4svjd-1{list-style-type:none}.lst-kix_rbhmpzgzmhdz-1>li:before{content:"\0025cb  "}.lst-kix_2mto5juffctz-7>li:before{content:"\0025cb  "}.lst-kix_3fnpvvr0i8os-4>li:before{content:"\0025cb  "}.lst-kix_7qzrf8aug8gu-2>li:before{content:"\0025a0  "}.lst-kix_gg06s1qsfn23-8>li:before{content:"\0025a0  "}.lst-kix_qhrkxnqxxds4-5>li:before{content:"\0025a0  "}.lst-kix_uest4hng52tf-3>li:before{content:"\0025cf  "}.lst-kix_xgfpveff5zr0-1>li:before{content:"\0025cb  "}ul.lst-kix_mm2j3zumujcl-3{list-style-type:none}ul.lst-kix_mm2j3zumujcl-2{list-style-type:none}.lst-kix_rwqhz1uwdird-7>li:before{content:"\0025cb  "}.lst-kix_e2bt8y2ywvrz-7>li:before{content:"\0025cb  "}ul.lst-kix_mm2j3zumujcl-1{list-style-type:none}ul.lst-kix_solhl6ysg65v-0{list-style-type:none}ul.lst-kix_mm2j3zumujcl-0{list-style-type:none}ul.lst-kix_solhl6ysg65v-1{list-style-type:none}ul.lst-kix_solhl6ysg65v-2{list-style-type:none}ul.lst-kix_solhl6ysg65v-3{list-style-type:none}ul.lst-kix_solhl6ysg65v-4{list-style-type:none}.lst-kix_gc2knt874en1-3>li:before{content:"\0025cf  "}.lst-kix_rwqhz1uwdird-3>li:before{content:"\0025cf  "}ul.lst-kix_mm2j3zumujcl-8{list-style-type:none}ul.lst-kix_mm2j3zumujcl-7{list-style-type:none}.lst-kix_pmg6qjdagav9-0>li:before{content:"\0025cf  "}.lst-kix_pmg6qjdagav9-8>li:before{content:"\0025a0  "}ul.lst-kix_mm2j3zumujcl-6{list-style-type:none}ul.lst-kix_mm2j3zumujcl-5{list-style-type:none}ul.lst-kix_mm2j3zumujcl-4{list-style-type:none}.lst-kix_gc2knt874en1-7>li:before{content:"\0025cb  "}ul.lst-kix_solhl6ysg65v-5{list-style-type:none}ul.lst-kix_solhl6ysg65v-6{list-style-type:none}ul.lst-kix_solhl6ysg65v-7{list-style-type:none}ul.lst-kix_solhl6ysg65v-8{list-style-type:none}.lst-kix_pmg6qjdagav9-4>li:before{content:"\0025cb  "}.lst-kix_p0fqpwscumob-5>li:before{content:"\0025a0  "}.lst-kix_qsdpvqxg1u78-0>li:before{content:"\0025cf  "}.lst-kix_qsdpvqxg1u78-8>li:before{content:"\0025a0  "}.lst-kix_vrgigcglf3we-8>li:before{content:"\0025a0  "}.lst-kix_e2bt8y2ywvrz-3>li:before{content:"\0025cf  "}.lst-kix_mm2j3zumujcl-2>li:before{content:"\0025a0  "}.lst-kix_qsdpvqxg1u78-4>li:before{content:"\0025cb  "}ul.lst-kix_b07y002136mr-1{list-style-type:none}ul.lst-kix_b07y002136mr-0{list-style-type:none}.lst-kix_ftgmvnn66fx0-1>li:before{content:"\0025cb  "}.lst-kix_ftgmvnn66fx0-5>li:before{content:"\0025a0  "}.lst-kix_vrgigcglf3we-0>li:before{content:"\0025cf  "}.lst-kix_474t6llskbh7-8>li:before{content:"\0025a0  "}ul.lst-kix_b07y002136mr-7{list-style-type:none}ul.lst-kix_b07y002136mr-6{list-style-type:none}.lst-kix_i0z4lx8deqnq-6>li:before{content:"\0025cf  "}ul.lst-kix_b07y002136mr-8{list-style-type:none}ul.lst-kix_b07y002136mr-3{list-style-type:none}.lst-kix_mm2j3zumujcl-6>li:before{content:"\0025cf  "}ul.lst-kix_b07y002136mr-2{list-style-type:none}ul.lst-kix_b07y002136mr-5{list-style-type:none}ul.lst-kix_b07y002136mr-4{list-style-type:none}.lst-kix_7q22x2z5v5y-5>li:before{content:"\0025a0  "}.lst-kix_i0z4lx8deqnq-2>li:before{content:"\0025a0  "}.lst-kix_vrgigcglf3we-4>li:before{content:"\0025cb  "}.lst-kix_474t6llskbh7-0>li:before{content:"\0025cf  "}.lst-kix_474t6llskbh7-4>li:before{content:"\0025cb  "}.lst-kix_kk7o9gi3ij0l-6>li:before{content:"\0025cf  "}.lst-kix_p0fqpwscumob-1>li:before{content:"\0025cb  "}.lst-kix_1qjopwyk3n75-5>li:before{content:"\0025a0  "}.lst-kix_4yao0efkrd31-5>li:before{content:"\0025a0  "}.lst-kix_4yao0efkrd31-1>li:before{content:"\0025cb  "}.lst-kix_6d5ax9c8k1xa-2>li:before{content:"\0025a0  "}.lst-kix_yv7c7gf6elha-7>li:before{content:"\0025cb  "}.lst-kix_fjl32nwuhu4u-3>li:before{content:"\0025cf  "}.lst-kix_mgyoyquoxrm1-6>li:before{content:"\0025cf  "}.lst-kix_nc0l9nyccr86-4>li:before{content:"\0025cb  "}.lst-kix_2nh9kwfbzbik-5>li:before{content:"\0025a0  "}.lst-kix_6d5ax9c8k1xa-6>li:before{content:"\0025cf  "}.lst-kix_yv7c7gf6elha-3>li:before{content:"\0025cf  "}.lst-kix_nc0l9nyccr86-8>li:before{content:"\0025a0  "}.lst-kix_d2ysoruvczg5-8>li:before{content:"\0025a0  "}.lst-kix_nc0l9nyccr86-0>li:before{content:"\0025cf  "}.lst-kix_74gx9bwnn8kt-0>li:before{content:"\0025cf  "}.lst-kix_kxfg6pdeancy-7>li:before{content:"\0025cb  "}.lst-kix_z8j4l8h7k6b3-8>li:before{content:"\0025a0  "}.lst-kix_s6xk062mdkv5-8>li:before{content:"\0025a0  "}.lst-kix_h99428b6laza-2>li:before{content:"\0025a0  "}.lst-kix_h99428b6laza-6>li:before{content:"\0025cf  "}.lst-kix_fjl32nwuhu4u-7>li:before{content:"\0025cb  "}.lst-kix_mgyoyquoxrm1-2>li:before{content:"\0025a0  "}.lst-kix_kkphisidn6cz-1>li:before{content:"\0025cb  "}.lst-kix_2nh9kwfbzbik-1>li:before{content:"\0025cb  "}ul.lst-kix_5fj31eh1jc7-0{list-style-type:none}ul.lst-kix_5fj31eh1jc7-1{list-style-type:none}.lst-kix_riff65m65tnf-8>li:before{content:"\0025a0  "}ul.lst-kix_5x48r7dz2gvp-8{list-style-type:none}ul.lst-kix_5x48r7dz2gvp-5{list-style-type:none}.lst-kix_scilllbfudea-7>li:before{content:"\0025cb  "}ul.lst-kix_5x48r7dz2gvp-4{list-style-type:none}ul.lst-kix_5x48r7dz2gvp-7{list-style-type:none}.lst-kix_riff65m65tnf-4>li:before{content:"\0025cb  "}ul.lst-kix_5x48r7dz2gvp-6{list-style-type:none}.lst-kix_7q22x2z5v5y-1>li:before{content:"\0025cb  "}.lst-kix_1xoigud6ruor-4>li:before{content:"\0025cb  "}ul.lst-kix_5x48r7dz2gvp-1{list-style-type:none}ul.lst-kix_5x48r7dz2gvp-0{list-style-type:none}ul.lst-kix_5x48r7dz2gvp-3{list-style-type:none}.lst-kix_74gx9bwnn8kt-8>li:before{content:"\0025a0  "}ul.lst-kix_5x48r7dz2gvp-2{list-style-type:none}.lst-kix_scilllbfudea-3>li:before{content:"\0025cf  "}.lst-kix_kkphisidn6cz-5>li:before{content:"\0025a0  "}ul.lst-kix_5fj31eh1jc7-8{list-style-type:none}ul.lst-kix_5fj31eh1jc7-6{list-style-type:none}ul.lst-kix_5fj31eh1jc7-7{list-style-type:none}.lst-kix_1xoigud6ruor-0>li:before{content:"\0025cf  "}ul.lst-kix_5fj31eh1jc7-4{list-style-type:none}ul.lst-kix_5fj31eh1jc7-5{list-style-type:none}.lst-kix_74gx9bwnn8kt-4>li:before{content:"\0025cb  "}ul.lst-kix_5fj31eh1jc7-2{list-style-type:none}ul.lst-kix_5fj31eh1jc7-3{list-style-type:none}.lst-kix_2ihanzwz4fgf-0>li:before{content:"\0025cf  "}.lst-kix_ut5bvwbkq6rt-4>li:before{content:"\0025cb  "}.lst-kix_nbuhcgd4ft21-6>li:before{content:"\0025cf  "}.lst-kix_bcdpfzfy81p5-8>li:before{content:"\0025a0  "}.lst-kix_2ihanzwz4fgf-4>li:before{content:"\0025cb  "}.lst-kix_ut5bvwbkq6rt-0>li:before{content:"\0025cf  "}.lst-kix_ut5bvwbkq6rt-8>li:before{content:"\0025a0  "}.lst-kix_7v1j337157vp-7>li:before{content:"\0025cb  "}.lst-kix_z4z7nvld905a-2>li:before{content:"\0025a0  "}.lst-kix_nbuhcgd4ft21-2>li:before{content:"\0025a0  "}.lst-kix_2ihanzwz4fgf-8>li:before{content:"\0025a0  "}.lst-kix_riff65m65tnf-0>li:before{content:"\0025cf  "}ul.lst-kix_9cwmoyhkmhfb-0{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-1{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-2{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-3{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-4{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-5{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-6{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-7{list-style-type:none}ul.lst-kix_9cwmoyhkmhfb-8{list-style-type:none}.lst-kix_z4z7nvld905a-6>li:before{content:"\0025cf  "}.lst-kix_z8j4l8h7k6b3-0>li:before{content:"\0025cf  "}.lst-kix_s6xk062mdkv5-4>li:before{content:"\0025cb  "}ul.lst-kix_i1kls5tk4gt6-0{list-style-type:none}ul.lst-kix_i1kls5tk4gt6-1{list-style-type:none}ul.lst-kix_i1kls5tk4gt6-2{list-style-type:none}.lst-kix_z8j4l8h7k6b3-4>li:before{content:"\0025cb  "}ul.lst-kix_i1kls5tk4gt6-3{list-style-type:none}.lst-kix_bcdpfzfy81p5-0>li:before{content:"\0025cf  "}.lst-kix_s6xk062mdkv5-0>li:before{content:"\0025cf  "}ul.lst-kix_i1kls5tk4gt6-4{list-style-type:none}ul.lst-kix_i1kls5tk4gt6-5{list-style-type:none}.lst-kix_d2ysoruvczg5-0>li:before{content:"\0025cf  "}ul.lst-kix_i1kls5tk4gt6-6{list-style-type:none}ul.lst-kix_i1kls5tk4gt6-7{list-style-type:none}ul.lst-kix_i1kls5tk4gt6-8{list-style-type:none}.lst-kix_bcdpfzfy81p5-4>li:before{content:"\0025cb  "}ul.lst-kix_8k70e8i8rdlm-8{list-style-type:none}.lst-kix_d2ysoruvczg5-4>li:before{content:"\0025cb  "}.lst-kix_kxfg6pdeancy-3>li:before{content:"\0025cf  "}ul.lst-kix_8k70e8i8rdlm-2{list-style-type:none}ul.lst-kix_8k70e8i8rdlm-3{list-style-type:none}ul.lst-kix_8k70e8i8rdlm-0{list-style-type:none}ul.lst-kix_8k70e8i8rdlm-1{list-style-type:none}ul.lst-kix_8k70e8i8rdlm-6{list-style-type:none}ul.lst-kix_8k70e8i8rdlm-7{list-style-type:none}ul.lst-kix_8k70e8i8rdlm-4{list-style-type:none}ul.lst-kix_8k70e8i8rdlm-5{list-style-type:none}.lst-kix_n21asioviqe4-1>li:before{content:"\0025cb  "}.lst-kix_xgfpveff5zr0-5>li:before{content:"\0025a0  "}ul.lst-kix_p3gj10803b4t-1{list-style-type:none}ul.lst-kix_p3gj10803b4t-2{list-style-type:none}ul.lst-kix_p3gj10803b4t-3{list-style-type:none}ul.lst-kix_p3gj10803b4t-4{list-style-type:none}.lst-kix_l47f7qjlu9cu-3>li:before{content:"\0025cf  "}.lst-kix_z1voh5a7dove-4>li:before{content:"\0025cb  "}ul.lst-kix_p3gj10803b4t-0{list-style-type:none}ul.lst-kix_nxvbdas3q36w-1{list-style-type:none}ul.lst-kix_nxvbdas3q36w-2{list-style-type:none}ul.lst-kix_nxvbdas3q36w-0{list-style-type:none}ul.lst-kix_p3gj10803b4t-5{list-style-type:none}ul.lst-kix_p3gj10803b4t-6{list-style-type:none}ul.lst-kix_p3gj10803b4t-7{list-style-type:none}ul.lst-kix_p3gj10803b4t-8{list-style-type:none}.lst-kix_z1voh5a7dove-8>li:before{content:"\0025a0  "}ul.lst-kix_nxvbdas3q36w-7{list-style-type:none}ul.lst-kix_nxvbdas3q36w-8{list-style-type:none}ul.lst-kix_nxvbdas3q36w-5{list-style-type:none}.lst-kix_gpgigcfykzff-3>li:before{content:"\0025cf  "}ul.lst-kix_nxvbdas3q36w-6{list-style-type:none}.lst-kix_uz5smpj76010-3>li:before{content:"\0025cf  "}ul.lst-kix_nxvbdas3q36w-3{list-style-type:none}ul.lst-kix_nxvbdas3q36w-4{list-style-type:none}.lst-kix_gpgigcfykzff-7>li:before{content:"\0025cb  "}.lst-kix_n21asioviqe4-5>li:before{content:"\0025a0  "}.lst-kix_7apmk6vwlj5r-0>li:before{content:"\0025cf  "}.lst-kix_7v1j337157vp-3>li:before{content:"\0025cf  "}.lst-kix_z1voh5a7dove-0>li:before{content:"\0025cf  "}ul.lst-kix_bjneibxfrx3r-4{list-style-type:none}ul.lst-kix_bjneibxfrx3r-5{list-style-type:none}.lst-kix_b474o1tvhevy-1>li:before{content:"\0025cb  "}ul.lst-kix_bjneibxfrx3r-2{list-style-type:none}ul.lst-kix_bjneibxfrx3r-3{list-style-type:none}ul.lst-kix_bjneibxfrx3r-0{list-style-type:none}ul.lst-kix_bjneibxfrx3r-1{list-style-type:none}.lst-kix_ex6gjyozs00-3>li:before{content:"\0025cf  "}.lst-kix_l4pre1xosssa-7>li:before{content:"\0025cb  "}.lst-kix_n1766bho68n4-2>li:before{content:"\0025a0  "}.lst-kix_2jmrp4ochkmc-3>li:before{content:"\0025cf  "}.lst-kix_b474o1tvhevy-5>li:before{content:"\0025a0  "}.lst-kix_1xoigud6ruor-8>li:before{content:"\0025a0  "}ul.lst-kix_bjneibxfrx3r-8{list-style-type:none}.lst-kix_ex6gjyozs00-7>li:before{content:"\0025cb  "}ul.lst-kix_bjneibxfrx3r-6{list-style-type:none}.lst-kix_l4pre1xosssa-3>li:before{content:"\0025cf  "}ul.lst-kix_bjneibxfrx3r-7{list-style-type:none}.lst-kix_n1766bho68n4-6>li:before{content:"\0025cf  "}.lst-kix_2jmrp4ochkmc-7>li:before{content:"\0025cb  "}.lst-kix_o14fwownm59e-1>li:before{content:"\0025cb  "}ul.lst-kix_qlax8iv93hxg-8{list-style-type:none}ul.lst-kix_qlax8iv93hxg-7{list-style-type:none}.lst-kix_vzf3xt3x8pov-3>li:before{content:"\0025cf  "}.lst-kix_tglcy61nnbu3-3>li:before{content:"\0025cf  "}.lst-kix_vebf56ofbjcp-5>li:before{content:"\0025a0  "}ul.lst-kix_jbw6p6lajlkh-1{list-style-type:none}ul.lst-kix_qlax8iv93hxg-0{list-style-type:none}ul.lst-kix_jbw6p6lajlkh-0{list-style-type:none}ul.lst-kix_jbw6p6lajlkh-3{list-style-type:none}ul.lst-kix_qlax8iv93hxg-2{list-style-type:none}ul.lst-kix_jbw6p6lajlkh-2{list-style-type:none}.lst-kix_vzf3xt3x8pov-7>li:before{content:"\0025cb  "}ul.lst-kix_qlax8iv93hxg-1{list-style-type:none}ul.lst-kix_jbw6p6lajlkh-5{list-style-type:none}ul.lst-kix_qlax8iv93hxg-4{list-style-type:none}ul.lst-kix_jbw6p6lajlkh-4{list-style-type:none}ul.lst-kix_qlax8iv93hxg-3{list-style-type:none}.lst-kix_o14fwownm59e-5>li:before{content:"\0025a0  "}ul.lst-kix_jbw6p6lajlkh-7{list-style-type:none}ul.lst-kix_qlax8iv93hxg-6{list-style-type:none}ul.lst-kix_jbw6p6lajlkh-6{list-style-type:none}ul.lst-kix_qlax8iv93hxg-5{list-style-type:none}ul.lst-kix_jbw6p6lajlkh-8{list-style-type:none}.lst-kix_l47f7qjlu9cu-7>li:before{content:"\0025cb  "}.lst-kix_tglcy61nnbu3-7>li:before{content:"\0025cb  "}.lst-kix_vebf56ofbjcp-1>li:before{content:"\0025cb  "}ul.lst-kix_qwp9c5bklrce-1{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-2{list-style-type:none}ul.lst-kix_qwp9c5bklrce-2{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-1{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-4{list-style-type:none}ul.lst-kix_qwp9c5bklrce-0{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-3{list-style-type:none}ul.lst-kix_qwp9c5bklrce-5{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-6{list-style-type:none}ul.lst-kix_qwp9c5bklrce-6{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-5{list-style-type:none}ul.lst-kix_qwp9c5bklrce-3{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-8{list-style-type:none}ul.lst-kix_qwp9c5bklrce-4{list-style-type:none}ul.lst-kix_vhrrjzyjvp5x-7{list-style-type:none}ul.lst-kix_qwp9c5bklrce-7{list-style-type:none}ul.lst-kix_qwp9c5bklrce-8{list-style-type:none}.lst-kix_kk8oms3v3iyb-2>li:before{content:"\0025a0  "}.lst-kix_dq6vem8xoqp9-8>li:before{content:"\0025a0  "}.lst-kix_sxaq135mthse-4>li:before{content:"\0025cb  "}.lst-kix_gogm0wicn297-3>li:before{content:"\0025cf  "}.lst-kix_u9ija913pfxg-7>li:before{content:"-  "}.lst-kix_6r73oe35erxy-1>li:before{content:"\0025cb  "}.lst-kix_ljjwvb506pet-8>li:before{content:"\0025a0  "}ul.lst-kix_vhrrjzyjvp5x-0{list-style-type:none}.lst-kix_edj3r8hgf2z3-5>li:before{content:"\0025a0  "}.lst-kix_jbw6p6lajlkh-6>li:before{content:"\0025cf  "}.lst-kix_nf1j1iafbq39-7>li:before{content:"\0025cb  "}ul.lst-kix_n1766bho68n4-8{list-style-type:none}.lst-kix_didibfga6ee5-7>li:before{content:"\0025cb  "}.lst-kix_m46mj27r3fpz-7>li:before{content:"\0025cb  "}ul.lst-kix_n1766bho68n4-5{list-style-type:none}ul.lst-kix_n1766bho68n4-4{list-style-type:none}ul.lst-kix_n1766bho68n4-7{list-style-type:none}.lst-kix_u82x83buy4jv-0>li:before{content:"\0025cf  "}ul.lst-kix_n1766bho68n4-6{list-style-type:none}.lst-kix_rcp8sez4svjd-6>li:before{content:"-  "}.lst-kix_ljjwvb506pet-0>li:before{content:"\0025cf  "}.lst-kix_5d66udn9quk7-4>li:before{content:"\0025cb  "}.lst-kix_u82x83buy4jv-8>li:before{content:"\0025a0  "}.lst-kix_douubz1tb8an-5>li:before{content:"\0025a0  "}.lst-kix_nbpnmy8xci8w-8>li:before{content:"\0025a0  "}.lst-kix_tg2hwffju8t9-4>li:before{content:"\0025cb  "}.lst-kix_b8dlmlw0p2p1-3>li:before{content:"\0025cf  "}.lst-kix_mxwp6np2u5xu-1>li:before{content:"-  "}.lst-kix_204pjqaavew-8>li:before{content:"\0025a0  "}.lst-kix_5kaugn46jpw6-3>li:before{content:"\0025cf  "}.lst-kix_x3vcornhe3kv-3>li:before{content:"\0025cf  "}.lst-kix_8hjy0hqh3rvg-1>li:before{content:"\0025cb  "}.lst-kix_7apmk6vwlj5r-8>li:before{content:"\0025a0  "}.lst-kix_398os0jr1iw3-7>li:before{content:"\0025cb  "}.lst-kix_5fj31eh1jc7-3>li:before{content:"\0025cf  "}.lst-kix_7b2y41o7s2k3-3>li:before{content:"\0025cf  "}.lst-kix_dq6vem8xoqp9-0>li:before{content:"\0025cf  "}.lst-kix_f3egesa43r6b-1>li:before{content:"\0025cb  "}ul.lst-kix_xx3xz98aq2wq-0{list-style-type:none}ul.lst-kix_xx3xz98aq2wq-1{list-style-type:none}ul.lst-kix_xx3xz98aq2wq-2{list-style-type:none}.lst-kix_nbpnmy8xci8w-0>li:before{content:"\0025cf  "}ul.lst-kix_xx3xz98aq2wq-3{list-style-type:none}ul.lst-kix_xx3xz98aq2wq-4{list-style-type:none}ul.lst-kix_xx3xz98aq2wq-5{list-style-type:none}ul.lst-kix_xx3xz98aq2wq-6{list-style-type:none}ul.lst-kix_xx3xz98aq2wq-7{list-style-type:none}ul.lst-kix_xx3xz98aq2wq-8{list-style-type:none}.lst-kix_hydge7xfbt8r-2>li:before{content:"\0025a0  "}ul.lst-kix_l4pre1xosssa-1{list-style-type:none}.lst-kix_6asjiojyuog6-8>li:before{content:"\0025a0  "}ul.lst-kix_l4pre1xosssa-0{list-style-type:none}ul.lst-kix_l4pre1xosssa-3{list-style-type:none}ul.lst-kix_l4pre1xosssa-2{list-style-type:none}ul.lst-kix_l4pre1xosssa-5{list-style-type:none}ul.lst-kix_l4pre1xosssa-4{list-style-type:none}.lst-kix_x5zq7qqqztpc-0>li:before{content:"\0025cf  "}.lst-kix_gwuatnbmuewh-2>li:before{content:"\0025a0  "}.lst-kix_wdjiiwxyq2d1-6>li:before{content:"\0025cf  "}.lst-kix_ucprmx270ar2-6>li:before{content:"\0025cf  "}.lst-kix_6asjiojyuog6-0>li:before{content:"\0025cf  "}.lst-kix_o5m7acjt2ju1-2>li:before{content:"\0025a0  "}ul.lst-kix_6asjiojyuog6-3{list-style-type:none}.lst-kix_iiujyt46s4u7-0>li:before{content:"\0025cf  "}ul.lst-kix_6asjiojyuog6-4{list-style-type:none}ul.lst-kix_6asjiojyuog6-1{list-style-type:none}ul.lst-kix_6asjiojyuog6-2{list-style-type:none}.lst-kix_l1rt7mvaz29b-7>li:before{content:"\0025cb  "}ul.lst-kix_6asjiojyuog6-7{list-style-type:none}ul.lst-kix_6asjiojyuog6-8{list-style-type:none}ul.lst-kix_6asjiojyuog6-5{list-style-type:none}ul.lst-kix_6asjiojyuog6-6{list-style-type:none}.lst-kix_xx3xz98aq2wq-7>li:before{content:"\0025cb  "}.lst-kix_mh0uhwysa91-3>li:before{content:"\0025cf  "}.lst-kix_204pjqaavew-0>li:before{content:"\0025cf  "}.lst-kix_jzarhjey1jrh-1>li:before{content:"\0025cb  "}ul.lst-kix_6asjiojyuog6-0{list-style-type:none}.lst-kix_4ykl6tjv6vdc-2>li:before{content:"\0025a0  "}ul.lst-kix_4ykl6tjv6vdc-0{list-style-type:none}ul.lst-kix_4ykl6tjv6vdc-1{list-style-type:none}.lst-kix_f51q5t2m61i8-2>li:before{content:"\0025a0  "}.lst-kix_crb89j7xw8q2-6>li:before{content:"\0025cf  "}ul.lst-kix_4ykl6tjv6vdc-8{list-style-type:none}ul.lst-kix_4ykl6tjv6vdc-6{list-style-type:none}ul.lst-kix_4ykl6tjv6vdc-7{list-style-type:none}ul.lst-kix_4ykl6tjv6vdc-4{list-style-type:none}ul.lst-kix_4ykl6tjv6vdc-5{list-style-type:none}.lst-kix_z1olrl2brpzr-4>li:before{content:"\0025cb  "}.lst-kix_x5zq7qqqztpc-8>li:before{content:"\0025a0  "}ul.lst-kix_4ykl6tjv6vdc-2{list-style-type:none}ul.lst-kix_4ykl6tjv6vdc-3{list-style-type:none}.lst-kix_iiujyt46s4u7-8>li:before{content:"\0025a0  "}.lst-kix_a9t5z6hxg4oz-2>li:before{content:"\0025a0  "}.lst-kix_ulntv5wergyk-5>li:before{content:"\0025a0  "}.lst-kix_5p6viq6b9z8b-7>li:before{content:"\0025cb  "}.lst-kix_o4ah8y8am4ct-4>li:before{content:"\0025cb  "}ul.lst-kix_n1766bho68n4-1{list-style-type:none}ul.lst-kix_n1766bho68n4-0{list-style-type:none}ul.lst-kix_n1766bho68n4-3{list-style-type:none}ul.lst-kix_n1766bho68n4-2{list-style-type:none}.lst-kix_9khvkfghhb3b-2>li:before{content:"\0025a0  "}.lst-kix_emy9ap8zeql8-6>li:before{content:"\0025cf  "}.lst-kix_ow9ix2mme95u-5>li:before{content:"\0025a0  "}.lst-kix_cst6lp3lb98n-5>li:before{content:"\0025a0  "}.lst-kix_o4d3wx7elf58-7>li:before{content:"\0025cb  "}.lst-kix_ct57w2de70tb-4>li:before{content:"\0025cb  "}ul.lst-kix_7apmk6vwlj5r-4{list-style-type:none}ul.lst-kix_7apmk6vwlj5r-3{list-style-type:none}ul.lst-kix_7apmk6vwlj5r-6{list-style-type:none}ul.lst-kix_7apmk6vwlj5r-5{list-style-type:none}ul.lst-kix_7apmk6vwlj5r-8{list-style-type:none}ul.lst-kix_7apmk6vwlj5r-7{list-style-type:none}.lst-kix_iigck7esqel5-0>li:before{content:"\0025cf  "}.lst-kix_saoi6g81wrq2-2>li:before{content:"\0025a0  "}.lst-kix_u6q708mtjbvx-7>li:before{content:"-  "}.lst-kix_nxvbdas3q36w-4>li:before{content:"\0025cb  "}.lst-kix_iigck7esqel5-8>li:before{content:"\0025a0  "}.lst-kix_24cmoxb3qmwl-6>li:before{content:"\0025cf  "}.lst-kix_p2mkzdwo7q6r-3>li:before{content:"\0025cf  "}ul.lst-kix_7apmk6vwlj5r-0{list-style-type:none}ul.lst-kix_7apmk6vwlj5r-2{list-style-type:none}.lst-kix_wq1o6elz9hy4-1>li:before{content:"\0025cb  "}ul.lst-kix_7apmk6vwlj5r-1{list-style-type:none}ul.lst-kix_isql72ps5u79-6{list-style-type:none}ul.lst-kix_isql72ps5u79-7{list-style-type:none}ul.lst-kix_isql72ps5u79-4{list-style-type:none}ul.lst-kix_isql72ps5u79-5{list-style-type:none}.lst-kix_8k70e8i8rdlm-6>li:before{content:"\0025cf  "}ul.lst-kix_isql72ps5u79-8{list-style-type:none}ul.lst-kix_isql72ps5u79-2{list-style-type:none}.lst-kix_16ygmg7toocv-0>li:before{content:"\0025cf  "}.lst-kix_16ygmg7toocv-8>li:before{content:"\0025a0  "}ul.lst-kix_isql72ps5u79-3{list-style-type:none}ul.lst-kix_isql72ps5u79-0{list-style-type:none}ul.lst-kix_isql72ps5u79-1{list-style-type:none}.lst-kix_b07y002136mr-0>li:before{content:"\0025cf  "}.lst-kix_emewkhtp1rfy-3>li:before{content:"\0025cf  "}ul.lst-kix_kk8oms3v3iyb-0{list-style-type:none}ul.lst-kix_kk8oms3v3iyb-1{list-style-type:none}.lst-kix_8cma8sbxfy2x-5>li:before{content:"\0025a0  "}ul.lst-kix_s3fvwoba1t7n-8{list-style-type:none}ul.lst-kix_kk8oms3v3iyb-4{list-style-type:none}ul.lst-kix_kk8oms3v3iyb-5{list-style-type:none}ul.lst-kix_kk8oms3v3iyb-2{list-style-type:none}ul.lst-kix_kk8oms3v3iyb-3{list-style-type:none}ul.lst-kix_kk8oms3v3iyb-8{list-style-type:none}ul.lst-kix_kk8oms3v3iyb-6{list-style-type:none}.lst-kix_vhrrjzyjvp5x-3>li:before{content:"\0025cf  "}ul.lst-kix_kk8oms3v3iyb-7{list-style-type:none}ul.lst-kix_s3fvwoba1t7n-0{list-style-type:none}ul.lst-kix_zgvzzrmreqeg-7{list-style-type:none}ul.lst-kix_s3fvwoba1t7n-1{list-style-type:none}.lst-kix_9cwmoyhkmhfb-5>li:before{content:"\0025a0  "}ul.lst-kix_zgvzzrmreqeg-8{list-style-type:none}ul.lst-kix_s3fvwoba1t7n-2{list-style-type:none}ul.lst-kix_zgvzzrmreqeg-5{list-style-type:none}ul.lst-kix_s3fvwoba1t7n-3{list-style-type:none}ul.lst-kix_zgvzzrmreqeg-6{list-style-type:none}ul.lst-kix_s3fvwoba1t7n-4{list-style-type:none}.lst-kix_pzunwido5p11-6>li:before{content:"\0025cf  "}ul.lst-kix_s3fvwoba1t7n-5{list-style-type:none}.lst-kix_w14ild1qeoey-7>li:before{content:"\0025cb  "}ul.lst-kix_s3fvwoba1t7n-6{list-style-type:none}.lst-kix_xrce7kijh7l1-3>li:before{content:"\0025cf  "}ul.lst-kix_s3fvwoba1t7n-7{list-style-type:none}.lst-kix_894copnhklgz-5>li:before{content:"\0025a0  "}.lst-kix_ukuyvb7r2sb4-0>li:before{content:"\0025cf  "}.lst-kix_yh6f66826ysk-6>li:before{content:"\0025cf  "}.lst-kix_jys0trw4gd1m-3>li:before{content:"\0025cf  "}.lst-kix_jhqr8d48vqqp-2>li:before{content:"\0025a0  "}.lst-kix_ehe7zglmx7i2-4>li:before{content:"\0025cb  "}.lst-kix_8z696e8bv17-4>li:before{content:"\0025cb  "}.lst-kix_6yojqqjec0st-7>li:before{content:"\0025cb  "}.lst-kix_juam6n4619t2-5>li:before{content:"\0025a0  "}ul.lst-kix_5p6viq6b9z8b-7{list-style-type:none}.lst-kix_dj4fsxw52t4l-1>li:before{content:"\0025cb  "}ul.lst-kix_5p6viq6b9z8b-8{list-style-type:none}ul.lst-kix_zgvzzrmreqeg-0{list-style-type:none}ul.lst-kix_zgvzzrmreqeg-3{list-style-type:none}ul.lst-kix_zgvzzrmreqeg-4{list-style-type:none}ul.lst-kix_zgvzzrmreqeg-1{list-style-type:none}.lst-kix_vapye4eddf48-4>li:before{content:"\0025cb  "}ul.lst-kix_zgvzzrmreqeg-2{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-2{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-1{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-4{list-style-type:none}.lst-kix_383wi8io125v-7>li:before{content:"\0025cb  "}ul.lst-kix_ftgmvnn66fx0-3{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-0{list-style-type:none}.lst-kix_rwfcya85pd71-3>li:before{content:"\0025cf  "}.lst-kix_o9z4cf2yq4mz-1>li:before{content:"\0025cb  "}.lst-kix_d1iyykra6bco-6>li:before{content:"\0025cf  "}ul.lst-kix_5p6viq6b9z8b-5{list-style-type:none}ul.lst-kix_5p6viq6b9z8b-6{list-style-type:none}ul.lst-kix_5p6viq6b9z8b-3{list-style-type:none}ul.lst-kix_5p6viq6b9z8b-4{list-style-type:none}ul.lst-kix_5p6viq6b9z8b-1{list-style-type:none}ul.lst-kix_5p6viq6b9z8b-2{list-style-type:none}.lst-kix_d4lbyvb2g3j-5>li:before{content:"\0025a0  "}ul.lst-kix_5p6viq6b9z8b-0{list-style-type:none}.lst-kix_d8ik6bmbwuj4-2>li:before{content:"\0025a0  "}ul.lst-kix_l4pre1xosssa-7{list-style-type:none}ul.lst-kix_l4pre1xosssa-6{list-style-type:none}ul.lst-kix_l4pre1xosssa-8{list-style-type:none}.lst-kix_4k4id0a37km3-5>li:before{content:"\0025a0  "}.lst-kix_xgsgxyckqw77-3>li:before{content:"\0025cf  "}ul.lst-kix_d995nc7613a9-8{list-style-type:none}ul.lst-kix_d995nc7613a9-6{list-style-type:none}.lst-kix_o38l9c2rqk2p-1>li:before{content:"\0025cb  "}.lst-kix_x5jl3p2g0327-6>li:before{content:"\0025cf  "}ul.lst-kix_d995nc7613a9-7{list-style-type:none}ul.lst-kix_d995nc7613a9-4{list-style-type:none}ul.lst-kix_d995nc7613a9-5{list-style-type:none}ul.lst-kix_d995nc7613a9-2{list-style-type:none}ul.lst-kix_d995nc7613a9-3{list-style-type:none}.lst-kix_ymmukz9l2n7v-7>li:before{content:"\0025cb  "}.lst-kix_cq59e9nktsby-1>li:before{content:"\0025cb  "}.lst-kix_gnzb8ea0t50l-7>li:before{content:"\0025cb  "}.lst-kix_8nsyajlscxcg-5>li:before{content:"\0025a0  "}.lst-kix_8wesot3ko53b-0>li:before{content:"\0025cf  "}.lst-kix_6ykz6yxb3o6x-3>li:before{content:"\0025cf  "}.lst-kix_pjyqz9f44g8s-6>li:before{content:"\0025cf  "}.lst-kix_79uewz4g8df3-3>li:before{content:"\0025cf  "}.lst-kix_klzhllb6xcyr-7>li:before{content:"\0025cb  "}.lst-kix_c2vvnsz0drek-2>li:before{content:"\0025a0  "}.lst-kix_8wesot3ko53b-8>li:before{content:"\0025a0  "}ul.lst-kix_8hjy0hqh3rvg-5{list-style-type:none}.lst-kix_ydb53brft35n-7>li:before{content:"\0025cb  "}ul.lst-kix_8hjy0hqh3rvg-6{list-style-type:none}.lst-kix_i1kls5tk4gt6-3>li:before{content:"\0025cf  "}ul.lst-kix_8hjy0hqh3rvg-3{list-style-type:none}.lst-kix_vsaybtg1hfvm-1>li:before{content:"\0025cb  "}ul.lst-kix_8hjy0hqh3rvg-4{list-style-type:none}.lst-kix_tlb25jkk5mkd-8>li:before{content:"\0025a0  "}.lst-kix_xkjgsygnpr2m-2>li:before{content:"\0025a0  "}ul.lst-kix_8hjy0hqh3rvg-7{list-style-type:none}ul.lst-kix_8hjy0hqh3rvg-8{list-style-type:none}.lst-kix_egb7bfw0xx0q-2>li:before{content:"\0025a0  "}.lst-kix_gswyhyotsm36-2>li:before{content:"\0025a0  "}.lst-kix_62is116n17wq-6>li:before{content:"\0025cf  "}.lst-kix_lyxc6qcmvco2-5>li:before{content:"\0025a0  "}.lst-kix_tltq6x9aaf31-6>li:before{content:"\0025cf  "}.lst-kix_41ikkbtdotge-4>li:before{content:"\0025cb  "}.lst-kix_3odao55mezu-1>li:before{content:"\0025cb  "}.lst-kix_ukuyvb7r2sb4-8>li:before{content:"\0025a0  "}.lst-kix_3de1jg1kuy24-3>li:before{content:"\0025cf  "}.lst-kix_1ig57j6lcs95-6>li:before{content:"\0025cf  "}.lst-kix_x2mjmu7myisq-6>li:before{content:"\0025cf  "}.lst-kix_313q3f5x4p2m-4>li:before{content:"\0025cb  "}.lst-kix_5ia1ign2sshz-0>li:before{content:"\0025cf  "}.lst-kix_5ia1ign2sshz-8>li:before{content:"\0025a0  "}ul.lst-kix_17is42bhveph-4{list-style-type:none}ul.lst-kix_17is42bhveph-5{list-style-type:none}ul.lst-kix_17is42bhveph-2{list-style-type:none}.lst-kix_xav8ikvidc9s-5>li:before{content:"\0025a0  "}ul.lst-kix_17is42bhveph-3{list-style-type:none}.lst-kix_ta3ofdl38yyn-7>li:before{content:"\0025cb  "}ul.lst-kix_17is42bhveph-0{list-style-type:none}ul.lst-kix_17is42bhveph-1{list-style-type:none}ul.lst-kix_8hjy0hqh3rvg-1{list-style-type:none}ul.lst-kix_8hjy0hqh3rvg-2{list-style-type:none}.lst-kix_8swzovi1ytnr-3>li:before{content:"\0025cf  "}ul.lst-kix_8hjy0hqh3rvg-0{list-style-type:none}.lst-kix_p98dexr65abh-7>li:before{content:"\0025cb  "}.lst-kix_470xpz4q6hoi-1>li:before{content:"\0025cb  "}.lst-kix_5x48r7dz2gvp-4>li:before{content:"\0025cb  "}.lst-kix_bq5ybp1zal0b-1>li:before{content:"\0025cb  "}.lst-kix_b07y002136mr-8>li:before{content:"\0025a0  "}.lst-kix_kxpudurk2j6e-3>li:before{content:"\0025cf  "}.lst-kix_u1clvxqptoo-4>li:before{content:"\0025cb  "}.lst-kix_fk4cszd8thar-7>li:before{content:"\0025cb  "}ul.lst-kix_hh7pij1hmwlh-0{list-style-type:none}ul.lst-kix_hh7pij1hmwlh-1{list-style-type:none}ul.lst-kix_17is42bhveph-8{list-style-type:none}ul.lst-kix_17is42bhveph-6{list-style-type:none}ul.lst-kix_17is42bhveph-7{list-style-type:none}ul.lst-kix_d995nc7613a9-0{list-style-type:none}ul.lst-kix_hh7pij1hmwlh-6{list-style-type:none}ul.lst-kix_d995nc7613a9-1{list-style-type:none}ul.lst-kix_hh7pij1hmwlh-7{list-style-type:none}ul.lst-kix_hh7pij1hmwlh-8{list-style-type:none}.lst-kix_2zf5bd1mem1t-1>li:before{content:"\0025cb  "}ul.lst-kix_hh7pij1hmwlh-2{list-style-type:none}ul.lst-kix_hh7pij1hmwlh-3{list-style-type:none}ul.lst-kix_hh7pij1hmwlh-4{list-style-type:none}.lst-kix_m8diz2yltkag-3>li:before{content:"\0025cf  "}ul.lst-kix_hh7pij1hmwlh-5{list-style-type:none}ul.lst-kix_z3si3pbri1de-1{list-style-type:none}ul.lst-kix_z3si3pbri1de-0{list-style-type:none}ul.lst-kix_z3si3pbri1de-3{list-style-type:none}ul.lst-kix_z3si3pbri1de-2{list-style-type:none}ul.lst-kix_z3si3pbri1de-5{list-style-type:none}.lst-kix_vjhjbjs9ubca-4>li:before{content:"\0025cb  "}.lst-kix_rr7hjjtid2vg-8>li:before{content:"\0025a0  "}ul.lst-kix_z3si3pbri1de-4{list-style-type:none}ul.lst-kix_z3si3pbri1de-7{list-style-type:none}ul.lst-kix_z3si3pbri1de-6{list-style-type:none}.lst-kix_rr7hjjtid2vg-7>li:before{content:"\0025cb  "}ul.lst-kix_z3si3pbri1de-8{list-style-type:none}.lst-kix_nwi1ck3fibs7-5>li:before{content:"\0025a0  "}.lst-kix_vjhjbjs9ubca-8>li:before{content:"\0025a0  "}.lst-kix_rr7hjjtid2vg-4>li:before{content:"\0025cb  "}.lst-kix_vjhjbjs9ubca-7>li:before{content:"\0025cb  "}.lst-kix_nwi1ck3fibs7-4>li:before{content:"\0025cb  "}.lst-kix_nwi1ck3fibs7-1>li:before{content:"\0025cb  "}ul.lst-kix_1ig57j6lcs95-6{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-4{list-style-type:none}ul.lst-kix_1ig57j6lcs95-7{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-3{list-style-type:none}.lst-kix_i5pnhlfopov3-1>li:before{content:"\0025cb  "}ul.lst-kix_1ig57j6lcs95-8{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-2{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-1{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-8{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-7{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-6{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-5{list-style-type:none}.lst-kix_i5pnhlfopov3-4>li:before{content:"\0025cb  "}.lst-kix_jourdftw4nrm-5>li:before{content:"\0025a0  "}.lst-kix_p3gj10803b4t-5>li:before{content:"\0025a0  "}ul.lst-kix_sxaq135mthse-3{list-style-type:none}.lst-kix_r8labycqo9zl-2>li:before{content:"\0025a0  "}ul.lst-kix_sxaq135mthse-2{list-style-type:none}.lst-kix_i5pnhlfopov3-8>li:before{content:"\0025a0  "}ul.lst-kix_sxaq135mthse-1{list-style-type:none}ul.lst-kix_sxaq135mthse-0{list-style-type:none}.lst-kix_i5pnhlfopov3-5>li:before{content:"\0025a0  "}.lst-kix_nwi1ck3fibs7-8>li:before{content:"\0025a0  "}ul.lst-kix_sxaq135mthse-8{list-style-type:none}ul.lst-kix_sxaq135mthse-7{list-style-type:none}ul.lst-kix_sxaq135mthse-6{list-style-type:none}ul.lst-kix_sxaq135mthse-5{list-style-type:none}ul.lst-kix_sxaq135mthse-4{list-style-type:none}.lst-kix_z0c6lix981m-8>li:before{content:"\0025a0  "}ul.lst-kix_o4ah8y8am4ct-6{list-style-type:none}ul.lst-kix_o4ah8y8am4ct-5{list-style-type:none}ul.lst-kix_o4ah8y8am4ct-4{list-style-type:none}ul.lst-kix_o4ah8y8am4ct-3{list-style-type:none}ul.lst-kix_o4ah8y8am4ct-8{list-style-type:none}.lst-kix_ltkzlkp2gxkt-6>li:before{content:"\0025cf  "}.lst-kix_hh7pij1hmwlh-8>li:before{content:"\0025a0  "}ul.lst-kix_o4ah8y8am4ct-7{list-style-type:none}.lst-kix_z0c6lix981m-3>li:before{content:"\0025cf  "}.lst-kix_ly0lmz6fcokr-6>li:before{content:"\0025cf  "}.lst-kix_vsaybtg1hfvm-8>li:before{content:"\0025a0  "}.lst-kix_zgvzzrmreqeg-2>li:before{content:"\0025a0  "}.lst-kix_vsaybtg1hfvm-5>li:before{content:"\0025a0  "}.lst-kix_jourdftw4nrm-8>li:before{content:"\0025a0  "}.lst-kix_481c98v2pt83-1>li:before{content:"\0025cb  "}.lst-kix_p3gj10803b4t-2>li:before{content:"\0025a0  "}ul.lst-kix_o4ah8y8am4ct-2{list-style-type:none}ul.lst-kix_o4ah8y8am4ct-1{list-style-type:none}ul.lst-kix_o4ah8y8am4ct-0{list-style-type:none}ul.lst-kix_yv7c7gf6elha-1{list-style-type:none}ul.lst-kix_yv7c7gf6elha-0{list-style-type:none}ul.lst-kix_yv7c7gf6elha-3{list-style-type:none}ul.lst-kix_yv7c7gf6elha-2{list-style-type:none}ul.lst-kix_yv7c7gf6elha-5{list-style-type:none}ul.lst-kix_yv7c7gf6elha-4{list-style-type:none}ul.lst-kix_yv7c7gf6elha-7{list-style-type:none}ul.lst-kix_yv7c7gf6elha-6{list-style-type:none}.lst-kix_s3fvwoba1t7n-8>li:before{content:"\0025a0  "}.lst-kix_47kviebmmd71-5>li:before{content:"\0025a0  "}.lst-kix_s3fvwoba1t7n-4>li:before{content:"\0025cb  "}.lst-kix_hh7pij1hmwlh-5>li:before{content:"\0025a0  "}.lst-kix_ly0lmz6fcokr-2>li:before{content:"\0025a0  "}.lst-kix_ly0lmz6fcokr-3>li:before{content:"\0025cf  "}ul.lst-kix_yv7c7gf6elha-8{list-style-type:none}.lst-kix_hh7pij1hmwlh-1>li:before{content:"\0025cb  "}.lst-kix_defeme42l2ge-2>li:before{content:"\0025a0  "}.lst-kix_47kviebmmd71-1>li:before{content:"\0025cb  "}.lst-kix_58o57c52tmut-6>li:before{content:"\0025cf  "}.lst-kix_rrs2j0rau6wp-7>li:before{content:"\0025cb  "}.lst-kix_7r3huctovklw-1>li:before{content:"\0025cb  "}.lst-kix_xkmlxx8lhm5d-0>li:before{content:"\0025cf  "}.lst-kix_yhn6vvmwi8p2-4>li:before{content:"\0025cb  "}.lst-kix_58o57c52tmut-3>li:before{content:"\0025cf  "}.lst-kix_uk8k6pzec0np-2>li:before{content:"\0025a0  "}.lst-kix_qlax8iv93hxg-5>li:before{content:"\0025a0  "}.lst-kix_o38l9c2rqk2p-8>li:before{content:"\0025a0  "}.lst-kix_yhn6vvmwi8p2-8>li:before{content:"\0025a0  "}.lst-kix_yhmx20qr5k0e-6>li:before{content:"\0025cf  "}.lst-kix_n8ye7s4j9pok-0>li:before{content:"\0025cf  "}.lst-kix_7r3huctovklw-5>li:before{content:"\0025a0  "}.lst-kix_gnzb8ea0t50l-4>li:before{content:"\0025cb  "}.lst-kix_vt0etbgrzlaw-0>li:before{content:"\0025cf  "}.lst-kix_uk8k6pzec0np-6>li:before{content:"\0025cf  "}.lst-kix_qlax8iv93hxg-1>li:before{content:"\0025cb  "}.lst-kix_481c98v2pt83-6>li:before{content:"\0025cf  "}.lst-kix_klzhllb6xcyr-6>li:before{content:"\0025cf  "}.lst-kix_n8ye7s4j9pok-4>li:before{content:"\0025cb  "}ul.lst-kix_ymmukz9l2n7v-5{list-style-type:none}ul.lst-kix_ymmukz9l2n7v-4{list-style-type:none}ul.lst-kix_ymmukz9l2n7v-7{list-style-type:none}ul.lst-kix_ymmukz9l2n7v-6{list-style-type:none}ul.lst-kix_ymmukz9l2n7v-1{list-style-type:none}.lst-kix_ydb53brft35n-0>li:before{content:"\0025cf  "}ul.lst-kix_ymmukz9l2n7v-0{list-style-type:none}ul.lst-kix_ymmukz9l2n7v-3{list-style-type:none}ul.lst-kix_ymmukz9l2n7v-2{list-style-type:none}.lst-kix_juam6n4619t2-2>li:before{content:"\0025a0  "}.lst-kix_gnzb8ea0t50l-8>li:before{content:"\0025a0  "}.lst-kix_d3k7hdh35j7a-5>li:before{content:"\0025a0  "}.lst-kix_17is42bhveph-3>li:before{content:"\0025cf  "}.lst-kix_xqqiyb3b5ag0-8>li:before{content:"\0025a0  "}.lst-kix_5ia1ign2sshz-3>li:before{content:"\0025cf  "}.lst-kix_cy2og2rt03zo-7>li:before{content:"\0025cb  "}.lst-kix_2rp5aiyjkdpd-6>li:before{content:"\0025cf  "}.lst-kix_cy2og2rt03zo-3>li:before{content:"\0025cf  "}.lst-kix_2s6hp75gzisz-2>li:before{content:"\0025a0  "}.lst-kix_ta3ofdl38yyn-6>li:before{content:"\0025cf  "}.lst-kix_60wjig3vq01-3>li:before{content:"\0025cf  "}.lst-kix_gr1y63l14uay-7>li:before{content:"-  "}.lst-kix_60wjig3vq01-7>li:before{content:"\0025cb  "}.lst-kix_r8labycqo9zl-5>li:before{content:"\0025a0  "}.lst-kix_r75b0bpeph0o-6>li:before{content:"\0025cf  "}ul.lst-kix_1ig57j6lcs95-0{list-style-type:none}.lst-kix_7jzvzpyn9bnk-1>li:before{content:"\0025cb  "}ul.lst-kix_1ig57j6lcs95-1{list-style-type:none}ul.lst-kix_1ig57j6lcs95-2{list-style-type:none}ul.lst-kix_7jzvzpyn9bnk-0{list-style-type:none}ul.lst-kix_1ig57j6lcs95-3{list-style-type:none}ul.lst-kix_1ig57j6lcs95-4{list-style-type:none}ul.lst-kix_1ig57j6lcs95-5{list-style-type:none}.lst-kix_vt0etbgrzlaw-4>li:before{content:"\0025cb  "}.lst-kix_p98dexr65abh-2>li:before{content:"\0025a0  "}.lst-kix_yhmx20qr5k0e-2>li:before{content:"\0025a0  "}.lst-kix_3fvzyqsgm5ef-4>li:before{content:"\0025cb  "}ul.lst-kix_cy2og2rt03zo-0{list-style-type:none}.lst-kix_x5jl3p2g0327-1>li:before{content:"\0025cb  "}ul.lst-kix_cy2og2rt03zo-3{list-style-type:none}ul.lst-kix_cy2og2rt03zo-4{list-style-type:none}ul.lst-kix_cy2og2rt03zo-1{list-style-type:none}ul.lst-kix_cy2og2rt03zo-2{list-style-type:none}ul.lst-kix_cy2og2rt03zo-7{list-style-type:none}.lst-kix_2zf5bd1mem1t-2>li:before{content:"\0025a0  "}ul.lst-kix_cy2og2rt03zo-8{list-style-type:none}.lst-kix_8fyyex2aqhl9-7>li:before{content:"\0025cb  "}ul.lst-kix_cy2og2rt03zo-5{list-style-type:none}ul.lst-kix_cy2og2rt03zo-6{list-style-type:none}ul.lst-kix_398os0jr1iw3-2{list-style-type:none}ul.lst-kix_398os0jr1iw3-1{list-style-type:none}ul.lst-kix_398os0jr1iw3-0{list-style-type:none}.lst-kix_kz1sfsbi2eng-7>li:before{content:"\0025cb  "}ul.lst-kix_398os0jr1iw3-6{list-style-type:none}ul.lst-kix_398os0jr1iw3-5{list-style-type:none}ul.lst-kix_398os0jr1iw3-4{list-style-type:none}ul.lst-kix_398os0jr1iw3-3{list-style-type:none}ul.lst-kix_398os0jr1iw3-8{list-style-type:none}ul.lst-kix_398os0jr1iw3-7{list-style-type:none}.lst-kix_g0l7lf51hdyf-2>li:before{content:"\0025a0  "}.lst-kix_369j2eishmb5-8>li:before{content:"\0025a0  "}.lst-kix_8k70e8i8rdlm-1>li:before{content:"\0025cb  "}.lst-kix_bhgkuyt4aopz-8>li:before{content:"\0025a0  "}.lst-kix_3fvzyqsgm5ef-3>li:before{content:"\0025cf  "}.lst-kix_bhgkuyt4aopz-4>li:before{content:"\0025cb  "}.lst-kix_3fvzyqsgm5ef-0>li:before{content:"\0025cf  "}.lst-kix_4fwhm2tiefhz-5>li:before{content:"\0025a0  "}ul.lst-kix_d2ysoruvczg5-8{list-style-type:none}ul.lst-kix_nwi1ck3fibs7-8{list-style-type:none}ul.lst-kix_d2ysoruvczg5-6{list-style-type:none}ul.lst-kix_d2ysoruvczg5-7{list-style-type:none}ul.lst-kix_d2ysoruvczg5-4{list-style-type:none}.lst-kix_pzunwido5p11-5>li:before{content:"\0025a0  "}ul.lst-kix_d2ysoruvczg5-5{list-style-type:none}ul.lst-kix_d2ysoruvczg5-2{list-style-type:none}.lst-kix_vhrrjzyjvp5x-4>li:before{content:"\0025cb  "}ul.lst-kix_d2ysoruvczg5-3{list-style-type:none}ul.lst-kix_d2ysoruvczg5-0{list-style-type:none}.lst-kix_yh6f66826ysk-3>li:before{content:"\0025cf  "}ul.lst-kix_d2ysoruvczg5-1{list-style-type:none}ul.lst-kix_o9z4cf2yq4mz-2{list-style-type:none}ul.lst-kix_o9z4cf2yq4mz-1{list-style-type:none}.lst-kix_z1vxl485jzzo-8>li:before{content:"\0025a0  "}.lst-kix_wq4pisfqksnu-1>li:before{content:"\0025cb  "}ul.lst-kix_o9z4cf2yq4mz-0{list-style-type:none}.lst-kix_spzfuoeekohi-3>li:before{content:"\0025cf  "}.lst-kix_w14ild1qeoey-2>li:before{content:"\0025a0  "}.lst-kix_369j2eishmb5-5>li:before{content:"\0025a0  "}.lst-kix_4fwhm2tiefhz-8>li:before{content:"\0025a0  "}ul.lst-kix_nwi1ck3fibs7-7{list-style-type:none}ul.lst-kix_nwi1ck3fibs7-6{list-style-type:none}ul.lst-kix_nwi1ck3fibs7-5{list-style-type:none}.lst-kix_xqfh3tf7tjc0-0>li:before{content:"\0025cf  "}ul.lst-kix_nwi1ck3fibs7-4{list-style-type:none}ul.lst-kix_nwi1ck3fibs7-3{list-style-type:none}ul.lst-kix_nwi1ck3fibs7-2{list-style-type:none}ul.lst-kix_nwi1ck3fibs7-1{list-style-type:none}.lst-kix_yh6f66826ysk-7>li:before{content:"\0025cb  "}ul.lst-kix_nwi1ck3fibs7-0{list-style-type:none}.lst-kix_swbzestlepme-6>li:before{content:"\0025cf  "}ul.lst-kix_7b2y41o7s2k3-7{list-style-type:none}.lst-kix_xqfh3tf7tjc0-4>li:before{content:"\0025cb  "}ul.lst-kix_7b2y41o7s2k3-8{list-style-type:none}ul.lst-kix_7b2y41o7s2k3-5{list-style-type:none}.lst-kix_2rp5aiyjkdpd-2>li:before{content:"\0025a0  "}ul.lst-kix_7b2y41o7s2k3-6{list-style-type:none}ul.lst-kix_7b2y41o7s2k3-3{list-style-type:none}ul.lst-kix_7b2y41o7s2k3-4{list-style-type:none}.lst-kix_juam6n4619t2-6>li:before{content:"\0025cf  "}.lst-kix_383wi8io125v-8>li:before{content:"\0025a0  "}.lst-kix_swbzestlepme-3>li:before{content:"\0025cf  "}ul.lst-kix_klzhllb6xcyr-5{list-style-type:none}ul.lst-kix_klzhllb6xcyr-4{list-style-type:none}ul.lst-kix_klzhllb6xcyr-3{list-style-type:none}ul.lst-kix_klzhllb6xcyr-2{list-style-type:none}ul.lst-kix_16ygmg7toocv-0{list-style-type:none}ul.lst-kix_klzhllb6xcyr-8{list-style-type:none}ul.lst-kix_16ygmg7toocv-1{list-style-type:none}ul.lst-kix_ymmukz9l2n7v-8{list-style-type:none}ul.lst-kix_klzhllb6xcyr-7{list-style-type:none}ul.lst-kix_16ygmg7toocv-2{list-style-type:none}ul.lst-kix_klzhllb6xcyr-6{list-style-type:none}ul.lst-kix_16ygmg7toocv-3{list-style-type:none}.lst-kix_383wi8io125v-4>li:before{content:"\0025cb  "}.lst-kix_swbzestlepme-2>li:before{content:"\0025a0  "}ul.lst-kix_klzhllb6xcyr-1{list-style-type:none}.lst-kix_vhrrjzyjvp5x-0>li:before{content:"\0025cf  "}ul.lst-kix_klzhllb6xcyr-0{list-style-type:none}ul.lst-kix_p98dexr65abh-0{list-style-type:none}ul.lst-kix_p98dexr65abh-1{list-style-type:none}ul.lst-kix_p98dexr65abh-2{list-style-type:none}ul.lst-kix_p98dexr65abh-3{list-style-type:none}ul.lst-kix_p98dexr65abh-4{list-style-type:none}ul.lst-kix_p98dexr65abh-5{list-style-type:none}ul.lst-kix_p98dexr65abh-6{list-style-type:none}ul.lst-kix_p98dexr65abh-7{list-style-type:none}ul.lst-kix_p98dexr65abh-8{list-style-type:none}ul.lst-kix_16ygmg7toocv-4{list-style-type:none}ul.lst-kix_16ygmg7toocv-5{list-style-type:none}ul.lst-kix_16ygmg7toocv-6{list-style-type:none}ul.lst-kix_16ygmg7toocv-7{list-style-type:none}ul.lst-kix_16ygmg7toocv-8{list-style-type:none}.lst-kix_gnt40hozc3kv-5>li:before{content:"\0025a0  "}ul.lst-kix_24cmoxb3qmwl-4{list-style-type:none}ul.lst-kix_24cmoxb3qmwl-5{list-style-type:none}.lst-kix_1a8ua5u4a26p-3>li:before{content:"\0025cf  "}ul.lst-kix_24cmoxb3qmwl-2{list-style-type:none}ul.lst-kix_24cmoxb3qmwl-3{list-style-type:none}.lst-kix_xgsgxyckqw77-0>li:before{content:"\0025cf  "}ul.lst-kix_24cmoxb3qmwl-8{list-style-type:none}ul.lst-kix_24cmoxb3qmwl-6{list-style-type:none}ul.lst-kix_24cmoxb3qmwl-7{list-style-type:none}.lst-kix_l1rt7mvaz29b-2>li:before{content:"\0025a0  "}ul.lst-kix_7b2y41o7s2k3-1{list-style-type:none}ul.lst-kix_7b2y41o7s2k3-2{list-style-type:none}.lst-kix_xgsgxyckqw77-4>li:before{content:"\0025cb  "}ul.lst-kix_7b2y41o7s2k3-0{list-style-type:none}.lst-kix_ho1wvdl93240-6>li:before{content:"\0025cf  "}ul.lst-kix_24cmoxb3qmwl-0{list-style-type:none}ul.lst-kix_24cmoxb3qmwl-1{list-style-type:none}.lst-kix_gnt40hozc3kv-1>li:before{content:"\0025cb  "}.lst-kix_1a8ua5u4a26p-0>li:before{content:"\0025cf  "}.lst-kix_58o57c52tmut-7>li:before{content:"\0025cb  "}.lst-kix_z1olrl2brpzr-7>li:before{content:"\0025cb  "}.lst-kix_d3k7hdh35j7a-2>li:before{content:"\0025a0  "}.lst-kix_z3si3pbri1de-2>li:before{content:"\0025a0  "}.lst-kix_qlax8iv93hxg-4>li:before{content:"\0025cb  "}.lst-kix_uk8k6pzec0np-3>li:before{content:"\0025cf  "}.lst-kix_yhmx20qr5k0e-3>li:before{content:"\0025cf  "}.lst-kix_yhn6vvmwi8p2-7>li:before{content:"\0025cb  "}.lst-kix_jzarhjey1jrh-6>li:before{content:"\0025cf  "}.lst-kix_n8ye7s4j9pok-7>li:before{content:"\0025cb  "}.lst-kix_k12n8xes8ofj-0>li:before{content:"\0025cf  "}.lst-kix_n8ye7s4j9pok-3>li:before{content:"\0025cf  "}.lst-kix_xqqiyb3b5ag0-5>li:before{content:"\0025a0  "}.lst-kix_z1olrl2brpzr-3>li:before{content:"\0025cf  "}.lst-kix_d3k7hdh35j7a-6>li:before{content:"\0025cf  "}.lst-kix_2rp5aiyjkdpd-5>li:before{content:"\0025a0  "}.lst-kix_cy2og2rt03zo-6>li:before{content:"\0025cf  "}.lst-kix_p7d8xi28lme6-3>li:before{content:"\0025cf  "}.lst-kix_68swn1licdyn-4>li:before{content:"\0025cb  "}.lst-kix_gr1y63l14uay-4>li:before{content:"-  "}.lst-kix_8g8vlw6j0asg-2>li:before{content:"\0025a0  "}ul.lst-kix_o9z4cf2yq4mz-8{list-style-type:none}ul.lst-kix_o9z4cf2yq4mz-7{list-style-type:none}ul.lst-kix_o9z4cf2yq4mz-6{list-style-type:none}ul.lst-kix_o9z4cf2yq4mz-5{list-style-type:none}.lst-kix_solhl6ysg65v-0>li:before{content:"\0025cf  "}ul.lst-kix_o9z4cf2yq4mz-4{list-style-type:none}ul.lst-kix_o9z4cf2yq4mz-3{list-style-type:none}ul.lst-kix_3odao55mezu-1{list-style-type:none}.lst-kix_gr1y63l14uay-8>li:before{content:"-  "}ul.lst-kix_3odao55mezu-0{list-style-type:none}ul.lst-kix_3odao55mezu-3{list-style-type:none}ul.lst-kix_3odao55mezu-2{list-style-type:none}ul.lst-kix_3odao55mezu-5{list-style-type:none}ul.lst-kix_acuhah6xdzsp-8{list-style-type:none}ul.lst-kix_3odao55mezu-4{list-style-type:none}.lst-kix_aui8m1kby916-8>li:before{content:"\0025a0  "}ul.lst-kix_3odao55mezu-7{list-style-type:none}ul.lst-kix_3odao55mezu-6{list-style-type:none}ul.lst-kix_acuhah6xdzsp-4{list-style-type:none}ul.lst-kix_3odao55mezu-8{list-style-type:none}.lst-kix_r8labycqo9zl-6>li:before{content:"\0025cf  "}ul.lst-kix_acuhah6xdzsp-5{list-style-type:none}ul.lst-kix_acuhah6xdzsp-6{list-style-type:none}ul.lst-kix_acuhah6xdzsp-7{list-style-type:none}ul.lst-kix_acuhah6xdzsp-0{list-style-type:none}ul.lst-kix_acuhah6xdzsp-1{list-style-type:none}ul.lst-kix_acuhah6xdzsp-2{list-style-type:none}.lst-kix_60wjig3vq01-6>li:before{content:"\0025cf  "}ul.lst-kix_acuhah6xdzsp-3{list-style-type:none}ul.lst-kix_rwqhz1uwdird-0{list-style-type:none}ul.lst-kix_rwqhz1uwdird-1{list-style-type:none}ul.lst-kix_rwqhz1uwdird-2{list-style-type:none}ul.lst-kix_rwqhz1uwdird-3{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-0{list-style-type:none}ul.lst-kix_rwqhz1uwdird-4{list-style-type:none}.lst-kix_isql72ps5u79-2>li:before{content:"\0025a0  "}.lst-kix_bjneibxfrx3r-2>li:before{content:"\0025a0  "}.lst-kix_j24rsp966ll1-1>li:before{content:"\0025cb  "}.lst-kix_3fvzyqsgm5ef-7>li:before{content:"\0025cb  "}.lst-kix_6jabh0roz5s1-0>li:before{content:"\0025cf  "}.lst-kix_68swn1licdyn-8>li:before{content:"\0025a0  "}.lst-kix_nxvbdas3q36w-1>li:before{content:"\0025cb  "}.lst-kix_nxvbdas3q36w-5>li:before{content:"\0025a0  "}ul.lst-kix_rwqhz1uwdird-5{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-2{list-style-type:none}.lst-kix_qlax8iv93hxg-8>li:before{content:"\0025a0  "}ul.lst-kix_rwqhz1uwdird-6{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-1{list-style-type:none}.lst-kix_iqbmivscsmun-0>li:before{content:"\0025cf  "}ul.lst-kix_rwqhz1uwdird-7{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-4{list-style-type:none}.lst-kix_iigck7esqel5-5>li:before{content:"\0025a0  "}ul.lst-kix_rwqhz1uwdird-8{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-3{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-6{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-5{list-style-type:none}ul.lst-kix_o5m7acjt2ju1-8{list-style-type:none}.lst-kix_j24rsp966ll1-5>li:before{content:"\0025a0  "}ul.lst-kix_o5m7acjt2ju1-7{list-style-type:none}.lst-kix_m46mj27r3fpz-3>li:before{content:"\0025cf  "}.lst-kix_kk8oms3v3iyb-1>li:before{content:"\0025cb  "}.lst-kix_m46mj27r3fpz-6>li:before{content:"\0025cf  "}.lst-kix_rcp8sez4svjd-2>li:before{content:"-  "}.lst-kix_gogm0wicn297-4>li:before{content:"\0025cb  "}.lst-kix_ljjwvb506pet-5>li:before{content:"\0025a0  "}.lst-kix_sxaq135mthse-0>li:before{content:"\0025cf  "}.lst-kix_gogm0wicn297-7>li:before{content:"\0025cb  "}ul.lst-kix_qsdpvqxg1u78-7{list-style-type:none}ul.lst-kix_qsdpvqxg1u78-6{list-style-type:none}ul.lst-kix_qsdpvqxg1u78-5{list-style-type:none}ul.lst-kix_qsdpvqxg1u78-4{list-style-type:none}ul.lst-kix_qsdpvqxg1u78-8{list-style-type:none}.lst-kix_edj3r8hgf2z3-4>li:before{content:"\0025cb  "}.lst-kix_ow9ix2mme95u-1>li:before{content:"\0025cb  "}.lst-kix_didibfga6ee5-8>li:before{content:"\0025a0  "}.lst-kix_xav8ikvidc9s-1>li:before{content:"\0025cb  "}ul.lst-kix_iqbmivscsmun-8{list-style-type:none}ul.lst-kix_iqbmivscsmun-6{list-style-type:none}ul.lst-kix_iqbmivscsmun-7{list-style-type:none}ul.lst-kix_iqbmivscsmun-4{list-style-type:none}ul.lst-kix_iqbmivscsmun-5{list-style-type:none}ul.lst-kix_iqbmivscsmun-2{list-style-type:none}ul.lst-kix_iqbmivscsmun-3{list-style-type:none}ul.lst-kix_iqbmivscsmun-0{list-style-type:none}ul.lst-kix_iqbmivscsmun-1{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-0{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-1{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-2{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-3{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-4{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-5{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-6{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-7{list-style-type:none}ul.lst-kix_z8j4l8h7k6b3-8{list-style-type:none}.lst-kix_5d66udn9quk7-8>li:before{content:"\0025a0  "}.lst-kix_8hjy0hqh3rvg-2>li:before{content:"\0025a0  "}.lst-kix_douubz1tb8an-1>li:before{content:"\0025cb  "}.lst-kix_204pjqaavew-7>li:before{content:"\0025cb  "}.lst-kix_tg2hwffju8t9-0>li:before{content:"\0025cf  "}.lst-kix_nbpnmy8xci8w-4>li:before{content:"\0025cb  "}.lst-kix_a9t5z6hxg4oz-6>li:before{content:"\0025cf  "}.lst-kix_mxwp6np2u5xu-2>li:before{content:"-  "}ul.lst-kix_2mto5juffctz-1{list-style-type:none}.lst-kix_c2vvnsz0drek-6>li:before{content:"\0025cf  "}ul.lst-kix_2mto5juffctz-0{list-style-type:none}.lst-kix_edj3r8hgf2z3-1>li:before{content:"\0025cb  "}.lst-kix_f3egesa43r6b-5>li:before{content:"\0025a0  "}.lst-kix_7b2y41o7s2k3-7>li:before{content:"\0025cb  "}.lst-kix_8hjy0hqh3rvg-5>li:before{content:"\0025a0  "}.lst-kix_mxwp6np2u5xu-5>li:before{content:"-  "}.lst-kix_5kaugn46jpw6-8>li:before{content:"\0025a0  "}.lst-kix_n21asioviqe4-0>li:before{content:"\0025cf  "}.lst-kix_ucprmx270ar2-2>li:before{content:"\0025a0  "}.lst-kix_z3si3pbri1de-5>li:before{content:"\0025a0  "}.lst-kix_8wesot3ko53b-1>li:before{content:"\0025cb  "}.lst-kix_ymmukz9l2n7v-8>li:before{content:"\0025a0  "}ul.lst-kix_vapye4eddf48-0{list-style-type:none}.lst-kix_x5zq7qqqztpc-4>li:before{content:"\0025cb  "}ul.lst-kix_vapye4eddf48-3{list-style-type:none}ul.lst-kix_vapye4eddf48-4{list-style-type:none}ul.lst-kix_vapye4eddf48-1{list-style-type:none}ul.lst-kix_vapye4eddf48-2{list-style-type:none}ul.lst-kix_vapye4eddf48-7{list-style-type:none}.lst-kix_6ykz6yxb3o6x-4>li:before{content:"\0025cb  "}ul.lst-kix_vapye4eddf48-8{list-style-type:none}.lst-kix_l47f7qjlu9cu-0>li:before{content:"\0025cf  "}ul.lst-kix_vapye4eddf48-5{list-style-type:none}ul.lst-kix_vapye4eddf48-6{list-style-type:none}.lst-kix_iiujyt46s4u7-4>li:before{content:"\0025cb  "}.lst-kix_o5m7acjt2ju1-6>li:before{content:"\0025cf  "}.lst-kix_c2vvnsz0drek-3>li:before{content:"\0025cf  "}ul.lst-kix_2mto5juffctz-5{list-style-type:none}ul.lst-kix_2mto5juffctz-4{list-style-type:none}.lst-kix_xx3xz98aq2wq-3>li:before{content:"\0025cf  "}ul.lst-kix_2mto5juffctz-3{list-style-type:none}.lst-kix_204pjqaavew-4>li:before{content:"\0025cb  "}ul.lst-kix_2mto5juffctz-2{list-style-type:none}ul.lst-kix_2mto5juffctz-8{list-style-type:none}ul.lst-kix_2mto5juffctz-7{list-style-type:none}ul.lst-kix_2mto5juffctz-6{list-style-type:none}.lst-kix_f51q5t2m61i8-6>li:before{content:"\0025cf  "}.lst-kix_n1766bho68n4-1>li:before{content:"\0025cb  "}ul.lst-kix_o14fwownm59e-1{list-style-type:none}.lst-kix_bj6mzrhm69bi-8>li:before{content:"\0025a0  "}ul.lst-kix_o14fwownm59e-0{list-style-type:none}.lst-kix_egb7bfw0xx0q-1>li:before{content:"\0025cb  "}.lst-kix_z1olrl2brpzr-0>li:before{content:"\0025cf  "}.lst-kix_4ykl6tjv6vdc-6>li:before{content:"\0025cf  "}ul.lst-kix_o14fwownm59e-5{list-style-type:none}ul.lst-kix_o14fwownm59e-4{list-style-type:none}.lst-kix_62is116n17wq-5>li:before{content:"\0025a0  "}ul.lst-kix_o14fwownm59e-3{list-style-type:none}ul.lst-kix_o14fwownm59e-2{list-style-type:none}.lst-kix_lyxc6qcmvco2-4>li:before{content:"\0025cb  "}ul.lst-kix_o14fwownm59e-8{list-style-type:none}ul.lst-kix_o14fwownm59e-7{list-style-type:none}ul.lst-kix_o14fwownm59e-6{list-style-type:none}.lst-kix_470xpz4q6hoi-0>li:before{content:"\0025cf  "}ul.lst-kix_pzunwido5p11-5{list-style-type:none}.lst-kix_p7d8xi28lme6-6>li:before{content:"\0025cf  "}.lst-kix_3de1jg1kuy24-4>li:before{content:"\0025cb  "}ul.lst-kix_pzunwido5p11-4{list-style-type:none}.lst-kix_3odao55mezu-0>li:before{content:"\0025cf  "}ul.lst-kix_pzunwido5p11-3{list-style-type:none}ul.lst-kix_pzunwido5p11-2{list-style-type:none}.lst-kix_68swn1licdyn-1>li:before{content:"\0025cb  "}ul.lst-kix_pzunwido5p11-1{list-style-type:none}.lst-kix_solhl6ysg65v-3>li:before{content:"\0025cf  "}ul.lst-kix_pzunwido5p11-0{list-style-type:none}.lst-kix_nxvbdas3q36w-8>li:before{content:"\0025a0  "}ul.lst-kix_gg06s1qsfn23-7{list-style-type:none}ul.lst-kix_gg06s1qsfn23-6{list-style-type:none}ul.lst-kix_gg06s1qsfn23-8{list-style-type:none}ul.lst-kix_gg06s1qsfn23-3{list-style-type:none}.lst-kix_b474o1tvhevy-2>li:before{content:"\0025a0  "}ul.lst-kix_gg06s1qsfn23-2{list-style-type:none}.lst-kix_ct57w2de70tb-0>li:before{content:"\0025cf  "}ul.lst-kix_gg06s1qsfn23-5{list-style-type:none}.lst-kix_xav8ikvidc9s-4>li:before{content:"\0025cb  "}ul.lst-kix_gg06s1qsfn23-4{list-style-type:none}ul.lst-kix_kxpudurk2j6e-4{list-style-type:none}ul.lst-kix_kxpudurk2j6e-5{list-style-type:none}ul.lst-kix_kxpudurk2j6e-2{list-style-type:none}ul.lst-kix_gg06s1qsfn23-1{list-style-type:none}ul.lst-kix_kxpudurk2j6e-3{list-style-type:none}.lst-kix_crb89j7xw8q2-2>li:before{content:"\0025a0  "}ul.lst-kix_gg06s1qsfn23-0{list-style-type:none}ul.lst-kix_kxpudurk2j6e-0{list-style-type:none}.lst-kix_o4ah8y8am4ct-8>li:before{content:"\0025a0  "}ul.lst-kix_kxpudurk2j6e-1{list-style-type:none}.lst-kix_5x48r7dz2gvp-3>li:before{content:"\0025cf  "}ul.lst-kix_kxpudurk2j6e-8{list-style-type:none}.lst-kix_aui8m1kby916-5>li:before{content:"\0025a0  "}ul.lst-kix_kxpudurk2j6e-6{list-style-type:none}ul.lst-kix_kxpudurk2j6e-7{list-style-type:none}.lst-kix_bq5ybp1zal0b-0>li:before{content:"\0025cf  "}ul.lst-kix_f51q5t2m61i8-2{list-style-type:none}.lst-kix_24cmoxb3qmwl-2>li:before{content:"\0025a0  "}ul.lst-kix_f51q5t2m61i8-3{list-style-type:none}ul.lst-kix_f51q5t2m61i8-4{list-style-type:none}.lst-kix_2jmrp4ochkmc-8>li:before{content:"\0025a0  "}ul.lst-kix_f51q5t2m61i8-5{list-style-type:none}.lst-kix_isql72ps5u79-5>li:before{content:"\0025a0  "}ul.lst-kix_f51q5t2m61i8-0{list-style-type:none}.lst-kix_o4d3wx7elf58-3>li:before{content:"\0025cf  "}ul.lst-kix_f51q5t2m61i8-1{list-style-type:none}.lst-kix_d995nc7613a9-1>li:before{content:"\0025cb  "}.lst-kix_mh0uhwysa91-7>li:before{content:"\0025cb  "}.lst-kix_j24rsp966ll1-8>li:before{content:"\0025a0  "}ul.lst-kix_f51q5t2m61i8-6{list-style-type:none}ul.lst-kix_f51q5t2m61i8-7{list-style-type:none}ul.lst-kix_f51q5t2m61i8-8{list-style-type:none}.lst-kix_6jabh0roz5s1-3>li:before{content:"\0025cf  "}ul.lst-kix_pzunwido5p11-8{list-style-type:none}.lst-kix_41ikkbtdotge-5>li:before{content:"\0025a0  "}ul.lst-kix_pzunwido5p11-7{list-style-type:none}.lst-kix_emy9ap8zeql8-2>li:before{content:"\0025a0  "}ul.lst-kix_pzunwido5p11-6{list-style-type:none}.lst-kix_yv7c7gf6elha-6>li:before{content:"\0025cf  "}.lst-kix_emewkhtp1rfy-7>li:before{content:"\0025cb  "}.lst-kix_894copnhklgz-1>li:before{content:"\0025cb  "}.lst-kix_16ygmg7toocv-1>li:before{content:"\0025cb  "}.lst-kix_2nh9kwfbzbik-6>li:before{content:"\0025cf  "}.lst-kix_l3zoug7b3vcw-0>li:before{content:"\0025cf  "}.lst-kix_gnt40hozc3kv-8>li:before{content:"\0025a0  "}.lst-kix_16ygmg7toocv-4>li:before{content:"\0025cb  "}.lst-kix_8cma8sbxfy2x-4>li:before{content:"\0025cb  "}.lst-kix_jys0trw4gd1m-7>li:before{content:"\0025cb  "}.lst-kix_u1clvxqptoo-8>li:before{content:"\0025a0  "}.lst-kix_xrce7kijh7l1-2>li:before{content:"\0025a0  "}.lst-kix_6k5yl8dbqqq2-2>li:before{content:"\0025a0  "}.lst-kix_jhqr8d48vqqp-6>li:before{content:"\0025cf  "}.lst-kix_riff65m65tnf-5>li:before{content:"\0025a0  "}.lst-kix_bhgkuyt4aopz-1>li:before{content:"\0025cb  "}.lst-kix_9cwmoyhkmhfb-1>li:before{content:"\0025cb  "}.lst-kix_9cwmoyhkmhfb-4>li:before{content:"\0025cb  "}.lst-kix_8swzovi1ytnr-7>li:before{content:"\0025cb  "}.lst-kix_vapye4eddf48-8>li:before{content:"\0025a0  "}.lst-kix_vhrrjzyjvp5x-7>li:before{content:"\0025cb  "}.lst-kix_z1vxl485jzzo-5>li:before{content:"\0025a0  "}.lst-kix_74gx9bwnn8kt-3>li:before{content:"\0025cf  "}.lst-kix_pzunwido5p11-2>li:before{content:"\0025a0  "}.lst-kix_jys0trw4gd1m-4>li:before{content:"\0025cb  "}.lst-kix_4k4id0a37km3-1>li:before{content:"\0025cb  "}.lst-kix_z4z7nvld905a-5>li:before{content:"\0025a0  "}.lst-kix_4k4id0a37km3-4>li:before{content:"\0025cb  "}.lst-kix_xgsgxyckqw77-7>li:before{content:"\0025cb  "}ul.lst-kix_nc0l9nyccr86-0{list-style-type:none}.lst-kix_d1iyykra6bco-2>li:before{content:"\0025a0  "}ul.lst-kix_nc0l9nyccr86-1{list-style-type:none}ul.lst-kix_nc0l9nyccr86-2{list-style-type:none}ul.lst-kix_nc0l9nyccr86-3{list-style-type:none}.lst-kix_ehe7zglmx7i2-0>li:before{content:"\0025cf  "}.lst-kix_6yojqqjec0st-3>li:before{content:"\0025cf  "}.lst-kix_dj4fsxw52t4l-5>li:before{content:"\0025a0  "}ul.lst-kix_nc0l9nyccr86-8{list-style-type:none}.lst-kix_s6xk062mdkv5-7>li:before{content:"\0025cb  "}.lst-kix_6yojqqjec0st-0>li:before{content:"\0025cf  "}ul.lst-kix_nc0l9nyccr86-4{list-style-type:none}ul.lst-kix_nc0l9nyccr86-5{list-style-type:none}ul.lst-kix_nc0l9nyccr86-6{list-style-type:none}ul.lst-kix_nc0l9nyccr86-7{list-style-type:none}ul.lst-kix_u9ija913pfxg-1{list-style-type:none}ul.lst-kix_u9ija913pfxg-2{list-style-type:none}ul.lst-kix_u9ija913pfxg-3{list-style-type:none}ul.lst-kix_u9ija913pfxg-4{list-style-type:none}.lst-kix_d8ik6bmbwuj4-1>li:before{content:"\0025cb  "}.lst-kix_d4lbyvb2g3j-1>li:before{content:"\0025cb  "}ul.lst-kix_u9ija913pfxg-0{list-style-type:none}.lst-kix_d2ysoruvczg5-7>li:before{content:"\0025cb  "}ul.lst-kix_u9ija913pfxg-5{list-style-type:none}.lst-kix_8z696e8bv17-0>li:before{content:"\0025cf  "}ul.lst-kix_u9ija913pfxg-6{list-style-type:none}ul.lst-kix_u9ija913pfxg-7{list-style-type:none}ul.lst-kix_u9ija913pfxg-8{list-style-type:none}.lst-kix_6ykz6yxb3o6x-7>li:before{content:"\0025cb  "}.lst-kix_gc2knt874en1-2>li:before{content:"\0025a0  "}.lst-kix_ulntv5wergyk-6>li:before{content:"\0025cf  "}.lst-kix_7r3huctovklw-8>li:before{content:"\0025a0  "}.lst-kix_pmg6qjdagav9-7>li:before{content:"\0025cb  "}.lst-kix_lyxc6qcmvco2-1>li:before{content:"\0025cb  "}.lst-kix_crt5jd7c2xq0-7>li:before{content:"\0025cb  "}.lst-kix_o38l9c2rqk2p-5>li:before{content:"\0025a0  "}.lst-kix_w1d9l4m1yg5x-7>li:before{content:"\0025cb  "}.lst-kix_mh0uhwysa91-4>li:before{content:"\0025cb  "}.lst-kix_x5zq7qqqztpc-1>li:before{content:"\0025cb  "}.lst-kix_ydb53brft35n-3>li:before{content:"\0025cf  "}.lst-kix_i1kls5tk4gt6-7>li:before{content:"\0025cb  "}.lst-kix_tlb25jkk5mkd-4>li:before{content:"\0025cb  "}.lst-kix_xx3xz98aq2wq-6>li:before{content:"\0025cf  "}.lst-kix_klzhllb6xcyr-3>li:before{content:"\0025cf  "}.lst-kix_a9t5z6hxg4oz-3>li:before{content:"\0025cf  "}.lst-kix_62is116n17wq-2>li:before{content:"\0025a0  "}.lst-kix_8wesot3ko53b-4>li:before{content:"\0025cb  "}.lst-kix_wdjiiwxyq2d1-7>li:before{content:"\0025cb  "}.lst-kix_xkjgsygnpr2m-6>li:before{content:"\0025cf  "}.lst-kix_iiujyt46s4u7-7>li:before{content:"\0025cb  "}ul.lst-kix_m8diz2yltkag-3{list-style-type:none}.lst-kix_9khvkfghhb3b-1>li:before{content:"\0025cb  "}ul.lst-kix_m8diz2yltkag-4{list-style-type:none}ul.lst-kix_m8diz2yltkag-1{list-style-type:none}ul.lst-kix_m8diz2yltkag-2{list-style-type:none}ul.lst-kix_m8diz2yltkag-7{list-style-type:none}ul.lst-kix_m8diz2yltkag-8{list-style-type:none}ul.lst-kix_m8diz2yltkag-5{list-style-type:none}ul.lst-kix_m8diz2yltkag-6{list-style-type:none}.lst-kix_41ikkbtdotge-8>li:before{content:"\0025a0  "}.lst-kix_3de1jg1kuy24-7>li:before{content:"\0025cb  "}.lst-kix_o4ah8y8am4ct-5>li:before{content:"\0025a0  "}ul.lst-kix_m8diz2yltkag-0{list-style-type:none}.lst-kix_5x48r7dz2gvp-0>li:before{content:"\0025cf  "}ul.lst-kix_gwuatnbmuewh-3{list-style-type:none}ul.lst-kix_gwuatnbmuewh-4{list-style-type:none}ul.lst-kix_gwuatnbmuewh-5{list-style-type:none}ul.lst-kix_gwuatnbmuewh-6{list-style-type:none}.lst-kix_313q3f5x4p2m-0>li:before{content:"\0025cf  "}ul.lst-kix_gwuatnbmuewh-0{list-style-type:none}ul.lst-kix_gwuatnbmuewh-1{list-style-type:none}.lst-kix_7q22x2z5v5y-6>li:before{content:"\0025cf  "}ul.lst-kix_gwuatnbmuewh-2{list-style-type:none}ul.lst-kix_gwuatnbmuewh-7{list-style-type:none}.lst-kix_ta3ofdl38yyn-3>li:before{content:"\0025cf  "}ul.lst-kix_gwuatnbmuewh-8{list-style-type:none}.lst-kix_kk7o9gi3ij0l-3>li:before{content:"\0025cf  "}.lst-kix_4yao0efkrd31-0>li:before{content:"\0025cf  "}.lst-kix_2zf5bd1mem1t-5>li:before{content:"\0025a0  "}.lst-kix_p2mkzdwo7q6r-2>li:before{content:"\0025a0  "}.lst-kix_kxpudurk2j6e-7>li:before{content:"\0025cb  "}.lst-kix_8cma8sbxfy2x-1>li:before{content:"\0025cb  "}.lst-kix_pjyqz9f44g8s-2>li:before{content:"\0025a0  "}.lst-kix_vt0etbgrzlaw-7>li:before{content:"\0025cb  "}.lst-kix_qp3zt329w8t5-3>li:before{content:"\0025cf  "}.lst-kix_bj2se7l3xxv-8>li:before{content:"\0025a0  "}ul.lst-kix_gc2knt874en1-3{list-style-type:none}ul.lst-kix_gc2knt874en1-4{list-style-type:none}.lst-kix_hl5oa0f6tf4f-8>li:before{content:"\0025a0  "}ul.lst-kix_gc2knt874en1-5{list-style-type:none}ul.lst-kix_gc2knt874en1-6{list-style-type:none}ul.lst-kix_gc2knt874en1-7{list-style-type:none}.lst-kix_ltwr0s349me7-4>li:before{content:"\0025cb  "}ul.lst-kix_gc2knt874en1-8{list-style-type:none}.lst-kix_ltwr0s349me7-6>li:before{content:"\0025cf  "}ul.lst-kix_gc2knt874en1-0{list-style-type:none}ul.lst-kix_gc2knt874en1-1{list-style-type:none}ul.lst-kix_gc2knt874en1-2{list-style-type:none}.lst-kix_bj2se7l3xxv-0>li:before{content:"\0025cf  "}.lst-kix_dowwk88w0dao-1>li:before{content:"\0025cb  "}.lst-kix_1qjopwyk3n75-2>li:before{content:"\0025a0  "}ul.lst-kix_aui8m1kby916-1{list-style-type:none}ul.lst-kix_aui8m1kby916-0{list-style-type:none}ul.lst-kix_rr7hjjtid2vg-1{list-style-type:none}ul.lst-kix_aui8m1kby916-5{list-style-type:none}ul.lst-kix_rr7hjjtid2vg-0{list-style-type:none}ul.lst-kix_aui8m1kby916-4{list-style-type:none}ul.lst-kix_rr7hjjtid2vg-3{list-style-type:none}ul.lst-kix_aui8m1kby916-3{list-style-type:none}ul.lst-kix_rr7hjjtid2vg-2{list-style-type:none}.lst-kix_acuhah6xdzsp-0>li:before{content:"\0025cf  "}ul.lst-kix_aui8m1kby916-2{list-style-type:none}.lst-kix_gsefpdxx2m31-4>li:before{content:"\0025cb  "}ul.lst-kix_rr7hjjtid2vg-5{list-style-type:none}ul.lst-kix_r75b0bpeph0o-7{list-style-type:none}ul.lst-kix_rr7hjjtid2vg-4{list-style-type:none}ul.lst-kix_aui8m1kby916-8{list-style-type:none}ul.lst-kix_r75b0bpeph0o-8{list-style-type:none}.lst-kix_axhbb6oyblrk-7>li:before{content:"\0025cb  "}ul.lst-kix_rr7hjjtid2vg-7{list-style-type:none}ul.lst-kix_aui8m1kby916-7{list-style-type:none}ul.lst-kix_r75b0bpeph0o-5{list-style-type:none}ul.lst-kix_rr7hjjtid2vg-6{list-style-type:none}ul.lst-kix_aui8m1kby916-6{list-style-type:none}.lst-kix_h4ky6k6cvoue-1>li:before{content:"\0025cb  "}ul.lst-kix_r75b0bpeph0o-6{list-style-type:none}ul.lst-kix_r75b0bpeph0o-3{list-style-type:none}ul.lst-kix_rr7hjjtid2vg-8{list-style-type:none}ul.lst-kix_r75b0bpeph0o-4{list-style-type:none}.lst-kix_hl5oa0f6tf4f-2>li:before{content:"\0025a0  "}ul.lst-kix_r75b0bpeph0o-1{list-style-type:none}.lst-kix_acuhah6xdzsp-2>li:before{content:"\0025a0  "}ul.lst-kix_r75b0bpeph0o-2{list-style-type:none}.lst-kix_gsefpdxx2m31-2>li:before{content:"\0025a0  "}ul.lst-kix_68swn1licdyn-2{list-style-type:none}ul.lst-kix_68swn1licdyn-1{list-style-type:none}ul.lst-kix_r75b0bpeph0o-0{list-style-type:none}ul.lst-kix_68swn1licdyn-4{list-style-type:none}ul.lst-kix_68swn1licdyn-3{list-style-type:none}ul.lst-kix_68swn1licdyn-0{list-style-type:none}.lst-kix_acuhah6xdzsp-8>li:before{content:"\0025a0  "}.lst-kix_bj2se7l3xxv-6>li:before{content:"\0025cf  "}.lst-kix_uest4hng52tf-4>li:before{content:"\0025cb  "}.lst-kix_pfssn45qm13m-2>li:before{content:"\0025a0  "}.lst-kix_2mto5juffctz-2>li:before{content:"\0025a0  "}.lst-kix_7qzrf8aug8gu-5>li:before{content:"\0025a0  "}.lst-kix_uest4hng52tf-0>li:before{content:"\0025cf  "}.lst-kix_uest4hng52tf-6>li:before{content:"\0025cf  "}.lst-kix_h4ky6k6cvoue-7>li:before{content:"\0025cb  "}ul.lst-kix_vrgigcglf3we-8{list-style-type:none}.lst-kix_qwp9c5bklrce-0>li:before{content:"\0025cf  "}ul.lst-kix_vrgigcglf3we-7{list-style-type:none}.lst-kix_pfssn45qm13m-8>li:before{content:"\0025a0  "}.lst-kix_qwp9c5bklrce-2>li:before{content:"\0025a0  "}.lst-kix_axhbb6oyblrk-1>li:before{content:"\0025cb  "}.lst-kix_axhbb6oyblrk-3>li:before{content:"\0025cf  "}.lst-kix_2mto5juffctz-0>li:before{content:"\0025cf  "}.lst-kix_qhrkxnqxxds4-4>li:before{content:"\0025cb  "}.lst-kix_qhrkxnqxxds4-6>li:before{content:"\0025cf  "}.lst-kix_uz5smpj76010-4>li:before{content:"\0025cb  "}.lst-kix_gg06s1qsfn23-1>li:before{content:"\0025cb  "}.lst-kix_3xkj88gevxle-2>li:before{content:"\0025a0  "}.lst-kix_qhrkxnqxxds4-0>li:before{content:"\0025cf  "}.lst-kix_qwp9c5bklrce-6>li:before{content:"\0025cf  "}.lst-kix_gg06s1qsfn23-3>li:before{content:"\0025cf  "}.lst-kix_qwp9c5bklrce-8>li:before{content:"\0025a0  "}.lst-kix_3fnpvvr0i8os-7>li:before{content:"\0025cb  "}ul.lst-kix_vrgigcglf3we-4{list-style-type:none}ul.lst-kix_vrgigcglf3we-3{list-style-type:none}.lst-kix_gg06s1qsfn23-7>li:before{content:"\0025cb  "}ul.lst-kix_vrgigcglf3we-6{list-style-type:none}ul.lst-kix_vrgigcglf3we-5{list-style-type:none}ul.lst-kix_vrgigcglf3we-0{list-style-type:none}ul.lst-kix_vrgigcglf3we-2{list-style-type:none}.lst-kix_2mto5juffctz-6>li:before{content:"\0025cf  "}.lst-kix_3fnpvvr0i8os-1>li:before{content:"\0025cb  "}ul.lst-kix_vrgigcglf3we-1{list-style-type:none}.lst-kix_rbhmpzgzmhdz-4>li:before{content:"\0025cb  "}.lst-kix_2mto5juffctz-8>li:before{content:"\0025a0  "}.lst-kix_3fnpvvr0i8os-3>li:before{content:"\0025cf  "}.lst-kix_3xkj88gevxle-4>li:before{content:"\0025cb  "}ul.lst-kix_tltq6x9aaf31-3{list-style-type:none}ul.lst-kix_tltq6x9aaf31-4{list-style-type:none}ul.lst-kix_tltq6x9aaf31-5{list-style-type:none}ul.lst-kix_tltq6x9aaf31-6{list-style-type:none}ul.lst-kix_tltq6x9aaf31-0{list-style-type:none}ul.lst-kix_tltq6x9aaf31-1{list-style-type:none}ul.lst-kix_tltq6x9aaf31-2{list-style-type:none}.lst-kix_z1voh5a7dove-3>li:before{content:"\0025cf  "}ul.lst-kix_tltq6x9aaf31-7{list-style-type:none}ul.lst-kix_tltq6x9aaf31-8{list-style-type:none}.lst-kix_z1voh5a7dove-5>li:before{content:"\0025a0  "}.lst-kix_xgfpveff5zr0-8>li:before{content:"\0025a0  "}.lst-kix_gpgigcfykzff-6>li:before{content:"\0025cf  "}.lst-kix_pjyqz9f44g8s-7>li:before{content:"\0025cb  "}.lst-kix_tglcy61nnbu3-0>li:before{content:"\0025cf  "}.lst-kix_8nsyajlscxcg-4>li:before{content:"\0025cb  "}.lst-kix_uz5smpj76010-2>li:before{content:"\0025a0  "}.lst-kix_79uewz4g8df3-4>li:before{content:"\0025cb  "}.lst-kix_tlb25jkk5mkd-5>li:before{content:"\0025a0  "}.lst-kix_tlb25jkk5mkd-7>li:before{content:"\0025cb  "}.lst-kix_gpgigcfykzff-8>li:before{content:"\0025a0  "}.lst-kix_tltq6x9aaf31-7>li:before{content:"\0025cb  "}.lst-kix_7v1j337157vp-2>li:before{content:"\0025a0  "}.lst-kix_7v1j337157vp-4>li:before{content:"\0025cb  "}.lst-kix_1ig57j6lcs95-3>li:before{content:"\0025cf  "}.lst-kix_1ig57j6lcs95-7>li:before{content:"\0025cb  "}.lst-kix_1ig57j6lcs95-1>li:before{content:"\0025cb  "}.lst-kix_tltq6x9aaf31-1>li:before{content:"\0025cb  "}.lst-kix_tltq6x9aaf31-3>li:before{content:"\0025cf  "}.lst-kix_8swzovi1ytnr-0>li:before{content:"\0025cf  "}.lst-kix_gpgigcfykzff-0>li:before{content:"\0025cf  "}.lst-kix_tglcy61nnbu3-6>li:before{content:"\0025cf  "}.lst-kix_gpgigcfykzff-2>li:before{content:"\0025a0  "}.lst-kix_b07y002136mr-7>li:before{content:"\0025cb  "}.lst-kix_vzf3xt3x8pov-0>li:before{content:"\0025cf  "}.lst-kix_u1clvxqptoo-5>li:before{content:"\0025a0  "}.lst-kix_vebf56ofbjcp-4>li:before{content:"\0025cb  "}.lst-kix_8cma8sbxfy2x-0>li:before{content:"\0025cf  "}.lst-kix_pjyqz9f44g8s-1>li:before{content:"\0025cb  "}.lst-kix_vzf3xt3x8pov-8>li:before{content:"\0025a0  "}.lst-kix_u1clvxqptoo-1>li:before{content:"\0025cb  "}.lst-kix_vzf3xt3x8pov-6>li:before{content:"\0025cf  "}ul.lst-kix_jzarhjey1jrh-6{list-style-type:none}ul.lst-kix_jzarhjey1jrh-5{list-style-type:none}.lst-kix_mgyoyquoxrm1-5>li:before{content:"\0025a0  "}.lst-kix_emewkhtp1rfy-8>li:before{content:"\0025a0  "}ul.lst-kix_jzarhjey1jrh-4{list-style-type:none}ul.lst-kix_jzarhjey1jrh-3{list-style-type:none}.lst-kix_nc0l9nyccr86-5>li:before{content:"\0025a0  "}.lst-kix_yv7c7gf6elha-8>li:before{content:"\0025a0  "}ul.lst-kix_jzarhjey1jrh-8{list-style-type:none}ul.lst-kix_jzarhjey1jrh-7{list-style-type:none}.lst-kix_2nh9kwfbzbik-4>li:before{content:"\0025cb  "}.lst-kix_h99428b6laza-7>li:before{content:"\0025cb  "}ul.lst-kix_jzarhjey1jrh-2{list-style-type:none}ul.lst-kix_jzarhjey1jrh-1{list-style-type:none}.lst-kix_fjl32nwuhu4u-4>li:before{content:"\0025cb  "}ul.lst-kix_jzarhjey1jrh-0{list-style-type:none}.lst-kix_kxfg6pdeancy-8>li:before{content:"\0025a0  "}.lst-kix_kkphisidn6cz-2>li:before{content:"\0025a0  "}.lst-kix_jhqr8d48vqqp-7>li:before{content:"\0025cb  "}.lst-kix_yv7c7gf6elha-0>li:before{content:"\0025cf  "}.lst-kix_16ygmg7toocv-3>li:before{content:"\0025cf  "}.lst-kix_8cma8sbxfy2x-6>li:before{content:"\0025cf  "}.lst-kix_fjl32nwuhu4u-8>li:before{content:"\0025a0  "}.lst-kix_xrce7kijh7l1-4>li:before{content:"\0025cb  "}.lst-kix_9cwmoyhkmhfb-6>li:before{content:"\0025cf  "}.lst-kix_8swzovi1ytnr-4>li:before{content:"\0025cb  "}.lst-kix_894copnhklgz-6>li:before{content:"\0025cf  "}.lst-kix_2jmrp4ochkmc-0>li:before{content:"\0025cf  "}.lst-kix_scilllbfudea-6>li:before{content:"\0025cf  "}.lst-kix_jys0trw4gd1m-2>li:before{content:"\0025a0  "}.lst-kix_74gx9bwnn8kt-5>li:before{content:"\0025a0  "}.lst-kix_ex6gjyozs00-0>li:before{content:"\0025cf  "}.lst-kix_dj4fsxw52t4l-0>li:before{content:"\0025cf  "}.lst-kix_z4z7nvld905a-3>li:before{content:"\0025cf  "}.lst-kix_ut5bvwbkq6rt-7>li:before{content:"\0025cb  "}.lst-kix_2ihanzwz4fgf-7>li:before{content:"\0025cb  "}.lst-kix_vapye4eddf48-5>li:before{content:"\0025a0  "}.lst-kix_dj4fsxw52t4l-8>li:before{content:"\0025a0  "}.lst-kix_d1iyykra6bco-1>li:before{content:"\0025cb  "}ul.lst-kix_h4ky6k6cvoue-8{list-style-type:none}ul.lst-kix_h4ky6k6cvoue-7{list-style-type:none}ul.lst-kix_h4ky6k6cvoue-6{list-style-type:none}.lst-kix_nbuhcgd4ft21-3>li:before{content:"\0025cf  "}ul.lst-kix_h4ky6k6cvoue-5{list-style-type:none}.lst-kix_riff65m65tnf-3>li:before{content:"\0025cf  "}ul.lst-kix_h4ky6k6cvoue-4{list-style-type:none}.lst-kix_vapye4eddf48-1>li:before{content:"\0025cb  "}ul.lst-kix_h4ky6k6cvoue-3{list-style-type:none}ul.lst-kix_h4ky6k6cvoue-2{list-style-type:none}ul.lst-kix_h4ky6k6cvoue-1{list-style-type:none}ul.lst-kix_h4ky6k6cvoue-0{list-style-type:none}ul.lst-kix_3fvzyqsgm5ef-7{list-style-type:none}ul.lst-kix_3fvzyqsgm5ef-8{list-style-type:none}.lst-kix_z8j4l8h7k6b3-1>li:before{content:"\0025cb  "}.lst-kix_xx3xz98aq2wq-8>li:before{content:"\0025a0  "}ul.lst-kix_3fvzyqsgm5ef-5{list-style-type:none}ul.lst-kix_3fvzyqsgm5ef-6{list-style-type:none}.lst-kix_s6xk062mdkv5-1>li:before{content:"\0025cb  "}.lst-kix_rwfcya85pd71-4>li:before{content:"\0025cb  "}ul.lst-kix_m46mj27r3fpz-7{list-style-type:none}.lst-kix_d4lbyvb2g3j-8>li:before{content:"\0025a0  "}ul.lst-kix_m46mj27r3fpz-8{list-style-type:none}ul.lst-kix_m46mj27r3fpz-3{list-style-type:none}ul.lst-kix_m46mj27r3fpz-4{list-style-type:none}ul.lst-kix_m46mj27r3fpz-5{list-style-type:none}ul.lst-kix_m46mj27r3fpz-6{list-style-type:none}.lst-kix_d4lbyvb2g3j-4>li:before{content:"\0025cb  "}.lst-kix_bcdpfzfy81p5-3>li:before{content:"\0025cf  "}.lst-kix_bcdpfzfy81p5-7>li:before{content:"\0025cb  "}ul.lst-kix_3fvzyqsgm5ef-0{list-style-type:none}.lst-kix_d8ik6bmbwuj4-3>li:before{content:"\0025cf  "}ul.lst-kix_3fvzyqsgm5ef-3{list-style-type:none}ul.lst-kix_3fvzyqsgm5ef-4{list-style-type:none}ul.lst-kix_3fvzyqsgm5ef-1{list-style-type:none}.lst-kix_8z696e8bv17-1>li:before{content:"\0025cb  "}ul.lst-kix_3fvzyqsgm5ef-2{list-style-type:none}.lst-kix_d2ysoruvczg5-5>li:before{content:"\0025a0  "}.lst-kix_4k4id0a37km3-6>li:before{content:"\0025cf  "}.lst-kix_xgfpveff5zr0-2>li:before{content:"\0025a0  "}.lst-kix_ucprmx270ar2-1>li:before{content:"\0025cb  "}.lst-kix_pmg6qjdagav9-5>li:before{content:"\0025a0  "}.lst-kix_gc2knt874en1-4>li:before{content:"\0025cb  "}.lst-kix_rwqhz1uwdird-2>li:before{content:"\0025a0  "}.lst-kix_wdjiiwxyq2d1-5>li:before{content:"\0025a0  "}ul.lst-kix_m46mj27r3fpz-0{list-style-type:none}ul.lst-kix_m46mj27r3fpz-1{list-style-type:none}ul.lst-kix_m46mj27r3fpz-2{list-style-type:none}ul.lst-kix_crt5jd7c2xq0-3{list-style-type:none}.lst-kix_iiujyt46s4u7-1>li:before{content:"\0025cb  "}ul.lst-kix_crt5jd7c2xq0-2{list-style-type:none}.lst-kix_o5m7acjt2ju1-3>li:before{content:"\0025cf  "}ul.lst-kix_crt5jd7c2xq0-5{list-style-type:none}ul.lst-kix_crt5jd7c2xq0-4{list-style-type:none}ul.lst-kix_crt5jd7c2xq0-1{list-style-type:none}ul.lst-kix_crt5jd7c2xq0-0{list-style-type:none}.lst-kix_qsdpvqxg1u78-5>li:before{content:"\0025a0  "}.lst-kix_p0fqpwscumob-4>li:before{content:"\0025cb  "}.lst-kix_204pjqaavew-1>li:before{content:"\0025cb  "}.lst-kix_p0fqpwscumob-8>li:before{content:"\0025a0  "}.lst-kix_4ykl6tjv6vdc-7>li:before{content:"\0025cb  "}.lst-kix_xx3xz98aq2wq-0>li:before{content:"\0025cf  "}.lst-kix_gwuatnbmuewh-7>li:before{content:"\0025cb  "}.lst-kix_e2bt8y2ywvrz-2>li:before{content:"\0025a0  "}.lst-kix_rwqhz1uwdird-6>li:before{content:"\0025cf  "}.lst-kix_ulntv5wergyk-4>li:before{content:"\0025cb  "}.lst-kix_ftgmvnn66fx0-2>li:before{content:"\0025a0  "}.lst-kix_o4ah8y8am4ct-3>li:before{content:"\0025cf  "}.lst-kix_mm2j3zumujcl-5>li:before{content:"\0025a0  "}.lst-kix_ow9ix2mme95u-6>li:before{content:"\0025cf  "}.lst-kix_474t6llskbh7-1>li:before{content:"\0025cb  "}ul.lst-kix_gswyhyotsm36-8{list-style-type:none}.lst-kix_crb89j7xw8q2-3>li:before{content:"\0025cf  "}ul.lst-kix_gswyhyotsm36-5{list-style-type:none}ul.lst-kix_gswyhyotsm36-4{list-style-type:none}ul.lst-kix_gswyhyotsm36-7{list-style-type:none}.lst-kix_vrgigcglf3we-1>li:before{content:"\0025cb  "}ul.lst-kix_gswyhyotsm36-6{list-style-type:none}ul.lst-kix_gswyhyotsm36-1{list-style-type:none}ul.lst-kix_gswyhyotsm36-0{list-style-type:none}ul.lst-kix_gswyhyotsm36-3{list-style-type:none}ul.lst-kix_gswyhyotsm36-2{list-style-type:none}.lst-kix_7q22x2z5v5y-4>li:before{content:"\0025cb  "}.lst-kix_kk7o9gi3ij0l-5>li:before{content:"\0025a0  "}.lst-kix_u6q708mtjbvx-0>li:before{content:"-  "}ul.lst-kix_crt5jd7c2xq0-7{list-style-type:none}ul.lst-kix_crt5jd7c2xq0-6{list-style-type:none}.lst-kix_1qjopwyk3n75-8>li:before{content:"\0025a0  "}ul.lst-kix_crt5jd7c2xq0-8{list-style-type:none}.lst-kix_o4d3wx7elf58-6>li:before{content:"\0025cf  "}.lst-kix_dowwk88w0dao-7>li:before{content:"\0025cb  "}.lst-kix_9khvkfghhb3b-7>li:before{content:"\0025cb  "}.lst-kix_4yao0efkrd31-2>li:before{content:"\0025a0  "}.lst-kix_24cmoxb3qmwl-5>li:before{content:"\0025a0  "}ul.lst-kix_tglcy61nnbu3-2{list-style-type:none}ul.lst-kix_tglcy61nnbu3-1{list-style-type:none}ul.lst-kix_tglcy61nnbu3-0{list-style-type:none}.lst-kix_rcp8sez4svjd-1>li:before{content:"-  "}.lst-kix_m46mj27r3fpz-4>li:before{content:"\0025cb  "}ul.lst-kix_tglcy61nnbu3-8{list-style-type:none}ul.lst-kix_tglcy61nnbu3-7{list-style-type:none}ul.lst-kix_tglcy61nnbu3-6{list-style-type:none}.lst-kix_kk8oms3v3iyb-3>li:before{content:"\0025cf  "}ul.lst-kix_tglcy61nnbu3-5{list-style-type:none}ul.lst-kix_tglcy61nnbu3-4{list-style-type:none}ul.lst-kix_tglcy61nnbu3-3{list-style-type:none}.lst-kix_m8diz2yltkag-8>li:before{content:"\0025a0  "}.lst-kix_dq6vem8xoqp9-5>li:before{content:"\0025a0  "}ul.lst-kix_d4lbyvb2g3j-1{list-style-type:none}.lst-kix_u9ija913pfxg-4>li:before{content:"-  "}ul.lst-kix_d4lbyvb2g3j-0{list-style-type:none}ul.lst-kix_d4lbyvb2g3j-5{list-style-type:none}.lst-kix_u9ija913pfxg-2>li:before{content:"-  "}ul.lst-kix_d4lbyvb2g3j-4{list-style-type:none}ul.lst-kix_d4lbyvb2g3j-3{list-style-type:none}ul.lst-kix_d4lbyvb2g3j-2{list-style-type:none}.lst-kix_sxaq135mthse-7>li:before{content:"\0025cb  "}ul.lst-kix_d4lbyvb2g3j-8{list-style-type:none}ul.lst-kix_d4lbyvb2g3j-7{list-style-type:none}ul.lst-kix_d4lbyvb2g3j-6{list-style-type:none}.lst-kix_gogm0wicn297-8>li:before{content:"\0025a0  "}.lst-kix_ow9ix2mme95u-0>li:before{content:"\0025cf  "}.lst-kix_nf1j1iafbq39-4>li:before{content:"\0025cb  "}.lst-kix_ljjwvb506pet-3>li:before{content:"\0025cf  "}.lst-kix_cst6lp3lb98n-2>li:before{content:"\0025a0  "}.lst-kix_nf1j1iafbq39-2>li:before{content:"\0025a0  "}.lst-kix_cst6lp3lb98n-0>li:before{content:"\0025cf  "}.lst-kix_didibfga6ee5-4>li:before{content:"\0025cb  "}.lst-kix_xav8ikvidc9s-0>li:before{content:"\0025cf  "}.lst-kix_398os0jr1iw3-2>li:before{content:"\0025a0  "}ul.lst-kix_8nsyajlscxcg-1{list-style-type:none}.lst-kix_5d66udn9quk7-5>li:before{content:"\0025a0  "}ul.lst-kix_8nsyajlscxcg-0{list-style-type:none}ul.lst-kix_8nsyajlscxcg-3{list-style-type:none}ul.lst-kix_8nsyajlscxcg-2{list-style-type:none}.lst-kix_5d66udn9quk7-7>li:before{content:"\0025cb  "}ul.lst-kix_8nsyajlscxcg-8{list-style-type:none}ul.lst-kix_8nsyajlscxcg-5{list-style-type:none}ul.lst-kix_8nsyajlscxcg-4{list-style-type:none}ul.lst-kix_8nsyajlscxcg-7{list-style-type:none}ul.lst-kix_8nsyajlscxcg-6{list-style-type:none}.lst-kix_u82x83buy4jv-3>li:before{content:"\0025cf  "}.lst-kix_jbw6p6lajlkh-1>li:before{content:"\0025cb  "}.lst-kix_jbw6p6lajlkh-3>li:before{content:"\0025cf  "}.lst-kix_edj3r8hgf2z3-0>li:before{content:"\0025cf  "}.lst-kix_398os0jr1iw3-8>li:before{content:"\0025a0  "}.lst-kix_douubz1tb8an-2>li:before{content:"\0025a0  "}.lst-kix_7apmk6vwlj5r-7>li:before{content:"\0025cb  "}.lst-kix_u82x83buy4jv-5>li:before{content:"\0025a0  "}.lst-kix_f3egesa43r6b-2>li:before{content:"\0025a0  "}.lst-kix_7b2y41o7s2k3-0>li:before{content:"\0025cf  "}.lst-kix_5kaugn46jpw6-6>li:before{content:"\0025cf  "}.lst-kix_mxwp6np2u5xu-4>li:before{content:"-  "}.lst-kix_7b2y41o7s2k3-6>li:before{content:"\0025cf  "}.lst-kix_dq6vem8xoqp9-3>li:before{content:"\0025cf  "}ul.lst-kix_1xoigud6ruor-3{list-style-type:none}ul.lst-kix_6k5yl8dbqqq2-5{list-style-type:none}ul.lst-kix_1xoigud6ruor-2{list-style-type:none}ul.lst-kix_6k5yl8dbqqq2-6{list-style-type:none}ul.lst-kix_1xoigud6ruor-5{list-style-type:none}.lst-kix_8hjy0hqh3rvg-4>li:before{content:"\0025cb  "}ul.lst-kix_6k5yl8dbqqq2-7{list-style-type:none}ul.lst-kix_1xoigud6ruor-4{list-style-type:none}ul.lst-kix_6k5yl8dbqqq2-8{list-style-type:none}ul.lst-kix_1xoigud6ruor-7{list-style-type:none}ul.lst-kix_1xoigud6ruor-6{list-style-type:none}ul.lst-kix_1xoigud6ruor-8{list-style-type:none}.lst-kix_hydge7xfbt8r-7>li:before{content:"\0025cb  "}ul.lst-kix_6k5yl8dbqqq2-0{list-style-type:none}.lst-kix_hydge7xfbt8r-5>li:before{content:"\0025a0  "}ul.lst-kix_6k5yl8dbqqq2-1{list-style-type:none}ul.lst-kix_6k5yl8dbqqq2-2{list-style-type:none}ul.lst-kix_1xoigud6ruor-1{list-style-type:none}.lst-kix_douubz1tb8an-8>li:before{content:"\0025a0  "}.lst-kix_f3egesa43r6b-4>li:before{content:"\0025cb  "}ul.lst-kix_6k5yl8dbqqq2-3{list-style-type:none}ul.lst-kix_1xoigud6ruor-0{list-style-type:none}ul.lst-kix_6k5yl8dbqqq2-4{list-style-type:none}ul.lst-kix_313q3f5x4p2m-1{list-style-type:none}ul.lst-kix_313q3f5x4p2m-0{list-style-type:none}ul.lst-kix_313q3f5x4p2m-3{list-style-type:none}.lst-kix_lyxc6qcmvco2-8>li:before{content:"\0025a0  "}ul.lst-kix_313q3f5x4p2m-2{list-style-type:none}ul.lst-kix_313q3f5x4p2m-5{list-style-type:none}.lst-kix_n21asioviqe4-2>li:before{content:"\0025a0  "}ul.lst-kix_313q3f5x4p2m-4{list-style-type:none}ul.lst-kix_3de1jg1kuy24-8{list-style-type:none}.lst-kix_l47f7qjlu9cu-6>li:before{content:"\0025cf  "}ul.lst-kix_313q3f5x4p2m-7{list-style-type:none}ul.lst-kix_3de1jg1kuy24-7{list-style-type:none}ul.lst-kix_313q3f5x4p2m-6{list-style-type:none}.lst-kix_z3si3pbri1de-7>li:before{content:"\0025cb  "}.lst-kix_lyxc6qcmvco2-2>li:before{content:"\0025a0  "}ul.lst-kix_7v1j337157vp-2{list-style-type:none}.lst-kix_6ykz6yxb3o6x-8>li:before{content:"\0025a0  "}ul.lst-kix_7v1j337157vp-1{list-style-type:none}ul.lst-kix_7v1j337157vp-0{list-style-type:none}.lst-kix_l47f7qjlu9cu-4>li:before{content:"\0025cb  "}ul.lst-kix_7v1j337157vp-6{list-style-type:none}.lst-kix_i1kls5tk4gt6-0>li:before{content:"\0025cf  "}ul.lst-kix_xav8ikvidc9s-3{list-style-type:none}.lst-kix_d995nc7613a9-5>li:before{content:"\0025a0  "}ul.lst-kix_7v1j337157vp-5{list-style-type:none}ul.lst-kix_xav8ikvidc9s-2{list-style-type:none}ul.lst-kix_7v1j337157vp-4{list-style-type:none}.lst-kix_z3si3pbri1de-1>li:before{content:"\0025cb  "}ul.lst-kix_xav8ikvidc9s-1{list-style-type:none}ul.lst-kix_7v1j337157vp-3{list-style-type:none}ul.lst-kix_xav8ikvidc9s-0{list-style-type:none}ul.lst-kix_xav8ikvidc9s-7{list-style-type:none}ul.lst-kix_xav8ikvidc9s-6{list-style-type:none}ul.lst-kix_7v1j337157vp-8{list-style-type:none}ul.lst-kix_xav8ikvidc9s-5{list-style-type:none}ul.lst-kix_7v1j337157vp-7{list-style-type:none}ul.lst-kix_xav8ikvidc9s-4{list-style-type:none}ul.lst-kix_z4z7nvld905a-3{list-style-type:none}ul.lst-kix_z4z7nvld905a-4{list-style-type:none}ul.lst-kix_313q3f5x4p2m-8{list-style-type:none}ul.lst-kix_z4z7nvld905a-5{list-style-type:none}ul.lst-kix_z4z7nvld905a-6{list-style-type:none}ul.lst-kix_z4z7nvld905a-0{list-style-type:none}ul.lst-kix_z4z7nvld905a-1{list-style-type:none}ul.lst-kix_z4z7nvld905a-2{list-style-type:none}ul.lst-kix_d3k7hdh35j7a-8{list-style-type:none}.lst-kix_i1kls5tk4gt6-6>li:before{content:"\0025cf  "}.lst-kix_xkjgsygnpr2m-5>li:before{content:"\0025a0  "}ul.lst-kix_d3k7hdh35j7a-2{list-style-type:none}ul.lst-kix_d3k7hdh35j7a-3{list-style-type:none}ul.lst-kix_d3k7hdh35j7a-0{list-style-type:none}ul.lst-kix_d3k7hdh35j7a-1{list-style-type:none}.lst-kix_gswyhyotsm36-7>li:before{content:"\0025cb  "}ul.lst-kix_d3k7hdh35j7a-6{list-style-type:none}.lst-kix_aui8m1kby916-1>li:before{content:"\0025cb  "}ul.lst-kix_d3k7hdh35j7a-7{list-style-type:none}.lst-kix_c2vvnsz0drek-5>li:before{content:"\0025a0  "}ul.lst-kix_d3k7hdh35j7a-4{list-style-type:none}ul.lst-kix_d3k7hdh35j7a-5{list-style-type:none}.lst-kix_egb7bfw0xx0q-5>li:before{content:"\0025a0  "}.lst-kix_62is116n17wq-3>li:before{content:"\0025cf  "}ul.lst-kix_3de1jg1kuy24-6{list-style-type:none}.lst-kix_gswyhyotsm36-5>li:before{content:"\0025a0  "}ul.lst-kix_3de1jg1kuy24-5{list-style-type:none}ul.lst-kix_3de1jg1kuy24-4{list-style-type:none}ul.lst-kix_3de1jg1kuy24-3{list-style-type:none}ul.lst-kix_3de1jg1kuy24-2{list-style-type:none}.lst-kix_8wesot3ko53b-5>li:before{content:"\0025a0  "}.lst-kix_p7d8xi28lme6-8>li:before{content:"\0025a0  "}ul.lst-kix_3de1jg1kuy24-1{list-style-type:none}ul.lst-kix_3de1jg1kuy24-0{list-style-type:none}.lst-kix_b474o1tvhevy-0>li:before{content:"\0025cf  "}.lst-kix_6jabh0roz5s1-7>li:before{content:"\0025cb  "}.lst-kix_41ikkbtdotge-7>li:before{content:"\0025cb  "}.lst-kix_egb7bfw0xx0q-7>li:before{content:"\0025cb  "}.lst-kix_3de1jg1kuy24-8>li:before{content:"\0025a0  "}.lst-kix_l4pre1xosssa-6>li:before{content:"\0025cf  "}.lst-kix_ex6gjyozs00-2>li:before{content:"\0025a0  "}.lst-kix_41ikkbtdotge-1>li:before{content:"\0025cb  "}.lst-kix_5x48r7dz2gvp-1>li:before{content:"\0025cb  "}.lst-kix_b474o1tvhevy-8>li:before{content:"\0025a0  "}.lst-kix_fk4cszd8thar-2>li:before{content:"\0025a0  "}.lst-kix_n1766bho68n4-3>li:before{content:"\0025cf  "}.lst-kix_bq5ybp1zal0b-6>li:before{content:"\0025cf  "}.lst-kix_fk4cszd8thar-4>li:before{content:"\0025cb  "}.lst-kix_bq5ybp1zal0b-4>li:before{content:"\0025cb  "}.lst-kix_p7d8xi28lme6-2>li:before{content:"\0025a0  "}.lst-kix_ex6gjyozs00-8>li:before{content:"\0025a0  "}.lst-kix_2jmrp4ochkmc-6>li:before{content:"\0025cf  "}.lst-kix_l4pre1xosssa-4>li:before{content:"\0025cb  "}.lst-kix_b474o1tvhevy-6>li:before{content:"\0025cf  "}.lst-kix_p7d8xi28lme6-0>li:before{content:"\0025cf  "}.lst-kix_iqbmivscsmun-5>li:before{content:"\0025a0  "}ul.lst-kix_kz1sfsbi2eng-8{list-style-type:none}.lst-kix_j24rsp966ll1-4>li:before{content:"\0025cb  "}.lst-kix_isql72ps5u79-7>li:before{content:"\0025cb  "}ul.lst-kix_z4z7nvld905a-7{list-style-type:none}.lst-kix_m8diz2yltkag-0>li:before{content:"\0025cf  "}ul.lst-kix_z4z7nvld905a-8{list-style-type:none}ul.lst-kix_kz1sfsbi2eng-1{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-3{list-style-type:none}ul.lst-kix_kz1sfsbi2eng-0{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-4{list-style-type:none}ul.lst-kix_kz1sfsbi2eng-3{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-5{list-style-type:none}.lst-kix_j24rsp966ll1-2>li:before{content:"\0025a0  "}ul.lst-kix_kz1sfsbi2eng-2{list-style-type:none}ul.lst-kix_xav8ikvidc9s-8{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-6{list-style-type:none}ul.lst-kix_kz1sfsbi2eng-5{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-7{list-style-type:none}ul.lst-kix_kz1sfsbi2eng-4{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-8{list-style-type:none}ul.lst-kix_kz1sfsbi2eng-7{list-style-type:none}ul.lst-kix_kz1sfsbi2eng-6{list-style-type:none}.lst-kix_m8diz2yltkag-6>li:before{content:"\0025cf  "}ul.lst-kix_tlb25jkk5mkd-0{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-1{list-style-type:none}ul.lst-kix_tlb25jkk5mkd-2{list-style-type:none}.lst-kix_isql72ps5u79-1>li:before{content:"\0025cb  "}.lst-kix_fjl32nwuhu4u-2>li:before{content:"\0025a0  "}.lst-kix_g0l7lf51hdyf-1>li:before{content:"\0025cb  "}ul.lst-kix_a9t5z6hxg4oz-3{list-style-type:none}ul.lst-kix_62is116n17wq-5{list-style-type:none}ul.lst-kix_a9t5z6hxg4oz-2{list-style-type:none}.lst-kix_yv7c7gf6elha-2>li:before{content:"\0025a0  "}ul.lst-kix_62is116n17wq-6{list-style-type:none}ul.lst-kix_a9t5z6hxg4oz-1{list-style-type:none}ul.lst-kix_62is116n17wq-3{list-style-type:none}ul.lst-kix_a9t5z6hxg4oz-0{list-style-type:none}ul.lst-kix_62is116n17wq-4{list-style-type:none}ul.lst-kix_62is116n17wq-7{list-style-type:none}ul.lst-kix_62is116n17wq-8{list-style-type:none}.lst-kix_z8j4l8h7k6b3-7>li:before{content:"\0025cb  "}ul.lst-kix_470xpz4q6hoi-0{list-style-type:none}ul.lst-kix_470xpz4q6hoi-2{list-style-type:none}.lst-kix_894copnhklgz-0>li:before{content:"\0025cf  "}ul.lst-kix_470xpz4q6hoi-1{list-style-type:none}.lst-kix_h99428b6laza-5>li:before{content:"\0025a0  "}.lst-kix_spzfuoeekohi-8>li:before{content:"\0025a0  "}.lst-kix_u1clvxqptoo-7>li:before{content:"\0025cb  "}.lst-kix_bhgkuyt4aopz-5>li:before{content:"\0025a0  "}ul.lst-kix_1a8ua5u4a26p-0{list-style-type:none}.lst-kix_4fwhm2tiefhz-3>li:before{content:"\0025cf  "}ul.lst-kix_1a8ua5u4a26p-1{list-style-type:none}ul.lst-kix_1a8ua5u4a26p-2{list-style-type:none}ul.lst-kix_1a8ua5u4a26p-3{list-style-type:none}ul.lst-kix_1a8ua5u4a26p-4{list-style-type:none}.lst-kix_yh6f66826ysk-1>li:before{content:"\0025cb  "}ul.lst-kix_1a8ua5u4a26p-5{list-style-type:none}.lst-kix_vhrrjzyjvp5x-6>li:before{content:"\0025cf  "}.lst-kix_5x48r7dz2gvp-7>li:before{content:"\0025cb  "}.lst-kix_jys0trw4gd1m-8>li:before{content:"\0025a0  "}.lst-kix_8swzovi1ytnr-6>li:before{content:"\0025cf  "}.lst-kix_wq4pisfqksnu-3>li:before{content:"\0025cf  "}.lst-kix_9cwmoyhkmhfb-0>li:before{content:"\0025cf  "}ul.lst-kix_62is116n17wq-1{list-style-type:none}ul.lst-kix_62is116n17wq-2{list-style-type:none}ul.lst-kix_62is116n17wq-0{list-style-type:none}ul.lst-kix_1a8ua5u4a26p-6{list-style-type:none}.lst-kix_kkphisidn6cz-8>li:before{content:"\0025a0  "}ul.lst-kix_1a8ua5u4a26p-7{list-style-type:none}.lst-kix_pzunwido5p11-3>li:before{content:"\0025cf  "}ul.lst-kix_1a8ua5u4a26p-8{list-style-type:none}.lst-kix_xqfh3tf7tjc0-1>li:before{content:"\0025cb  "}.lst-kix_spzfuoeekohi-0>li:before{content:"\0025cf  "}.lst-kix_2rp5aiyjkdpd-3>li:before{content:"\0025cf  "}.lst-kix_juam6n4619t2-8>li:before{content:"\0025a0  "}.lst-kix_8z696e8bv17-7>li:before{content:"\0025cb  "}.lst-kix_ut5bvwbkq6rt-1>li:before{content:"\0025cb  "}.lst-kix_4k4id0a37km3-0>li:before{content:"\0025cf  "}.lst-kix_7v1j337157vp-8>li:before{content:"\0025a0  "}.lst-kix_xgsgxyckqw77-6>li:before{content:"\0025cf  "}.lst-kix_vapye4eddf48-7>li:before{content:"\0025cb  "}.lst-kix_nbuhcgd4ft21-5>li:before{content:"\0025a0  "}ul.lst-kix_ydb53brft35n-5{list-style-type:none}ul.lst-kix_ydb53brft35n-4{list-style-type:none}ul.lst-kix_ydb53brft35n-7{list-style-type:none}ul.lst-kix_ydb53brft35n-6{list-style-type:none}ul.lst-kix_ydb53brft35n-1{list-style-type:none}ul.lst-kix_ydb53brft35n-0{list-style-type:none}.lst-kix_ehe7zglmx7i2-1>li:before{content:"\0025cb  "}ul.lst-kix_ydb53brft35n-3{list-style-type:none}ul.lst-kix_ydb53brft35n-2{list-style-type:none}.lst-kix_dj4fsxw52t4l-6>li:before{content:"\0025cf  "}.lst-kix_s6xk062mdkv5-3>li:before{content:"\0025cf  "}.lst-kix_bcdpfzfy81p5-1>li:before{content:"\0025cb  "}.lst-kix_o9z4cf2yq4mz-6>li:before{content:"\0025cf  "}ul.lst-kix_ydb53brft35n-8{list-style-type:none}.lst-kix_gnt40hozc3kv-4>li:before{content:"\0025cb  "}ul.lst-kix_a9t5z6hxg4oz-8{list-style-type:none}ul.lst-kix_a9t5z6hxg4oz-7{list-style-type:none}.lst-kix_d4lbyvb2g3j-2>li:before{content:"\0025a0  "}ul.lst-kix_a9t5z6hxg4oz-6{list-style-type:none}ul.lst-kix_a9t5z6hxg4oz-5{list-style-type:none}ul.lst-kix_a9t5z6hxg4oz-4{list-style-type:none}.lst-kix_rwqhz1uwdird-8>li:before{content:"\0025a0  "}.lst-kix_d3k7hdh35j7a-0>li:before{content:"\0025cf  "}ul.lst-kix_pmg6qjdagav9-6{list-style-type:none}ul.lst-kix_pmg6qjdagav9-7{list-style-type:none}ul.lst-kix_pmg6qjdagav9-8{list-style-type:none}ul.lst-kix_pmg6qjdagav9-2{list-style-type:none}ul.lst-kix_pmg6qjdagav9-3{list-style-type:none}ul.lst-kix_pmg6qjdagav9-4{list-style-type:none}ul.lst-kix_pmg6qjdagav9-5{list-style-type:none}.lst-kix_xqqiyb3b5ag0-3>li:before{content:"\0025cf  "}ul.lst-kix_ex6gjyozs00-1{list-style-type:none}ul.lst-kix_ex6gjyozs00-0{list-style-type:none}.lst-kix_rwqhz1uwdird-0>li:before{content:"\0025cf  "}ul.lst-kix_ex6gjyozs00-3{list-style-type:none}ul.lst-kix_ex6gjyozs00-2{list-style-type:none}ul.lst-kix_xgsgxyckqw77-7{list-style-type:none}ul.lst-kix_xgsgxyckqw77-8{list-style-type:none}ul.lst-kix_xgsgxyckqw77-5{list-style-type:none}ul.lst-kix_xgsgxyckqw77-6{list-style-type:none}ul.lst-kix_xgsgxyckqw77-3{list-style-type:none}ul.lst-kix_xgsgxyckqw77-4{list-style-type:none}ul.lst-kix_xgsgxyckqw77-1{list-style-type:none}ul.lst-kix_xgsgxyckqw77-2{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-7{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-8{list-style-type:none}ul.lst-kix_xgsgxyckqw77-0{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-5{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-6{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-3{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-4{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-1{list-style-type:none}ul.lst-kix_o38l9c2rqk2p-2{list-style-type:none}.lst-kix_o5m7acjt2ju1-5>li:before{content:"\0025a0  "}.lst-kix_r75b0bpeph0o-7>li:before{content:"\0025cb  "}ul.lst-kix_o38l9c2rqk2p-0{list-style-type:none}.lst-kix_jzarhjey1jrh-4>li:before{content:"\0025cb  "}.lst-kix_d3k7hdh35j7a-8>li:before{content:"\0025a0  "}.lst-kix_iiujyt46s4u7-3>li:before{content:"\0025cf  "}.lst-kix_204pjqaavew-3>li:before{content:"\0025cf  "}.lst-kix_gwuatnbmuewh-5>li:before{content:"\0025a0  "}.lst-kix_xkmlxx8lhm5d-7>li:before{content:"\0025cb  "}.lst-kix_xx3xz98aq2wq-2>li:before{content:"\0025a0  "}.lst-kix_f51q5t2m61i8-7>li:before{content:"\0025cb  "}.lst-kix_qsdpvqxg1u78-3>li:before{content:"\0025cf  "}ul.lst-kix_b8dlmlw0p2p1-8{list-style-type:none}ul.lst-kix_b8dlmlw0p2p1-6{list-style-type:none}ul.lst-kix_b8dlmlw0p2p1-7{list-style-type:none}ul.lst-kix_b8dlmlw0p2p1-4{list-style-type:none}ul.lst-kix_b8dlmlw0p2p1-5{list-style-type:none}ul.lst-kix_b8dlmlw0p2p1-2{list-style-type:none}.lst-kix_r8labycqo9zl-8>li:before{content:"\0025a0  "}ul.lst-kix_b8dlmlw0p2p1-3{list-style-type:none}ul.lst-kix_b8dlmlw0p2p1-0{list-style-type:none}ul.lst-kix_b8dlmlw0p2p1-1{list-style-type:none}.lst-kix_solhl6ysg65v-2>li:before{content:"\0025a0  "}.lst-kix_5p6viq6b9z8b-2>li:before{content:"\0025a0  "}ul.lst-kix_68swn1licdyn-6{list-style-type:none}ul.lst-kix_68swn1licdyn-5{list-style-type:none}ul.lst-kix_68swn1licdyn-8{list-style-type:none}ul.lst-kix_68swn1licdyn-7{list-style-type:none}ul.lst-kix_7qzrf8aug8gu-6{list-style-type:none}.lst-kix_ct57w2de70tb-7>li:before{content:"\0025cb  "}ul.lst-kix_7qzrf8aug8gu-7{list-style-type:none}ul.lst-kix_7qzrf8aug8gu-8{list-style-type:none}.lst-kix_474t6llskbh7-3>li:before{content:"\0025cf  "}ul.lst-kix_7qzrf8aug8gu-2{list-style-type:none}ul.lst-kix_7qzrf8aug8gu-3{list-style-type:none}ul.lst-kix_7qzrf8aug8gu-4{list-style-type:none}ul.lst-kix_7qzrf8aug8gu-5{list-style-type:none}.lst-kix_7jzvzpyn9bnk-0>li:before{content:"\0025cf  "}ul.lst-kix_ljjwvb506pet-3{list-style-type:none}ul.lst-kix_ljjwvb506pet-2{list-style-type:none}.lst-kix_cst6lp3lb98n-8>li:before{content:"\0025a0  "}ul.lst-kix_7qzrf8aug8gu-0{list-style-type:none}ul.lst-kix_ljjwvb506pet-5{list-style-type:none}ul.lst-kix_7qzrf8aug8gu-1{list-style-type:none}ul.lst-kix_ljjwvb506pet-4{list-style-type:none}ul.lst-kix_ljjwvb506pet-1{list-style-type:none}ul.lst-kix_ljjwvb506pet-0{list-style-type:none}ul.lst-kix_470xpz4q6hoi-8{list-style-type:none}ul.lst-kix_470xpz4q6hoi-7{list-style-type:none}ul.lst-kix_470xpz4q6hoi-4{list-style-type:none}ul.lst-kix_ljjwvb506pet-7{list-style-type:none}.lst-kix_p0fqpwscumob-2>li:before{content:"\0025a0  "}ul.lst-kix_470xpz4q6hoi-3{list-style-type:none}ul.lst-kix_ljjwvb506pet-6{list-style-type:none}ul.lst-kix_470xpz4q6hoi-6{list-style-type:none}.lst-kix_24cmoxb3qmwl-3>li:before{content:"\0025cf  "}ul.lst-kix_470xpz4q6hoi-5{list-style-type:none}ul.lst-kix_ljjwvb506pet-8{list-style-type:none}.lst-kix_wq1o6elz9hy4-6>li:before{content:"\0025cf  "}ul.lst-kix_ex6gjyozs00-8{list-style-type:none}.lst-kix_mh0uhwysa91-8>li:before{content:"\0025a0  "}ul.lst-kix_uz5smpj76010-5{list-style-type:none}ul.lst-kix_ex6gjyozs00-5{list-style-type:none}.lst-kix_o4d3wx7elf58-4>li:before{content:"\0025cb  "}ul.lst-kix_uz5smpj76010-6{list-style-type:none}ul.lst-kix_ex6gjyozs00-4{list-style-type:none}ul.lst-kix_uz5smpj76010-7{list-style-type:none}ul.lst-kix_ex6gjyozs00-7{list-style-type:none}ul.lst-kix_uz5smpj76010-8{list-style-type:none}ul.lst-kix_ex6gjyozs00-6{list-style-type:none}ul.lst-kix_uz5smpj76010-1{list-style-type:none}ul.lst-kix_uz5smpj76010-2{list-style-type:none}ul.lst-kix_uz5smpj76010-3{list-style-type:none}ul.lst-kix_pmg6qjdagav9-0{list-style-type:none}ul.lst-kix_uz5smpj76010-4{list-style-type:none}ul.lst-kix_pmg6qjdagav9-1{list-style-type:none}.lst-kix_p2mkzdwo7q6r-6>li:before{content:"\0025cf  "}ul.lst-kix_n21asioviqe4-0{list-style-type:none}ul.lst-kix_uz5smpj76010-0{list-style-type:none}ul.lst-kix_n21asioviqe4-2{list-style-type:none}.lst-kix_9khvkfghhb3b-5>li:before{content:"\0025a0  "}ul.lst-kix_n21asioviqe4-1{list-style-type:none}ul.lst-kix_n21asioviqe4-4{list-style-type:none}.lst-kix_emy9ap8zeql8-1>li:before{content:"\0025cb  "}ul.lst-kix_n21asioviqe4-3{list-style-type:none}ul.lst-kix_n21asioviqe4-6{list-style-type:none}ul.lst-kix_n21asioviqe4-5{list-style-type:none}.lst-kix_kz1sfsbi2eng-2>li:before{content:"\0025a0  "}ul.lst-kix_n21asioviqe4-8{list-style-type:none}ul.lst-kix_n21asioviqe4-7{list-style-type:none}.lst-kix_vjhjbjs9ubca-0>li:before{content:"\0025cf  "}.lst-kix_vjhjbjs9ubca-3>li:before{content:"\0025cf  "}ul.lst-kix_74gx9bwnn8kt-3{list-style-type:none}ul.lst-kix_74gx9bwnn8kt-2{list-style-type:none}ul.lst-kix_74gx9bwnn8kt-1{list-style-type:none}.lst-kix_nwi1ck3fibs7-2>li:before{content:"\0025a0  "}ul.lst-kix_74gx9bwnn8kt-0{list-style-type:none}ul.lst-kix_74gx9bwnn8kt-7{list-style-type:none}.lst-kix_vjhjbjs9ubca-6>li:before{content:"\0025cf  "}.lst-kix_rr7hjjtid2vg-6>li:before{content:"\0025cf  "}ul.lst-kix_74gx9bwnn8kt-6{list-style-type:none}ul.lst-kix_74gx9bwnn8kt-5{list-style-type:none}ul.lst-kix_74gx9bwnn8kt-4{list-style-type:none}ul.lst-kix_74gx9bwnn8kt-8{list-style-type:none}.lst-kix_rr7hjjtid2vg-3>li:before{content:"\0025cf  "}ul.lst-kix_crb89j7xw8q2-8{list-style-type:none}ul.lst-kix_crb89j7xw8q2-5{list-style-type:none}.lst-kix_i5pnhlfopov3-0>li:before{content:"\0025cf  "}ul.lst-kix_crb89j7xw8q2-4{list-style-type:none}ul.lst-kix_crb89j7xw8q2-7{list-style-type:none}.lst-kix_jourdftw4nrm-0>li:before{content:"\0025cf  "}ul.lst-kix_crb89j7xw8q2-6{list-style-type:none}ul.lst-kix_crb89j7xw8q2-1{list-style-type:none}ul.lst-kix_crb89j7xw8q2-0{list-style-type:none}ul.lst-kix_crb89j7xw8q2-3{list-style-type:none}ul.lst-kix_crb89j7xw8q2-2{list-style-type:none}.lst-kix_rr7hjjtid2vg-0>li:before{content:"\0025cf  "}.lst-kix_p3gj10803b4t-4>li:before{content:"\0025cb  "}.lst-kix_i5pnhlfopov3-3>li:before{content:"\0025cf  "}.lst-kix_60wjig3vq01-2>li:before{content:"\0025a0  "}.lst-kix_jourdftw4nrm-3>li:before{content:"\0025cf  "}.lst-kix_p3gj10803b4t-7>li:before{content:"\0025cb  "}.lst-kix_i5pnhlfopov3-6>li:before{content:"\0025cf  "}.lst-kix_z0c6lix981m-7>li:before{content:"\0025cb  "}.lst-kix_ltkzlkp2gxkt-1>li:before{content:"\0025cb  "}ul.lst-kix_481c98v2pt83-1{list-style-type:none}.lst-kix_z0c6lix981m-4>li:before{content:"\0025cb  "}ul.lst-kix_481c98v2pt83-2{list-style-type:none}ul.lst-kix_481c98v2pt83-3{list-style-type:none}ul.lst-kix_481c98v2pt83-4{list-style-type:none}.lst-kix_hh7pij1hmwlh-7>li:before{content:"\0025cb  "}ul.lst-kix_481c98v2pt83-5{list-style-type:none}ul.lst-kix_481c98v2pt83-6{list-style-type:none}ul.lst-kix_481c98v2pt83-7{list-style-type:none}.lst-kix_ltkzlkp2gxkt-4>li:before{content:"\0025cb  "}ul.lst-kix_481c98v2pt83-8{list-style-type:none}.lst-kix_zgvzzrmreqeg-3>li:before{content:"\0025cf  "}.lst-kix_vsaybtg1hfvm-6>li:before{content:"\0025cf  "}ul.lst-kix_i5pnhlfopov3-5{list-style-type:none}ul.lst-kix_i5pnhlfopov3-4{list-style-type:none}.lst-kix_z0c6lix981m-1>li:before{content:"\0025cb  "}ul.lst-kix_i5pnhlfopov3-3{list-style-type:none}.lst-kix_jourdftw4nrm-6>li:before{content:"\0025cf  "}ul.lst-kix_i5pnhlfopov3-2{list-style-type:none}ul.lst-kix_i5pnhlfopov3-1{list-style-type:none}ul.lst-kix_ulntv5wergyk-8{list-style-type:none}ul.lst-kix_i5pnhlfopov3-0{list-style-type:none}ul.lst-kix_481c98v2pt83-0{list-style-type:none}ul.lst-kix_ulntv5wergyk-5{list-style-type:none}ul.lst-kix_ulntv5wergyk-4{list-style-type:none}ul.lst-kix_ulntv5wergyk-7{list-style-type:none}.lst-kix_zgvzzrmreqeg-0>li:before{content:"\0025cf  "}.lst-kix_vsaybtg1hfvm-3>li:before{content:"\0025cf  "}.lst-kix_ltkzlkp2gxkt-7>li:before{content:"\0025cb  "}ul.lst-kix_ulntv5wergyk-6{list-style-type:none}ul.lst-kix_ulntv5wergyk-1{list-style-type:none}ul.lst-kix_ulntv5wergyk-0{list-style-type:none}ul.lst-kix_i5pnhlfopov3-8{list-style-type:none}ul.lst-kix_ulntv5wergyk-3{list-style-type:none}.lst-kix_klzhllb6xcyr-8>li:before{content:"\0025a0  "}ul.lst-kix_i5pnhlfopov3-7{list-style-type:none}ul.lst-kix_ulntv5wergyk-2{list-style-type:none}.lst-kix_p3gj10803b4t-1>li:before{content:"\0025cb  "}.lst-kix_k12n8xes8ofj-5>li:before{content:"\0025a0  "}ul.lst-kix_i5pnhlfopov3-6{list-style-type:none}.lst-kix_rrs2j0rau6wp-2>li:before{content:"\0025a0  "}.lst-kix_k12n8xes8ofj-8>li:before{content:"\0025a0  "}ul.lst-kix_kk7o9gi3ij0l-1{list-style-type:none}ul.lst-kix_kk7o9gi3ij0l-0{list-style-type:none}ul.lst-kix_kk7o9gi3ij0l-3{list-style-type:none}.lst-kix_defeme42l2ge-7>li:before{content:"\0025cb  "}ul.lst-kix_kk7o9gi3ij0l-2{list-style-type:none}.lst-kix_s3fvwoba1t7n-3>li:before{content:"\0025cf  "}ul.lst-kix_kk7o9gi3ij0l-5{list-style-type:none}ul.lst-kix_kk7o9gi3ij0l-4{list-style-type:none}ul.lst-kix_kk7o9gi3ij0l-7{list-style-type:none}ul.lst-kix_kk7o9gi3ij0l-6{list-style-type:none}.lst-kix_47kviebmmd71-2>li:before{content:"\0025a0  "}ul.lst-kix_kk7o9gi3ij0l-8{list-style-type:none}ul.lst-kix_emy9ap8zeql8-8{list-style-type:none}.lst-kix_rrs2j0rau6wp-5>li:before{content:"\0025a0  "}.lst-kix_hh7pij1hmwlh-4>li:before{content:"\0025cb  "}.lst-kix_w1d9l4m1yg5x-2>li:before{content:"\0025a0  "}ul.lst-kix_emy9ap8zeql8-7{list-style-type:none}ul.lst-kix_emy9ap8zeql8-6{list-style-type:none}.lst-kix_s3fvwoba1t7n-6>li:before{content:"\0025cf  "}ul.lst-kix_emy9ap8zeql8-5{list-style-type:none}.lst-kix_zgvzzrmreqeg-6>li:before{content:"\0025cf  "}ul.lst-kix_emy9ap8zeql8-4{list-style-type:none}.lst-kix_rrs2j0rau6wp-8>li:before{content:"\0025a0  "}.lst-kix_ly0lmz6fcokr-0>li:before{content:"\0025cf  "}ul.lst-kix_emy9ap8zeql8-3{list-style-type:none}ul.lst-kix_emy9ap8zeql8-2{list-style-type:none}ul.lst-kix_emy9ap8zeql8-1{list-style-type:none}ul.lst-kix_emy9ap8zeql8-0{list-style-type:none}ul.lst-kix_ukuyvb7r2sb4-0{list-style-type:none}.lst-kix_58o57c52tmut-5>li:before{content:"\0025a0  "}.lst-kix_yhn6vvmwi8p2-0>li:before{content:"\0025cf  "}ul.lst-kix_ukuyvb7r2sb4-3{list-style-type:none}ul.lst-kix_7q22x2z5v5y-7{list-style-type:none}ul.lst-kix_ukuyvb7r2sb4-4{list-style-type:none}ul.lst-kix_7q22x2z5v5y-6{list-style-type:none}ul.lst-kix_ukuyvb7r2sb4-1{list-style-type:none}.lst-kix_58o57c52tmut-8>li:before{content:"\0025a0  "}ul.lst-kix_ukuyvb7r2sb4-2{list-style-type:none}ul.lst-kix_7q22x2z5v5y-8{list-style-type:none}ul.lst-kix_ukuyvb7r2sb4-7{list-style-type:none}ul.lst-kix_7q22x2z5v5y-3{list-style-type:none}ul.lst-kix_ukuyvb7r2sb4-8{list-style-type:none}ul.lst-kix_7q22x2z5v5y-2{list-style-type:none}ul.lst-kix_ukuyvb7r2sb4-5{list-style-type:none}ul.lst-kix_7q22x2z5v5y-5{list-style-type:none}ul.lst-kix_ukuyvb7r2sb4-6{list-style-type:none}ul.lst-kix_7q22x2z5v5y-4{list-style-type:none}ul.lst-kix_7q22x2z5v5y-1{list-style-type:none}ul.lst-kix_7q22x2z5v5y-0{list-style-type:none}.lst-kix_gnzb8ea0t50l-6>li:before{content:"\0025cf  "}.lst-kix_17is42bhveph-7>li:before{content:"\0025cb  "}.lst-kix_xkmlxx8lhm5d-4>li:before{content:"\0025cb  "}.lst-kix_uk8k6pzec0np-4>li:before{content:"\0025cb  "}.lst-kix_crt5jd7c2xq0-8>li:before{content:"\0025a0  "}.lst-kix_xqqiyb3b5ag0-0>li:before{content:"\0025cf  "}.lst-kix_yhn6vvmwi8p2-6>li:before{content:"\0025cf  "}.lst-kix_yhmx20qr5k0e-4>li:before{content:"\0025cb  "}.lst-kix_7r3huctovklw-3>li:before{content:"\0025cf  "}.lst-kix_o38l9c2rqk2p-4>li:before{content:"\0025cb  "}.lst-kix_w1d9l4m1yg5x-8>li:before{content:"\0025a0  "}ul.lst-kix_8swzovi1ytnr-7{list-style-type:none}ul.lst-kix_8swzovi1ytnr-8{list-style-type:none}ul.lst-kix_8swzovi1ytnr-5{list-style-type:none}ul.lst-kix_8swzovi1ytnr-6{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-6{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-5{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-8{list-style-type:none}ul.lst-kix_ftgmvnn66fx0-7{list-style-type:none}.lst-kix_n8ye7s4j9pok-8>li:before{content:"\0025a0  "}.lst-kix_vsaybtg1hfvm-0>li:before{content:"\0025cf  "}.lst-kix_n8ye7s4j9pok-2>li:before{content:"\0025a0  "}.lst-kix_cy2og2rt03zo-1>li:before{content:"\0025cb  "}.lst-kix_klzhllb6xcyr-2>li:before{content:"\0025a0  "}ul.lst-kix_8swzovi1ytnr-0{list-style-type:none}ul.lst-kix_8swzovi1ytnr-3{list-style-type:none}ul.lst-kix_8swzovi1ytnr-4{list-style-type:none}.lst-kix_ydb53brft35n-4>li:before{content:"\0025cb  "}ul.lst-kix_8swzovi1ytnr-1{list-style-type:none}.lst-kix_17is42bhveph-1>li:before{content:"\0025cb  "}ul.lst-kix_8swzovi1ytnr-2{list-style-type:none}.lst-kix_7jzvzpyn9bnk-3>li:before{content:"\0025cf  "}.lst-kix_gr1y63l14uay-3>li:before{content:"-  "}.lst-kix_ta3ofdl38yyn-8>li:before{content:"\0025a0  "}ul.lst-kix_o4d3wx7elf58-4{list-style-type:none}ul.lst-kix_o4d3wx7elf58-3{list-style-type:none}.lst-kix_2s6hp75gzisz-4>li:before{content:"\0025cb  "}ul.lst-kix_o4d3wx7elf58-2{list-style-type:none}ul.lst-kix_o4d3wx7elf58-1{list-style-type:none}ul.lst-kix_6r73oe35erxy-1{list-style-type:none}ul.lst-kix_o4d3wx7elf58-8{list-style-type:none}ul.lst-kix_6r73oe35erxy-2{list-style-type:none}ul.lst-kix_o4d3wx7elf58-7{list-style-type:none}ul.lst-kix_o4d3wx7elf58-6{list-style-type:none}ul.lst-kix_6r73oe35erxy-0{list-style-type:none}ul.lst-kix_o4d3wx7elf58-5{list-style-type:none}.lst-kix_60wjig3vq01-5>li:before{content:"\0025a0  "}.lst-kix_6k5yl8dbqqq2-7>li:before{content:"\0025cb  "}.lst-kix_r75b0bpeph0o-4>li:before{content:"\0025cb  "}.lst-kix_ta3ofdl38yyn-2>li:before{content:"\0025a0  "}ul.lst-kix_41ikkbtdotge-6{list-style-type:none}.lst-kix_2zf5bd1mem1t-4>li:before{content:"\0025cb  "}ul.lst-kix_41ikkbtdotge-7{list-style-type:none}ul.lst-kix_41ikkbtdotge-4{list-style-type:none}ul.lst-kix_41ikkbtdotge-5{list-style-type:none}.lst-kix_vt0etbgrzlaw-2>li:before{content:"\0025a0  "}.lst-kix_kxpudurk2j6e-6>li:before{content:"\0025cf  "}ul.lst-kix_41ikkbtdotge-2{list-style-type:none}ul.lst-kix_41ikkbtdotge-3{list-style-type:none}ul.lst-kix_41ikkbtdotge-0{list-style-type:none}.lst-kix_8fyyex2aqhl9-5>li:before{content:"\0025a0  "}ul.lst-kix_41ikkbtdotge-1{list-style-type:none}.lst-kix_bjneibxfrx3r-3>li:before{content:"\0025cf  "}.lst-kix_gnzb8ea0t50l-0>li:before{content:"\0025cf  "}ul.lst-kix_6r73oe35erxy-5{list-style-type:none}ul.lst-kix_6r73oe35erxy-6{list-style-type:none}ul.lst-kix_6r73oe35erxy-3{list-style-type:none}ul.lst-kix_6r73oe35erxy-4{list-style-type:none}ul.lst-kix_o4d3wx7elf58-0{list-style-type:none}ul.lst-kix_6r73oe35erxy-7{list-style-type:none}ul.lst-kix_6r73oe35erxy-8{list-style-type:none}.lst-kix_crt5jd7c2xq0-2>li:before{content:"\0025a0  "}.lst-kix_vt0etbgrzlaw-8>li:before{content:"\0025a0  "}ul.lst-kix_41ikkbtdotge-8{list-style-type:none}.lst-kix_8k70e8i8rdlm-8>li:before{content:"\0025a0  "}ul.lst-kix_60wjig3vq01-8{list-style-type:none}ul.lst-kix_60wjig3vq01-7{list-style-type:none}ul.lst-kix_60wjig3vq01-6{list-style-type:none}.lst-kix_369j2eishmb5-1>li:before{content:"\0025cb  "}ul.lst-kix_60wjig3vq01-5{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-7{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-8{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-5{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-6{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-3{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-4{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-1{list-style-type:none}ul.lst-kix_mgyoyquoxrm1-2{list-style-type:none}.lst-kix_g0l7lf51hdyf-7>li:before{content:"\0025cb  "}.lst-kix_4fwhm2tiefhz-0>li:before{content:"\0025cf  "}.lst-kix_wq4pisfqksnu-6>li:before{content:"\0025cf  "}ul.lst-kix_60wjig3vq01-4{list-style-type:none}.lst-kix_8k70e8i8rdlm-5>li:before{content:"\0025a0  "}.lst-kix_1a8ua5u4a26p-7>li:before{content:"\0025cb  "}ul.lst-kix_60wjig3vq01-3{list-style-type:none}ul.lst-kix_60wjig3vq01-2{list-style-type:none}ul.lst-kix_60wjig3vq01-1{list-style-type:none}ul.lst-kix_60wjig3vq01-0{list-style-type:none}.lst-kix_l3zoug7b3vcw-2>li:before{content:"\0025a0  "}.lst-kix_pzunwido5p11-0>li:before{content:"\0025cf  "}.lst-kix_6k5yl8dbqqq2-1>li:before{content:"\0025cb  "}ul.lst-kix_mgyoyquoxrm1-0{list-style-type:none}.lst-kix_w14ild1qeoey-6>li:before{content:"\0025cf  "}.lst-kix_369j2eishmb5-4>li:before{content:"\0025cb  "}.lst-kix_x2mjmu7myisq-8>li:before{content:"\0025a0  "}.lst-kix_gr1y63l14uay-0>li:before{content:"-  "}.lst-kix_swbzestlepme-7>li:before{content:"\0025cb  "}.lst-kix_z1vxl485jzzo-0>li:before{content:"\0025cf  "}.lst-kix_bj6mzrhm69bi-4>li:before{content:"\0025cb  "}.lst-kix_z1vxl485jzzo-3>li:before{content:"\0025cf  "}.lst-kix_solhl6ysg65v-8>li:before{content:"\0025a0  "}.lst-kix_6yojqqjec0st-5>li:before{content:"\0025a0  "}.lst-kix_6yojqqjec0st-2>li:before{content:"\0025a0  "}.lst-kix_l3zoug7b3vcw-5>li:before{content:"\0025a0  "}.lst-kix_l1rt7mvaz29b-6>li:before{content:"\0025cf  "}ul.lst-kix_u6q708mtjbvx-4{list-style-type:none}ul.lst-kix_u6q708mtjbvx-3{list-style-type:none}ul.lst-kix_u6q708mtjbvx-2{list-style-type:none}ul.lst-kix_u6q708mtjbvx-1{list-style-type:none}ul.lst-kix_u6q708mtjbvx-8{list-style-type:none}ul.lst-kix_u6q708mtjbvx-7{list-style-type:none}.lst-kix_1a8ua5u4a26p-4>li:before{content:"\0025cb  "}ul.lst-kix_u6q708mtjbvx-6{list-style-type:none}ul.lst-kix_u6q708mtjbvx-5{list-style-type:none}.lst-kix_l1rt7mvaz29b-3>li:before{content:"\0025cf  "}ul.lst-kix_47kviebmmd71-1{list-style-type:none}ul.lst-kix_u6q708mtjbvx-0{list-style-type:none}ul.lst-kix_47kviebmmd71-2{list-style-type:none}ul.lst-kix_47kviebmmd71-0{list-style-type:none}ul.lst-kix_47kviebmmd71-5{list-style-type:none}ul.lst-kix_47kviebmmd71-6{list-style-type:none}ul.lst-kix_47kviebmmd71-3{list-style-type:none}ul.lst-kix_47kviebmmd71-4{list-style-type:none}ul.lst-kix_47kviebmmd71-7{list-style-type:none}ul.lst-kix_47kviebmmd71-8{list-style-type:none}.lst-kix_6asjiojyuog6-7>li:before{content:"\0025cb  "}.lst-kix_uk8k6pzec0np-7>li:before{content:"\0025cb  "}.lst-kix_o38l9c2rqk2p-7>li:before{content:"\0025cb  "}ul.lst-kix_gsefpdxx2m31-8{list-style-type:none}ul.lst-kix_gsefpdxx2m31-7{list-style-type:none}.lst-kix_yhn6vvmwi8p2-3>li:before{content:"\0025cf  "}.lst-kix_crt5jd7c2xq0-5>li:before{content:"\0025a0  "}ul.lst-kix_gnt40hozc3kv-6{list-style-type:none}ul.lst-kix_gnt40hozc3kv-7{list-style-type:none}ul.lst-kix_gnt40hozc3kv-4{list-style-type:none}ul.lst-kix_gnt40hozc3kv-5{list-style-type:none}.lst-kix_w1d9l4m1yg5x-5>li:before{content:"\0025a0  "}.lst-kix_yhmx20qr5k0e-7>li:before{content:"\0025cb  "}.lst-kix_7r3huctovklw-6>li:before{content:"\0025cf  "}ul.lst-kix_gnt40hozc3kv-8{list-style-type:none}.lst-kix_x5zq7qqqztpc-3>li:before{content:"\0025cf  "}.lst-kix_iqbmivscsmun-8>li:before{content:"\0025a0  "}.lst-kix_xkmlxx8lhm5d-1>li:before{content:"\0025cb  "}ul.lst-kix_gnt40hozc3kv-2{list-style-type:none}ul.lst-kix_gnt40hozc3kv-3{list-style-type:none}ul.lst-kix_gnt40hozc3kv-0{list-style-type:none}ul.lst-kix_gnt40hozc3kv-1{list-style-type:none}.lst-kix_bjneibxfrx3r-6>li:before{content:"\0025cf  "}.lst-kix_klzhllb6xcyr-5>li:before{content:"\0025a0  "}.lst-kix_mh0uhwysa91-2>li:before{content:"\0025a0  "}.lst-kix_a9t5z6hxg4oz-5>li:before{content:"\0025a0  "}.lst-kix_ydb53brft35n-1>li:before{content:"\0025cb  "}.lst-kix_f51q5t2m61i8-1>li:before{content:"\0025cb  "}.lst-kix_8wesot3ko53b-2>li:before{content:"\0025a0  "}.lst-kix_bj6mzrhm69bi-7>li:before{content:"\0025cb  "}.lst-kix_emy9ap8zeql8-7>li:before{content:"\0025cb  "}.lst-kix_68swn1licdyn-0>li:before{content:"\0025cf  "}.lst-kix_ta3ofdl38yyn-5>li:before{content:"\0025a0  "}.lst-kix_7jzvzpyn9bnk-6>li:before{content:"\0025cf  "}.lst-kix_ct57w2de70tb-1>li:before{content:"\0025cb  "}ul.lst-kix_riff65m65tnf-4{list-style-type:none}.lst-kix_8g8vlw6j0asg-6>li:before{content:"\0025cf  "}ul.lst-kix_riff65m65tnf-3{list-style-type:none}.lst-kix_r75b0bpeph0o-1>li:before{content:"\0025cb  "}ul.lst-kix_riff65m65tnf-6{list-style-type:none}ul.lst-kix_riff65m65tnf-5{list-style-type:none}ul.lst-kix_riff65m65tnf-8{list-style-type:none}ul.lst-kix_riff65m65tnf-7{list-style-type:none}.lst-kix_aui8m1kby916-4>li:before{content:"\0025cb  "}.lst-kix_62is116n17wq-0>li:before{content:"\0025cf  "}ul.lst-kix_riff65m65tnf-0{list-style-type:none}.lst-kix_6k5yl8dbqqq2-4>li:before{content:"\0025cb  "}.lst-kix_3de1jg1kuy24-5>li:before{content:"\0025a0  "}ul.lst-kix_riff65m65tnf-2{list-style-type:none}ul.lst-kix_riff65m65tnf-1{list-style-type:none}.lst-kix_8fyyex2aqhl9-2>li:before{content:"\0025a0  "}.lst-kix_p2mkzdwo7q6r-0>li:before{content:"\0025cf  "}.lst-kix_iigck7esqel5-1>li:before{content:"\0025cb  "}.lst-kix_vt0etbgrzlaw-5>li:before{content:"\0025a0  "}.lst-kix_2zf5bd1mem1t-7>li:before{content:"\0025cb  "}.lst-kix_d995nc7613a9-2>li:before{content:"\0025a0  "}ul.lst-kix_bcdpfzfy81p5-4{list-style-type:none}ul.lst-kix_bcdpfzfy81p5-3{list-style-type:none}.lst-kix_2s6hp75gzisz-7>li:before{content:"\0025cb  "}ul.lst-kix_bcdpfzfy81p5-6{list-style-type:none}ul.lst-kix_bcdpfzfy81p5-5{list-style-type:none}ul.lst-kix_bcdpfzfy81p5-0{list-style-type:none}ul.lst-kix_bcdpfzfy81p5-2{list-style-type:none}ul.lst-kix_bcdpfzfy81p5-1{list-style-type:none}.lst-kix_6jabh0roz5s1-4>li:before{content:"\0025cb  "}ul.lst-kix_bcdpfzfy81p5-8{list-style-type:none}ul.lst-kix_bcdpfzfy81p5-7{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-3{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-5{list-style-type:none}.lst-kix_m46mj27r3fpz-1>li:before{content:"\0025cb  "}ul.lst-kix_vjhjbjs9ubca-2{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-6{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-5{list-style-type:none}ul.lst-kix_emewkhtp1rfy-1{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-7{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-4{list-style-type:none}ul.lst-kix_emewkhtp1rfy-0{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-8{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-7{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-1{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-6{list-style-type:none}.lst-kix_6r73oe35erxy-2>li:before{content:"\0025a0  "}ul.lst-kix_i0z4lx8deqnq-2{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-3{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-8{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-4{list-style-type:none}ul.lst-kix_emewkhtp1rfy-7{list-style-type:none}.lst-kix_rcp8sez4svjd-4>li:before{content:"-  "}ul.lst-kix_emewkhtp1rfy-6{list-style-type:none}ul.lst-kix_emewkhtp1rfy-8{list-style-type:none}ul.lst-kix_i0z4lx8deqnq-0{list-style-type:none}ul.lst-kix_emewkhtp1rfy-3{list-style-type:none}.lst-kix_6r73oe35erxy-5>li:before{content:"\0025a0  "}ul.lst-kix_emewkhtp1rfy-2{list-style-type:none}ul.lst-kix_emewkhtp1rfy-5{list-style-type:none}ul.lst-kix_emewkhtp1rfy-4{list-style-type:none}ul.lst-kix_1qjopwyk3n75-4{list-style-type:none}ul.lst-kix_1qjopwyk3n75-5{list-style-type:none}ul.lst-kix_1qjopwyk3n75-2{list-style-type:none}ul.lst-kix_1qjopwyk3n75-3{list-style-type:none}.lst-kix_gogm0wicn297-2>li:before{content:"\0025a0  "}ul.lst-kix_1qjopwyk3n75-8{list-style-type:none}.lst-kix_sxaq135mthse-1>li:before{content:"\0025cb  "}ul.lst-kix_1qjopwyk3n75-6{list-style-type:none}ul.lst-kix_1qjopwyk3n75-7{list-style-type:none}ul.lst-kix_1qjopwyk3n75-0{list-style-type:none}ul.lst-kix_1qjopwyk3n75-1{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-1{list-style-type:none}ul.lst-kix_vjhjbjs9ubca-0{list-style-type:none}.lst-kix_gogm0wicn297-5>li:before{content:"\0025a0  "}.lst-kix_edj3r8hgf2z3-6>li:before{content:"\0025cf  "}ul.lst-kix_xkmlxx8lhm5d-5{list-style-type:none}ul.lst-kix_xkmlxx8lhm5d-6{list-style-type:none}ul.lst-kix_xkmlxx8lhm5d-3{list-style-type:none}.lst-kix_xav8ikvidc9s-3>li:before{content:"\0025cf  "}ul.lst-kix_xkmlxx8lhm5d-4{list-style-type:none}ul.lst-kix_xkmlxx8lhm5d-1{list-style-type:none}ul.lst-kix_xkmlxx8lhm5d-2{list-style-type:none}ul.lst-kix_xkmlxx8lhm5d-0{list-style-type:none}.lst-kix_rcp8sez4svjd-7>li:before{content:"-  "}ul.lst-kix_rrs2j0rau6wp-0{list-style-type:none}ul.lst-kix_rrs2j0rau6wp-2{list-style-type:none}.lst-kix_tg2hwffju8t9-8>li:before{content:"\0025a0  "}.lst-kix_5fj31eh1jc7-7>li:before{content:"\0025cb  "}ul.lst-kix_rrs2j0rau6wp-1{list-style-type:none}.lst-kix_b8dlmlw0p2p1-7>li:before{content:"\0025cb  "}.lst-kix_5fj31eh1jc7-4>li:before{content:"\0025cb  "}.lst-kix_o4ah8y8am4ct-0>li:before{content:"\0025cf  "}.lst-kix_b8dlmlw0p2p1-4>li:before{content:"\0025cb  "}.lst-kix_x3vcornhe3kv-7>li:before{content:"\0025cb  "}ul.lst-kix_qp3zt329w8t5-8{list-style-type:none}ul.lst-kix_qp3zt329w8t5-7{list-style-type:none}.lst-kix_tg2hwffju8t9-5>li:before{content:"\0025a0  "}ul.lst-kix_qp3zt329w8t5-4{list-style-type:none}ul.lst-kix_qp3zt329w8t5-3{list-style-type:none}ul.lst-kix_qp3zt329w8t5-6{list-style-type:none}ul.lst-kix_qp3zt329w8t5-5{list-style-type:none}ul.lst-kix_qp3zt329w8t5-0{list-style-type:none}.lst-kix_7apmk6vwlj5r-4>li:before{content:"\0025cb  "}.lst-kix_nbpnmy8xci8w-6>li:before{content:"\0025cf  "}.lst-kix_a9t5z6hxg4oz-8>li:before{content:"\0025a0  "}ul.lst-kix_qp3zt329w8t5-2{list-style-type:none}ul.lst-kix_qp3zt329w8t5-1{list-style-type:none}.lst-kix_nbpnmy8xci8w-3>li:before{content:"\0025cf  "}ul.lst-kix_rrs2j0rau6wp-8{list-style-type:none}ul.lst-kix_rrs2j0rau6wp-7{list-style-type:none}.lst-kix_x3vcornhe3kv-4>li:before{content:"\0025cb  "}ul.lst-kix_rrs2j0rau6wp-4{list-style-type:none}ul.lst-kix_rrs2j0rau6wp-3{list-style-type:none}.lst-kix_5kaugn46jpw6-0>li:before{content:"\0025cf  "}ul.lst-kix_rrs2j0rau6wp-6{list-style-type:none}ul.lst-kix_rrs2j0rau6wp-5{list-style-type:none}.lst-kix_8hjy0hqh3rvg-7>li:before{content:"\0025cb  "}.lst-kix_kk8oms3v3iyb-6>li:before{content:"\0025cf  "}ul.lst-kix_hydge7xfbt8r-0{list-style-type:none}.lst-kix_ho1wvdl93240-2>li:before{content:"\0025a0  "}.lst-kix_z1olrl2brpzr-8>li:before{content:"\0025a0  "}ul.lst-kix_lyxc6qcmvco2-2{list-style-type:none}.lst-kix_6ykz6yxb3o6x-2>li:before{content:"\0025a0  "}.lst-kix_cq59e9nktsby-8>li:before{content:"\0025a0  "}ul.lst-kix_lyxc6qcmvco2-3{list-style-type:none}ul.lst-kix_lyxc6qcmvco2-0{list-style-type:none}ul.lst-kix_lyxc6qcmvco2-1{list-style-type:none}.lst-kix_ymmukz9l2n7v-0>li:before{content:"\0025cf  "}.lst-kix_cq59e9nktsby-2>li:before{content:"\0025a0  "}.lst-kix_x5zq7qqqztpc-6>li:before{content:"\0025cf  "}.lst-kix_n21asioviqe4-8>li:before{content:"\0025a0  "}ul.lst-kix_lyxc6qcmvco2-8{list-style-type:none}.lst-kix_wdjiiwxyq2d1-2>li:before{content:"\0025a0  "}ul.lst-kix_lyxc6qcmvco2-6{list-style-type:none}.lst-kix_ucprmx270ar2-4>li:before{content:"\0025cb  "}ul.lst-kix_lyxc6qcmvco2-7{list-style-type:none}ul.lst-kix_lyxc6qcmvco2-4{list-style-type:none}ul.lst-kix_lyxc6qcmvco2-5{list-style-type:none}.lst-kix_saoi6g81wrq2-0>li:before{content:"\0025cf  "}.lst-kix_ymmukz9l2n7v-6>li:before{content:"\0025cf  "}.lst-kix_6asjiojyuog6-4>li:before{content:"\0025cb  "}ul.lst-kix_xqfh3tf7tjc0-0{list-style-type:none}.lst-kix_4ykl6tjv6vdc-4>li:before{content:"\0025cb  "}ul.lst-kix_xqfh3tf7tjc0-6{list-style-type:none}ul.lst-kix_xqfh3tf7tjc0-5{list-style-type:none}ul.lst-kix_xqfh3tf7tjc0-8{list-style-type:none}.lst-kix_ulntv5wergyk-1>li:before{content:"\0025cb  "}ul.lst-kix_xqfh3tf7tjc0-7{list-style-type:none}ul.lst-kix_xqfh3tf7tjc0-2{list-style-type:none}ul.lst-kix_xqfh3tf7tjc0-1{list-style-type:none}.lst-kix_7apmk6vwlj5r-1>li:before{content:"\0025cb  "}ul.lst-kix_xqfh3tf7tjc0-4{list-style-type:none}ul.lst-kix_xqfh3tf7tjc0-3{list-style-type:none}ul.lst-kix_hydge7xfbt8r-8{list-style-type:none}ul.lst-kix_hydge7xfbt8r-7{list-style-type:none}ul.lst-kix_hydge7xfbt8r-6{list-style-type:none}ul.lst-kix_hydge7xfbt8r-5{list-style-type:none}ul.lst-kix_hydge7xfbt8r-4{list-style-type:none}ul.lst-kix_hydge7xfbt8r-3{list-style-type:none}ul.lst-kix_hydge7xfbt8r-2{list-style-type:none}.lst-kix_z1olrl2brpzr-2>li:before{content:"\0025a0  "}ul.lst-kix_hydge7xfbt8r-1{list-style-type:none}.lst-kix_3odao55mezu-2>li:before{content:"\0025a0  "}.lst-kix_ukuyvb7r2sb4-7>li:before{content:"\0025cb  "}.lst-kix_313q3f5x4p2m-5>li:before{content:"\0025a0  "}.lst-kix_470xpz4q6hoi-2>li:before{content:"\0025a0  "}.lst-kix_3de1jg1kuy24-2>li:before{content:"\0025a0  "}.lst-kix_xav8ikvidc9s-6>li:before{content:"\0025cf  "}.lst-kix_8g8vlw6j0asg-3>li:before{content:"\0025cf  "}.lst-kix_nxvbdas3q36w-6>li:before{content:"\0025cf  "}.lst-kix_x2mjmu7myisq-5>li:before{content:"\0025a0  "}.lst-kix_aui8m1kby916-7>li:before{content:"\0025cb  "}ul.lst-kix_gogm0wicn297-8{list-style-type:none}ul.lst-kix_gogm0wicn297-7{list-style-type:none}ul.lst-kix_gogm0wicn297-6{list-style-type:none}ul.lst-kix_gogm0wicn297-5{list-style-type:none}.lst-kix_68swn1licdyn-3>li:before{content:"\0025cf  "}.lst-kix_crb89j7xw8q2-0>li:before{content:"\0025cf  "}ul.lst-kix_gogm0wicn297-4{list-style-type:none}ul.lst-kix_gogm0wicn297-3{list-style-type:none}ul.lst-kix_gogm0wicn297-2{list-style-type:none}ul.lst-kix_gogm0wicn297-1{list-style-type:none}ul.lst-kix_gogm0wicn297-0{list-style-type:none}.lst-kix_u6q708mtjbvx-3>li:before{content:"-  "}.lst-kix_nxvbdas3q36w-0>li:before{content:"\0025cf  "}.lst-kix_o14fwownm59e-0>li:before{content:"\0025cf  "}.lst-kix_470xpz4q6hoi-8>li:before{content:"\0025a0  "}.lst-kix_6r73oe35erxy-8>li:before{content:"\0025a0  "}.lst-kix_3odao55mezu-8>li:before{content:"\0025a0  "}.lst-kix_6jabh0roz5s1-1>li:before{content:"\0025cb  "}.lst-kix_o14fwownm59e-6>li:before{content:"\0025cf  "}.lst-kix_iigck7esqel5-4>li:before{content:"\0025cb  "}.lst-kix_saoi6g81wrq2-6>li:before{content:"\0025cf  "}ul.lst-kix_z1voh5a7dove-8{list-style-type:none}.lst-kix_nc0l9nyccr86-2>li:before{content:"\0025a0  "}ul.lst-kix_z1voh5a7dove-4{list-style-type:none}ul.lst-kix_z1voh5a7dove-5{list-style-type:none}ul.lst-kix_z1voh5a7dove-6{list-style-type:none}ul.lst-kix_z1voh5a7dove-7{list-style-type:none}.lst-kix_kxfg6pdeancy-5>li:before{content:"\0025a0  "}.lst-kix_6d5ax9c8k1xa-5>li:before{content:"\0025a0  "}.lst-kix_scilllbfudea-0>li:before{content:"\0025cf  "}.lst-kix_369j2eishmb5-7>li:before{content:"\0025cb  "}.lst-kix_emewkhtp1rfy-2>li:before{content:"\0025a0  "}ul.lst-kix_rwfcya85pd71-8{list-style-type:none}ul.lst-kix_rwfcya85pd71-7{list-style-type:none}.lst-kix_8k70e8i8rdlm-2>li:before{content:"\0025a0  "}ul.lst-kix_rwfcya85pd71-6{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-0{list-style-type:none}.lst-kix_vebf56ofbjcp-7>li:before{content:"\0025cb  "}ul.lst-kix_rwfcya85pd71-5{list-style-type:none}ul.lst-kix_rwfcya85pd71-4{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-2{list-style-type:none}ul.lst-kix_z1voh5a7dove-0{list-style-type:none}ul.lst-kix_rwfcya85pd71-3{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-1{list-style-type:none}ul.lst-kix_z1voh5a7dove-1{list-style-type:none}ul.lst-kix_rwfcya85pd71-2{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-4{list-style-type:none}ul.lst-kix_z1voh5a7dove-2{list-style-type:none}ul.lst-kix_rwfcya85pd71-1{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-3{list-style-type:none}ul.lst-kix_z1voh5a7dove-3{list-style-type:none}ul.lst-kix_rwfcya85pd71-0{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-0{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-6{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-1{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-5{list-style-type:none}ul.lst-kix_p2mkzdwo7q6r-8{list-style-type:none}.lst-kix_b07y002136mr-1>li:before{content:"\0025cb  "}ul.lst-kix_p2mkzdwo7q6r-7{list-style-type:none}.lst-kix_1xoigud6ruor-6>li:before{content:"\0025cf  "}ul.lst-kix_6d5ax9c8k1xa-4{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-5{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-2{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-3{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-8{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-6{list-style-type:none}ul.lst-kix_6d5ax9c8k1xa-7{list-style-type:none}.lst-kix_1xoigud6ruor-3>li:before{content:"\0025cf  "}.lst-kix_ukuyvb7r2sb4-1>li:before{content:"\0025cb  "}.lst-kix_jhqr8d48vqqp-1>li:before{content:"\0025cb  "}.lst-kix_w14ild1qeoey-3>li:before{content:"\0025cf  "}.lst-kix_2ihanzwz4fgf-1>li:before{content:"\0025cb  "}.lst-kix_6yojqqjec0st-8>li:before{content:"\0025a0  "}.lst-kix_swbzestlepme-4>li:before{content:"\0025cb  "}.lst-kix_swbzestlepme-1>li:before{content:"\0025cb  "}.lst-kix_xrce7kijh7l1-7>li:before{content:"\0025cb  "}.lst-kix_l3zoug7b3vcw-8>li:before{content:"\0025a0  "}.lst-kix_d8ik6bmbwuj4-6>li:before{content:"\0025cf  "}ul.lst-kix_u82x83buy4jv-0{list-style-type:none}ul.lst-kix_u82x83buy4jv-2{list-style-type:none}.lst-kix_d1iyykra6bco-7>li:before{content:"\0025cb  "}ul.lst-kix_u82x83buy4jv-1{list-style-type:none}.lst-kix_d2ysoruvczg5-2>li:before{content:"\0025a0  "}.lst-kix_ho1wvdl93240-8>li:before{content:"\0025a0  "}.lst-kix_6d5ax9c8k1xa-8>li:before{content:"\0025a0  "}.lst-kix_1a8ua5u4a26p-1>li:before{content:"\0025cb  "}.lst-kix_rwfcya85pd71-7>li:before{content:"\0025cb  "}.lst-kix_kxfg6pdeancy-2>li:before{content:"\0025a0  "}.lst-kix_l1rt7mvaz29b-0>li:before{content:"\0025cf  "}.lst-kix_ymmukz9l2n7v-3>li:before{content:"\0025cf  "}.lst-kix_cq59e9nktsby-5>li:before{content:"\0025a0  "}.lst-kix_e2bt8y2ywvrz-8>li:before{content:"\0025a0  "}.lst-kix_ho1wvdl93240-5>li:before{content:"\0025a0  "}.lst-kix_z1olrl2brpzr-5>li:before{content:"\0025a0  "}.lst-kix_s3fvwoba1t7n-0>li:before{content:"\0025cf  "}.lst-kix_7r3huctovklw-0>li:before{content:"\0025cf  "}.lst-kix_58o57c52tmut-2>li:before{content:"\0025a0  "}ul.lst-kix_u82x83buy4jv-8{list-style-type:none}.lst-kix_ucprmx270ar2-7>li:before{content:"\0025cb  "}ul.lst-kix_u82x83buy4jv-7{list-style-type:none}.lst-kix_6asjiojyuog6-1>li:before{content:"\0025cb  "}ul.lst-kix_u82x83buy4jv-4{list-style-type:none}ul.lst-kix_u82x83buy4jv-3{list-style-type:none}ul.lst-kix_u82x83buy4jv-6{list-style-type:none}ul.lst-kix_u82x83buy4jv-5{list-style-type:none}.lst-kix_79uewz4g8df3-7>li:before{content:"\0025cb  "}.lst-kix_gnzb8ea0t50l-3>li:before{content:"\0025cf  "}.lst-kix_4ykl6tjv6vdc-1>li:before{content:"\0025cb  "}ul.lst-kix_uk8k6pzec0np-8{list-style-type:none}ul.lst-kix_uk8k6pzec0np-7{list-style-type:none}ul.lst-kix_uk8k6pzec0np-6{list-style-type:none}ul.lst-kix_uk8k6pzec0np-5{list-style-type:none}ul.lst-kix_uk8k6pzec0np-4{list-style-type:none}.lst-kix_vrgigcglf3we-7>li:before{content:"\0025cb  "}ul.lst-kix_z0c6lix981m-0{list-style-type:none}.lst-kix_17is42bhveph-4>li:before{content:"\0025cb  "}ul.lst-kix_z0c6lix981m-1{list-style-type:none}ul.lst-kix_z0c6lix981m-2{list-style-type:none}ul.lst-kix_z0c6lix981m-3{list-style-type:none}ul.lst-kix_z0c6lix981m-4{list-style-type:none}ul.lst-kix_z0c6lix981m-5{list-style-type:none}ul.lst-kix_z0c6lix981m-6{list-style-type:none}ul.lst-kix_z0c6lix981m-7{list-style-type:none}.lst-kix_n8ye7s4j9pok-5>li:before{content:"\0025a0  "}ul.lst-kix_z0c6lix981m-8{list-style-type:none}.lst-kix_8g8vlw6j0asg-0>li:before{content:"\0025cf  "}.lst-kix_60wjig3vq01-8>li:before{content:"\0025a0  "}.lst-kix_313q3f5x4p2m-8>li:before{content:"\0025a0  "}.lst-kix_ukuyvb7r2sb4-4>li:before{content:"\0025cb  "}.lst-kix_ftgmvnn66fx0-8>li:before{content:"\0025a0  "}.lst-kix_470xpz4q6hoi-5>li:before{content:"\0025a0  "}.lst-kix_w14ild1qeoey-0>li:before{content:"\0025cf  "}ul.lst-kix_xkmlxx8lhm5d-7{list-style-type:none}.lst-kix_i0z4lx8deqnq-5>li:before{content:"\0025a0  "}.lst-kix_68swn1licdyn-6>li:before{content:"\0025cf  "}ul.lst-kix_xkmlxx8lhm5d-8{list-style-type:none}ul.lst-kix_uk8k6pzec0np-3{list-style-type:none}.lst-kix_4yao0efkrd31-8>li:before{content:"\0025a0  "}ul.lst-kix_uk8k6pzec0np-2{list-style-type:none}ul.lst-kix_uk8k6pzec0np-1{list-style-type:none}ul.lst-kix_uk8k6pzec0np-0{list-style-type:none}.lst-kix_gr1y63l14uay-6>li:before{content:"-  "}.lst-kix_x2mjmu7myisq-2>li:before{content:"\0025a0  "}.lst-kix_b07y002136mr-4>li:before{content:"\0025cb  "}.lst-kix_8nsyajlscxcg-1>li:before{content:"\0025cb  "}.lst-kix_yhmx20qr5k0e-1>li:before{content:"\0025cb  "}.lst-kix_saoi6g81wrq2-3>li:before{content:"\0025cf  "}ul.lst-kix_egb7bfw0xx0q-8{list-style-type:none}.lst-kix_uk8k6pzec0np-1>li:before{content:"\0025cb  "}.lst-kix_u6q708mtjbvx-6>li:before{content:"-  "}.lst-kix_iigck7esqel5-7>li:before{content:"\0025cb  "}ul.lst-kix_egb7bfw0xx0q-4{list-style-type:none}ul.lst-kix_egb7bfw0xx0q-5{list-style-type:none}.lst-kix_bjneibxfrx3r-0>li:before{content:"\0025cf  "}ul.lst-kix_egb7bfw0xx0q-6{list-style-type:none}ul.lst-kix_egb7bfw0xx0q-7{list-style-type:none}ul.lst-kix_egb7bfw0xx0q-0{list-style-type:none}.lst-kix_3odao55mezu-5>li:before{content:"\0025a0  "}ul.lst-kix_egb7bfw0xx0q-1{list-style-type:none}ul.lst-kix_egb7bfw0xx0q-2{list-style-type:none}ul.lst-kix_egb7bfw0xx0q-3{list-style-type:none}.lst-kix_o14fwownm59e-3>li:before{content:"\0025cf  "}.lst-kix_nxvbdas3q36w-3>li:before{content:"\0025cf  "}.lst-kix_qp3zt329w8t5-2>li:before{content:"\0025a0  "}.lst-kix_dowwk88w0dao-0>li:before{content:"\0025cf  "}.lst-kix_qp3zt329w8t5-6>li:before{content:"\0025cf  "}.lst-kix_1qjopwyk3n75-3>li:before{content:"\0025cf  "}.lst-kix_ltwr0s349me7-5>li:before{content:"\0025a0  "}.lst-kix_bj2se7l3xxv-1>li:before{content:"\0025cb  "}.lst-kix_dowwk88w0dao-4>li:before{content:"\0025cb  "}.lst-kix_hl5oa0f6tf4f-3>li:before{content:"\0025cf  "}.lst-kix_hl5oa0f6tf4f-7>li:before{content:"\0025cb  "}ul.lst-kix_x3vcornhe3kv-4{list-style-type:none}ul.lst-kix_x3vcornhe3kv-3{list-style-type:none}ul.lst-kix_x3vcornhe3kv-6{list-style-type:none}ul.lst-kix_x3vcornhe3kv-5{list-style-type:none}ul.lst-kix_x3vcornhe3kv-8{list-style-type:none}ul.lst-kix_x3vcornhe3kv-7{list-style-type:none}.lst-kix_gsefpdxx2m31-7>li:before{content:"\0025cb  "}.lst-kix_kk7o9gi3ij0l-0>li:before{content:"\0025cf  "}.lst-kix_acuhah6xdzsp-1>li:before{content:"\0025cb  "}.lst-kix_gsefpdxx2m31-3>li:before{content:"\0025cf  "}.lst-kix_axhbb6oyblrk-8>li:before{content:"\0025a0  "}ul.lst-kix_ehe7zglmx7i2-8{list-style-type:none}ul.lst-kix_ehe7zglmx7i2-7{list-style-type:none}ul.lst-kix_ehe7zglmx7i2-6{list-style-type:none}.lst-kix_h4ky6k6cvoue-0>li:before{content:"\0025cf  "}ul.lst-kix_ehe7zglmx7i2-5{list-style-type:none}ul.lst-kix_ehe7zglmx7i2-4{list-style-type:none}ul.lst-kix_ehe7zglmx7i2-3{list-style-type:none}ul.lst-kix_ehe7zglmx7i2-2{list-style-type:none}.lst-kix_acuhah6xdzsp-5>li:before{content:"\0025a0  "}ul.lst-kix_ehe7zglmx7i2-1{list-style-type:none}ul.lst-kix_ehe7zglmx7i2-0{list-style-type:none}.lst-kix_bj2se7l3xxv-5>li:before{content:"\0025a0  "}.lst-kix_7qzrf8aug8gu-8>li:before{content:"\0025a0  "}.lst-kix_2mto5juffctz-1>li:before{content:"\0025cb  "}.lst-kix_pfssn45qm13m-3>li:before{content:"\0025cf  "}.lst-kix_uest4hng52tf-1>li:before{content:"\0025cb  "}.lst-kix_uest4hng52tf-5>li:before{content:"\0025a0  "}.lst-kix_7qzrf8aug8gu-4>li:before{content:"\0025cb  "}.lst-kix_2mto5juffctz-5>li:before{content:"\0025a0  "}.lst-kix_axhbb6oyblrk-0>li:before{content:"\0025cf  "}.lst-kix_h4ky6k6cvoue-8>li:before{content:"\0025a0  "}ul.lst-kix_3xkj88gevxle-1{list-style-type:none}ul.lst-kix_3xkj88gevxle-0{list-style-type:none}ul.lst-kix_3xkj88gevxle-3{list-style-type:none}ul.lst-kix_3xkj88gevxle-2{list-style-type:none}ul.lst-kix_ut5bvwbkq6rt-1{list-style-type:none}ul.lst-kix_ut5bvwbkq6rt-0{list-style-type:none}ul.lst-kix_ut5bvwbkq6rt-3{list-style-type:none}.lst-kix_pfssn45qm13m-7>li:before{content:"\0025cb  "}ul.lst-kix_ut5bvwbkq6rt-2{list-style-type:none}.lst-kix_tlb25jkk5mkd-2>li:before{content:"\0025a0  "}.lst-kix_axhbb6oyblrk-4>li:before{content:"\0025cb  "}.lst-kix_qwp9c5bklrce-3>li:before{content:"\0025cf  "}.lst-kix_h4ky6k6cvoue-4>li:before{content:"\0025cb  "}.lst-kix_rbhmpzgzmhdz-7>li:before{content:"\0025cb  "}.lst-kix_3xkj88gevxle-7>li:before{content:"\0025cb  "}ul.lst-kix_x3vcornhe3kv-0{list-style-type:none}.lst-kix_qhrkxnqxxds4-3>li:before{content:"\0025cf  "}.lst-kix_qhrkxnqxxds4-7>li:before{content:"\0025cb  "}.lst-kix_uz5smpj76010-5>li:before{content:"\0025a0  "}ul.lst-kix_x3vcornhe3kv-2{list-style-type:none}ul.lst-kix_x3vcornhe3kv-1{list-style-type:none}.lst-kix_gg06s1qsfn23-2>li:before{content:"\0025a0  "}.lst-kix_qwp9c5bklrce-7>li:before{content:"\0025cb  "}.lst-kix_7qzrf8aug8gu-0>li:before{content:"\0025cf  "}.lst-kix_gg06s1qsfn23-6>li:before{content:"\0025cf  "}.lst-kix_3fnpvvr0i8os-6>li:before{content:"\0025cf  "}.lst-kix_rbhmpzgzmhdz-3>li:before{content:"\0025cf  "}.lst-kix_ltwr0s349me7-1>li:before{content:"\0025cb  "}.lst-kix_3fnpvvr0i8os-2>li:before{content:"\0025a0  "}.lst-kix_3xkj88gevxle-3>li:before{content:"\0025cf  "}.lst-kix_xgfpveff5zr0-3>li:before{content:"\0025cf  "}.lst-kix_pmg6qjdagav9-6>li:before{content:"\0025cf  "}.lst-kix_gc2knt874en1-5>li:before{content:"\0025a0  "}.lst-kix_rwqhz1uwdird-1>li:before{content:"\0025cb  "}.lst-kix_e2bt8y2ywvrz-1>li:before{content:"\0025cb  "}ul.lst-kix_2s6hp75gzisz-0{list-style-type:none}ul.lst-kix_2s6hp75gzisz-2{list-style-type:none}ul.lst-kix_2s6hp75gzisz-1{list-style-type:none}ul.lst-kix_2s6hp75gzisz-4{list-style-type:none}ul.lst-kix_2s6hp75gzisz-3{list-style-type:none}ul.lst-kix_2s6hp75gzisz-6{list-style-type:none}ul.lst-kix_2s6hp75gzisz-5{list-style-type:none}.lst-kix_pmg6qjdagav9-2>li:before{content:"\0025a0  "}ul.lst-kix_2s6hp75gzisz-8{list-style-type:none}ul.lst-kix_2s6hp75gzisz-7{list-style-type:none}.lst-kix_p0fqpwscumob-3>li:before{content:"\0025cf  "}.lst-kix_qsdpvqxg1u78-2>li:before{content:"\0025a0  "}.lst-kix_qsdpvqxg1u78-6>li:before{content:"\0025cf  "}.lst-kix_p0fqpwscumob-7>li:before{content:"\0025cb  "}.lst-kix_mm2j3zumujcl-0>li:before{content:"\0025cf  "}.lst-kix_vrgigcglf3we-6>li:before{content:"\0025cf  "}.lst-kix_z4z7nvld905a-0>li:before{content:"\0025cf  "}.lst-kix_i0z4lx8deqnq-8>li:before{content:"\0025a0  "}.lst-kix_rwqhz1uwdird-5>li:before{content:"\0025a0  "}.lst-kix_e2bt8y2ywvrz-5>li:before{content:"\0025a0  "}.lst-kix_ftgmvnn66fx0-3>li:before{content:"\0025cf  "}.lst-kix_mm2j3zumujcl-8>li:before{content:"\0025a0  "}ul.lst-kix_vebf56ofbjcp-7{list-style-type:none}.lst-kix_mm2j3zumujcl-4>li:before{content:"\0025cb  "}.lst-kix_ftgmvnn66fx0-7>li:before{content:"\0025cb  "}ul.lst-kix_vebf56ofbjcp-8{list-style-type:none}ul.lst-kix_vebf56ofbjcp-5{list-style-type:none}ul.lst-kix_vebf56ofbjcp-6{list-style-type:none}ul.lst-kix_vebf56ofbjcp-3{list-style-type:none}.lst-kix_7q22x2z5v5y-7>li:before{content:"\0025cb  "}ul.lst-kix_vebf56ofbjcp-4{list-style-type:none}ul.lst-kix_vebf56ofbjcp-1{list-style-type:none}ul.lst-kix_vebf56ofbjcp-2{list-style-type:none}.lst-kix_474t6llskbh7-6>li:before{content:"\0025cf  "}.lst-kix_i0z4lx8deqnq-4>li:before{content:"\0025cb  "}.lst-kix_4yao0efkrd31-7>li:before{content:"\0025cb  "}.lst-kix_vrgigcglf3we-2>li:before{content:"\0025a0  "}.lst-kix_i0z4lx8deqnq-0>li:before{content:"\0025cf  "}.lst-kix_7q22x2z5v5y-3>li:before{content:"\0025cf  "}.lst-kix_474t6llskbh7-2>li:before{content:"\0025a0  "}.lst-kix_kk7o9gi3ij0l-4>li:before{content:"\0025cb  "}.lst-kix_4yao0efkrd31-3>li:before{content:"\0025cf  "}.lst-kix_1qjopwyk3n75-7>li:before{content:"\0025cb  "}.lst-kix_kk7o9gi3ij0l-8>li:before{content:"\0025a0  "}.lst-kix_dowwk88w0dao-8>li:before{content:"\0025a0  "}ul.lst-kix_2rp5aiyjkdpd-6{list-style-type:none}ul.lst-kix_2rp5aiyjkdpd-7{list-style-type:none}.lst-kix_16ygmg7toocv-6>li:before{content:"\0025cf  "}ul.lst-kix_2rp5aiyjkdpd-8{list-style-type:none}.lst-kix_jhqr8d48vqqp-0>li:before{content:"\0025cf  "}.lst-kix_emewkhtp1rfy-5>li:before{content:"\0025a0  "}.lst-kix_16ygmg7toocv-2>li:before{content:"\0025a0  "}.lst-kix_894copnhklgz-3>li:before{content:"\0025cf  "}.lst-kix_emewkhtp1rfy-1>li:before{content:"\0025cb  "}.lst-kix_b07y002136mr-2>li:before{content:"\0025a0  "}ul.lst-kix_j24rsp966ll1-3{list-style-type:none}ul.lst-kix_j24rsp966ll1-2{list-style-type:none}.lst-kix_jhqr8d48vqqp-8>li:before{content:"\0025a0  "}ul.lst-kix_j24rsp966ll1-5{list-style-type:none}ul.lst-kix_2rp5aiyjkdpd-0{list-style-type:none}ul.lst-kix_j24rsp966ll1-4{list-style-type:none}ul.lst-kix_2rp5aiyjkdpd-1{list-style-type:none}ul.lst-kix_2rp5aiyjkdpd-2{list-style-type:none}ul.lst-kix_2rp5aiyjkdpd-3{list-style-type:none}ul.lst-kix_j24rsp966ll1-1{list-style-type:none}ul.lst-kix_2rp5aiyjkdpd-4{list-style-type:none}ul.lst-kix_j24rsp966ll1-0{list-style-type:none}ul.lst-kix_2rp5aiyjkdpd-5{list-style-type:none}ul.lst-kix_j24rsp966ll1-7{list-style-type:none}ul.lst-kix_j24rsp966ll1-6{list-style-type:none}.lst-kix_xrce7kijh7l1-1>li:before{content:"\0025cb  "}.lst-kix_xrce7kijh7l1-5>li:before{content:"\0025a0  "}ul.lst-kix_j24rsp966ll1-8{list-style-type:none}.lst-kix_8cma8sbxfy2x-7>li:before{content:"\0025cb  "}.lst-kix_9cwmoyhkmhfb-7>li:before{content:"\0025cb  "}.lst-kix_8swzovi1ytnr-5>li:before{content:"\0025a0  "}.lst-kix_jys0trw4gd1m-1>li:before{content:"\0025cb  "}ul.lst-kix_79uewz4g8df3-8{list-style-type:none}.lst-kix_jhqr8d48vqqp-4>li:before{content:"\0025cb  "}ul.lst-kix_79uewz4g8df3-5{list-style-type:none}ul.lst-kix_79uewz4g8df3-4{list-style-type:none}ul.lst-kix_79uewz4g8df3-7{list-style-type:none}.lst-kix_jys0trw4gd1m-5>li:before{content:"\0025a0  "}ul.lst-kix_79uewz4g8df3-6{list-style-type:none}ul.lst-kix_79uewz4g8df3-1{list-style-type:none}.lst-kix_894copnhklgz-7>li:before{content:"\0025cb  "}.lst-kix_9cwmoyhkmhfb-3>li:before{content:"\0025cf  "}ul.lst-kix_79uewz4g8df3-0{list-style-type:none}ul.lst-kix_79uewz4g8df3-3{list-style-type:none}ul.lst-kix_79uewz4g8df3-2{list-style-type:none}ul.lst-kix_xgfpveff5zr0-4{list-style-type:none}ul.lst-kix_xgfpveff5zr0-5{list-style-type:none}ul.lst-kix_xgfpveff5zr0-2{list-style-type:none}ul.lst-kix_xgfpveff5zr0-3{list-style-type:none}ul.lst-kix_xgfpveff5zr0-0{list-style-type:none}ul.lst-kix_xgfpveff5zr0-1{list-style-type:none}.lst-kix_dj4fsxw52t4l-3>li:before{content:"\0025cf  "}.lst-kix_8z696e8bv17-6>li:before{content:"\0025cf  "}.lst-kix_4k4id0a37km3-3>li:before{content:"\0025cf  "}.lst-kix_vapye4eddf48-6>li:before{content:"\0025cf  "}.lst-kix_xkjgsygnpr2m-0>li:before{content:"\0025cf  "}.lst-kix_d1iyykra6bco-0>li:before{content:"\0025cf  "}.lst-kix_d1iyykra6bco-4>li:before{content:"\0025cb  "}.lst-kix_dj4fsxw52t4l-7>li:before{content:"\0025cb  "}ul.lst-kix_xgfpveff5zr0-8{list-style-type:none}ul.lst-kix_xgfpveff5zr0-6{list-style-type:none}.lst-kix_vapye4eddf48-2>li:before{content:"\0025a0  "}ul.lst-kix_xgfpveff5zr0-7{list-style-type:none}.lst-kix_d1iyykra6bco-8>li:before{content:"\0025a0  "}.lst-kix_rwfcya85pd71-1>li:before{content:"\0025cb  "}.lst-kix_rwfcya85pd71-5>li:before{content:"\0025a0  "}.lst-kix_d4lbyvb2g3j-7>li:before{content:"\0025cb  "}.lst-kix_d8ik6bmbwuj4-0>li:before{content:"\0025cf  "}.lst-kix_d8ik6bmbwuj4-8>li:before{content:"\0025a0  "}.lst-kix_8z696e8bv17-2>li:before{content:"\0025a0  "}.lst-kix_4k4id0a37km3-7>li:before{content:"\0025cb  "}ul.lst-kix_wq1o6elz9hy4-1{list-style-type:none}.lst-kix_gc2knt874en1-1>li:before{content:"\0025cb  "}ul.lst-kix_wq1o6elz9hy4-0{list-style-type:none}.lst-kix_d4lbyvb2g3j-3>li:before{content:"\0025cf  "}.lst-kix_d8ik6bmbwuj4-4>li:before{content:"\0025cb  "}ul.lst-kix_dowwk88w0dao-7{list-style-type:none}ul.lst-kix_wq1o6elz9hy4-5{list-style-type:none}ul.lst-kix_dowwk88w0dao-6{list-style-type:none}ul.lst-kix_wq1o6elz9hy4-4{list-style-type:none}ul.lst-kix_dowwk88w0dao-5{list-style-type:none}ul.lst-kix_wq1o6elz9hy4-3{list-style-type:none}ul.lst-kix_dowwk88w0dao-4{list-style-type:none}ul.lst-kix_wq1o6elz9hy4-2{list-style-type:none}ul.lst-kix_dowwk88w0dao-3{list-style-type:none}ul.lst-kix_dowwk88w0dao-2{list-style-type:none}ul.lst-kix_wq1o6elz9hy4-8{list-style-type:none}ul.lst-kix_dowwk88w0dao-1{list-style-type:none}ul.lst-kix_wq1o6elz9hy4-7{list-style-type:none}ul.lst-kix_dowwk88w0dao-0{list-style-type:none}.lst-kix_lyxc6qcmvco2-7>li:before{content:"\0025cb  "}ul.lst-kix_wq1o6elz9hy4-6{list-style-type:none}ul.lst-kix_383wi8io125v-3{list-style-type:none}ul.lst-kix_383wi8io125v-4{list-style-type:none}ul.lst-kix_383wi8io125v-1{list-style-type:none}ul.lst-kix_383wi8io125v-2{list-style-type:none}ul.lst-kix_383wi8io125v-7{list-style-type:none}.lst-kix_6ykz6yxb3o6x-1>li:before{content:"\0025cb  "}ul.lst-kix_383wi8io125v-8{list-style-type:none}ul.lst-kix_383wi8io125v-5{list-style-type:none}ul.lst-kix_383wi8io125v-6{list-style-type:none}.lst-kix_pjyqz9f44g8s-4>li:before{content:"\0025cb  "}.lst-kix_i1kls5tk4gt6-1>li:before{content:"\0025cb  "}ul.lst-kix_ho1wvdl93240-1{list-style-type:none}.lst-kix_8nsyajlscxcg-7>li:before{content:"\0025cb  "}ul.lst-kix_ho1wvdl93240-0{list-style-type:none}ul.lst-kix_ho1wvdl93240-3{list-style-type:none}ul.lst-kix_ho1wvdl93240-2{list-style-type:none}ul.lst-kix_ho1wvdl93240-5{list-style-type:none}ul.lst-kix_ho1wvdl93240-4{list-style-type:none}ul.lst-kix_ho1wvdl93240-7{list-style-type:none}ul.lst-kix_ho1wvdl93240-6{list-style-type:none}.lst-kix_pjyqz9f44g8s-8>li:before{content:"\0025a0  "}.lst-kix_8nsyajlscxcg-3>li:before{content:"\0025cf  "}ul.lst-kix_ho1wvdl93240-8{list-style-type:none}.lst-kix_6ykz6yxb3o6x-5>li:before{content:"\0025a0  "}.lst-kix_xkjgsygnpr2m-8>li:before{content:"\0025a0  "}ul.lst-kix_wdjiiwxyq2d1-8{list-style-type:none}.lst-kix_tlb25jkk5mkd-6>li:before{content:"\0025cf  "}.lst-kix_xkjgsygnpr2m-4>li:before{content:"\0025cb  "}ul.lst-kix_3xkj88gevxle-8{list-style-type:none}.lst-kix_i1kls5tk4gt6-5>li:before{content:"\0025a0  "}.lst-kix_79uewz4g8df3-5>li:before{content:"\0025a0  "}ul.lst-kix_3xkj88gevxle-5{list-style-type:none}ul.lst-kix_3xkj88gevxle-4{list-style-type:none}.lst-kix_gswyhyotsm36-8>li:before{content:"\0025a0  "}ul.lst-kix_3xkj88gevxle-7{list-style-type:none}ul.lst-kix_3xkj88gevxle-6{list-style-type:none}ul.lst-kix_ta3ofdl38yyn-7{list-style-type:none}.lst-kix_egb7bfw0xx0q-0>li:before{content:"\0025cf  "}.lst-kix_egb7bfw0xx0q-4>li:before{content:"\0025cb  "}ul.lst-kix_ta3ofdl38yyn-8{list-style-type:none}ul.lst-kix_ta3ofdl38yyn-5{list-style-type:none}.lst-kix_lyxc6qcmvco2-3>li:before{content:"\0025cf  "}ul.lst-kix_ta3ofdl38yyn-6{list-style-type:none}.lst-kix_tltq6x9aaf31-4>li:before{content:"\0025cb  "}.lst-kix_tltq6x9aaf31-8>li:before{content:"\0025a0  "}ul.lst-kix_wdjiiwxyq2d1-3{list-style-type:none}ul.lst-kix_ta3ofdl38yyn-3{list-style-type:none}ul.lst-kix_wdjiiwxyq2d1-2{list-style-type:none}ul.lst-kix_ta3ofdl38yyn-4{list-style-type:none}ul.lst-kix_wdjiiwxyq2d1-1{list-style-type:none}ul.lst-kix_ta3ofdl38yyn-1{list-style-type:none}ul.lst-kix_wdjiiwxyq2d1-0{list-style-type:none}ul.lst-kix_ta3ofdl38yyn-2{list-style-type:none}.lst-kix_79uewz4g8df3-1>li:before{content:"\0025cb  "}ul.lst-kix_wdjiiwxyq2d1-7{list-style-type:none}ul.lst-kix_wdjiiwxyq2d1-6{list-style-type:none}ul.lst-kix_ta3ofdl38yyn-0{list-style-type:none}.lst-kix_gswyhyotsm36-4>li:before{content:"\0025cb  "}ul.lst-kix_wdjiiwxyq2d1-5{list-style-type:none}ul.lst-kix_dowwk88w0dao-8{list-style-type:none}ul.lst-kix_wdjiiwxyq2d1-4{list-style-type:none}.lst-kix_313q3f5x4p2m-6>li:before{content:"\0025cf  "}.lst-kix_1ig57j6lcs95-4>li:before{content:"\0025cb  "}.lst-kix_egb7bfw0xx0q-8>li:before{content:"\0025a0  "}.lst-kix_470xpz4q6hoi-3>li:before{content:"\0025cf  "}.lst-kix_tltq6x9aaf31-0>li:before{content:"\0025cf  "}.lst-kix_313q3f5x4p2m-2>li:before{content:"\0025a0  "}.lst-kix_1ig57j6lcs95-0>li:before{content:"\0025cf  "}.lst-kix_1ig57j6lcs95-8>li:before{content:"\0025a0  "}.lst-kix_nf1j1iafbq39-1>li:before{content:"\0025cb  "}.lst-kix_41ikkbtdotge-2>li:before{content:"\0025a0  "}.lst-kix_3odao55mezu-3>li:before{content:"\0025cf  "}.lst-kix_gswyhyotsm36-0>li:before{content:"\0025cf  "}.lst-kix_8swzovi1ytnr-1>li:before{content:"\0025cb  "}.lst-kix_8cma8sbxfy2x-3>li:before{content:"\0025cf  "}.lst-kix_u1clvxqptoo-6>li:before{content:"\0025cf  "}ul.lst-kix_x2mjmu7myisq-8{list-style-type:none}ul.lst-kix_x2mjmu7myisq-7{list-style-type:none}ul.lst-kix_x2mjmu7myisq-6{list-style-type:none}ul.lst-kix_x2mjmu7myisq-5{list-style-type:none}ul.lst-kix_x2mjmu7myisq-4{list-style-type:none}ul.lst-kix_x2mjmu7myisq-3{list-style-type:none}.lst-kix_b07y002136mr-6>li:before{content:"\0025cf  "}ul.lst-kix_x2mjmu7myisq-2{list-style-type:none}.lst-kix_u1clvxqptoo-2>li:before{content:"\0025a0  "}ul.lst-kix_x2mjmu7myisq-1{list-style-type:none}ul.lst-kix_x2mjmu7myisq-0{list-style-type:none}.lst-kix_6r73oe35erxy-7>li:before{content:"\0025cb  "}ul.lst-kix_383wi8io125v-0{list-style-type:none}.lst-kix_470xpz4q6hoi-7>li:before{content:"\0025cb  "}.lst-kix_pjyqz9f44g8s-0>li:before{content:"\0025cf  "}.lst-kix_41ikkbtdotge-6>li:before{content:"\0025cf  "}.lst-kix_3odao55mezu-7>li:before{content:"\0025cb  "}.lst-kix_rcp8sez4svjd-0>li:before{content:"-  "}ul.lst-kix_hl5oa0f6tf4f-8{list-style-type:none}ul.lst-kix_iigck7esqel5-8{list-style-type:none}ul.lst-kix_iigck7esqel5-6{list-style-type:none}ul.lst-kix_iigck7esqel5-7{list-style-type:none}.lst-kix_m46mj27r3fpz-5>li:before{content:"\0025a0  "}ul.lst-kix_iigck7esqel5-4{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-3{list-style-type:none}.lst-kix_6r73oe35erxy-3>li:before{content:"\0025cf  "}ul.lst-kix_iigck7esqel5-5{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-2{list-style-type:none}.lst-kix_hydge7xfbt8r-0>li:before{content:"\0025cf  "}ul.lst-kix_iigck7esqel5-2{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-1{list-style-type:none}ul.lst-kix_iigck7esqel5-3{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-0{list-style-type:none}ul.lst-kix_iigck7esqel5-0{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-7{list-style-type:none}ul.lst-kix_iigck7esqel5-1{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-6{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-5{list-style-type:none}ul.lst-kix_hl5oa0f6tf4f-4{list-style-type:none}.lst-kix_gogm0wicn297-1>li:before{content:"\0025cb  "}.lst-kix_dq6vem8xoqp9-6>li:before{content:"\0025cf  "}.lst-kix_sxaq135mthse-2>li:before{content:"\0025a0  "}.lst-kix_ljjwvb506pet-6>li:before{content:"\0025cf  "}.lst-kix_kk8oms3v3iyb-0>li:before{content:"\0025cf  "}.lst-kix_u9ija913pfxg-5>li:before{content:"-  "}.lst-kix_u82x83buy4jv-2>li:before{content:"\0025a0  "}.lst-kix_cst6lp3lb98n-3>li:before{content:"\0025cf  "}.lst-kix_rcp8sez4svjd-8>li:before{content:"-  "}.lst-kix_nf1j1iafbq39-5>li:before{content:"\0025a0  "}ul.lst-kix_douubz1tb8an-5{list-style-type:none}ul.lst-kix_douubz1tb8an-4{list-style-type:none}ul.lst-kix_douubz1tb8an-7{list-style-type:none}ul.lst-kix_douubz1tb8an-6{list-style-type:none}ul.lst-kix_douubz1tb8an-1{list-style-type:none}ul.lst-kix_douubz1tb8an-0{list-style-type:none}ul.lst-kix_douubz1tb8an-3{list-style-type:none}ul.lst-kix_douubz1tb8an-2{list-style-type:none}.lst-kix_b8dlmlw0p2p1-1>li:before{content:"\0025cb  "}.lst-kix_douubz1tb8an-3>li:before{content:"\0025cf  "}.lst-kix_8hjy0hqh3rvg-3>li:before{content:"\0025cf  "}.lst-kix_7apmk6vwlj5r-2>li:before{content:"\0025a0  "}.lst-kix_5d66udn9quk7-2>li:before{content:"\0025a0  "}.lst-kix_x3vcornhe3kv-1>li:before{content:"\0025cb  "}.lst-kix_c2vvnsz0drek-8>li:before{content:"\0025a0  "}.lst-kix_5kaugn46jpw6-1>li:before{content:"\0025cb  "}.lst-kix_204pjqaavew-6>li:before{content:"\0025cf  "}.lst-kix_398os0jr1iw3-5>li:before{content:"\0025a0  "}.lst-kix_edj3r8hgf2z3-3>li:before{content:"\0025cf  "}.lst-kix_tg2hwffju8t9-2>li:before{content:"\0025a0  "}.lst-kix_5fj31eh1jc7-1>li:before{content:"\0025cb  "}ul.lst-kix_douubz1tb8an-8{list-style-type:none}.lst-kix_jbw6p6lajlkh-4>li:before{content:"\0025cb  "}ul.lst-kix_bq5ybp1zal0b-6{list-style-type:none}ul.lst-kix_p0fqpwscumob-4{list-style-type:none}ul.lst-kix_bq5ybp1zal0b-7{list-style-type:none}ul.lst-kix_p0fqpwscumob-5{list-style-type:none}ul.lst-kix_bq5ybp1zal0b-8{list-style-type:none}ul.lst-kix_p0fqpwscumob-6{list-style-type:none}ul.lst-kix_p0fqpwscumob-7{list-style-type:none}ul.lst-kix_bq5ybp1zal0b-2{list-style-type:none}ul.lst-kix_p0fqpwscumob-8{list-style-type:none}ul.lst-kix_bq5ybp1zal0b-3{list-style-type:none}.lst-kix_7b2y41o7s2k3-5>li:before{content:"\0025a0  "}ul.lst-kix_bq5ybp1zal0b-4{list-style-type:none}ul.lst-kix_bq5ybp1zal0b-5{list-style-type:none}.lst-kix_f3egesa43r6b-7>li:before{content:"\0025cb  "}ul.lst-kix_bq5ybp1zal0b-0{list-style-type:none}.lst-kix_nbpnmy8xci8w-2>li:before{content:"\0025a0  "}.lst-kix_hydge7xfbt8r-8>li:before{content:"\0025a0  "}ul.lst-kix_bq5ybp1zal0b-1{list-style-type:none}ul.lst-kix_5kaugn46jpw6-1{list-style-type:none}ul.lst-kix_gsefpdxx2m31-4{list-style-type:none}ul.lst-kix_5kaugn46jpw6-2{list-style-type:none}ul.lst-kix_gsefpdxx2m31-3{list-style-type:none}ul.lst-kix_5kaugn46jpw6-3{list-style-type:none}ul.lst-kix_gsefpdxx2m31-6{list-style-type:none}ul.lst-kix_5kaugn46jpw6-4{list-style-type:none}ul.lst-kix_gsefpdxx2m31-5{list-style-type:none}ul.lst-kix_5kaugn46jpw6-5{list-style-type:none}ul.lst-kix_gsefpdxx2m31-0{list-style-type:none}ul.lst-kix_5kaugn46jpw6-6{list-style-type:none}ul.lst-kix_5kaugn46jpw6-7{list-style-type:none}ul.lst-kix_gsefpdxx2m31-2{list-style-type:none}ul.lst-kix_5kaugn46jpw6-8{list-style-type:none}ul.lst-kix_gsefpdxx2m31-1{list-style-type:none}.lst-kix_kk8oms3v3iyb-8>li:before{content:"\0025a0  "}ul.lst-kix_p0fqpwscumob-0{list-style-type:none}.lst-kix_mxwp6np2u5xu-7>li:before{content:"-  "}ul.lst-kix_p0fqpwscumob-1{list-style-type:none}ul.lst-kix_p0fqpwscumob-2{list-style-type:none}ul.lst-kix_p0fqpwscumob-3{list-style-type:none}ul.lst-kix_5kaugn46jpw6-0{list-style-type:none}.lst-kix_gwuatnbmuewh-0>li:before{content:"\0025cf  "}.lst-kix_ucprmx270ar2-0>li:before{content:"\0025cf  "}.lst-kix_ulntv5wergyk-7>li:before{content:"\0025cb  "}.lst-kix_ho1wvdl93240-4>li:before{content:"\0025cb  "}.lst-kix_z1olrl2brpzr-6>li:before{content:"\0025cf  "}.lst-kix_o5m7acjt2ju1-0>li:before{content:"\0025cf  "}.lst-kix_ucprmx270ar2-8>li:before{content:"\0025a0  "}.lst-kix_6asjiojyuog6-2>li:before{content:"\0025a0  "}.lst-kix_mh0uhwysa91-5>li:before{content:"\0025a0  "}.lst-kix_jzarhjey1jrh-7>li:before{content:"\0025cb  "}.lst-kix_wdjiiwxyq2d1-0>li:before{content:"\0025cf  "}.lst-kix_x5zq7qqqztpc-2>li:before{content:"\0025a0  "}.lst-kix_4ykl6tjv6vdc-0>li:before{content:"\0025cf  "}.lst-kix_o5m7acjt2ju1-8>li:before{content:"\0025a0  "}.lst-kix_iiujyt46s4u7-6>li:before{content:"\0025cf  "}.lst-kix_xx3xz98aq2wq-5>li:before{content:"\0025a0  "}ul.lst-kix_gpgigcfykzff-1{list-style-type:none}ul.lst-kix_gpgigcfykzff-0{list-style-type:none}ul.lst-kix_gpgigcfykzff-3{list-style-type:none}.lst-kix_a9t5z6hxg4oz-4>li:before{content:"\0025cb  "}ul.lst-kix_gpgigcfykzff-2{list-style-type:none}ul.lst-kix_gpgigcfykzff-5{list-style-type:none}ul.lst-kix_gpgigcfykzff-4{list-style-type:none}ul.lst-kix_gpgigcfykzff-7{list-style-type:none}ul.lst-kix_gpgigcfykzff-6{list-style-type:none}ul.lst-kix_gpgigcfykzff-8{list-style-type:none}.lst-kix_f51q5t2m61i8-4>li:before{content:"\0025cb  "}.lst-kix_gwuatnbmuewh-8>li:before{content:"\0025a0  "}.lst-kix_wdjiiwxyq2d1-8>li:before{content:"\0025a0  "}.lst-kix_4ykl6tjv6vdc-8>li:before{content:"\0025a0  "}.lst-kix_9khvkfghhb3b-0>li:before{content:"\0025cf  "}.lst-kix_5p6viq6b9z8b-5>li:before{content:"\0025a0  "}.lst-kix_didibfga6ee5-1>li:before{content:"\0025cb  "}.lst-kix_ct57w2de70tb-2>li:before{content:"\0025a0  "}.lst-kix_emy9ap8zeql8-4>li:before{content:"\0025cb  "}.lst-kix_w14ild1qeoey-1>li:before{content:"\0025cb  "}.lst-kix_o4ah8y8am4ct-6>li:before{content:"\0025cf  "}ul.lst-kix_4yao0efkrd31-1{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-0{list-style-type:none}ul.lst-kix_4yao0efkrd31-0{list-style-type:none}ul.lst-kix_4yao0efkrd31-3{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-2{list-style-type:none}ul.lst-kix_4yao0efkrd31-2{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-1{list-style-type:none}.lst-kix_crb89j7xw8q2-4>li:before{content:"\0025cb  "}ul.lst-kix_mxwp6np2u5xu-5{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-8{list-style-type:none}ul.lst-kix_mxwp6np2u5xu-4{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-7{list-style-type:none}ul.lst-kix_mxwp6np2u5xu-3{list-style-type:none}ul.lst-kix_mxwp6np2u5xu-2{list-style-type:none}ul.lst-kix_mxwp6np2u5xu-1{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-4{list-style-type:none}.lst-kix_ow9ix2mme95u-3>li:before{content:"\0025cf  "}ul.lst-kix_mxwp6np2u5xu-0{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-3{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-6{list-style-type:none}ul.lst-kix_vsaybtg1hfvm-5{list-style-type:none}ul.lst-kix_mh0uhwysa91-3{list-style-type:none}ul.lst-kix_mh0uhwysa91-4{list-style-type:none}ul.lst-kix_mh0uhwysa91-1{list-style-type:none}.lst-kix_p2mkzdwo7q6r-1>li:before{content:"\0025cb  "}ul.lst-kix_mh0uhwysa91-2{list-style-type:none}ul.lst-kix_mh0uhwysa91-7{list-style-type:none}.lst-kix_24cmoxb3qmwl-0>li:before{content:"\0025cf  "}ul.lst-kix_mh0uhwysa91-8{list-style-type:none}ul.lst-kix_mh0uhwysa91-5{list-style-type:none}.lst-kix_wq1o6elz9hy4-3>li:before{content:"\0025cf  "}ul.lst-kix_mh0uhwysa91-6{list-style-type:none}ul.lst-kix_k12n8xes8ofj-0{list-style-type:none}ul.lst-kix_k12n8xes8ofj-1{list-style-type:none}ul.lst-kix_4yao0efkrd31-8{list-style-type:none}ul.lst-kix_k12n8xes8ofj-4{list-style-type:none}ul.lst-kix_4yao0efkrd31-5{list-style-type:none}ul.lst-kix_mh0uhwysa91-0{list-style-type:none}ul.lst-kix_k12n8xes8ofj-5{list-style-type:none}ul.lst-kix_4yao0efkrd31-4{list-style-type:none}ul.lst-kix_k12n8xes8ofj-2{list-style-type:none}ul.lst-kix_4yao0efkrd31-7{list-style-type:none}ul.lst-kix_k12n8xes8ofj-3{list-style-type:none}ul.lst-kix_4yao0efkrd31-6{list-style-type:none}ul.lst-kix_k12n8xes8ofj-8{list-style-type:none}.lst-kix_saoi6g81wrq2-4>li:before{content:"\0025cb  "}.lst-kix_9khvkfghhb3b-8>li:before{content:"\0025a0  "}ul.lst-kix_k12n8xes8ofj-6{list-style-type:none}.lst-kix_24cmoxb3qmwl-8>li:before{content:"\0025a0  "}ul.lst-kix_k12n8xes8ofj-7{list-style-type:none}.lst-kix_u6q708mtjbvx-5>li:before{content:"-  "}.lst-kix_nxvbdas3q36w-2>li:before{content:"\0025a0  "}.lst-kix_o4d3wx7elf58-1>li:before{content:"\0025cb  "}.lst-kix_iigck7esqel5-6>li:before{content:"\0025cf  "}.lst-kix_kz1sfsbi2eng-5>li:before{content:"\0025a0  "}.lst-kix_6d5ax9c8k1xa-4>li:before{content:"\0025cb  "}.lst-kix_yv7c7gf6elha-5>li:before{content:"\0025a0  "}.lst-kix_fjl32nwuhu4u-5>li:before{content:"\0025a0  "}.lst-kix_mgyoyquoxrm1-8>li:before{content:"\0025a0  "}ul.lst-kix_5d66udn9quk7-6{list-style-type:none}ul.lst-kix_5d66udn9quk7-7{list-style-type:none}ul.lst-kix_5d66udn9quk7-4{list-style-type:none}.lst-kix_nc0l9nyccr86-6>li:before{content:"\0025cf  "}.lst-kix_2nh9kwfbzbik-7>li:before{content:"\0025cb  "}ul.lst-kix_5d66udn9quk7-5{list-style-type:none}ul.lst-kix_5d66udn9quk7-2{list-style-type:none}.lst-kix_scilllbfudea-1>li:before{content:"\0025cb  "}ul.lst-kix_5d66udn9quk7-3{list-style-type:none}ul.lst-kix_5d66udn9quk7-0{list-style-type:none}.lst-kix_h99428b6laza-8>li:before{content:"\0025a0  "}ul.lst-kix_5d66udn9quk7-1{list-style-type:none}.lst-kix_kkphisidn6cz-3>li:before{content:"\0025cf  "}.lst-kix_1a8ua5u4a26p-5>li:before{content:"\0025a0  "}.lst-kix_mgyoyquoxrm1-0>li:before{content:"\0025cf  "}.lst-kix_g0l7lf51hdyf-4>li:before{content:"\0025cb  "}.lst-kix_3fvzyqsgm5ef-1>li:before{content:"\0025cb  "}.lst-kix_wq4pisfqksnu-8>li:before{content:"\0025a0  "}.lst-kix_bhgkuyt4aopz-2>li:before{content:"\0025a0  "}ul.lst-kix_mxwp6np2u5xu-8{list-style-type:none}ul.lst-kix_mxwp6np2u5xu-7{list-style-type:none}ul.lst-kix_mxwp6np2u5xu-6{list-style-type:none}.lst-kix_1xoigud6ruor-2>li:before{content:"\0025a0  "}.lst-kix_spzfuoeekohi-5>li:before{content:"\0025a0  "}.lst-kix_riff65m65tnf-6>li:before{content:"\0025cf  "}.lst-kix_4fwhm2tiefhz-6>li:before{content:"\0025cf  "}.lst-kix_2jmrp4ochkmc-1>li:before{content:"\0025cb  "}.lst-kix_369j2eishmb5-6>li:before{content:"\0025cf  "}.lst-kix_74gx9bwnn8kt-2>li:before{content:"\0025a0  "}ul.lst-kix_5d66udn9quk7-8{list-style-type:none}.lst-kix_z1vxl485jzzo-6>li:before{content:"\0025cf  "}.lst-kix_wq4pisfqksnu-0>li:before{content:"\0025cf  "}.lst-kix_7v1j337157vp-5>li:before{content:"\0025a0  "}.lst-kix_solhl6ysg65v-5>li:before{content:"\0025a0  "}.lst-kix_bj6mzrhm69bi-1>li:before{content:"\0025cb  "}ul.lst-kix_nf1j1iafbq39-7{list-style-type:none}.lst-kix_2ihanzwz4fgf-2>li:before{content:"\0025a0  "}ul.lst-kix_nf1j1iafbq39-6{list-style-type:none}.lst-kix_z4z7nvld905a-4>li:before{content:"\0025cb  "}ul.lst-kix_nf1j1iafbq39-8{list-style-type:none}ul.lst-kix_nf1j1iafbq39-3{list-style-type:none}.lst-kix_2rp5aiyjkdpd-0>li:before{content:"\0025cf  "}ul.lst-kix_nf1j1iafbq39-2{list-style-type:none}ul.lst-kix_nf1j1iafbq39-5{list-style-type:none}.lst-kix_xqfh3tf7tjc0-6>li:before{content:"\0025cf  "}ul.lst-kix_nf1j1iafbq39-4{list-style-type:none}.lst-kix_ut5bvwbkq6rt-6>li:before{content:"\0025cf  "}.lst-kix_swbzestlepme-5>li:before{content:"\0025a0  "}ul.lst-kix_nf1j1iafbq39-1{list-style-type:none}ul.lst-kix_nf1j1iafbq39-0{list-style-type:none}.lst-kix_nbuhcgd4ft21-0>li:before{content:"\0025cf  "}.lst-kix_gnt40hozc3kv-7>li:before{content:"\0025cb  "}.lst-kix_z8j4l8h7k6b3-2>li:before{content:"\0025a0  "}.lst-kix_s6xk062mdkv5-6>li:before{content:"\0025cf  "}.lst-kix_l3zoug7b3vcw-7>li:before{content:"\0025cb  "}.lst-kix_h99428b6laza-0>li:before{content:"\0025cf  "}.lst-kix_bcdpfzfy81p5-6>li:before{content:"\0025cf  "}.lst-kix_l1rt7mvaz29b-1>li:before{content:"\0025cb  "}.lst-kix_d2ysoruvczg5-6>li:before{content:"\0025cf  "}.lst-kix_kxfg6pdeancy-1>li:before{content:"\0025cb  "}ul.lst-kix_ucprmx270ar2-7{list-style-type:none}ul.lst-kix_ucprmx270ar2-6{list-style-type:none}ul.lst-kix_ucprmx270ar2-5{list-style-type:none}ul.lst-kix_ucprmx270ar2-4{list-style-type:none}ul.lst-kix_ucprmx270ar2-3{list-style-type:none}ul.lst-kix_ucprmx270ar2-2{list-style-type:none}ul.lst-kix_ucprmx270ar2-1{list-style-type:none}ul.lst-kix_ucprmx270ar2-0{list-style-type:none}ul.lst-kix_ucprmx270ar2-8{list-style-type:none}.lst-kix_l47f7qjlu9cu-1>li:before{content:"\0025cb  "}.lst-kix_n8ye7s4j9pok-1>li:before{content:"\0025cb  "}.lst-kix_z3si3pbri1de-4>li:before{content:"\0025cb  "}.lst-kix_qlax8iv93hxg-2>li:before{content:"\0025a0  "}.lst-kix_gpgigcfykzff-5>li:before{content:"\0025a0  "}.lst-kix_d995nc7613a9-8>li:before{content:"\0025a0  "}.lst-kix_rrs2j0rau6wp-0>li:before{content:"\0025cf  "}.lst-kix_yhmx20qr5k0e-5>li:before{content:"\0025a0  "}.lst-kix_uz5smpj76010-1>li:before{content:"\0025cb  "}.lst-kix_k12n8xes8ofj-2>li:before{content:"\0025a0  "}.lst-kix_n21asioviqe4-7>li:before{content:"\0025cb  "}.lst-kix_z1voh5a7dove-2>li:before{content:"\0025a0  "}.lst-kix_n1766bho68n4-0>li:before{content:"\0025cf  "}.lst-kix_nbuhcgd4ft21-8>li:before{content:"\0025a0  "}ul.lst-kix_8g8vlw6j0asg-7{list-style-type:none}.lst-kix_p7d8xi28lme6-5>li:before{content:"\0025a0  "}ul.lst-kix_8g8vlw6j0asg-6{list-style-type:none}ul.lst-kix_8g8vlw6j0asg-5{list-style-type:none}ul.lst-kix_8g8vlw6j0asg-4{list-style-type:none}ul.lst-kix_edj3r8hgf2z3-8{list-style-type:none}ul.lst-kix_8g8vlw6j0asg-3{list-style-type:none}ul.lst-kix_edj3r8hgf2z3-7{list-style-type:none}.lst-kix_2s6hp75gzisz-1>li:before{content:"\0025cb  "}ul.lst-kix_8g8vlw6j0asg-2{list-style-type:none}ul.lst-kix_8g8vlw6j0asg-1{list-style-type:none}.lst-kix_l4pre1xosssa-1>li:before{content:"\0025cb  "}.lst-kix_n1766bho68n4-8>li:before{content:"\0025a0  "}ul.lst-kix_8g8vlw6j0asg-0{list-style-type:none}ul.lst-kix_edj3r8hgf2z3-4{list-style-type:none}.lst-kix_cy2og2rt03zo-4>li:before{content:"\0025cb  "}ul.lst-kix_edj3r8hgf2z3-3{list-style-type:none}ul.lst-kix_edj3r8hgf2z3-6{list-style-type:none}ul.lst-kix_edj3r8hgf2z3-5{list-style-type:none}ul.lst-kix_edj3r8hgf2z3-0{list-style-type:none}.lst-kix_b474o1tvhevy-3>li:before{content:"\0025cf  "}ul.lst-kix_edj3r8hgf2z3-2{list-style-type:none}.lst-kix_o14fwownm59e-7>li:before{content:"\0025cb  "}ul.lst-kix_edj3r8hgf2z3-1{list-style-type:none}ul.lst-kix_8g8vlw6j0asg-8{list-style-type:none}.lst-kix_ex6gjyozs00-5>li:before{content:"\0025a0  "}.lst-kix_aui8m1kby916-6>li:before{content:"\0025cf  "}.lst-kix_8g8vlw6j0asg-4>li:before{content:"\0025cb  "}.lst-kix_60wjig3vq01-4>li:before{content:"\0025cb  "}.lst-kix_68swn1licdyn-2>li:before{content:"\0025a0  "}.lst-kix_tglcy61nnbu3-5>li:before{content:"\0025a0  "}.lst-kix_8fyyex2aqhl9-0>li:before{content:"\0025cf  "}.lst-kix_vebf56ofbjcp-3>li:before{content:"\0025cf  "}.lst-kix_j24rsp966ll1-7>li:before{content:"\0025cb  "}.lst-kix_vzf3xt3x8pov-1>li:before{content:"\0025cb  "}.lst-kix_isql72ps5u79-4>li:before{content:"\0025cb  "}.lst-kix_8fyyex2aqhl9-8>li:before{content:"\0025a0  "}.lst-kix_d995nc7613a9-0>li:before{content:"\0025cf  "}.lst-kix_iqbmivscsmun-2>li:before{content:"\0025a0  "}.lst-kix_6jabh0roz5s1-2>li:before{content:"\0025a0  "}ul.lst-kix_p7d8xi28lme6-8{list-style-type:none}.lst-kix_vjhjbjs9ubca-1>li:before{content:"\0025cb  "}ul.lst-kix_p7d8xi28lme6-7{list-style-type:none}ul.lst-kix_p7d8xi28lme6-6{list-style-type:none}ul.lst-kix_p7d8xi28lme6-5{list-style-type:none}.lst-kix_vjhjbjs9ubca-2>li:before{content:"\0025a0  "}ul.lst-kix_p7d8xi28lme6-0{list-style-type:none}.lst-kix_vjhjbjs9ubca-5>li:before{content:"\0025a0  "}ul.lst-kix_p7d8xi28lme6-4{list-style-type:none}ul.lst-kix_p7d8xi28lme6-3{list-style-type:none}ul.lst-kix_p7d8xi28lme6-2{list-style-type:none}ul.lst-kix_p7d8xi28lme6-1{list-style-type:none}.lst-kix_rr7hjjtid2vg-5>li:before{content:"\0025a0  "}ul.lst-kix_ct57w2de70tb-8{list-style-type:none}ul.lst-kix_ct57w2de70tb-7{list-style-type:none}ul.lst-kix_ct57w2de70tb-6{list-style-type:none}ul.lst-kix_ct57w2de70tb-5{list-style-type:none}ul.lst-kix_ct57w2de70tb-4{list-style-type:none}ul.lst-kix_ct57w2de70tb-3{list-style-type:none}ul.lst-kix_ct57w2de70tb-2{list-style-type:none}ul.lst-kix_ct57w2de70tb-1{list-style-type:none}ul.lst-kix_ct57w2de70tb-0{list-style-type:none}.lst-kix_rr7hjjtid2vg-1>li:before{content:"\0025cb  "}.lst-kix_i5pnhlfopov3-2>li:before{content:"\0025a0  "}.lst-kix_p3gj10803b4t-8>li:before{content:"\0025a0  "}.lst-kix_r8labycqo9zl-0>li:before{content:"\0025cf  "}ul.lst-kix_ltkzlkp2gxkt-8{list-style-type:none}.lst-kix_60wjig3vq01-0>li:before{content:"\0025cf  "}ul.lst-kix_ltkzlkp2gxkt-7{list-style-type:none}.lst-kix_nwi1ck3fibs7-7>li:before{content:"\0025cb  "}ul.lst-kix_ltkzlkp2gxkt-6{list-style-type:none}ul.lst-kix_ltkzlkp2gxkt-5{list-style-type:none}ul.lst-kix_ltkzlkp2gxkt-4{list-style-type:none}ul.lst-kix_ltkzlkp2gxkt-3{list-style-type:none}.lst-kix_jourdftw4nrm-2>li:before{content:"\0025a0  "}ul.lst-kix_ltkzlkp2gxkt-2{list-style-type:none}ul.lst-kix_ltkzlkp2gxkt-1{list-style-type:none}.lst-kix_i5pnhlfopov3-7>li:before{content:"\0025cb  "}ul.lst-kix_ltkzlkp2gxkt-0{list-style-type:none}.lst-kix_ltkzlkp2gxkt-0>li:before{content:"\0025cf  "}.lst-kix_ltkzlkp2gxkt-3>li:before{content:"\0025cf  "}.lst-kix_z0c6lix981m-6>li:before{content:"\0025cf  "}ul.lst-kix_wq4pisfqksnu-7{list-style-type:none}ul.lst-kix_wq4pisfqksnu-8{list-style-type:none}.lst-kix_z0c6lix981m-5>li:before{content:"\0025a0  "}.lst-kix_ly0lmz6fcokr-5>li:before{content:"\0025a0  "}.lst-kix_defeme42l2ge-8>li:before{content:"\0025a0  "}ul.lst-kix_fjl32nwuhu4u-5{list-style-type:none}ul.lst-kix_fjl32nwuhu4u-4{list-style-type:none}ul.lst-kix_fjl32nwuhu4u-3{list-style-type:none}ul.lst-kix_fjl32nwuhu4u-2{list-style-type:none}.lst-kix_481c98v2pt83-0>li:before{content:"\0025cf  "}.lst-kix_z0c6lix981m-2>li:before{content:"\0025a0  "}.lst-kix_zgvzzrmreqeg-1>li:before{content:"\0025cb  "}ul.lst-kix_fjl32nwuhu4u-1{list-style-type:none}ul.lst-kix_fjl32nwuhu4u-0{list-style-type:none}.lst-kix_vsaybtg1hfvm-2>li:before{content:"\0025a0  "}.lst-kix_ly0lmz6fcokr-8>li:before{content:"\0025a0  "}ul.lst-kix_fjl32nwuhu4u-8{list-style-type:none}.lst-kix_481c98v2pt83-3>li:before{content:"\0025cf  "}ul.lst-kix_fjl32nwuhu4u-7{list-style-type:none}ul.lst-kix_fjl32nwuhu4u-6{list-style-type:none}.lst-kix_47kviebmmd71-8>li:before{content:"\0025a0  "}.lst-kix_k12n8xes8ofj-6>li:before{content:"\0025cf  "}.lst-kix_47kviebmmd71-7>li:before{content:"\0025cb  "}.lst-kix_defeme42l2ge-1>li:before{content:"\0025cb  "}.lst-kix_47kviebmmd71-4>li:before{content:"\0025cb  "}ul.lst-kix_z1olrl2brpzr-2{list-style-type:none}ul.lst-kix_z1olrl2brpzr-1{list-style-type:none}.lst-kix_s3fvwoba1t7n-2>li:before{content:"\0025a0  "}.lst-kix_zgvzzrmreqeg-4>li:before{content:"\0025cb  "}ul.lst-kix_z1olrl2brpzr-4{list-style-type:none}.lst-kix_rrs2j0rau6wp-4>li:before{content:"\0025cb  "}ul.lst-kix_z1olrl2brpzr-3{list-style-type:none}.lst-kix_zgvzzrmreqeg-5>li:before{content:"\0025a0  "}.lst-kix_defeme42l2ge-5>li:before{content:"\0025a0  "}ul.lst-kix_z1olrl2brpzr-0{list-style-type:none}ul.lst-kix_wq4pisfqksnu-5{list-style-type:none}.lst-kix_defeme42l2ge-4>li:before{content:"\0025cb  "}ul.lst-kix_wq4pisfqksnu-6{list-style-type:none}ul.lst-kix_wq4pisfqksnu-3{list-style-type:none}.lst-kix_zgvzzrmreqeg-8>li:before{content:"\0025a0  "}ul.lst-kix_wq4pisfqksnu-4{list-style-type:none}.lst-kix_w1d9l4m1yg5x-1>li:before{content:"\0025cb  "}.lst-kix_s3fvwoba1t7n-5>li:before{content:"\0025a0  "}ul.lst-kix_wq4pisfqksnu-1{list-style-type:none}ul.lst-kix_z1olrl2brpzr-6{list-style-type:none}ul.lst-kix_wq4pisfqksnu-2{list-style-type:none}ul.lst-kix_z1olrl2brpzr-5{list-style-type:none}ul.lst-kix_z1olrl2brpzr-8{list-style-type:none}ul.lst-kix_wq4pisfqksnu-0{list-style-type:none}.lst-kix_hh7pij1hmwlh-2>li:before{content:"\0025a0  "}ul.lst-kix_z1olrl2brpzr-7{list-style-type:none}.lst-kix_w1d9l4m1yg5x-0>li:before{content:"\0025cf  "}.lst-kix_o38l9c2rqk2p-2>li:before{content:"\0025a0  "}ul.lst-kix_4fwhm2tiefhz-8{list-style-type:none}ul.lst-kix_2nh9kwfbzbik-1{list-style-type:none}ul.lst-kix_4fwhm2tiefhz-7{list-style-type:none}ul.lst-kix_2nh9kwfbzbik-0{list-style-type:none}ul.lst-kix_4fwhm2tiefhz-6{list-style-type:none}.lst-kix_58o57c52tmut-1>li:before{content:"\0025cb  "}ul.lst-kix_4fwhm2tiefhz-5{list-style-type:none}.lst-kix_xqqiyb3b5ag0-2>li:before{content:"\0025a0  "}.lst-kix_s3fvwoba1t7n-1>li:before{content:"\0025cb  "}ul.lst-kix_4fwhm2tiefhz-4{list-style-type:none}ul.lst-kix_4fwhm2tiefhz-3{list-style-type:none}.lst-kix_x5jl3p2g0327-3>li:before{content:"\0025cf  "}ul.lst-kix_4fwhm2tiefhz-2{list-style-type:none}ul.lst-kix_4fwhm2tiefhz-1{list-style-type:none}.lst-kix_w1d9l4m1yg5x-4>li:before{content:"\0025cb  "}.lst-kix_xkmlxx8lhm5d-6>li:before{content:"\0025cf  "}ul.lst-kix_4fwhm2tiefhz-0{list-style-type:none}ul.lst-kix_z1vxl485jzzo-7{list-style-type:none}ul.lst-kix_z1vxl485jzzo-6{list-style-type:none}.lst-kix_rrs2j0rau6wp-1>li:before{content:"\0025cb  "}ul.lst-kix_z1vxl485jzzo-8{list-style-type:none}ul.lst-kix_z1vxl485jzzo-3{list-style-type:none}ul.lst-kix_z1vxl485jzzo-2{list-style-type:none}ul.lst-kix_z1vxl485jzzo-5{list-style-type:none}ul.lst-kix_z1vxl485jzzo-4{list-style-type:none}ul.lst-kix_z1vxl485jzzo-1{list-style-type:none}ul.lst-kix_z1vxl485jzzo-0{list-style-type:none}.lst-kix_x5jl3p2g0327-7>li:before{content:"\0025cb  "}ul.lst-kix_d8ik6bmbwuj4-7{list-style-type:none}ul.lst-kix_d8ik6bmbwuj4-6{list-style-type:none}ul.lst-kix_d8ik6bmbwuj4-8{list-style-type:none}.lst-kix_481c98v2pt83-4>li:before{content:"\0025cb  "}ul.lst-kix_d8ik6bmbwuj4-3{list-style-type:none}ul.lst-kix_d8ik6bmbwuj4-2{list-style-type:none}ul.lst-kix_d8ik6bmbwuj4-5{list-style-type:none}.lst-kix_k12n8xes8ofj-3>li:before{content:"\0025cf  "}ul.lst-kix_d8ik6bmbwuj4-4{list-style-type:none}.lst-kix_17is42bhveph-5>li:before{content:"\0025a0  "}.lst-kix_383wi8io125v-2>li:before{content:"\0025a0  "}ul.lst-kix_2nh9kwfbzbik-8{list-style-type:none}ul.lst-kix_2nh9kwfbzbik-7{list-style-type:none}ul.lst-kix_2nh9kwfbzbik-6{list-style-type:none}.lst-kix_juam6n4619t2-0>li:before{content:"\0025cf  "}ul.lst-kix_2nh9kwfbzbik-5{list-style-type:none}ul.lst-kix_2nh9kwfbzbik-4{list-style-type:none}.lst-kix_d3k7hdh35j7a-3>li:before{content:"\0025cf  "}ul.lst-kix_2nh9kwfbzbik-3{list-style-type:none}ul.lst-kix_2nh9kwfbzbik-2{list-style-type:none}ul.lst-kix_x5zq7qqqztpc-8{list-style-type:none}.lst-kix_ydb53brft35n-6>li:before{content:"\0025cf  "}ul.lst-kix_x5zq7qqqztpc-6{list-style-type:none}ul.lst-kix_x5zq7qqqztpc-7{list-style-type:none}.lst-kix_xqqiyb3b5ag0-6>li:before{content:"\0025cf  "}.lst-kix_klzhllb6xcyr-0>li:before{content:"\0025cf  "}.lst-kix_n8ye7s4j9pok-6>li:before{content:"\0025cf  "}ul.lst-kix_x5zq7qqqztpc-0{list-style-type:none}ul.lst-kix_x5zq7qqqztpc-1{list-style-type:none}.lst-kix_5ia1ign2sshz-5>li:before{content:"\0025a0  "}ul.lst-kix_x5zq7qqqztpc-4{list-style-type:none}ul.lst-kix_x5zq7qqqztpc-5{list-style-type:none}ul.lst-kix_x5zq7qqqztpc-2{list-style-type:none}ul.lst-kix_x5zq7qqqztpc-3{list-style-type:none}.lst-kix_2rp5aiyjkdpd-8>li:before{content:"\0025a0  "}.lst-kix_7jzvzpyn9bnk-7>li:before{content:"\0025cb  "}ul.lst-kix_uest4hng52tf-0{list-style-type:none}.lst-kix_r8labycqo9zl-3>li:before{content:"\0025cf  "}ul.lst-kix_uest4hng52tf-1{list-style-type:none}.lst-kix_r75b0bpeph0o-0>li:before{content:"\0025cf  "}ul.lst-kix_uest4hng52tf-2{list-style-type:none}.lst-kix_gr1y63l14uay-5>li:before{content:"-  "}ul.lst-kix_uest4hng52tf-3{list-style-type:none}ul.lst-kix_uest4hng52tf-4{list-style-type:none}ul.lst-kix_uest4hng52tf-5{list-style-type:none}ul.lst-kix_uest4hng52tf-6{list-style-type:none}ul.lst-kix_uest4hng52tf-7{list-style-type:none}.lst-kix_p98dexr65abh-8>li:before{content:"\0025a0  "}ul.lst-kix_uest4hng52tf-8{list-style-type:none}.lst-kix_ta3ofdl38yyn-0>li:before{content:"\0025cf  "}.lst-kix_2zf5bd1mem1t-8>li:before{content:"\0025a0  "}.lst-kix_bjneibxfrx3r-5>li:before{content:"\0025a0  "}.lst-kix_6k5yl8dbqqq2-5>li:before{content:"\0025a0  "}.lst-kix_kxpudurk2j6e-4>li:before{content:"\0025cb  "}.lst-kix_gnzb8ea0t50l-2>li:before{content:"\0025a0  "}.lst-kix_8fyyex2aqhl9-1>li:before{content:"\0025cb  "}.lst-kix_p98dexr65abh-4>li:before{content:"\0025cb  "}.lst-kix_yhmx20qr5k0e-0>li:before{content:"\0025cf  "}.lst-kix_bjneibxfrx3r-1>li:before{content:"\0025cb  "}.lst-kix_uk8k6pzec0np-0>li:before{content:"\0025cf  "}.lst-kix_2s6hp75gzisz-8>li:before{content:"\0025a0  "}.lst-kix_3fvzyqsgm5ef-6>li:before{content:"\0025cf  "}.lst-kix_crt5jd7c2xq0-4>li:before{content:"\0025cb  "}.lst-kix_qlax8iv93hxg-7>li:before{content:"\0025cb  "}.lst-kix_crt5jd7c2xq0-0>li:before{content:"\0025cf  "}.lst-kix_kxpudurk2j6e-0>li:before{content:"\0025cf  "}.lst-kix_kz1sfsbi2eng-4>li:before{content:"\0025cb  "}.lst-kix_pzunwido5p11-8>li:before{content:"\0025a0  "}.lst-kix_8k70e8i8rdlm-0>li:before{content:"\0025cf  "}.lst-kix_bhgkuyt4aopz-7>li:before{content:"\0025cb  "}.lst-kix_kz1sfsbi2eng-8>li:before{content:"\0025a0  "}.lst-kix_8k70e8i8rdlm-4>li:before{content:"\0025cb  "}.lst-kix_yh6f66826ysk-0>li:before{content:"\0025cf  "}.lst-kix_yh6f66826ysk-4>li:before{content:"\0025cb  "}.lst-kix_5x48r7dz2gvp-6>li:before{content:"\0025cf  "}.lst-kix_spzfuoeekohi-6>li:before{content:"\0025cf  "}.lst-kix_w14ild1qeoey-5>li:before{content:"\0025a0  "}.lst-kix_spzfuoeekohi-2>li:before{content:"\0025a0  "}ul.lst-kix_c2vvnsz0drek-1{list-style-type:none}ul.lst-kix_c2vvnsz0drek-2{list-style-type:none}ul.lst-kix_c2vvnsz0drek-0{list-style-type:none}.lst-kix_ukuyvb7r2sb4-3>li:before{content:"\0025cf  "}.lst-kix_ukuyvb7r2sb4-2>li:before{content:"\0025a0  "}.lst-kix_gr1y63l14uay-1>li:before{content:"-  "}.lst-kix_xqfh3tf7tjc0-3>li:before{content:"\0025cf  "}ul.lst-kix_c2vvnsz0drek-5{list-style-type:none}.lst-kix_xqfh3tf7tjc0-7>li:before{content:"\0025cb  "}ul.lst-kix_c2vvnsz0drek-6{list-style-type:none}ul.lst-kix_c2vvnsz0drek-3{list-style-type:none}ul.lst-kix_c2vvnsz0drek-4{list-style-type:none}.lst-kix_6yojqqjec0st-6>li:before{content:"\0025cf  "}.lst-kix_ehe7zglmx7i2-3>li:before{content:"\0025cf  "}ul.lst-kix_c2vvnsz0drek-7{list-style-type:none}ul.lst-kix_c2vvnsz0drek-8{list-style-type:none}.lst-kix_383wi8io125v-5>li:before{content:"\0025a0  "}ul.lst-kix_d8ik6bmbwuj4-1{list-style-type:none}ul.lst-kix_d8ik6bmbwuj4-0{list-style-type:none}.lst-kix_vhrrjzyjvp5x-1>li:before{content:"\0025cb  "}ul.lst-kix_n8ye7s4j9pok-7{list-style-type:none}ul.lst-kix_n8ye7s4j9pok-6{list-style-type:none}.lst-kix_o9z4cf2yq4mz-3>li:before{content:"\0025cf  "}ul.lst-kix_n8ye7s4j9pok-8{list-style-type:none}.lst-kix_o9z4cf2yq4mz-0>li:before{content:"\0025cf  "}.lst-kix_o9z4cf2yq4mz-4>li:before{content:"\0025cb  "}.lst-kix_o9z4cf2yq4mz-7>li:before{content:"\0025cb  "}.lst-kix_l3zoug7b3vcw-6>li:before{content:"\0025cf  "}.lst-kix_xgsgxyckqw77-1>li:before{content:"\0025cb  "}.lst-kix_ehe7zglmx7i2-6>li:before{content:"\0025cf  "}.lst-kix_gnt40hozc3kv-2>li:before{content:"\0025a0  "}.lst-kix_ehe7zglmx7i2-7>li:before{content:"\0025cb  "}.lst-kix_ho1wvdl93240-7>li:before{content:"\0025cb  "}ul.lst-kix_7r3huctovklw-8{list-style-type:none}.lst-kix_cq59e9nktsby-3>li:before{content:"\0025cf  "}.lst-kix_cq59e9nktsby-7>li:before{content:"\0025cb  "}.lst-kix_7r3huctovklw-2>li:before{content:"\0025a0  "}.lst-kix_58o57c52tmut-4>li:before{content:"\0025cb  "}.lst-kix_x5jl3p2g0327-4>li:before{content:"\0025cb  "}.lst-kix_ymmukz9l2n7v-1>li:before{content:"\0025cb  "}.lst-kix_ho1wvdl93240-3>li:before{content:"\0025cf  "}ul.lst-kix_vzf3xt3x8pov-8{list-style-type:none}.lst-kix_17is42bhveph-6>li:before{content:"\0025cf  "}.lst-kix_saoi6g81wrq2-1>li:before{content:"\0025cb  "}.lst-kix_58o57c52tmut-0>li:before{content:"\0025cf  "}.lst-kix_6asjiojyuog6-3>li:before{content:"\0025cf  "}ul.lst-kix_vzf3xt3x8pov-0{list-style-type:none}ul.lst-kix_vzf3xt3x8pov-1{list-style-type:none}ul.lst-kix_vzf3xt3x8pov-2{list-style-type:none}.lst-kix_gnzb8ea0t50l-5>li:before{content:"\0025a0  "}ul.lst-kix_n8ye7s4j9pok-1{list-style-type:none}ul.lst-kix_vzf3xt3x8pov-3{list-style-type:none}ul.lst-kix_n8ye7s4j9pok-0{list-style-type:none}ul.lst-kix_vzf3xt3x8pov-4{list-style-type:none}ul.lst-kix_n8ye7s4j9pok-3{list-style-type:none}ul.lst-kix_vzf3xt3x8pov-5{list-style-type:none}.lst-kix_ymmukz9l2n7v-5>li:before{content:"\0025a0  "}ul.lst-kix_n8ye7s4j9pok-2{list-style-type:none}ul.lst-kix_vzf3xt3x8pov-6{list-style-type:none}ul.lst-kix_n8ye7s4j9pok-5{list-style-type:none}ul.lst-kix_vzf3xt3x8pov-7{list-style-type:none}ul.lst-kix_n8ye7s4j9pok-4{list-style-type:none}.lst-kix_c2vvnsz0drek-0>li:before{content:"\0025cf  "}.lst-kix_62is116n17wq-8>li:before{content:"\0025a0  "}.lst-kix_383wi8io125v-1>li:before{content:"\0025cb  "}.lst-kix_481c98v2pt83-7>li:before{content:"\0025cb  "}.lst-kix_x5zq7qqqztpc-7>li:before{content:"\0025cb  "}.lst-kix_juam6n4619t2-3>li:before{content:"\0025cf  "}ul.lst-kix_7r3huctovklw-2{list-style-type:none}.lst-kix_17is42bhveph-2>li:before{content:"\0025a0  "}ul.lst-kix_7r3huctovklw-3{list-style-type:none}ul.lst-kix_7r3huctovklw-0{list-style-type:none}ul.lst-kix_7r3huctovklw-1{list-style-type:none}ul.lst-kix_7r3huctovklw-6{list-style-type:none}ul.lst-kix_7r3huctovklw-7{list-style-type:none}ul.lst-kix_7r3huctovklw-4{list-style-type:none}ul.lst-kix_7r3huctovklw-5{list-style-type:none}.lst-kix_x2mjmu7myisq-4>li:before{content:"\0025cb  "}.lst-kix_5p6viq6b9z8b-4>li:before{content:"\0025cb  "}.lst-kix_5p6viq6b9z8b-8>li:before{content:"\0025a0  "}.lst-kix_didibfga6ee5-2>li:before{content:"\0025a0  "}.lst-kix_5ia1ign2sshz-2>li:before{content:"\0025a0  "}.lst-kix_6k5yl8dbqqq2-8>li:before{content:"\0025a0  "}.lst-kix_ukuyvb7r2sb4-6>li:before{content:"\0025cf  "}.lst-kix_3de1jg1kuy24-1>li:before{content:"\0025cb  "}.lst-kix_p2mkzdwo7q6r-8>li:before{content:"\0025a0  "}.lst-kix_bq5ybp1zal0b-7>li:before{content:"\0025cb  "}ul.lst-kix_qhrkxnqxxds4-7{list-style-type:none}ul.lst-kix_qhrkxnqxxds4-8{list-style-type:none}.lst-kix_fk4cszd8thar-1>li:before{content:"\0025cb  "}.lst-kix_fk4cszd8thar-5>li:before{content:"\0025a0  "}.lst-kix_cst6lp3lb98n-6>li:before{content:"\0025cf  "}.lst-kix_bq5ybp1zal0b-3>li:before{content:"\0025cf  "}.lst-kix_x2mjmu7myisq-0>li:before{content:"\0025cf  "}.lst-kix_xav8ikvidc9s-7>li:before{content:"\0025cb  "}.lst-kix_5ia1ign2sshz-6>li:before{content:"\0025cf  "}.lst-kix_gnzb8ea0t50l-1>li:before{content:"\0025cb  "}.lst-kix_wq1o6elz9hy4-0>li:before{content:"\0025cf  "}.lst-kix_wq1o6elz9hy4-4>li:before{content:"\0025cb  "}.lst-kix_vt0etbgrzlaw-1>li:before{content:"\0025cb  "}ul.lst-kix_qhrkxnqxxds4-3{list-style-type:none}ul.lst-kix_qhrkxnqxxds4-4{list-style-type:none}ul.lst-kix_qhrkxnqxxds4-5{list-style-type:none}ul.lst-kix_qhrkxnqxxds4-6{list-style-type:none}ul.lst-kix_qhrkxnqxxds4-0{list-style-type:none}ul.lst-kix_qhrkxnqxxds4-1{list-style-type:none}ul.lst-kix_qhrkxnqxxds4-2{list-style-type:none}.lst-kix_p98dexr65abh-5>li:before{content:"\0025a0  "}.lst-kix_x5jl3p2g0327-0>li:before{content:"\0025cf  "}.lst-kix_saoi6g81wrq2-5>li:before{content:"\0025a0  "}.lst-kix_m8diz2yltkag-5>li:before{content:"\0025a0  "}.lst-kix_crt5jd7c2xq0-1>li:before{content:"\0025cb  "}.lst-kix_p98dexr65abh-1>li:before{content:"\0025cb  "}.lst-kix_kxpudurk2j6e-1>li:before{content:"\0025cb  "}.lst-kix_6r73oe35erxy-0>li:before{content:"\0025cf  "}.lst-kix_m46mj27r3fpz-0>li:before{content:"\0025cf  "}.lst-kix_hydge7xfbt8r-1>li:before{content:"\0025cb  "}.lst-kix_sxaq135mthse-3>li:before{content:"\0025cf  "}.lst-kix_wq1o6elz9hy4-7>li:before{content:"\0025cb  "}.lst-kix_u9ija913pfxg-8>li:before{content:"-  "}.lst-kix_edj3r8hgf2z3-7>li:before{content:"\0025cb  "}.lst-kix_u9ija913pfxg-1>li:before{content:"-  "}.lst-kix_nf1j1iafbq39-8>li:before{content:"\0025a0  "}ul.lst-kix_8cma8sbxfy2x-8{list-style-type:none}.lst-kix_sxaq135mthse-6>li:before{content:"\0025cf  "}.lst-kix_jbw6p6lajlkh-7>li:before{content:"\0025cb  "}ul.lst-kix_8cma8sbxfy2x-0{list-style-type:none}ul.lst-kix_8cma8sbxfy2x-1{list-style-type:none}ul.lst-kix_8cma8sbxfy2x-2{list-style-type:none}ul.lst-kix_8cma8sbxfy2x-3{list-style-type:none}ul.lst-kix_8cma8sbxfy2x-4{list-style-type:none}ul.lst-kix_8cma8sbxfy2x-5{list-style-type:none}.lst-kix_rcp8sez4svjd-5>li:before{content:"-  "}.lst-kix_ljjwvb506pet-2>li:before{content:"\0025a0  "}ul.lst-kix_8cma8sbxfy2x-6{list-style-type:none}ul.lst-kix_8cma8sbxfy2x-7{list-style-type:none}.lst-kix_didibfga6ee5-5>li:before{content:"\0025a0  "}.lst-kix_398os0jr1iw3-1>li:before{content:"\0025cb  "}.lst-kix_tg2hwffju8t9-6>li:before{content:"\0025cf  "}.lst-kix_5fj31eh1jc7-5>li:before{content:"\0025a0  "}.lst-kix_b8dlmlw0p2p1-5>li:before{content:"\0025a0  "}.lst-kix_398os0jr1iw3-4>li:before{content:"\0025cb  "}.lst-kix_5d66udn9quk7-1>li:before{content:"\0025cb  "}.lst-kix_douubz1tb8an-4>li:before{content:"\0025cb  "}.lst-kix_jbw6p6lajlkh-0>li:before{content:"\0025cf  "}.lst-kix_b8dlmlw0p2p1-2>li:before{content:"\0025a0  "}.lst-kix_5kaugn46jpw6-2>li:before{content:"\0025a0  "}.lst-kix_7apmk6vwlj5r-3>li:before{content:"\0025cf  "}.lst-kix_x3vcornhe3kv-2>li:before{content:"\0025a0  "}.lst-kix_tg2hwffju8t9-3>li:before{content:"\0025cf  "}.lst-kix_5fj31eh1jc7-2>li:before{content:"\0025a0  "}.lst-kix_7apmk6vwlj5r-6>li:before{content:"\0025cf  "}.lst-kix_u82x83buy4jv-6>li:before{content:"\0025cf  "}.lst-kix_x3vcornhe3kv-5>li:before{content:"\0025a0  "}ul.lst-kix_w1d9l4m1yg5x-6{list-style-type:none}.lst-kix_7b2y41o7s2k3-4>li:before{content:"\0025cb  "}ul.lst-kix_w1d9l4m1yg5x-5{list-style-type:none}ul.lst-kix_w1d9l4m1yg5x-8{list-style-type:none}.lst-kix_mxwp6np2u5xu-8>li:before{content:"-  "}ul.lst-kix_w1d9l4m1yg5x-7{list-style-type:none}.lst-kix_7b2y41o7s2k3-1>li:before{content:"\0025cb  "}.lst-kix_nbpnmy8xci8w-1>li:before{content:"\0025cb  "}.lst-kix_dq6vem8xoqp9-2>li:before{content:"\0025a0  "}.lst-kix_5kaugn46jpw6-5>li:before{content:"\0025a0  "}.lst-kix_f3egesa43r6b-8>li:before{content:"\0025a0  "}.lst-kix_kk8oms3v3iyb-4>li:before{content:"\0025cb  "}.lst-kix_hydge7xfbt8r-4>li:before{content:"\0025cb  "}.lst-kix_8hjy0hqh3rvg-8>li:before{content:"\0025a0  "}ul.lst-kix_w1d9l4m1yg5x-0{list-style-type:none}.lst-kix_douubz1tb8an-7>li:before{content:"\0025cb  "}ul.lst-kix_w1d9l4m1yg5x-2{list-style-type:none}.lst-kix_kk8oms3v3iyb-7>li:before{content:"\0025cb  "}ul.lst-kix_w1d9l4m1yg5x-1{list-style-type:none}ul.lst-kix_w1d9l4m1yg5x-4{list-style-type:none}ul.lst-kix_w1d9l4m1yg5x-3{list-style-type:none}.lst-kix_6asjiojyuog6-6>li:before{content:"\0025cf  "}ul.lst-kix_gnzb8ea0t50l-0{list-style-type:none}ul.lst-kix_gnzb8ea0t50l-1{list-style-type:none}ul.lst-kix_gnzb8ea0t50l-2{list-style-type:none}.lst-kix_cq59e9nktsby-0>li:before{content:"\0025cf  "}ul.lst-kix_gnzb8ea0t50l-3{list-style-type:none}ul.lst-kix_gnzb8ea0t50l-4{list-style-type:none}ul.lst-kix_gnzb8ea0t50l-5{list-style-type:none}ul.lst-kix_gnzb8ea0t50l-6{list-style-type:none}ul.lst-kix_gnzb8ea0t50l-7{list-style-type:none}ul.lst-kix_gnzb8ea0t50l-8{list-style-type:none}ul.lst-kix_4k4id0a37km3-6{list-style-type:none}ul.lst-kix_4k4id0a37km3-5{list-style-type:none}ul.lst-kix_4k4id0a37km3-8{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-8{list-style-type:none}ul.lst-kix_4k4id0a37km3-7{list-style-type:none}.lst-kix_wdjiiwxyq2d1-4>li:before{content:"\0025cb  "}ul.lst-kix_dj4fsxw52t4l-6{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-7{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-4{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-5{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-2{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-3{list-style-type:none}ul.lst-kix_4k4id0a37km3-0{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-0{list-style-type:none}ul.lst-kix_dj4fsxw52t4l-1{list-style-type:none}ul.lst-kix_4k4id0a37km3-2{list-style-type:none}.lst-kix_ho1wvdl93240-0>li:before{content:"\0025cf  "}.lst-kix_d995nc7613a9-7>li:before{content:"\0025cb  "}ul.lst-kix_4k4id0a37km3-1{list-style-type:none}ul.lst-kix_4k4id0a37km3-4{list-style-type:none}ul.lst-kix_4k4id0a37km3-3{list-style-type:none}.lst-kix_a9t5z6hxg4oz-0>li:before{content:"\0025cf  "}.lst-kix_mh0uhwysa91-1>li:before{content:"\0025cb  "}.lst-kix_jzarhjey1jrh-3>li:before{content:"\0025cf  "}.lst-kix_i1kls5tk4gt6-4>li:before{content:"\0025cb  "}.lst-kix_f51q5t2m61i8-0>li:before{content:"\0025cf  "}.lst-kix_xkjgsygnpr2m-3>li:before{content:"\0025cf  "}.lst-kix_gswyhyotsm36-1>li:before{content:"\0025cb  "}.lst-kix_ulntv5wergyk-3>li:before{content:"\0025cf  "}.lst-kix_gwuatnbmuewh-4>li:before{content:"\0025cb  "}.lst-kix_n21asioviqe4-6>li:before{content:"\0025cf  "}.lst-kix_8wesot3ko53b-7>li:before{content:"\0025cb  "}.lst-kix_crb89j7xw8q2-8>li:before{content:"\0025a0  "}.lst-kix_o4ah8y8am4ct-2>li:before{content:"\0025a0  "}.lst-kix_l4pre1xosssa-0>li:before{content:"\0025cf  "}.lst-kix_emy9ap8zeql8-8>li:before{content:"\0025a0  "}.lst-kix_x2mjmu7myisq-7>li:before{content:"\0025cb  "}.lst-kix_5p6viq6b9z8b-1>li:before{content:"\0025cb  "}.lst-kix_313q3f5x4p2m-3>li:before{content:"\0025cf  "}.lst-kix_o14fwownm59e-8>li:before{content:"\0025a0  "}.lst-kix_ow9ix2mme95u-7>li:before{content:"\0025cb  "}ul.lst-kix_2zf5bd1mem1t-0{list-style-type:none}.lst-kix_8g8vlw6j0asg-5>li:before{content:"\0025a0  "}.lst-kix_2jmrp4ochkmc-2>li:before{content:"\0025a0  "}.lst-kix_ct57w2de70tb-6>li:before{content:"\0025cf  "}.lst-kix_n1766bho68n4-7>li:before{content:"\0025cb  "}.lst-kix_ex6gjyozs00-6>li:before{content:"\0025cf  "}.lst-kix_iqbmivscsmun-3>li:before{content:"\0025cf  "}.lst-kix_u6q708mtjbvx-1>li:before{content:"-  "}.lst-kix_fk4cszd8thar-8>li:before{content:"\0025a0  "}ul.lst-kix_2zf5bd1mem1t-7{list-style-type:none}ul.lst-kix_2zf5bd1mem1t-8{list-style-type:none}ul.lst-kix_2zf5bd1mem1t-5{list-style-type:none}ul.lst-kix_2zf5bd1mem1t-6{list-style-type:none}ul.lst-kix_2zf5bd1mem1t-3{list-style-type:none}ul.lst-kix_2zf5bd1mem1t-4{list-style-type:none}ul.lst-kix_2zf5bd1mem1t-1{list-style-type:none}ul.lst-kix_2zf5bd1mem1t-2{list-style-type:none}.lst-kix_iigck7esqel5-2>li:before{content:"\0025a0  "}ul.lst-kix_8fyyex2aqhl9-5{list-style-type:none}ul.lst-kix_8fyyex2aqhl9-6{list-style-type:none}ul.lst-kix_8fyyex2aqhl9-7{list-style-type:none}ul.lst-kix_8fyyex2aqhl9-8{list-style-type:none}ul.lst-kix_8fyyex2aqhl9-1{list-style-type:none}.lst-kix_saoi6g81wrq2-8>li:before{content:"\0025a0  "}.lst-kix_m8diz2yltkag-2>li:before{content:"\0025a0  "}ul.lst-kix_8fyyex2aqhl9-2{list-style-type:none}.lst-kix_9khvkfghhb3b-4>li:before{content:"\0025cb  "}.lst-kix_kz1sfsbi2eng-1>li:before{content:"\0025cb  "}ul.lst-kix_8fyyex2aqhl9-3{list-style-type:none}.lst-kix_p2mkzdwo7q6r-5>li:before{content:"\0025a0  "}ul.lst-kix_8fyyex2aqhl9-4{list-style-type:none}ul.lst-kix_8fyyex2aqhl9-0{list-style-type:none}.lst-kix_2nh9kwfbzbik-3>li:before{content:"\0025cf  "}.lst-kix_369j2eishmb5-2>li:before{content:"\0025a0  "}.lst-kix_6d5ax9c8k1xa-0>li:before{content:"\0025cf  "}.lst-kix_fjl32nwuhu4u-1>li:before{content:"\0025cb  "}.lst-kix_mgyoyquoxrm1-4>li:before{content:"\0025cb  "}ul.lst-kix_xkjgsygnpr2m-8{list-style-type:none}ul.lst-kix_xkjgsygnpr2m-7{list-style-type:none}.lst-kix_nc0l9nyccr86-7>li:before{content:"\0025cb  "}.lst-kix_wq4pisfqksnu-4>li:before{content:"\0025cb  "}.lst-kix_8k70e8i8rdlm-7>li:before{content:"\0025cb  "}.lst-kix_mgyoyquoxrm1-7>li:before{content:"\0025cb  "}.lst-kix_z8j4l8h7k6b3-6>li:before{content:"\0025cf  "}.lst-kix_g0l7lf51hdyf-8>li:before{content:"\0025a0  "}.lst-kix_1a8ua5u4a26p-6>li:before{content:"\0025cf  "}.lst-kix_wq4pisfqksnu-7>li:before{content:"\0025cb  "}.lst-kix_g0l7lf51hdyf-5>li:before{content:"\0025a0  "}.lst-kix_l3zoug7b3vcw-3>li:before{content:"\0025cf  "}.lst-kix_h99428b6laza-4>li:before{content:"\0025cb  "}.lst-kix_4fwhm2tiefhz-2>li:before{content:"\0025a0  "}.lst-kix_74gx9bwnn8kt-6>li:before{content:"\0025cf  "}.lst-kix_w14ild1qeoey-8>li:before{content:"\0025a0  "}.lst-kix_scilllbfudea-8>li:before{content:"\0025a0  "}ul.lst-kix_l1rt7mvaz29b-8{list-style-type:none}ul.lst-kix_l1rt7mvaz29b-7{list-style-type:none}.lst-kix_kkphisidn6cz-4>li:before{content:"\0025cb  "}ul.lst-kix_l1rt7mvaz29b-6{list-style-type:none}ul.lst-kix_l1rt7mvaz29b-5{list-style-type:none}.lst-kix_894copnhklgz-4>li:before{content:"\0025cb  "}ul.lst-kix_l1rt7mvaz29b-4{list-style-type:none}ul.lst-kix_l1rt7mvaz29b-3{list-style-type:none}ul.lst-kix_l1rt7mvaz29b-2{list-style-type:none}ul.lst-kix_l1rt7mvaz29b-1{list-style-type:none}ul.lst-kix_l1rt7mvaz29b-0{list-style-type:none}ul.lst-kix_xkjgsygnpr2m-6{list-style-type:none}ul.lst-kix_xkjgsygnpr2m-5{list-style-type:none}ul.lst-kix_xkjgsygnpr2m-4{list-style-type:none}.lst-kix_kkphisidn6cz-7>li:before{content:"\0025cb  "}ul.lst-kix_xkjgsygnpr2m-3{list-style-type:none}ul.lst-kix_xkjgsygnpr2m-2{list-style-type:none}.lst-kix_scilllbfudea-5>li:before{content:"\0025a0  "}ul.lst-kix_xkjgsygnpr2m-1{list-style-type:none}ul.lst-kix_xkjgsygnpr2m-0{list-style-type:none}.lst-kix_bj6mzrhm69bi-2>li:before{content:"\0025a0  "}.lst-kix_solhl6ysg65v-6>li:before{content:"\0025cf  "}.lst-kix_z1vxl485jzzo-2>li:before{content:"\0025a0  "}.lst-kix_ut5bvwbkq6rt-2>li:before{content:"\0025a0  "}.lst-kix_z4z7nvld905a-8>li:before{content:"\0025a0  "}.lst-kix_ut5bvwbkq6rt-5>li:before{content:"\0025a0  "}.lst-kix_dj4fsxw52t4l-2>li:before{content:"\0025a0  "}.lst-kix_2ihanzwz4fgf-6>li:before{content:"\0025cf  "}.lst-kix_riff65m65tnf-2>li:before{content:"\0025a0  "}.lst-kix_rwfcya85pd71-2>li:before{content:"\0025a0  "}.lst-kix_z8j4l8h7k6b3-3>li:before{content:"\0025cf  "}.lst-kix_l1rt7mvaz29b-5>li:before{content:"\0025a0  "}.lst-kix_h99428b6laza-1>li:before{content:"\0025cb  "}ul.lst-kix_yhn6vvmwi8p2-1{list-style-type:none}ul.lst-kix_8z696e8bv17-2{list-style-type:none}ul.lst-kix_yhn6vvmwi8p2-2{list-style-type:none}ul.lst-kix_8z696e8bv17-1{list-style-type:none}ul.lst-kix_8z696e8bv17-0{list-style-type:none}ul.lst-kix_yhn6vvmwi8p2-0{list-style-type:none}ul.lst-kix_yhn6vvmwi8p2-5{list-style-type:none}ul.lst-kix_yhn6vvmwi8p2-6{list-style-type:none}ul.lst-kix_yhn6vvmwi8p2-3{list-style-type:none}.lst-kix_8z696e8bv17-3>li:before{content:"\0025cf  "}ul.lst-kix_yhn6vvmwi8p2-4{list-style-type:none}ul.lst-kix_yhn6vvmwi8p2-7{list-style-type:none}ul.lst-kix_8z696e8bv17-8{list-style-type:none}ul.lst-kix_yhn6vvmwi8p2-8{list-style-type:none}ul.lst-kix_8z696e8bv17-7{list-style-type:none}ul.lst-kix_8z696e8bv17-6{list-style-type:none}ul.lst-kix_8z696e8bv17-5{list-style-type:none}ul.lst-kix_8z696e8bv17-4{list-style-type:none}ul.lst-kix_8z696e8bv17-3{list-style-type:none}ul.lst-kix_58o57c52tmut-8{list-style-type:none}ul.lst-kix_58o57c52tmut-7{list-style-type:none}.lst-kix_z3si3pbri1de-8>li:before{content:"\0025a0  "}.lst-kix_yhn6vvmwi8p2-1>li:before{content:"\0025cb  "}.lst-kix_gwuatnbmuewh-1>li:before{content:"\0025cb  "}.lst-kix_xgfpveff5zr0-7>li:before{content:"\0025cb  "}.lst-kix_d995nc7613a9-4>li:before{content:"\0025cb  "}.lst-kix_e2bt8y2ywvrz-0>li:before{content:"\0025cf  "}.lst-kix_z1voh5a7dove-6>li:before{content:"\0025cf  "}ul.lst-kix_58o57c52tmut-0{list-style-type:none}.lst-kix_tglcy61nnbu3-1>li:before{content:"\0025cb  "}ul.lst-kix_58o57c52tmut-2{list-style-type:none}ul.lst-kix_58o57c52tmut-1{list-style-type:none}.lst-kix_xkmlxx8lhm5d-3>li:before{content:"\0025cf  "}ul.lst-kix_58o57c52tmut-4{list-style-type:none}ul.lst-kix_58o57c52tmut-3{list-style-type:none}ul.lst-kix_58o57c52tmut-6{list-style-type:none}ul.lst-kix_58o57c52tmut-5{list-style-type:none}.lst-kix_l1rt7mvaz29b-8>li:before{content:"\0025a0  "}.lst-kix_isql72ps5u79-8>li:before{content:"\0025a0  "}.lst-kix_bjneibxfrx3r-8>li:before{content:"\0025a0  "}.lst-kix_jzarhjey1jrh-0>li:before{content:"\0025cf  "}.lst-kix_aui8m1kby916-2>li:before{content:"\0025a0  "}.lst-kix_crb89j7xw8q2-5>li:before{content:"\0025a0  "}.lst-kix_cy2og2rt03zo-0>li:before{content:"\0025cf  "}.lst-kix_mm2j3zumujcl-3>li:before{content:"\0025cf  "}.lst-kix_f51q5t2m61i8-3>li:before{content:"\0025cf  "}.lst-kix_n21asioviqe4-3>li:before{content:"\0025cf  "}.lst-kix_bj6mzrhm69bi-5>li:before{content:"\0025a0  "}.lst-kix_7jzvzpyn9bnk-4>li:before{content:"\0025cb  "}.lst-kix_emy9ap8zeql8-5>li:before{content:"\0025a0  "}.lst-kix_7v1j337157vp-1>li:before{content:"\0025cb  "}.lst-kix_ftgmvnn66fx0-0>li:before{content:"\0025cf  "}.lst-kix_474t6llskbh7-7>li:before{content:"\0025cb  "}.lst-kix_6jabh0roz5s1-6>li:before{content:"\0025cf  "}ul.lst-kix_bj2se7l3xxv-6{list-style-type:none}.lst-kix_8g8vlw6j0asg-8>li:before{content:"\0025a0  "}ul.lst-kix_bj2se7l3xxv-5{list-style-type:none}.lst-kix_r75b0bpeph0o-3>li:before{content:"\0025cf  "}ul.lst-kix_bj2se7l3xxv-8{list-style-type:none}.lst-kix_n1766bho68n4-4>li:before{content:"\0025cb  "}ul.lst-kix_bj2se7l3xxv-7{list-style-type:none}.lst-kix_ct57w2de70tb-3>li:before{content:"\0025cf  "}.lst-kix_2jmrp4ochkmc-5>li:before{content:"\0025a0  "}.lst-kix_ow9ix2mme95u-4>li:before{content:"\0025cb  "}.lst-kix_iqbmivscsmun-6>li:before{content:"\0025cf  "}.lst-kix_8fyyex2aqhl9-4>li:before{content:"\0025cb  "}ul.lst-kix_bj2se7l3xxv-2{list-style-type:none}ul.lst-kix_bj2se7l3xxv-1{list-style-type:none}ul.lst-kix_bj2se7l3xxv-4{list-style-type:none}ul.lst-kix_bj2se7l3xxv-3{list-style-type:none}ul.lst-kix_bj2se7l3xxv-0{list-style-type:none}.lst-kix_vzf3xt3x8pov-5>li:before{content:"\0025a0  "}.lst-kix_2s6hp75gzisz-5>li:before{content:"\0025a0  "}.lst-kix_o4d3wx7elf58-0>li:before{content:"\0025cf  "}.lst-kix_qp3zt329w8t5-1>li:before{content:"\0025cb  "}.lst-kix_qp3zt329w8t5-5>li:before{content:"\0025a0  "}.lst-kix_qp3zt329w8t5-7>li:before{content:"\0025cb  "}.lst-kix_hl5oa0f6tf4f-4>li:before{content:"\0025cb  "}.lst-kix_dowwk88w0dao-5>li:before{content:"\0025a0  "}.lst-kix_1qjopwyk3n75-0>li:before{content:"\0025cf  "}.lst-kix_1qjopwyk3n75-4>li:before{content:"\0025cb  "}.lst-kix_gsefpdxx2m31-0>li:before{content:"\0025cf  "}.lst-kix_dowwk88w0dao-3>li:before{content:"\0025cf  "}.lst-kix_ltwr0s349me7-8>li:before{content:"\0025a0  "}.lst-kix_hl5oa0f6tf4f-6>li:before{content:"\0025cf  "}ul.lst-kix_u1clvxqptoo-2{list-style-type:none}ul.lst-kix_u1clvxqptoo-1{list-style-type:none}ul.lst-kix_u1clvxqptoo-0{list-style-type:none}.lst-kix_hl5oa0f6tf4f-0>li:before{content:"\0025cf  "}.lst-kix_gsefpdxx2m31-8>li:before{content:"\0025a0  "}.lst-kix_kk7o9gi3ij0l-1>li:before{content:"\0025cb  "}ul.lst-kix_u1clvxqptoo-8{list-style-type:none}ul.lst-kix_u1clvxqptoo-7{list-style-type:none}ul.lst-kix_u1clvxqptoo-6{list-style-type:none}ul.lst-kix_u1clvxqptoo-5{list-style-type:none}ul.lst-kix_u1clvxqptoo-4{list-style-type:none}ul.lst-kix_u1clvxqptoo-3{list-style-type:none}.lst-kix_bj2se7l3xxv-2>li:before{content:"\0025a0  "}.lst-kix_acuhah6xdzsp-4>li:before{content:"\0025cb  "}.lst-kix_bj2se7l3xxv-4>li:before{content:"\0025cb  "}.lst-kix_acuhah6xdzsp-6>li:before{content:"\0025cf  "}.lst-kix_gsefpdxx2m31-6>li:before{content:"\0025cf  "}.lst-kix_pfssn45qm13m-0>li:before{content:"\0025cf  "}ul.lst-kix_spzfuoeekohi-2{list-style-type:none}ul.lst-kix_spzfuoeekohi-1{list-style-type:none}ul.lst-kix_spzfuoeekohi-4{list-style-type:none}.lst-kix_uest4hng52tf-2>li:before{content:"\0025a0  "}ul.lst-kix_spzfuoeekohi-3{list-style-type:none}ul.lst-kix_spzfuoeekohi-6{list-style-type:none}.lst-kix_7qzrf8aug8gu-7>li:before{content:"\0025cb  "}ul.lst-kix_spzfuoeekohi-5{list-style-type:none}ul.lst-kix_spzfuoeekohi-8{list-style-type:none}ul.lst-kix_spzfuoeekohi-7{list-style-type:none}.lst-kix_uest4hng52tf-8>li:before{content:"\0025a0  "}ul.lst-kix_spzfuoeekohi-0{list-style-type:none}.lst-kix_2mto5juffctz-4>li:before{content:"\0025cb  "}.lst-kix_h4ky6k6cvoue-5>li:before{content:"\0025a0  "}.lst-kix_pfssn45qm13m-6>li:before{content:"\0025cf  "}.lst-kix_pfssn45qm13m-4>li:before{content:"\0025cb  "}.lst-kix_axhbb6oyblrk-5>li:before{content:"\0025a0  "}.lst-kix_h4ky6k6cvoue-3>li:before{content:"\0025cf  "}.lst-kix_qwp9c5bklrce-4>li:before{content:"\0025cb  "}.lst-kix_tlb25jkk5mkd-3>li:before{content:"\0025cf  "}.lst-kix_rbhmpzgzmhdz-8>li:before{content:"\0025a0  "}.lst-kix_qhrkxnqxxds4-2>li:before{content:"\0025a0  "}.lst-kix_3xkj88gevxle-6>li:before{content:"\0025cf  "}.lst-kix_tlb25jkk5mkd-1>li:before{content:"\0025cb  "}.lst-kix_qhrkxnqxxds4-8>li:before{content:"\0025a0  "}.lst-kix_uz5smpj76010-6>li:before{content:"\0025cf  "}.lst-kix_rbhmpzgzmhdz-6>li:before{content:"\0025cf  "}.lst-kix_3xkj88gevxle-0>li:before{content:"\0025cf  "}.lst-kix_3xkj88gevxle-8>li:before{content:"\0025a0  "}.lst-kix_rbhmpzgzmhdz-0>li:before{content:"\0025cf  "}.lst-kix_7qzrf8aug8gu-1>li:before{content:"\0025cb  "}.lst-kix_uz5smpj76010-8>li:before{content:"\0025a0  "}.lst-kix_7qzrf8aug8gu-3>li:before{content:"\0025cf  "}.lst-kix_gg06s1qsfn23-5>li:before{content:"\0025a0  "}.lst-kix_3fnpvvr0i8os-5>li:before{content:"\0025a0  "}.lst-kix_ltwr0s349me7-2>li:before{content:"\0025a0  "}.lst-kix_xgfpveff5zr0-4>li:before{content:"\0025cb  "}.lst-kix_rbhmpzgzmhdz-2>li:before{content:"\0025a0  "}.lst-kix_ltwr0s349me7-0>li:before{content:"\0025cf  "}.lst-kix_gc2knt874en1-0>li:before{content:"\0025cf  "}.lst-kix_xgfpveff5zr0-6>li:before{content:"\0025cf  "}.lst-kix_8nsyajlscxcg-8>li:before{content:"\0025a0  "}.lst-kix_pjyqz9f44g8s-3>li:before{content:"\0025cf  "}ul.lst-kix_6ykz6yxb3o6x-8{list-style-type:none}ul.lst-kix_6ykz6yxb3o6x-7{list-style-type:none}.lst-kix_pjyqz9f44g8s-5>li:before{content:"\0025a0  "}ul.lst-kix_6ykz6yxb3o6x-6{list-style-type:none}ul.lst-kix_6ykz6yxb3o6x-5{list-style-type:none}ul.lst-kix_6ykz6yxb3o6x-4{list-style-type:none}.lst-kix_8nsyajlscxcg-2>li:before{content:"\0025a0  "}.lst-kix_8nsyajlscxcg-6>li:before{content:"\0025cf  "}.lst-kix_gpgigcfykzff-4>li:before{content:"\0025cb  "}.lst-kix_79uewz4g8df3-8>li:before{content:"\0025a0  "}ul.lst-kix_xqqiyb3b5ag0-1{list-style-type:none}.lst-kix_z1voh5a7dove-7>li:before{content:"\0025cb  "}ul.lst-kix_xqqiyb3b5ag0-2{list-style-type:none}ul.lst-kix_xqqiyb3b5ag0-0{list-style-type:none}.lst-kix_79uewz4g8df3-2>li:before{content:"\0025a0  "}.lst-kix_uz5smpj76010-0>li:before{content:"\0025cf  "}ul.lst-kix_6ykz6yxb3o6x-3{list-style-type:none}ul.lst-kix_6ykz6yxb3o6x-2{list-style-type:none}ul.lst-kix_6ykz6yxb3o6x-1{list-style-type:none}.lst-kix_79uewz4g8df3-6>li:before{content:"\0025cf  "}ul.lst-kix_6ykz6yxb3o6x-0{list-style-type:none}.lst-kix_tltq6x9aaf31-5>li:before{content:"\0025a0  "}.lst-kix_z1voh5a7dove-1>li:before{content:"\0025cb  "}.lst-kix_79uewz4g8df3-0>li:before{content:"\0025cf  "}.lst-kix_1ig57j6lcs95-5>li:before{content:"\0025a0  "}ul.lst-kix_l3zoug7b3vcw-0{list-style-type:none}.lst-kix_7v1j337157vp-0>li:before{content:"\0025cf  "}.lst-kix_8swzovi1ytnr-2>li:before{content:"\0025a0  "}ul.lst-kix_l3zoug7b3vcw-8{list-style-type:none}ul.lst-kix_l3zoug7b3vcw-7{list-style-type:none}ul.lst-kix_l3zoug7b3vcw-6{list-style-type:none}ul.lst-kix_l3zoug7b3vcw-5{list-style-type:none}ul.lst-kix_l3zoug7b3vcw-4{list-style-type:none}ul.lst-kix_l3zoug7b3vcw-3{list-style-type:none}.lst-kix_1xoigud6ruor-7>li:before{content:"\0025cb  "}ul.lst-kix_l3zoug7b3vcw-2{list-style-type:none}ul.lst-kix_l3zoug7b3vcw-1{list-style-type:none}.lst-kix_vebf56ofbjcp-2>li:before{content:"\0025a0  "}.lst-kix_8cma8sbxfy2x-2>li:before{content:"\0025a0  "}.lst-kix_tglcy61nnbu3-4>li:before{content:"\0025cb  "}.lst-kix_vebf56ofbjcp-6>li:before{content:"\0025cf  "}.lst-kix_tglcy61nnbu3-2>li:before{content:"\0025a0  "}.lst-kix_b07y002136mr-5>li:before{content:"\0025a0  "}.lst-kix_8nsyajlscxcg-0>li:before{content:"\0025cf  "}.lst-kix_vzf3xt3x8pov-2>li:before{content:"\0025a0  "}.lst-kix_u1clvxqptoo-3>li:before{content:"\0025cf  "}.lst-kix_vzf3xt3x8pov-4>li:before{content:"\0025cb  "}.lst-kix_tglcy61nnbu3-8>li:before{content:"\0025a0  "}.lst-kix_vebf56ofbjcp-0>li:before{content:"\0025cf  "}.lst-kix_6d5ax9c8k1xa-3>li:before{content:"\0025cf  "}.lst-kix_16ygmg7toocv-7>li:before{content:"\0025cb  "}.lst-kix_kxfg6pdeancy-0>li:before{content:"\0025cf  "}.lst-kix_yv7c7gf6elha-4>li:before{content:"\0025cb  "}.lst-kix_2nh9kwfbzbik-0>li:before{content:"\0025cf  "}.lst-kix_nc0l9nyccr86-1>li:before{content:"\0025cb  "}.lst-kix_6d5ax9c8k1xa-7>li:before{content:"\0025cb  "}.lst-kix_kxfg6pdeancy-4>li:before{content:"\0025cb  "}.lst-kix_emewkhtp1rfy-4>li:before{content:"\0025cb  "}.lst-kix_894copnhklgz-2>li:before{content:"\0025a0  "}.lst-kix_h99428b6laza-3>li:before{content:"\0025cf  "}.lst-kix_emewkhtp1rfy-0>li:before{content:"\0025cf  "}.lst-kix_b07y002136mr-3>li:before{content:"\0025cf  "}.lst-kix_mgyoyquoxrm1-1>li:before{content:"\0025cb  "}.lst-kix_vebf56ofbjcp-8>li:before{content:"\0025a0  "}ul.lst-kix_yh6f66826ysk-0{list-style-type:none}ul.lst-kix_yh6f66826ysk-3{list-style-type:none}ul.lst-kix_yh6f66826ysk-4{list-style-type:none}.lst-kix_1xoigud6ruor-5>li:before{content:"\0025a0  "}ul.lst-kix_yh6f66826ysk-1{list-style-type:none}.lst-kix_riff65m65tnf-7>li:before{content:"\0025cb  "}ul.lst-kix_yh6f66826ysk-2{list-style-type:none}.lst-kix_jys0trw4gd1m-6>li:before{content:"\0025cf  "}ul.lst-kix_s6xk062mdkv5-3{list-style-type:none}ul.lst-kix_yh6f66826ysk-7{list-style-type:none}ul.lst-kix_s6xk062mdkv5-2{list-style-type:none}ul.lst-kix_yh6f66826ysk-8{list-style-type:none}ul.lst-kix_s6xk062mdkv5-1{list-style-type:none}ul.lst-kix_yh6f66826ysk-5{list-style-type:none}ul.lst-kix_s6xk062mdkv5-0{list-style-type:none}ul.lst-kix_yh6f66826ysk-6{list-style-type:none}.lst-kix_7q22x2z5v5y-0>li:before{content:"\0025cf  "}.lst-kix_74gx9bwnn8kt-1>li:before{content:"\0025cb  "}.lst-kix_9cwmoyhkmhfb-2>li:before{content:"\0025a0  "}ul.lst-kix_rbhmpzgzmhdz-0{list-style-type:none}ul.lst-kix_rbhmpzgzmhdz-1{list-style-type:none}.lst-kix_jhqr8d48vqqp-3>li:before{content:"\0025cf  "}ul.lst-kix_rbhmpzgzmhdz-2{list-style-type:none}.lst-kix_kkphisidn6cz-6>li:before{content:"\0025cf  "}ul.lst-kix_rbhmpzgzmhdz-3{list-style-type:none}.lst-kix_1xoigud6ruor-1>li:before{content:"\0025cb  "}ul.lst-kix_rbhmpzgzmhdz-4{list-style-type:none}.lst-kix_scilllbfudea-2>li:before{content:"\0025a0  "}ul.lst-kix_rbhmpzgzmhdz-5{list-style-type:none}ul.lst-kix_rbhmpzgzmhdz-6{list-style-type:none}.lst-kix_xrce7kijh7l1-0>li:before{content:"\0025cf  "}ul.lst-kix_rbhmpzgzmhdz-7{list-style-type:none}ul.lst-kix_rbhmpzgzmhdz-8{list-style-type:none}.lst-kix_8swzovi1ytnr-8>li:before{content:"\0025a0  "}.lst-kix_fjl32nwuhu4u-0>li:before{content:"\0025cf  "}.lst-kix_z4z7nvld905a-7>li:before{content:"\0025cb  "}ul.lst-kix_scilllbfudea-5{list-style-type:none}ul.lst-kix_scilllbfudea-6{list-style-type:none}.lst-kix_7v1j337157vp-6>li:before{content:"\0025cf  "}ul.lst-kix_scilllbfudea-3{list-style-type:none}ul.lst-kix_scilllbfudea-4{list-style-type:none}ul.lst-kix_scilllbfudea-1{list-style-type:none}ul.lst-kix_scilllbfudea-2{list-style-type:none}.lst-kix_nbuhcgd4ft21-7>li:before{content:"\0025cb  "}ul.lst-kix_scilllbfudea-0{list-style-type:none}.lst-kix_2ihanzwz4fgf-3>li:before{content:"\0025cf  "}.lst-kix_d4lbyvb2g3j-0>li:before{content:"\0025cf  "}.lst-kix_8z696e8bv17-5>li:before{content:"\0025a0  "}ul.lst-kix_scilllbfudea-7{list-style-type:none}.lst-kix_4k4id0a37km3-2>li:before{content:"\0025a0  "}ul.lst-kix_scilllbfudea-8{list-style-type:none}ul.lst-kix_s6xk062mdkv5-8{list-style-type:none}ul.lst-kix_s6xk062mdkv5-7{list-style-type:none}ul.lst-kix_s6xk062mdkv5-6{list-style-type:none}ul.lst-kix_s6xk062mdkv5-5{list-style-type:none}.lst-kix_xkjgsygnpr2m-1>li:before{content:"\0025cb  "}ul.lst-kix_s6xk062mdkv5-4{list-style-type:none}.lst-kix_dj4fsxw52t4l-4>li:before{content:"\0025cb  "}.lst-kix_xrce7kijh7l1-8>li:before{content:"\0025a0  "}.lst-kix_ut5bvwbkq6rt-3>li:before{content:"\0025cf  "}ul.lst-kix_xqqiyb3b5ag0-5{list-style-type:none}ul.lst-kix_xqqiyb3b5ag0-6{list-style-type:none}ul.lst-kix_xqqiyb3b5ag0-3{list-style-type:none}ul.lst-kix_xqqiyb3b5ag0-4{list-style-type:none}.lst-kix_d8ik6bmbwuj4-7>li:before{content:"\0025cb  "}.lst-kix_s6xk062mdkv5-5>li:before{content:"\0025a0  "}.lst-kix_rwfcya85pd71-0>li:before{content:"\0025cf  "}ul.lst-kix_xqqiyb3b5ag0-7{list-style-type:none}ul.lst-kix_xqqiyb3b5ag0-8{list-style-type:none}.lst-kix_z8j4l8h7k6b3-5>li:before{content:"\0025a0  "}.lst-kix_d1iyykra6bco-5>li:before{content:"\0025a0  "}.lst-kix_d2ysoruvczg5-1>li:before{content:"\0025cb  "}.lst-kix_rwfcya85pd71-8>li:before{content:"\0025a0  "}.lst-kix_e2bt8y2ywvrz-6>li:before{content:"\0025cf  "}.lst-kix_ulntv5wergyk-8>li:before{content:"\0025a0  "}.lst-kix_gc2knt874en1-8>li:before{content:"\0025a0  "}.lst-kix_ucprmx270ar2-5>li:before{content:"\0025a0  "}.lst-kix_pmg6qjdagav9-1>li:before{content:"\0025cb  "}.lst-kix_wdjiiwxyq2d1-1>li:before{content:"\0025cb  "}.lst-kix_o5m7acjt2ju1-7>li:before{content:"\0025cb  "}.lst-kix_iiujyt46s4u7-5>li:before{content:"\0025a0  "}ul.lst-kix_2ihanzwz4fgf-4{list-style-type:none}.lst-kix_4ykl6tjv6vdc-3>li:before{content:"\0025cf  "}.lst-kix_qsdpvqxg1u78-1>li:before{content:"\0025cb  "}ul.lst-kix_2ihanzwz4fgf-3{list-style-type:none}.lst-kix_xx3xz98aq2wq-4>li:before{content:"\0025cb  "}ul.lst-kix_2ihanzwz4fgf-2{list-style-type:none}ul.lst-kix_2ihanzwz4fgf-1{list-style-type:none}ul.lst-kix_2ihanzwz4fgf-8{list-style-type:none}ul.lst-kix_2ihanzwz4fgf-7{list-style-type:none}ul.lst-kix_2ihanzwz4fgf-6{list-style-type:none}ul.lst-kix_2ihanzwz4fgf-5{list-style-type:none}.lst-kix_ulntv5wergyk-0>li:before{content:"\0025cf  "}ul.lst-kix_2ihanzwz4fgf-0{list-style-type:none}ul.lst-kix_r8labycqo9zl-8{list-style-type:none}.lst-kix_gwuatnbmuewh-3>li:before{content:"\0025cf  "}ul.lst-kix_r8labycqo9zl-7{list-style-type:none}ul.lst-kix_r8labycqo9zl-6{list-style-type:none}ul.lst-kix_r8labycqo9zl-5{list-style-type:none}.lst-kix_crb89j7xw8q2-7>li:before{content:"\0025cb  "}.lst-kix_mm2j3zumujcl-1>li:before{content:"\0025cb  "}.lst-kix_9khvkfghhb3b-3>li:before{content:"\0025cf  "}.lst-kix_i0z4lx8deqnq-7>li:before{content:"\0025cb  "}ul.lst-kix_r8labycqo9zl-4{list-style-type:none}ul.lst-kix_r8labycqo9zl-3{list-style-type:none}ul.lst-kix_r8labycqo9zl-2{list-style-type:none}.lst-kix_ftgmvnn66fx0-6>li:before{content:"\0025cf  "}ul.lst-kix_r8labycqo9zl-1{list-style-type:none}.lst-kix_474t6llskbh7-5>li:before{content:"\0025a0  "}ul.lst-kix_r8labycqo9zl-0{list-style-type:none}.lst-kix_7q22x2z5v5y-8>li:before{content:"\0025a0  "}.lst-kix_vrgigcglf3we-5>li:before{content:"\0025a0  "}.lst-kix_o4ah8y8am4ct-7>li:before{content:"\0025cb  "}.lst-kix_i0z4lx8deqnq-3>li:before{content:"\0025cf  "}.lst-kix_ow9ix2mme95u-2>li:before{content:"\0025a0  "}.lst-kix_o4d3wx7elf58-2>li:before{content:"\0025a0  "}ul.lst-kix_qsdpvqxg1u78-3{list-style-type:none}ul.lst-kix_qsdpvqxg1u78-2{list-style-type:none}ul.lst-kix_qsdpvqxg1u78-1{list-style-type:none}ul.lst-kix_qsdpvqxg1u78-0{list-style-type:none}.lst-kix_p0fqpwscumob-0>li:before{content:"\0025cf  "}.lst-kix_u6q708mtjbvx-4>li:before{content:"-  "}.lst-kix_4yao0efkrd31-6>li:before{content:"\0025cf  "}.lst-kix_24cmoxb3qmwl-1>li:before{content:"\0025cb  "}ul.lst-kix_h99428b6laza-7{list-style-type:none}ul.lst-kix_h99428b6laza-8{list-style-type:none}.lst-kix_u6q708mtjbvx-8>li:before{content:"-  "}.lst-kix_2nh9kwfbzbik-8>li:before{content:"\0025a0  "}ul.lst-kix_h99428b6laza-0{list-style-type:none}ul.lst-kix_h99428b6laza-1{list-style-type:none}ul.lst-kix_h99428b6laza-2{list-style-type:none}ul.lst-kix_h99428b6laza-3{list-style-type:none}ul.lst-kix_h99428b6laza-4{list-style-type:none}ul.lst-kix_h99428b6laza-5{list-style-type:none}ul.lst-kix_h99428b6laza-6{list-style-type:none}.lst-kix_m46mj27r3fpz-2>li:before{content:"\0025a0  "}.lst-kix_6r73oe35erxy-4>li:before{content:"\0025cb  "}.lst-kix_gogm0wicn297-0>li:before{content:"\0025cf  "}.lst-kix_rcp8sez4svjd-3>li:before{content:"-  "}.lst-kix_sxaq135mthse-5>li:before{content:"\0025a0  "}.lst-kix_gogm0wicn297-6>li:before{content:"\0025cf  "}ul.lst-kix_kkphisidn6cz-5{list-style-type:none}ul.lst-kix_kkphisidn6cz-6{list-style-type:none}ul.lst-kix_kkphisidn6cz-7{list-style-type:none}ul.lst-kix_kkphisidn6cz-8{list-style-type:none}.lst-kix_edj3r8hgf2z3-8>li:before{content:"\0025a0  "}ul.lst-kix_jourdftw4nrm-8{list-style-type:none}ul.lst-kix_jourdftw4nrm-7{list-style-type:none}ul.lst-kix_jourdftw4nrm-6{list-style-type:none}ul.lst-kix_jourdftw4nrm-5{list-style-type:none}.lst-kix_ljjwvb506pet-1>li:before{content:"\0025cb  "}.lst-kix_xav8ikvidc9s-2>li:before{content:"\0025a0  "}.lst-kix_tg2hwffju8t9-7>li:before{content:"\0025cb  "}.lst-kix_5fj31eh1jc7-6>li:before{content:"\0025cf  "}.lst-kix_o4ah8y8am4ct-1>li:before{content:"\0025cb  "}ul.lst-kix_b474o1tvhevy-8{list-style-type:none}ul.lst-kix_b474o1tvhevy-7{list-style-type:none}.lst-kix_x3vcornhe3kv-8>li:before{content:"\0025a0  "}ul.lst-kix_b474o1tvhevy-6{list-style-type:none}.lst-kix_398os0jr1iw3-0>li:before{content:"\0025cf  "}.lst-kix_5fj31eh1jc7-8>li:before{content:"\0025a0  "}.lst-kix_b8dlmlw0p2p1-8>li:before{content:"\0025a0  "}ul.lst-kix_b474o1tvhevy-1{list-style-type:none}ul.lst-kix_b474o1tvhevy-0{list-style-type:none}ul.lst-kix_b474o1tvhevy-5{list-style-type:none}ul.lst-kix_b474o1tvhevy-4{list-style-type:none}ul.lst-kix_b474o1tvhevy-3{list-style-type:none}ul.lst-kix_b474o1tvhevy-2{list-style-type:none}.lst-kix_nbpnmy8xci8w-5>li:before{content:"\0025a0  "}.lst-kix_a9t5z6hxg4oz-7>li:before{content:"\0025cb  "}.lst-kix_nbpnmy8xci8w-7>li:before{content:"\0025cb  "}.lst-kix_5fj31eh1jc7-0>li:before{content:"\0025cf  "}.lst-kix_7apmk6vwlj5r-5>li:before{content:"\0025a0  "}.lst-kix_c2vvnsz0drek-7>li:before{content:"\0025cb  "}.lst-kix_edj3r8hgf2z3-2>li:before{content:"\0025a0  "}.lst-kix_x3vcornhe3kv-6>li:before{content:"\0025cf  "}.lst-kix_tg2hwffju8t9-1>li:before{content:"\0025cb  "}.lst-kix_douubz1tb8an-0>li:before{content:"\0025cf  "}.lst-kix_b8dlmlw0p2p1-6>li:before{content:"\0025cf  "}.lst-kix_7b2y41o7s2k3-8>li:before{content:"\0025a0  "}.lst-kix_5kaugn46jpw6-4>li:before{content:"\0025cb  "}.lst-kix_kk8oms3v3iyb-5>li:before{content:"\0025a0  "}.lst-kix_8hjy0hqh3rvg-6>li:before{content:"\0025cf  "}.lst-kix_mxwp6np2u5xu-6>li:before{content:"-  "}.lst-kix_6ykz6yxb3o6x-6>li:before{content:"\0025cf  "}.lst-kix_ymmukz9l2n7v-2>li:before{content:"\0025a0  "}.lst-kix_cq59e9nktsby-4>li:before{content:"\0025cb  "}.lst-kix_6ykz6yxb3o6x-0>li:before{content:"\0025cf  "}.lst-kix_lyxc6qcmvco2-0>li:before{content:"\0025cf  "}.lst-kix_ymmukz9l2n7v-4>li:before{content:"\0025cb  "}.lst-kix_cq59e9nktsby-6>li:before{content:"\0025cf  "}.lst-kix_i1kls5tk4gt6-8>li:before{content:"\0025a0  "}ul.lst-kix_8wesot3ko53b-1{list-style-type:none}ul.lst-kix_8wesot3ko53b-0{list-style-type:none}.lst-kix_aui8m1kby916-3>li:before{content:"\0025cf  "}.lst-kix_8wesot3ko53b-3>li:before{content:"\0025cf  "}.lst-kix_x3vcornhe3kv-0>li:before{content:"\0025cf  "}.lst-kix_b8dlmlw0p2p1-0>li:before{content:"\0025cf  "}.lst-kix_bj6mzrhm69bi-6>li:before{content:"\0025cf  "}.lst-kix_n21asioviqe4-4>li:before{content:"\0025cb  "}.lst-kix_xkjgsygnpr2m-7>li:before{content:"\0025cb  "}.lst-kix_313q3f5x4p2m-7>li:before{content:"\0025cb  "}.lst-kix_3de1jg1kuy24-6>li:before{content:"\0025cf  "}.lst-kix_x2mjmu7myisq-3>li:before{content:"\0025cf  "}.lst-kix_470xpz4q6hoi-4>li:before{content:"\0025cb  "}ul.lst-kix_gr1y63l14uay-4{list-style-type:none}.lst-kix_3de1jg1kuy24-0>li:before{content:"\0025cf  "}ul.lst-kix_gr1y63l14uay-3{list-style-type:none}.lst-kix_3odao55mezu-4>li:before{content:"\0025cb  "}.lst-kix_6jabh0roz5s1-5>li:before{content:"\0025a0  "}ul.lst-kix_gr1y63l14uay-6{list-style-type:none}.lst-kix_ukuyvb7r2sb4-5>li:before{content:"\0025a0  "}.lst-kix_313q3f5x4p2m-1>li:before{content:"\0025cb  "}.lst-kix_8g8vlw6j0asg-1>li:before{content:"\0025cb  "}ul.lst-kix_gr1y63l14uay-5{list-style-type:none}.lst-kix_68swn1licdyn-5>li:before{content:"\0025a0  "}ul.lst-kix_gr1y63l14uay-8{list-style-type:none}ul.lst-kix_gr1y63l14uay-7{list-style-type:none}.lst-kix_62is116n17wq-1>li:before{content:"\0025cb  "}.lst-kix_8g8vlw6j0asg-7>li:before{content:"\0025cb  "}ul.lst-kix_gr1y63l14uay-0{list-style-type:none}ul.lst-kix_axhbb6oyblrk-8{list-style-type:none}ul.lst-kix_gr1y63l14uay-2{list-style-type:none}ul.lst-kix_axhbb6oyblrk-7{list-style-type:none}ul.lst-kix_gr1y63l14uay-1{list-style-type:none}ul.lst-kix_axhbb6oyblrk-6{list-style-type:none}ul.lst-kix_8wesot3ko53b-7{list-style-type:none}ul.lst-kix_axhbb6oyblrk-5{list-style-type:none}ul.lst-kix_8wesot3ko53b-6{list-style-type:none}ul.lst-kix_axhbb6oyblrk-4{list-style-type:none}.lst-kix_2jmrp4ochkmc-4>li:before{content:"\0025cb  "}ul.lst-kix_axhbb6oyblrk-3{list-style-type:none}ul.lst-kix_8wesot3ko53b-8{list-style-type:none}.lst-kix_n1766bho68n4-5>li:before{content:"\0025a0  "}ul.lst-kix_axhbb6oyblrk-2{list-style-type:none}ul.lst-kix_8wesot3ko53b-3{list-style-type:none}ul.lst-kix_axhbb6oyblrk-1{list-style-type:none}ul.lst-kix_8wesot3ko53b-2{list-style-type:none}ul.lst-kix_axhbb6oyblrk-0{list-style-type:none}ul.lst-kix_8wesot3ko53b-5{list-style-type:none}ul.lst-kix_8wesot3ko53b-4{list-style-type:none}.lst-kix_x2mjmu7myisq-1>li:before{content:"\0025cb  "}.lst-kix_xav8ikvidc9s-8>li:before{content:"\0025a0  "}ul.lst-kix_kkphisidn6cz-1{list-style-type:none}ul.lst-kix_kkphisidn6cz-2{list-style-type:none}ul.lst-kix_kkphisidn6cz-3{list-style-type:none}ul.lst-kix_kkphisidn6cz-4{list-style-type:none}.lst-kix_iqbmivscsmun-7>li:before{content:"\0025cb  "}ul.lst-kix_kkphisidn6cz-0{list-style-type:none}.lst-kix_d995nc7613a9-3>li:before{content:"\0025cf  "}ul.lst-kix_ow9ix2mme95u-1{list-style-type:none}ul.lst-kix_ow9ix2mme95u-2{list-style-type:none}ul.lst-kix_ow9ix2mme95u-0{list-style-type:none}.lst-kix_68swn1licdyn-7>li:before{content:"\0025cb  "}.lst-kix_470xpz4q6hoi-6>li:before{content:"\0025cf  "}.lst-kix_6r73oe35erxy-6>li:before{content:"\0025cf  "}.lst-kix_o14fwownm59e-2>li:before{content:"\0025a0  "}.lst-kix_3odao55mezu-6>li:before{content:"\0025cf  "}ul.lst-kix_ow9ix2mme95u-7{list-style-type:none}ul.lst-kix_ow9ix2mme95u-8{list-style-type:none}.lst-kix_o14fwownm59e-4>li:before{content:"\0025cb  "}ul.lst-kix_ow9ix2mme95u-5{list-style-type:none}ul.lst-kix_ow9ix2mme95u-6{list-style-type:none}ul.lst-kix_ow9ix2mme95u-3{list-style-type:none}ul.lst-kix_ow9ix2mme95u-4{list-style-type:none}.lst-kix_nc0l9nyccr86-3>li:before{content:"\0025cf  "}.lst-kix_emewkhtp1rfy-6>li:before{content:"\0025cf  "}.lst-kix_kxfg6pdeancy-6>li:before{content:"\0025cf  "}.lst-kix_kkphisidn6cz-0>li:before{content:"\0025cf  "}.lst-kix_6d5ax9c8k1xa-1>li:before{content:"\0025cb  "}.lst-kix_8k70e8i8rdlm-3>li:before{content:"\0025cf  "}.lst-kix_16ygmg7toocv-5>li:before{content:"\0025a0  "}.lst-kix_mgyoyquoxrm1-3>li:before{content:"\0025cf  "}.lst-kix_2nh9kwfbzbik-2>li:before{content:"\0025a0  "}.lst-kix_xrce7kijh7l1-6>li:before{content:"\0025cf  "}ul.lst-kix_swbzestlepme-0{list-style-type:none}.lst-kix_9cwmoyhkmhfb-8>li:before{content:"\0025a0  "}ul.lst-kix_swbzestlepme-1{list-style-type:none}ul.lst-kix_swbzestlepme-2{list-style-type:none}ul.lst-kix_swbzestlepme-3{list-style-type:none}ul.lst-kix_swbzestlepme-4{list-style-type:none}ul.lst-kix_swbzestlepme-5{list-style-type:none}ul.lst-kix_swbzestlepme-6{list-style-type:none}ul.lst-kix_swbzestlepme-7{list-style-type:none}.lst-kix_74gx9bwnn8kt-7>li:before{content:"\0025cb  "}ul.lst-kix_swbzestlepme-8{list-style-type:none}.lst-kix_8cma8sbxfy2x-8>li:before{content:"\0025a0  "}.lst-kix_jhqr8d48vqqp-5>li:before{content:"\0025a0  "}ul.lst-kix_f3egesa43r6b-0{list-style-type:none}ul.lst-kix_f3egesa43r6b-1{list-style-type:none}ul.lst-kix_f3egesa43r6b-2{list-style-type:none}ul.lst-kix_f3egesa43r6b-3{list-style-type:none}ul.lst-kix_f3egesa43r6b-4{list-style-type:none}ul.lst-kix_f3egesa43r6b-5{list-style-type:none}.lst-kix_894copnhklgz-8>li:before{content:"\0025a0  "}.lst-kix_w14ild1qeoey-4>li:before{content:"\0025cb  "}.lst-kix_scilllbfudea-4>li:before{content:"\0025cb  "}.lst-kix_369j2eishmb5-3>li:before{content:"\0025cf  "}.lst-kix_z1vxl485jzzo-1>li:before{content:"\0025cb  "}.lst-kix_6yojqqjec0st-4>li:before{content:"\0025cb  "}.lst-kix_d1iyykra6bco-3>li:before{content:"\0025cf  "}.lst-kix_2ihanzwz4fgf-5>li:before{content:"\0025a0  "}ul.lst-kix_bj6mzrhm69bi-2{list-style-type:none}ul.lst-kix_5ia1ign2sshz-0{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-1{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-0{list-style-type:none}ul.lst-kix_5ia1ign2sshz-3{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-6{list-style-type:none}ul.lst-kix_5ia1ign2sshz-4{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-5{list-style-type:none}ul.lst-kix_5ia1ign2sshz-1{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-4{list-style-type:none}ul.lst-kix_5ia1ign2sshz-2{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-3{list-style-type:none}ul.lst-kix_5ia1ign2sshz-7{list-style-type:none}ul.lst-kix_5ia1ign2sshz-8{list-style-type:none}.lst-kix_swbzestlepme-0>li:before{content:"\0025cf  "}ul.lst-kix_5ia1ign2sshz-5{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-8{list-style-type:none}.lst-kix_riff65m65tnf-1>li:before{content:"\0025cb  "}ul.lst-kix_5ia1ign2sshz-6{list-style-type:none}ul.lst-kix_bj6mzrhm69bi-7{list-style-type:none}.lst-kix_l3zoug7b3vcw-4>li:before{content:"\0025cb  "}.lst-kix_l1rt7mvaz29b-4>li:before{content:"\0025cb  "}.lst-kix_1a8ua5u4a26p-2>li:before{content:"\0025a0  "}ul.lst-kix_cq59e9nktsby-8{list-style-type:none}.lst-kix_d2ysoruvczg5-3>li:before{content:"\0025cf  "}.lst-kix_4k4id0a37km3-8>li:before{content:"\0025a0  "}.lst-kix_rwfcya85pd71-6>li:before{content:"\0025cf  "}.lst-kix_d8ik6bmbwuj4-5>li:before{content:"\0025a0  "}.lst-kix_swbzestlepme-8>li:before{content:"\0025a0  "}.lst-kix_xgfpveff5zr0-0>li:before{content:"\0025cf  "}.lst-kix_6asjiojyuog6-5>li:before{content:"\0025a0  "}.lst-kix_ho1wvdl93240-1>li:before{content:"\0025cb  "}ul.lst-kix_369j2eishmb5-8{list-style-type:none}.lst-kix_ucprmx270ar2-3>li:before{content:"\0025cf  "}.lst-kix_x5zq7qqqztpc-5>li:before{content:"\0025a0  "}.lst-kix_17is42bhveph-8>li:before{content:"\0025a0  "}.lst-kix_gc2knt874en1-6>li:before{content:"\0025cf  "}.lst-kix_yhn6vvmwi8p2-5>li:before{content:"\0025a0  "}.lst-kix_wdjiiwxyq2d1-3>li:before{content:"\0025cf  "}ul.lst-kix_369j2eishmb5-7{list-style-type:none}.lst-kix_uk8k6pzec0np-5>li:before{content:"\0025a0  "}ul.lst-kix_369j2eishmb5-6{list-style-type:none}ul.lst-kix_369j2eishmb5-5{list-style-type:none}ul.lst-kix_369j2eishmb5-4{list-style-type:none}ul.lst-kix_369j2eishmb5-3{list-style-type:none}ul.lst-kix_369j2eishmb5-2{list-style-type:none}.lst-kix_7r3huctovklw-4>li:before{content:"\0025cb  "}.lst-kix_pmg6qjdagav9-3>li:before{content:"\0025cf  "}ul.lst-kix_369j2eishmb5-1{list-style-type:none}ul.lst-kix_369j2eishmb5-0{list-style-type:none}.lst-kix_17is42bhveph-0>li:before{content:"\0025cf  "}.lst-kix_mh0uhwysa91-0>li:before{content:"\0025cf  "}.lst-kix_z1olrl2brpzr-1>li:before{content:"\0025cb  "}.lst-kix_z4z7nvld905a-1>li:before{content:"\0025cb  "}.lst-kix_4ykl6tjv6vdc-5>li:before{content:"\0025a0  "}.lst-kix_ulntv5wergyk-2>li:before{content:"\0025a0  "}ul.lst-kix_dq6vem8xoqp9-5{list-style-type:none}ul.lst-kix_dq6vem8xoqp9-6{list-style-type:none}.lst-kix_e2bt8y2ywvrz-4>li:before{content:"\0025cb  "}ul.lst-kix_dq6vem8xoqp9-7{list-style-type:none}ul.lst-kix_dq6vem8xoqp9-8{list-style-type:none}ul.lst-kix_dq6vem8xoqp9-1{list-style-type:none}ul.lst-kix_dq6vem8xoqp9-2{list-style-type:none}ul.lst-kix_dq6vem8xoqp9-3{list-style-type:none}ul.lst-kix_dq6vem8xoqp9-4{list-style-type:none}.lst-kix_ftgmvnn66fx0-4>li:before{content:"\0025cb  "}ul.lst-kix_dq6vem8xoqp9-0{list-style-type:none}.lst-kix_gr1y63l14uay-2>li:before{content:"-  "}ul.lst-kix_ly0lmz6fcokr-8{list-style-type:none}.lst-kix_mm2j3zumujcl-7>li:before{content:"\0025cb  "}ul.lst-kix_f3egesa43r6b-6{list-style-type:none}ul.lst-kix_f3egesa43r6b-7{list-style-type:none}ul.lst-kix_f3egesa43r6b-8{list-style-type:none}.lst-kix_ow9ix2mme95u-8>li:before{content:"\0025a0  "}.lst-kix_nxvbdas3q36w-7>li:before{content:"\0025cb  "}.lst-kix_jys0trw4gd1m-0>li:before{content:"\0025cf  "}ul.lst-kix_jourdftw4nrm-4{list-style-type:none}ul.lst-kix_pfssn45qm13m-0{list-style-type:none}.lst-kix_6k5yl8dbqqq2-6>li:before{content:"\0025cf  "}ul.lst-kix_jourdftw4nrm-3{list-style-type:none}ul.lst-kix_jourdftw4nrm-2{list-style-type:none}ul.lst-kix_pfssn45qm13m-2{list-style-type:none}ul.lst-kix_jourdftw4nrm-1{list-style-type:none}ul.lst-kix_pfssn45qm13m-1{list-style-type:none}ul.lst-kix_jourdftw4nrm-0{list-style-type:none}.lst-kix_7q22x2z5v5y-2>li:before{content:"\0025a0  "}ul.lst-kix_jys0trw4gd1m-8{list-style-type:none}ul.lst-kix_pfssn45qm13m-8{list-style-type:none}ul.lst-kix_jys0trw4gd1m-7{list-style-type:none}ul.lst-kix_pfssn45qm13m-7{list-style-type:none}ul.lst-kix_jys0trw4gd1m-6{list-style-type:none}.lst-kix_vrgigcglf3we-3>li:before{content:"\0025cf  "}ul.lst-kix_jys0trw4gd1m-5{list-style-type:none}ul.lst-kix_jys0trw4gd1m-4{list-style-type:none}ul.lst-kix_pfssn45qm13m-4{list-style-type:none}ul.lst-kix_jys0trw4gd1m-3{list-style-type:none}ul.lst-kix_pfssn45qm13m-3{list-style-type:none}ul.lst-kix_jys0trw4gd1m-2{list-style-type:none}ul.lst-kix_pfssn45qm13m-6{list-style-type:none}.lst-kix_i0z4lx8deqnq-1>li:before{content:"\0025cb  "}ul.lst-kix_jys0trw4gd1m-1{list-style-type:none}ul.lst-kix_pfssn45qm13m-5{list-style-type:none}.lst-kix_crb89j7xw8q2-1>li:before{content:"\0025cb  "}ul.lst-kix_jys0trw4gd1m-0{list-style-type:none}.lst-kix_bjneibxfrx3r-4>li:before{content:"\0025cb  "}.lst-kix_4yao0efkrd31-4>li:before{content:"\0025cb  "}.lst-kix_u6q708mtjbvx-2>li:before{content:"-  "}.lst-kix_iigck7esqel5-3>li:before{content:"\0025cf  "}.lst-kix_kk7o9gi3ij0l-7>li:before{content:"\0025cb  "}.lst-kix_1qjopwyk3n75-6>li:before{content:"\0025cf  "}.lst-kix_vt0etbgrzlaw-3>li:before{content:"\0025cf  "}ul.lst-kix_g0l7lf51hdyf-1{list-style-type:none}ul.lst-kix_g0l7lf51hdyf-0{list-style-type:none}ul.lst-kix_g0l7lf51hdyf-5{list-style-type:none}.lst-kix_crt5jd7c2xq0-3>li:before{content:"\0025cf  "}ul.lst-kix_g0l7lf51hdyf-4{list-style-type:none}ul.lst-kix_g0l7lf51hdyf-3{list-style-type:none}ul.lst-kix_g0l7lf51hdyf-2{list-style-type:none}ul.lst-kix_ly0lmz6fcokr-5{list-style-type:none}ul.lst-kix_g0l7lf51hdyf-8{list-style-type:none}.lst-kix_7jzvzpyn9bnk-8>li:before{content:"\0025a0  "}ul.lst-kix_ly0lmz6fcokr-4{list-style-type:none}ul.lst-kix_g0l7lf51hdyf-7{list-style-type:none}ul.lst-kix_ly0lmz6fcokr-7{list-style-type:none}ul.lst-kix_g0l7lf51hdyf-6{list-style-type:none}ul.lst-kix_ly0lmz6fcokr-6{list-style-type:none}ul.lst-kix_ly0lmz6fcokr-1{list-style-type:none}ul.lst-kix_ly0lmz6fcokr-0{list-style-type:none}ul.lst-kix_ly0lmz6fcokr-3{list-style-type:none}.lst-kix_saoi6g81wrq2-7>li:before{content:"\0025cb  "}ul.lst-kix_ly0lmz6fcokr-2{list-style-type:none}.lst-kix_nwi1ck3fibs7-6>li:before{content:"\0025cf  "}.lst-kix_rr7hjjtid2vg-2>li:before{content:"\0025a0  "}.lst-kix_nwi1ck3fibs7-3>li:before{content:"\0025cf  "}.lst-kix_nwi1ck3fibs7-0>li:before{content:"\0025cf  "}ul.lst-kix_nbuhcgd4ft21-0{list-style-type:none}ul.lst-kix_nbuhcgd4ft21-1{list-style-type:none}ul.lst-kix_nbuhcgd4ft21-2{list-style-type:none}ul.lst-kix_nbuhcgd4ft21-3{list-style-type:none}ul.lst-kix_nbuhcgd4ft21-4{list-style-type:none}.lst-kix_jourdftw4nrm-4>li:before{content:"\0025cb  "}ul.lst-kix_nbuhcgd4ft21-5{list-style-type:none}ul.lst-kix_nbuhcgd4ft21-6{list-style-type:none}ul.lst-kix_saoi6g81wrq2-0{list-style-type:none}ul.lst-kix_nbuhcgd4ft21-7{list-style-type:none}ul.lst-kix_saoi6g81wrq2-1{list-style-type:none}ul.lst-kix_nbuhcgd4ft21-8{list-style-type:none}ul.lst-kix_saoi6g81wrq2-2{list-style-type:none}ul.lst-kix_saoi6g81wrq2-3{list-style-type:none}.lst-kix_r8labycqo9zl-1>li:before{content:"\0025cb  "}ul.lst-kix_saoi6g81wrq2-4{list-style-type:none}ul.lst-kix_saoi6g81wrq2-5{list-style-type:none}.lst-kix_p3gj10803b4t-6>li:before{content:"\0025cf  "}ul.lst-kix_saoi6g81wrq2-6{list-style-type:none}ul.lst-kix_saoi6g81wrq2-7{list-style-type:none}.lst-kix_60wjig3vq01-1>li:before{content:"\0025cb  "}ul.lst-kix_saoi6g81wrq2-8{list-style-type:none}.lst-kix_jourdftw4nrm-1>li:before{content:"\0025cb  "}.lst-kix_ltkzlkp2gxkt-2>li:before{content:"\0025a0  "}.lst-kix_ltkzlkp2gxkt-5>li:before{content:"\0025a0  "}.lst-kix_hh7pij1hmwlh-6>li:before{content:"\0025cf  "}.lst-kix_z0c6lix981m-0>li:before{content:"\0025cf  "}.lst-kix_vsaybtg1hfvm-7>li:before{content:"\0025cb  "}.lst-kix_jourdftw4nrm-7>li:before{content:"\0025cb  "}.lst-kix_p3gj10803b4t-3>li:before{content:"\0025cf  "}.lst-kix_481c98v2pt83-2>li:before{content:"\0025a0  "}.lst-kix_ly0lmz6fcokr-7>li:before{content:"\0025cb  "}.lst-kix_vsaybtg1hfvm-4>li:before{content:"\0025cb  "}.lst-kix_ltkzlkp2gxkt-8>li:before{content:"\0025a0  "}.lst-kix_defeme42l2ge-0>li:before{content:"\0025cf  "}.lst-kix_p3gj10803b4t-0>li:before{content:"\0025cf  "}.lst-kix_47kviebmmd71-6>li:before{content:"\0025cf  "}.lst-kix_k12n8xes8ofj-7>li:before{content:"\0025cb  "}.lst-kix_s3fvwoba1t7n-7>li:before{content:"\0025cb  "}.lst-kix_defeme42l2ge-6>li:before{content:"\0025cf  "}.lst-kix_rrs2j0rau6wp-3>li:before{content:"\0025cf  "}.lst-kix_47kviebmmd71-0>li:before{content:"\0025cf  "}.lst-kix_rrs2j0rau6wp-6>li:before{content:"\0025cf  "}.lst-kix_hh7pij1hmwlh-3>li:before{content:"\0025cf  "}.lst-kix_w1d9l4m1yg5x-3>li:before{content:"\0025cf  "}.lst-kix_ly0lmz6fcokr-4>li:before{content:"\0025cb  "}.lst-kix_47kviebmmd71-3>li:before{content:"\0025cf  "}.lst-kix_hh7pij1hmwlh-0>li:before{content:"\0025cf  "}.lst-kix_zgvzzrmreqeg-7>li:before{content:"\0025cb  "}.lst-kix_defeme42l2ge-3>li:before{content:"\0025cf  "}.lst-kix_ly0lmz6fcokr-1>li:before{content:"\0025cb  "}.lst-kix_uk8k6pzec0np-8>li:before{content:"\0025a0  "}.lst-kix_yhn6vvmwi8p2-2>li:before{content:"\0025a0  "}.lst-kix_o38l9c2rqk2p-6>li:before{content:"\0025cf  "}.lst-kix_x5jl3p2g0327-5>li:before{content:"\0025a0  "}ul.lst-kix_cq59e9nktsby-5{list-style-type:none}ul.lst-kix_cq59e9nktsby-4{list-style-type:none}.lst-kix_d3k7hdh35j7a-1>li:before{content:"\0025cb  "}ul.lst-kix_cq59e9nktsby-7{list-style-type:none}ul.lst-kix_cq59e9nktsby-6{list-style-type:none}.lst-kix_yhmx20qr5k0e-8>li:before{content:"\0025a0  "}.lst-kix_7r3huctovklw-7>li:before{content:"\0025cb  "}ul.lst-kix_cq59e9nktsby-1{list-style-type:none}ul.lst-kix_cq59e9nktsby-0{list-style-type:none}ul.lst-kix_cq59e9nktsby-3{list-style-type:none}.lst-kix_o38l9c2rqk2p-0>li:before{content:"\0025cf  "}ul.lst-kix_cq59e9nktsby-2{list-style-type:none}ul.lst-kix_iiujyt46s4u7-8{list-style-type:none}.lst-kix_crt5jd7c2xq0-6>li:before{content:"\0025cf  "}ul.lst-kix_iiujyt46s4u7-6{list-style-type:none}ul.lst-kix_iiujyt46s4u7-7{list-style-type:none}ul.lst-kix_iiujyt46s4u7-4{list-style-type:none}.lst-kix_qlax8iv93hxg-3>li:before{content:"\0025cf  "}ul.lst-kix_iiujyt46s4u7-5{list-style-type:none}.lst-kix_w1d9l4m1yg5x-6>li:before{content:"\0025cf  "}ul.lst-kix_iiujyt46s4u7-2{list-style-type:none}ul.lst-kix_iiujyt46s4u7-3{list-style-type:none}ul.lst-kix_iiujyt46s4u7-0{list-style-type:none}ul.lst-kix_iiujyt46s4u7-1{list-style-type:none}.lst-kix_kxpudurk2j6e-8>li:before{content:"\0025a0  "}.lst-kix_xkmlxx8lhm5d-2>li:before{content:"\0025a0  "}.lst-kix_ydb53brft35n-2>li:before{content:"\0025a0  "}.lst-kix_bjneibxfrx3r-7>li:before{content:"\0025cb  "}.lst-kix_481c98v2pt83-8>li:before{content:"\0025a0  "}.lst-kix_383wi8io125v-0>li:before{content:"\0025cf  "}.lst-kix_ydb53brft35n-8>li:before{content:"\0025a0  "}.lst-kix_r75b0bpeph0o-8>li:before{content:"\0025a0  "}.lst-kix_klzhllb6xcyr-4>li:before{content:"\0025cb  "}.lst-kix_k12n8xes8ofj-1>li:before{content:"\0025cb  "}.lst-kix_xqqiyb3b5ag0-4>li:before{content:"\0025cb  "}.lst-kix_juam6n4619t2-4>li:before{content:"\0025cb  "}.lst-kix_d3k7hdh35j7a-7>li:before{content:"\0025cb  "}.lst-kix_xkmlxx8lhm5d-8>li:before{content:"\0025a0  "}.lst-kix_2s6hp75gzisz-0>li:before{content:"\0025cf  "}.lst-kix_7jzvzpyn9bnk-5>li:before{content:"\0025a0  "}.lst-kix_5ia1ign2sshz-1>li:before{content:"\0025cb  "}.lst-kix_cy2og2rt03zo-5>li:before{content:"\0025a0  "}.lst-kix_r75b0bpeph0o-2>li:before{content:"\0025a0  "}ul.lst-kix_fk4cszd8thar-8{list-style-type:none}ul.lst-kix_fk4cszd8thar-7{list-style-type:none}.lst-kix_6k5yl8dbqqq2-3>li:before{content:"\0025cf  "}.lst-kix_ta3ofdl38yyn-4>li:before{content:"\0025cb  "}ul.lst-kix_fk4cszd8thar-6{list-style-type:none}ul.lst-kix_fk4cszd8thar-5{list-style-type:none}ul.lst-kix_fk4cszd8thar-4{list-style-type:none}ul.lst-kix_fk4cszd8thar-3{list-style-type:none}.lst-kix_5ia1ign2sshz-7>li:before{content:"\0025cb  "}ul.lst-kix_fk4cszd8thar-2{list-style-type:none}ul.lst-kix_fk4cszd8thar-1{list-style-type:none}ul.lst-kix_fk4cszd8thar-0{list-style-type:none}.lst-kix_r8labycqo9zl-7>li:before{content:"\0025cb  "}.lst-kix_p98dexr65abh-0>li:before{content:"\0025cf  "}.lst-kix_p98dexr65abh-6>li:before{content:"\0025cf  "}.lst-kix_2zf5bd1mem1t-6>li:before{content:"\0025cf  "}.lst-kix_8fyyex2aqhl9-3>li:before{content:"\0025cf  "}.lst-kix_3fvzyqsgm5ef-8>li:before{content:"\0025a0  "}.lst-kix_2zf5bd1mem1t-0>li:before{content:"\0025cf  "}.lst-kix_vt0etbgrzlaw-6>li:before{content:"\0025cf  "}.lst-kix_kxpudurk2j6e-2>li:before{content:"\0025a0  "}.lst-kix_2s6hp75gzisz-6>li:before{content:"\0025cf  "}ul.lst-kix_894copnhklgz-4{list-style-type:none}.lst-kix_369j2eishmb5-0>li:before{content:"\0025cf  "}ul.lst-kix_894copnhklgz-5{list-style-type:none}ul.lst-kix_894copnhklgz-2{list-style-type:none}ul.lst-kix_894copnhklgz-3{list-style-type:none}ul.lst-kix_894copnhklgz-8{list-style-type:none}.lst-kix_kz1sfsbi2eng-3>li:before{content:"\0025cf  "}ul.lst-kix_894copnhklgz-6{list-style-type:none}ul.lst-kix_894copnhklgz-7{list-style-type:none}.lst-kix_wq4pisfqksnu-5>li:before{content:"\0025a0  "}.lst-kix_g0l7lf51hdyf-3>li:before{content:"\0025cf  "}.lst-kix_l3zoug7b3vcw-1>li:before{content:"\0025cb  "}.lst-kix_4fwhm2tiefhz-1>li:before{content:"\0025cb  "}.lst-kix_1a8ua5u4a26p-8>li:before{content:"\0025a0  "}.lst-kix_g0l7lf51hdyf-6>li:before{content:"\0025cf  "}ul.lst-kix_x5jl3p2g0327-6{list-style-type:none}.lst-kix_4fwhm2tiefhz-4>li:before{content:"\0025cb  "}ul.lst-kix_x5jl3p2g0327-7{list-style-type:none}ul.lst-kix_x5jl3p2g0327-4{list-style-type:none}.lst-kix_pzunwido5p11-1>li:before{content:"\0025cb  "}ul.lst-kix_x5jl3p2g0327-5{list-style-type:none}.lst-kix_bhgkuyt4aopz-0>li:before{content:"\0025cf  "}ul.lst-kix_x5jl3p2g0327-2{list-style-type:none}ul.lst-kix_x5jl3p2g0327-3{list-style-type:none}.lst-kix_spzfuoeekohi-7>li:before{content:"\0025cb  "}ul.lst-kix_x5jl3p2g0327-0{list-style-type:none}ul.lst-kix_x5jl3p2g0327-1{list-style-type:none}.lst-kix_bhgkuyt4aopz-3>li:before{content:"\0025cf  "}.lst-kix_6k5yl8dbqqq2-0>li:before{content:"\0025cf  "}.lst-kix_vhrrjzyjvp5x-5>li:before{content:"\0025a0  "}.lst-kix_wq4pisfqksnu-2>li:before{content:"\0025a0  "}.lst-kix_pzunwido5p11-4>li:before{content:"\0025cb  "}.lst-kix_yh6f66826ysk-8>li:before{content:"\0025a0  "}.lst-kix_z1vxl485jzzo-7>li:before{content:"\0025cb  "}.lst-kix_vhrrjzyjvp5x-8>li:before{content:"\0025a0  "}.lst-kix_solhl6ysg65v-4>li:before{content:"\0025cb  "}ul.lst-kix_9khvkfghhb3b-4{list-style-type:none}ul.lst-kix_9khvkfghhb3b-5{list-style-type:none}ul.lst-kix_9khvkfghhb3b-6{list-style-type:none}.lst-kix_bj6mzrhm69bi-3>li:before{content:"\0025cf  "}ul.lst-kix_9khvkfghhb3b-7{list-style-type:none}.lst-kix_xgsgxyckqw77-8>li:before{content:"\0025a0  "}ul.lst-kix_9khvkfghhb3b-8{list-style-type:none}.lst-kix_bj6mzrhm69bi-0>li:before{content:"\0025cf  "}.lst-kix_solhl6ysg65v-7>li:before{content:"\0025cb  "}.lst-kix_juam6n4619t2-7>li:before{content:"\0025cb  "}.lst-kix_xgsgxyckqw77-5>li:before{content:"\0025a0  "}.lst-kix_z1vxl485jzzo-4>li:before{content:"\0025cb  "}.lst-kix_2rp5aiyjkdpd-1>li:before{content:"\0025cb  "}ul.lst-kix_9khvkfghhb3b-0{list-style-type:none}ul.lst-kix_9khvkfghhb3b-1{list-style-type:none}ul.lst-kix_9khvkfghhb3b-2{list-style-type:none}ul.lst-kix_9khvkfghhb3b-3{list-style-type:none}.lst-kix_ehe7zglmx7i2-2>li:before{content:"\0025a0  "}ul.lst-kix_3fnpvvr0i8os-2{list-style-type:none}ul.lst-kix_3fnpvvr0i8os-1{list-style-type:none}ul.lst-kix_3fnpvvr0i8os-0{list-style-type:none}.lst-kix_6yojqqjec0st-1>li:before{content:"\0025cb  "}ul.lst-kix_3fnpvvr0i8os-6{list-style-type:none}ul.lst-kix_3fnpvvr0i8os-5{list-style-type:none}.lst-kix_xqfh3tf7tjc0-8>li:before{content:"\0025a0  "}ul.lst-kix_3fnpvvr0i8os-4{list-style-type:none}ul.lst-kix_3fnpvvr0i8os-3{list-style-type:none}ul.lst-kix_x5jl3p2g0327-8{list-style-type:none}ul.lst-kix_3fnpvvr0i8os-8{list-style-type:none}ul.lst-kix_3fnpvvr0i8os-7{list-style-type:none}.lst-kix_gnt40hozc3kv-6>li:before{content:"\0025cf  "}ul.lst-kix_nbpnmy8xci8w-0{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-1{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-2{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-3{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-4{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-5{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-6{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-7{list-style-type:none}ul.lst-kix_nbpnmy8xci8w-8{list-style-type:none}ul.lst-kix_cst6lp3lb98n-5{list-style-type:none}ul.lst-kix_d1iyykra6bco-4{list-style-type:none}ul.lst-kix_cst6lp3lb98n-6{list-style-type:none}ul.lst-kix_d1iyykra6bco-3{list-style-type:none}ul.lst-kix_cst6lp3lb98n-7{list-style-type:none}ul.lst-kix_d1iyykra6bco-6{list-style-type:none}ul.lst-kix_cst6lp3lb98n-8{list-style-type:none}.lst-kix_o38l9c2rqk2p-3>li:before{content:"\0025cf  "}.lst-kix_x5jl3p2g0327-8>li:before{content:"\0025a0  "}ul.lst-kix_d1iyykra6bco-5{list-style-type:none}ul.lst-kix_d1iyykra6bco-8{list-style-type:none}ul.lst-kix_d1iyykra6bco-7{list-style-type:none}.lst-kix_z3si3pbri1de-6>li:before{content:"\0025cf  "}ul.lst-kix_d1iyykra6bco-0{list-style-type:none}ul.lst-kix_d1iyykra6bco-2{list-style-type:none}ul.lst-kix_d1iyykra6bco-1{list-style-type:none}.lst-kix_xkmlxx8lhm5d-5>li:before{content:"\0025a0  "}.lst-kix_xqqiyb3b5ag0-1>li:before{content:"\0025cb  "}.lst-kix_qlax8iv93hxg-0>li:before{content:"\0025cf  "}.lst-kix_d995nc7613a9-6>li:before{content:"\0025cf  "}.lst-kix_k12n8xes8ofj-4>li:before{content:"\0025cb  "}.lst-kix_aui8m1kby916-0>li:before{content:"\0025cf  "}ul.lst-kix_juam6n4619t2-0{list-style-type:none}ul.lst-kix_juam6n4619t2-1{list-style-type:none}ul.lst-kix_juam6n4619t2-2{list-style-type:none}.lst-kix_c2vvnsz0drek-4>li:before{content:"\0025cb  "}ul.lst-kix_juam6n4619t2-3{list-style-type:none}ul.lst-kix_juam6n4619t2-4{list-style-type:none}ul.lst-kix_juam6n4619t2-5{list-style-type:none}ul.lst-kix_juam6n4619t2-6{list-style-type:none}ul.lst-kix_juam6n4619t2-7{list-style-type:none}ul.lst-kix_juam6n4619t2-8{list-style-type:none}.lst-kix_jzarhjey1jrh-2>li:before{content:"\0025a0  "}.lst-kix_cy2og2rt03zo-2>li:before{content:"\0025a0  "}.lst-kix_klzhllb6xcyr-1>li:before{content:"\0025cb  "}.lst-kix_62is116n17wq-4>li:before{content:"\0025cb  "}.lst-kix_8wesot3ko53b-6>li:before{content:"\0025cf  "}.lst-kix_a9t5z6hxg4oz-1>li:before{content:"\0025cb  "}.lst-kix_ydb53brft35n-5>li:before{content:"\0025a0  "}.lst-kix_f51q5t2m61i8-5>li:before{content:"\0025a0  "}.lst-kix_6jabh0roz5s1-8>li:before{content:"\0025a0  "}.lst-kix_p7d8xi28lme6-7>li:before{content:"\0025cb  "}ul.lst-kix_kxfg6pdeancy-3{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-1{list-style-type:none}ul.lst-kix_kxfg6pdeancy-4{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-0{list-style-type:none}.lst-kix_2s6hp75gzisz-3>li:before{content:"\0025cf  "}ul.lst-kix_kxfg6pdeancy-1{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-3{list-style-type:none}ul.lst-kix_kxfg6pdeancy-2{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-2{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-5{list-style-type:none}ul.lst-kix_kxfg6pdeancy-0{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-4{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-7{list-style-type:none}ul.lst-kix_jhqr8d48vqqp-6{list-style-type:none}.lst-kix_ta3ofdl38yyn-1>li:before{content:"\0025cb  "}ul.lst-kix_jhqr8d48vqqp-8{list-style-type:none}ul.lst-kix_kxfg6pdeancy-7{list-style-type:none}ul.lst-kix_kxfg6pdeancy-8{list-style-type:none}ul.lst-kix_kxfg6pdeancy-5{list-style-type:none}.lst-kix_5x48r7dz2gvp-2>li:before{content:"\0025a0  "}ul.lst-kix_kxfg6pdeancy-6{list-style-type:none}.lst-kix_r75b0bpeph0o-5>li:before{content:"\0025a0  "}.lst-kix_7jzvzpyn9bnk-2>li:before{content:"\0025a0  "}.lst-kix_ct57w2de70tb-5>li:before{content:"\0025a0  "}.lst-kix_iqbmivscsmun-4>li:before{content:"\0025cb  "}.lst-kix_2zf5bd1mem1t-3>li:before{content:"\0025cf  "}.lst-kix_m8diz2yltkag-1>li:before{content:"\0025cb  "}.lst-kix_isql72ps5u79-6>li:before{content:"\0025cf  "}.lst-kix_mh0uhwysa91-6>li:before{content:"\0025cf  "}.lst-kix_kxpudurk2j6e-5>li:before{content:"\0025a0  "}ul.lst-kix_tg2hwffju8t9-5{list-style-type:none}ul.lst-kix_tg2hwffju8t9-6{list-style-type:none}ul.lst-kix_tg2hwffju8t9-7{list-style-type:none}ul.lst-kix_tg2hwffju8t9-8{list-style-type:none}ul.lst-kix_xrce7kijh7l1-8{list-style-type:none}ul.lst-kix_xrce7kijh7l1-7{list-style-type:none}.lst-kix_5p6viq6b9z8b-0>li:before{content:"\0025cf  "}ul.lst-kix_xrce7kijh7l1-6{list-style-type:none}ul.lst-kix_xrce7kijh7l1-5{list-style-type:none}ul.lst-kix_xrce7kijh7l1-4{list-style-type:none}.lst-kix_8fyyex2aqhl9-6>li:before{content:"\0025cf  "}.lst-kix_p2mkzdwo7q6r-4>li:before{content:"\0025cb  "}ul.lst-kix_xrce7kijh7l1-3{list-style-type:none}.lst-kix_kz1sfsbi2eng-0>li:before{content:"\0025cf  "}ul.lst-kix_xrce7kijh7l1-2{list-style-type:none}ul.lst-kix_xrce7kijh7l1-1{list-style-type:none}ul.lst-kix_cst6lp3lb98n-0{list-style-type:none}.lst-kix_emy9ap8zeql8-3>li:before{content:"\0025cf  "}ul.lst-kix_tg2hwffju8t9-0{list-style-type:none}ul.lst-kix_xrce7kijh7l1-0{list-style-type:none}ul.lst-kix_cst6lp3lb98n-1{list-style-type:none}ul.lst-kix_tg2hwffju8t9-1{list-style-type:none}ul.lst-kix_894copnhklgz-0{list-style-type:none}ul.lst-kix_cst6lp3lb98n-2{list-style-type:none}ul.lst-kix_tg2hwffju8t9-2{list-style-type:none}ul.lst-kix_894copnhklgz-1{list-style-type:none}ul.lst-kix_cst6lp3lb98n-3{list-style-type:none}ul.lst-kix_tg2hwffju8t9-3{list-style-type:none}ul.lst-kix_cst6lp3lb98n-4{list-style-type:none}ul.lst-kix_tg2hwffju8t9-4{list-style-type:none}.lst-kix_ljjwvb506pet-4>li:before{content:"\0025cb  "}.lst-kix_m8diz2yltkag-7>li:before{content:"\0025cb  "}.lst-kix_dq6vem8xoqp9-4>li:before{content:"\0025cb  "}.lst-kix_wq1o6elz9hy4-8>li:before{content:"\0025a0  "}.lst-kix_ljjwvb506pet-7>li:before{content:"\0025cb  "}.lst-kix_dq6vem8xoqp9-7>li:before{content:"\0025cb  "}ul.lst-kix_yhmx20qr5k0e-7{list-style-type:none}ul.lst-kix_yhmx20qr5k0e-8{list-style-type:none}.lst-kix_u9ija913pfxg-3>li:before{content:"-  "}ul.lst-kix_yhmx20qr5k0e-5{list-style-type:none}ul.lst-kix_yhmx20qr5k0e-6{list-style-type:none}.lst-kix_u9ija913pfxg-6>li:before{content:"-  "}.lst-kix_sxaq135mthse-8>li:before{content:"\0025a0  "}ul.lst-kix_yhmx20qr5k0e-0{list-style-type:none}ul.lst-kix_yhmx20qr5k0e-3{list-style-type:none}ul.lst-kix_yhmx20qr5k0e-4{list-style-type:none}ul.lst-kix_yhmx20qr5k0e-1{list-style-type:none}.lst-kix_ct57w2de70tb-8>li:before{content:"\0025a0  "}ul.lst-kix_yhmx20qr5k0e-2{list-style-type:none}.lst-kix_jbw6p6lajlkh-8>li:before{content:"\0025a0  "}.lst-kix_nf1j1iafbq39-3>li:before{content:"\0025cf  "}.lst-kix_m46mj27r3fpz-8>li:before{content:"\0025a0  "}.lst-kix_u82x83buy4jv-1>li:before{content:"\0025cb  "}.lst-kix_cst6lp3lb98n-1>li:before{content:"\0025cb  "}.lst-kix_nf1j1iafbq39-6>li:before{content:"\0025cf  "}.lst-kix_didibfga6ee5-6>li:before{content:"\0025cf  "}.lst-kix_u9ija913pfxg-0>li:before{content:"-  "}.lst-kix_didibfga6ee5-3>li:before{content:"\0025cf  "}ul.lst-kix_pjyqz9f44g8s-2{list-style-type:none}ul.lst-kix_pjyqz9f44g8s-3{list-style-type:none}.lst-kix_398os0jr1iw3-3>li:before{content:"\0025cf  "}ul.lst-kix_pjyqz9f44g8s-0{list-style-type:none}ul.lst-kix_pjyqz9f44g8s-1{list-style-type:none}.lst-kix_5d66udn9quk7-3>li:before{content:"\0025cf  "}.lst-kix_douubz1tb8an-6>li:before{content:"\0025cf  "}.lst-kix_mxwp6np2u5xu-0>li:before{content:"-  "}.lst-kix_204pjqaavew-5>li:before{content:"\0025a0  "}.lst-kix_u82x83buy4jv-4>li:before{content:"\0025cb  "}.lst-kix_jbw6p6lajlkh-2>li:before{content:"\0025a0  "}.lst-kix_398os0jr1iw3-6>li:before{content:"\0025cf  "}ul.lst-kix_pjyqz9f44g8s-8{list-style-type:none}.lst-kix_8hjy0hqh3rvg-0>li:before{content:"\0025cf  "}.lst-kix_u82x83buy4jv-7>li:before{content:"\0025cb  "}.lst-kix_jbw6p6lajlkh-5>li:before{content:"\0025a0  "}ul.lst-kix_pjyqz9f44g8s-6{list-style-type:none}ul.lst-kix_pjyqz9f44g8s-7{list-style-type:none}ul.lst-kix_pjyqz9f44g8s-4{list-style-type:none}.lst-kix_5d66udn9quk7-6>li:before{content:"\0025cf  "}ul.lst-kix_pjyqz9f44g8s-5{list-style-type:none}.lst-kix_f3egesa43r6b-3>li:before{content:"\0025cf  "}.lst-kix_5kaugn46jpw6-7>li:before{content:"\0025cb  "}.lst-kix_dq6vem8xoqp9-1>li:before{content:"\0025cb  "}.lst-kix_f3egesa43r6b-6>li:before{content:"\0025cf  "}.lst-kix_mxwp6np2u5xu-3>li:before{content:"-  "}.lst-kix_f3egesa43r6b-0>li:before{content:"\0025cf  "}.lst-kix_hydge7xfbt8r-3>li:before{content:"\0025cf  "}.lst-kix_5d66udn9quk7-0>li:before{content:"\0025cf  "}.lst-kix_hydge7xfbt8r-6>li:before{content:"\0025cf  "}.lst-kix_7b2y41o7s2k3-2>li:before{content:"\0025a0  "}.lst-kix_lyxc6qcmvco2-6>li:before{content:"\0025cf  "}.lst-kix_f51q5t2m61i8-8>li:before{content:"\0025a0  "}.lst-kix_z3si3pbri1de-3>li:before{content:"\0025cf  "}ul.lst-kix_bhgkuyt4aopz-7{list-style-type:none}ul.lst-kix_bhgkuyt4aopz-8{list-style-type:none}.lst-kix_i1kls5tk4gt6-2>li:before{content:"\0025a0  "}.lst-kix_o9z4cf2yq4mz-8>li:before{content:"\0025a0  "}ul.lst-kix_bhgkuyt4aopz-5{list-style-type:none}ul.lst-kix_bhgkuyt4aopz-6{list-style-type:none}.lst-kix_l47f7qjlu9cu-2>li:before{content:"\0025a0  "}ul.lst-kix_bhgkuyt4aopz-3{list-style-type:none}ul.lst-kix_bhgkuyt4aopz-4{list-style-type:none}ul.lst-kix_bhgkuyt4aopz-1{list-style-type:none}.lst-kix_o5m7acjt2ju1-4>li:before{content:"\0025cb  "}ul.lst-kix_bhgkuyt4aopz-2{list-style-type:none}.lst-kix_jzarhjey1jrh-5>li:before{content:"\0025a0  "}ul.lst-kix_bhgkuyt4aopz-0{list-style-type:none}.lst-kix_iiujyt46s4u7-2>li:before{content:"\0025a0  "}.lst-kix_62is116n17wq-7>li:before{content:"\0025cb  "}.lst-kix_204pjqaavew-2>li:before{content:"\0025a0  "}.lst-kix_gwuatnbmuewh-6>li:before{content:"\0025cf  "}.lst-kix_egb7bfw0xx0q-3>li:before{content:"\0025cf  "}.lst-kix_xx3xz98aq2wq-1>li:before{content:"\0025cb  "}.lst-kix_gswyhyotsm36-3>li:before{content:"\0025cf  "}.lst-kix_c2vvnsz0drek-1>li:before{content:"\0025cb  "}ul.lst-kix_ltwr0s349me7-5{list-style-type:none}ul.lst-kix_ltwr0s349me7-4{list-style-type:none}ul.lst-kix_ltwr0s349me7-7{list-style-type:none}.lst-kix_l4pre1xosssa-2>li:before{content:"\0025a0  "}ul.lst-kix_ltwr0s349me7-6{list-style-type:none}.lst-kix_5p6viq6b9z8b-3>li:before{content:"\0025cf  "}ul.lst-kix_ltwr0s349me7-8{list-style-type:none}.lst-kix_41ikkbtdotge-3>li:before{content:"\0025cf  "}.lst-kix_nf1j1iafbq39-0>li:before{content:"\0025cf  "}.lst-kix_b474o1tvhevy-4>li:before{content:"\0025cb  "}.lst-kix_solhl6ysg65v-1>li:before{content:"\0025cb  "}.lst-kix_p7d8xi28lme6-4>li:before{content:"\0025cb  "}.lst-kix_bq5ybp1zal0b-8>li:before{content:"\0025a0  "}.lst-kix_ex6gjyozs00-4>li:before{content:"\0025cb  "}.lst-kix_fk4cszd8thar-0>li:before{content:"\0025cf  "}.lst-kix_cst6lp3lb98n-7>li:before{content:"\0025cb  "}.lst-kix_5x48r7dz2gvp-5>li:before{content:"\0025a0  "}ul.lst-kix_ltwr0s349me7-1{list-style-type:none}ul.lst-kix_ltwr0s349me7-0{list-style-type:none}ul.lst-kix_ltwr0s349me7-3{list-style-type:none}ul.lst-kix_ltwr0s349me7-2{list-style-type:none}.lst-kix_j24rsp966ll1-6>li:before{content:"\0025cf  "}.lst-kix_bq5ybp1zal0b-2>li:before{content:"\0025a0  "}.lst-kix_24cmoxb3qmwl-4>li:before{content:"\0025cb  "}.lst-kix_fk4cszd8thar-6>li:before{content:"\0025cf  "}.lst-kix_j24rsp966ll1-0>li:before{content:"\0025cf  "}.lst-kix_isql72ps5u79-3>li:before{content:"\0025cf  "}.lst-kix_o4d3wx7elf58-5>li:before{content:"\0025a0  "}.lst-kix_wq1o6elz9hy4-5>li:before{content:"\0025a0  "}.lst-kix_p2mkzdwo7q6r-7>li:before{content:"\0025cb  "}.lst-kix_l4pre1xosssa-8>li:before{content:"\0025a0  "}.lst-kix_m8diz2yltkag-4>li:before{content:"\0025cb  "}.lst-kix_emy9ap8zeql8-0>li:before{content:"\0025cf  "}.lst-kix_iqbmivscsmun-1>li:before{content:"\0025cb  "}.lst-kix_9khvkfghhb3b-6>li:before{content:"\0025cf  "}.lst-kix_l47f7qjlu9cu-8>li:before{content:"\0025a0  "}.lst-kix_g0l7lf51hdyf-0>li:before{content:"\0025cf  "}.lst-kix_kz1sfsbi2eng-6>li:before{content:"\0025cf  "}ul.lst-kix_vt0etbgrzlaw-8{list-style-type:none}ul.lst-kix_defeme42l2ge-4{list-style-type:none}ul.lst-kix_vt0etbgrzlaw-7{list-style-type:none}ul.lst-kix_defeme42l2ge-3{list-style-type:none}ul.lst-kix_vt0etbgrzlaw-6{list-style-type:none}ul.lst-kix_defeme42l2ge-6{list-style-type:none}.lst-kix_pzunwido5p11-7>li:before{content:"\0025cb  "}ul.lst-kix_vt0etbgrzlaw-5{list-style-type:none}ul.lst-kix_defeme42l2ge-5{list-style-type:none}ul.lst-kix_defeme42l2ge-0{list-style-type:none}.lst-kix_yv7c7gf6elha-1>li:before{content:"\0025cb  "}ul.lst-kix_defeme42l2ge-2{list-style-type:none}ul.lst-kix_defeme42l2ge-1{list-style-type:none}ul.lst-kix_vt0etbgrzlaw-0{list-style-type:none}.lst-kix_fjl32nwuhu4u-6>li:before{content:"\0025cf  "}.lst-kix_3fvzyqsgm5ef-2>li:before{content:"\0025a0  "}.lst-kix_bhgkuyt4aopz-6>li:before{content:"\0025cf  "}ul.lst-kix_vt0etbgrzlaw-4{list-style-type:none}ul.lst-kix_vt0etbgrzlaw-3{list-style-type:none}ul.lst-kix_vt0etbgrzlaw-2{list-style-type:none}ul.lst-kix_vt0etbgrzlaw-1{list-style-type:none}ul.lst-kix_2jmrp4ochkmc-2{list-style-type:none}.lst-kix_vhrrjzyjvp5x-2>li:before{content:"\0025a0  "}.lst-kix_5x48r7dz2gvp-8>li:before{content:"\0025a0  "}ul.lst-kix_2jmrp4ochkmc-1{list-style-type:none}ul.lst-kix_2jmrp4ochkmc-0{list-style-type:none}.lst-kix_yh6f66826ysk-2>li:before{content:"\0025a0  "}.lst-kix_yh6f66826ysk-5>li:before{content:"\0025a0  "}.lst-kix_4fwhm2tiefhz-7>li:before{content:"\0025cb  "}ul.lst-kix_defeme42l2ge-8{list-style-type:none}ul.lst-kix_defeme42l2ge-7{list-style-type:none}.lst-kix_spzfuoeekohi-4>li:before{content:"\0025cb  "}ul.lst-kix_vebf56ofbjcp-0{list-style-type:none}.lst-kix_spzfuoeekohi-1>li:before{content:"\0025cb  "}ul.lst-kix_2jmrp4ochkmc-8{list-style-type:none}ul.lst-kix_2jmrp4ochkmc-7{list-style-type:none}ul.lst-kix_2jmrp4ochkmc-6{list-style-type:none}ul.lst-kix_2jmrp4ochkmc-5{list-style-type:none}ul.lst-kix_2jmrp4ochkmc-4{list-style-type:none}ul.lst-kix_2jmrp4ochkmc-3{list-style-type:none}.lst-kix_2rp5aiyjkdpd-4>li:before{content:"\0025cb  "}.lst-kix_xqfh3tf7tjc0-2>li:before{content:"\0025a0  "}.lst-kix_nbuhcgd4ft21-4>li:before{content:"\0025cb  "}.lst-kix_xqfh3tf7tjc0-5>li:before{content:"\0025a0  "}.lst-kix_383wi8io125v-3>li:before{content:"\0025cf  "}.lst-kix_vapye4eddf48-3>li:before{content:"\0025cf  "}.lst-kix_8z696e8bv17-8>li:before{content:"\0025a0  "}.lst-kix_nbuhcgd4ft21-1>li:before{content:"\0025cb  "}.lst-kix_383wi8io125v-6>li:before{content:"\0025cf  "}ul.lst-kix_6jabh0roz5s1-7{list-style-type:none}.lst-kix_o9z4cf2yq4mz-2>li:before{content:"\0025a0  "}ul.lst-kix_6jabh0roz5s1-6{list-style-type:none}.lst-kix_s6xk062mdkv5-2>li:before{content:"\0025a0  "}ul.lst-kix_6jabh0roz5s1-8{list-style-type:none}.lst-kix_vapye4eddf48-0>li:before{content:"\0025cf  "}.lst-kix_gnt40hozc3kv-3>li:before{content:"\0025cf  "}.lst-kix_bcdpfzfy81p5-2>li:before{content:"\0025a0  "}ul.lst-kix_6jabh0roz5s1-1{list-style-type:none}.lst-kix_o9z4cf2yq4mz-5>li:before{content:"\0025a0  "}ul.lst-kix_6jabh0roz5s1-0{list-style-type:none}ul.lst-kix_6jabh0roz5s1-3{list-style-type:none}ul.lst-kix_6jabh0roz5s1-2{list-style-type:none}ul.lst-kix_204pjqaavew-1{list-style-type:none}ul.lst-kix_6jabh0roz5s1-5{list-style-type:none}ul.lst-kix_204pjqaavew-0{list-style-type:none}ul.lst-kix_6jabh0roz5s1-4{list-style-type:none}.lst-kix_bcdpfzfy81p5-5>li:before{content:"\0025a0  "}ul.lst-kix_204pjqaavew-3{list-style-type:none}ul.lst-kix_204pjqaavew-2{list-style-type:none}ul.lst-kix_204pjqaavew-5{list-style-type:none}ul.lst-kix_204pjqaavew-4{list-style-type:none}.lst-kix_xgsgxyckqw77-2>li:before{content:"\0025a0  "}ul.lst-kix_204pjqaavew-7{list-style-type:none}.lst-kix_d4lbyvb2g3j-6>li:before{content:"\0025cf  "}ul.lst-kix_204pjqaavew-6{list-style-type:none}.lst-kix_ehe7zglmx7i2-5>li:before{content:"\0025a0  "}ul.lst-kix_204pjqaavew-8{list-style-type:none}.lst-kix_ehe7zglmx7i2-8>li:before{content:"\0025a0  "}.lst-kix_gnt40hozc3kv-0>li:before{content:"\0025cf  "}ul.lst-kix_didibfga6ee5-7{list-style-type:none}ul.lst-kix_w14ild1qeoey-2{list-style-type:none}ul.lst-kix_didibfga6ee5-6{list-style-type:none}ul.lst-kix_w14ild1qeoey-3{list-style-type:none}ul.lst-kix_w14ild1qeoey-0{list-style-type:none}ul.lst-kix_didibfga6ee5-8{list-style-type:none}ul.lst-kix_w14ild1qeoey-1{list-style-type:none}ul.lst-kix_didibfga6ee5-3{list-style-type:none}ul.lst-kix_didibfga6ee5-2{list-style-type:none}ul.lst-kix_didibfga6ee5-5{list-style-type:none}ul.lst-kix_didibfga6ee5-4{list-style-type:none}.lst-kix_l47f7qjlu9cu-5>li:before{content:"\0025a0  "}ul.lst-kix_didibfga6ee5-1{list-style-type:none}ul.lst-kix_w14ild1qeoey-8{list-style-type:none}ul.lst-kix_didibfga6ee5-0{list-style-type:none}ul.lst-kix_w14ild1qeoey-6{list-style-type:none}ul.lst-kix_w14ild1qeoey-7{list-style-type:none}ul.lst-kix_w14ild1qeoey-4{list-style-type:none}ul.lst-kix_w14ild1qeoey-5{list-style-type:none}ul.lst-kix_474t6llskbh7-7{list-style-type:none}ul.lst-kix_474t6llskbh7-8{list-style-type:none}.lst-kix_z3si3pbri1de-0>li:before{content:"\0025cf  "}ul.lst-kix_474t6llskbh7-5{list-style-type:none}.lst-kix_o5m7acjt2ju1-1>li:before{content:"\0025cb  "}ul.lst-kix_474t6llskbh7-6{list-style-type:none}ul.lst-kix_474t6llskbh7-3{list-style-type:none}.lst-kix_jzarhjey1jrh-8>li:before{content:"\0025a0  "}ul.lst-kix_474t6llskbh7-4{list-style-type:none}ul.lst-kix_474t6llskbh7-1{list-style-type:none}ul.lst-kix_474t6llskbh7-2{list-style-type:none}ul.lst-kix_474t6llskbh7-0{list-style-type:none}.lst-kix_p0fqpwscumob-6>li:before{content:"\0025cf  "}.lst-kix_481c98v2pt83-5>li:before{content:"\0025a0  "}.lst-kix_gswyhyotsm36-6>li:before{content:"\0025cf  "}ul.lst-kix_ut5bvwbkq6rt-5{list-style-type:none}ul.lst-kix_ut5bvwbkq6rt-4{list-style-type:none}ul.lst-kix_ut5bvwbkq6rt-7{list-style-type:none}ul.lst-kix_ut5bvwbkq6rt-6{list-style-type:none}.lst-kix_qsdpvqxg1u78-7>li:before{content:"\0025cb  "}ul.lst-kix_ut5bvwbkq6rt-8{list-style-type:none}.lst-kix_d3k7hdh35j7a-4>li:before{content:"\0025cb  "}.lst-kix_rwqhz1uwdird-4>li:before{content:"\0025cb  "}.lst-kix_juam6n4619t2-1>li:before{content:"\0025cb  "}.lst-kix_xqqiyb3b5ag0-7>li:before{content:"\0025cb  "}.lst-kix_5ia1ign2sshz-4>li:before{content:"\0025cb  "}.lst-kix_cy2og2rt03zo-8>li:before{content:"\0025a0  "}.lst-kix_5p6viq6b9z8b-6>li:before{content:"\0025cf  "}.lst-kix_l4pre1xosssa-5>li:before{content:"\0025a0  "}.lst-kix_41ikkbtdotge-0>li:before{content:"\0025cf  "}.lst-kix_egb7bfw0xx0q-6>li:before{content:"\0025cf  "}.lst-kix_ex6gjyozs00-1>li:before{content:"\0025cb  "}.lst-kix_1ig57j6lcs95-2>li:before{content:"\0025a0  "}.lst-kix_2rp5aiyjkdpd-7>li:before{content:"\0025cb  "}.lst-kix_didibfga6ee5-0>li:before{content:"\0025cf  "}.lst-kix_tltq6x9aaf31-2>li:before{content:"\0025a0  "}.lst-kix_cst6lp3lb98n-4>li:before{content:"\0025cb  "}.lst-kix_b474o1tvhevy-7>li:before{content:"\0025cb  "}.lst-kix_r8labycqo9zl-4>li:before{content:"\0025cb  "}.lst-kix_p7d8xi28lme6-1>li:before{content:"\0025cb  "}.lst-kix_bq5ybp1zal0b-5>li:before{content:"\0025a0  "}.lst-kix_o4d3wx7elf58-8>li:before{content:"\0025a0  "}.lst-kix_fk4cszd8thar-3>li:before{content:"\0025cf  "}ul.lst-kix_l47f7qjlu9cu-3{list-style-type:none}ul.lst-kix_e2bt8y2ywvrz-8{list-style-type:none}.lst-kix_wq1o6elz9hy4-2>li:before{content:"\0025a0  "}.lst-kix_gpgigcfykzff-1>li:before{content:"\0025cb  "}ul.lst-kix_l47f7qjlu9cu-2{list-style-type:none}ul.lst-kix_e2bt8y2ywvrz-7{list-style-type:none}ul.lst-kix_l47f7qjlu9cu-1{list-style-type:none}ul.lst-kix_l47f7qjlu9cu-0{list-style-type:none}ul.lst-kix_l47f7qjlu9cu-7{list-style-type:none}ul.lst-kix_e2bt8y2ywvrz-4{list-style-type:none}ul.lst-kix_l47f7qjlu9cu-6{list-style-type:none}ul.lst-kix_e2bt8y2ywvrz-3{list-style-type:none}ul.lst-kix_l47f7qjlu9cu-5{list-style-type:none}ul.lst-kix_e2bt8y2ywvrz-6{list-style-type:none}.lst-kix_j24rsp966ll1-3>li:before{content:"\0025cf  "}ul.lst-kix_l47f7qjlu9cu-4{list-style-type:none}.lst-kix_p98dexr65abh-3>li:before{content:"\0025cf  "}ul.lst-kix_e2bt8y2ywvrz-5{list-style-type:none}.lst-kix_3fvzyqsgm5ef-5>li:before{content:"\0025a0  "}.lst-kix_u1clvxqptoo-0>li:before{content:"\0025cf  "}.lst-kix_x5jl3p2g0327-2>li:before{content:"\0025a0  "}.lst-kix_qlax8iv93hxg-6>li:before{content:"\0025cf  "}.lst-kix_24cmoxb3qmwl-7>li:before{content:"\0025cb  "}ul.lst-kix_e2bt8y2ywvrz-0{list-style-type:none}ul.lst-kix_e2bt8y2ywvrz-2{list-style-type:none}ul.lst-kix_l47f7qjlu9cu-8{list-style-type:none}ul.lst-kix_e2bt8y2ywvrz-1{list-style-type:none}.lst-kix_isql72ps5u79-0>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c39{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:284.2pt;border-top-color:#000000;border-bottom-style:solid}.c45{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:184.5pt;border-top-color:#000000;border-bottom-style:solid}.c50{color:#000000;font-weight:normal;text-decoration:none;vertical-align:baseline;font-family:"Julius Sans One";font-style:normal}.c33{color:#000000;font-weight:normal;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c35{border-spacing:0;border-collapse:collapse;margin-right:auto}.c48{background-color:#f0f3fc;font-size:10.5pt;font-family:"Verdana";color:#222222}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c49{padding-top:0pt;padding-bottom:0pt;line-height:1.0}.c5{page-break-after:avoid;orphans:2;widows:2}.c3{orphans:2;widows:2;height:11pt}.c8{font-size:10pt;font-family:"Helvetica Neue";color:#212121}.c6{orphans:2;widows:2}.c17{margin-left:108pt;padding-left:0pt}.c14{color:#b7b7b7;text-decoration:underline}.c20{font-size:8pt;font-style:italic}.c22{margin-left:72pt;padding-left:0pt}.c7{margin-left:36pt;padding-left:0pt}.c19{color:#999999;text-decoration:underline}.c2{padding:0;margin:0}.c43{font-size:10pt;font-family:"Helvetica Neue"}.c16{background-color:#cccccc;color:#1155cc}.c37{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c0{color:inherit;text-decoration:inherit}.c11{color:#efefef;text-decoration:underline}.c42{background-color:#cccccc;color:#333333}.c4{color:#1155cc;text-decoration:underline}.c26{font-size:8pt}.c36{color:#efefef}.c41{font-size:36pt}.c9{font-weight:bold}.c23{height:60pt}.c12{color:#990000}.c44{height:208pt}.c1{background-color:#ffffff}.c27{color:#cccccc}.c21{font-style:italic}.c31{text-align:right}.c10{color:#cc0000}.c29{text-decoration:underline}.c30{text-align:left}.c24{color:#b7b7b7}.c18{font-size:24pt}.c25{line-height:1.4625}.c40{height:11pt}.c28{color:#38761d}.c32{color:#ff0000}.c34{color:#999999}.c13{font-size:11pt}.c46{color:#7e57c2}.c47{color:#d9d9d9}.c38{font-size:12pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:0pt;color:#000000;font-size:60pt;padding-bottom:0pt;font-family:"Julius Sans One";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:right}h2{padding-top:0pt;color:#000000;font-size:24pt;padding-bottom:0pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:0pt;color:#000000;font-size:14pt;padding-bottom:0pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:right}h5{padding-top:0pt;color:#b7b7b7;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c1 c37"><p class="c3"><span></span></p><a id="t.128b40fe24ec44c7c1e11d20f4229873bd06c889"></a><a id="t.0"></a><table class="c35"><tbody><tr class="c44"><td class="c45" colspan="1" rowspan="1"><p class="c15"><span class="c33 c13">A list of papers, with summaries of the relevant points, for use as a reference of current SOTA when actually making things. I was tired of paper lists that required me to read everything for it to be useful.</span></p><p class="c15 c40"><span class="c33 c13"></span></p><p class="c15"><span>Please don&#39;t share this with anyone not on the invite list, we&#39;re within few enough years of agi for it to be a real concern.</span></p></td><td class="c39" colspan="1" rowspan="1"><h1 class="c5 c31 c49" id="h.u1ye7hc88r6o"><span class="c41">How to Implement Cutting Edge Neural Networks</span></h1></td></tr></tbody></table><hr><p class="c3"><span></span></p><p class="c3"><span class="c8 c1"></span></p><p class="c3"><span></span></p><p class="c6"><span>I have a bunch of headings, and vaguely categorized papers. You might need to scroll through and check a few related headings if you don&#39;t have the same quirks of interpretation about the headings as I do. It&#39;s meant to be skimmed.</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6"><span>keywords I use in the descriptions sometimes:</span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_gr1y63l14uay-0 start"><li class="c6 c7"><span>application: uses other techniques for a new dataset, not focused on making new techniques; usually is about architecture and how to combine techniques</span></li><li class="c6 c7"><span>technique: adds or replaces an option for how to do things. research on the methods themselves, rather than how to use them. may also have &quot;how to use them&quot;.</span></li><li class="c6 c7"><span>theory: introduces model for why something works, rather than introducing something that works better.</span></li><li class="c6 c7"><span>&hellip; improvement &hellip; sota &hellip;: empirically a big-deal improvement. usually goes with a technique.</span></li></ul><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.bxplvi4p27q6"><span></span></h1><h1 class="c5" id="h.726ft4k37yh8"><span>Basics</span></h1><ul class="c2 lst-kix_acuhah6xdzsp-0 start"><li class="c6 c7"><span>initial context: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://frnsys.com/ai_notes/&amp;sa=D&amp;ust=1465142037677000&amp;usg=AFQjCNEzvgD__nCxCjmydcZN5tboHfYXyg">http://frnsys.com/ai_notes/</a></span></li></ul><ul class="c2 lst-kix_kxfg6pdeancy-0 start"><li class="c6 c7"><span>courses: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://docs.google.com/spreadsheets/d/1NSbURoynPVnOvSCtmaIX6zV8wl6n3ybacnNGMyb-v-0/edit%23gid%3D0&amp;sa=D&amp;ust=1465142037679000&amp;usg=AFQjCNEwDEquKxxXi-40hQn3zl4WLjTguw">https://docs.google.com/spreadsheets/d/1NSbURoynPVnOvSCtmaIX6zV8wl6n3ybacnNGMyb-v-0/edit#gid=0</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_u1clvxqptoo-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://vkrakovna.wordpress.com/2016/01/16/to-contribute-to-ai-safety-consider-doing-ai-research/&amp;sa=D&amp;ust=1465142037680000&amp;usg=AFQjCNGQbo3yPqA6dmaaNg3SSsc03oK0Ew">https://vkrakovna.wordpress.com/2016/01/16/to-contribute-to-ai-safety-consider-doing-ai-research/</a></span><span>&nbsp;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1305.0445v2.pdf&amp;sa=D&amp;ust=1465142037681000&amp;usg=AFQjCNGbZwrzJC_52BRnAlCUf8YHyDqoeA">http://arxiv.org/pdf/1305.0445v2.pdf</a></span><span>&nbsp;open problems in deep learning from 2013</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/wiki/index&amp;sa=D&amp;ust=1465142037682000&amp;usg=AFQjCNH4NDG6jXUchXOTOXRWkaudPrua4w">https://www.reddit.com/r/MachineLearning/wiki/index</a></span><span>&nbsp;a bunch of resources for getting started including moocs and books. has an index of dataset indexes, so should go in &quot;datasets&quot;.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.2v697i7vajfz"><span>Libraries</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_d1iyykra6bco-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/eladhoffer/ImageNet-Training&amp;sa=D&amp;ust=1465142037684000&amp;usg=AFQjCNG2uxF38IuoLLlQODLsD-SuCAieXg">https://github.com/eladhoffer/ImageNet-Training</a></span><span>&nbsp;- example for training on imagenet in torch. Uses a lot of bells and whistles, shows you how to set stuff up.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_l3zoug7b3vcw-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/karpathy/char-rnn&amp;sa=D&amp;ust=1465142037685000&amp;usg=AFQjCNHWgwXGrtd-hFQTFQQE3OjbbtONHw">https://github.com/karpathy/char-rnn</a></span><span>&nbsp;- similar, but for recurrent nets on text. not as much of the bells and whistles, since text is a fair bit easier.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_l4pre1xosssa-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/Atcold/Machine-learning-with-Torch/blob/master/MLP-regression/src/regression.lua&amp;sa=D&amp;ust=1465142037686000&amp;usg=AFQjCNFSETF0paJK4ATAvj7SLb00aVagvw">https://github.com/Atcold/Machine-learning-with-Torch/blob/master/MLP-regression/src/regression.lua</a></span><span>&nbsp;- example with training graphing.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.kdnuggets.com/2015/12/tensor-flow-terrific-deep-learning-library.html&amp;sa=D&amp;ust=1465142037687000&amp;usg=AFQjCNETZCEgBqO_te93yJgNeao4ttvuhQ">http://www.kdnuggets.com/2015/12/tensor-flow-terrific-deep-learning-library.html</a></span><span>&nbsp;argument for tensorflow being awesome; I am not fully convinced</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://blogs.microsoft.com/next/2016/01/25/microsoft-releases-cntk-its-open-source-deep-learning-toolkit-on-github/&amp;sa=D&amp;ust=1465142037688000&amp;usg=AFQjCNGzRvALm2uMrrLb4Nk-Y78EQugkDQ">http://blogs.microsoft.com/next/2016/01/25/microsoft-releases-cntk-its-open-source-deep-learning-toolkit-on-github/</a></span><span>&nbsp;new ms thing, better cross-gpu scalability than tensorflow</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://torch.ch/blog/2016/02/04/resnets.html&amp;sa=D&amp;ust=1465142037689000&amp;usg=AFQjCNH2AkqXjQXJSqRYfVmd68OWKhSgWw">http://torch.ch/blog/2016/02/04/resnets.html</a></span><span>&nbsp;analysis of resnets. should also go in &quot;architectures&quot; next to resnets, &quot;parallelization&quot;, and &quot;examples&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.deepdetect.com/applications/text_model/&amp;sa=D&amp;ust=1465142037690000&amp;usg=AFQjCNEVn5CByxwGMPBn_Vaa8gyVIDIafA">http://www.deepdetect.com/applications/text_model/</a></span><span>&nbsp;pretrained text classification conv models. </span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/Element-Research/rnn&amp;sa=D&amp;ust=1465142037690000&amp;usg=AFQjCNE2F71GW3O8vTBAohRikZ_suBMDDQ">https://github.com/Element-Research/rnn</a></span><span>&nbsp;torch rnn tool that doesn&#39;t suck like char-rnn does</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://developer.nvidia.com/cudnn&amp;sa=D&amp;ust=1465142037691000&amp;usg=AFQjCNHWzch08xVjZsHYlCR-jg5P-J99dg">https://developer.nvidia.com/cudnn</a></span><span>&nbsp;v4 is out, and it&#39;s faster</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/htwaijry/npy4th&amp;sa=D&amp;ust=1465142037691000&amp;usg=AFQjCNESWJLoudxt5WHJzs3BMFJD2uLaJw">https://github.com/htwaijry/npy4th</a></span><span>&nbsp;numpy data loading (one way, looks like) for torch. useful if you want to manage communication from python (which you do).</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h2 class="c5" id="h.8mmo7i63ffzi"><span>Debugging</span></h2><p class="c3"><span class="c18"></span></p><ul class="c2 lst-kix_swbzestlepme-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DghEmQSxT6tw&amp;sa=D&amp;ust=1465142037694000&amp;usg=AFQjCNH-0jjzk5VkPns9UU9x7VHOqHZovQ">https://www.youtube.com/watch?v=ghEmQSxT6tw</a></span><span>&nbsp;talk on network vis by one of the clarifai folks. Details</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_p3gj10803b4t-0 start"><li class="c6 c7"><span>also, deep dream</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_xgfpveff5zr0-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.01066&amp;sa=D&amp;ust=1465142037695000&amp;usg=AFQjCNG2iRzgcTDvFesJg6bvFpv_t6e4iQ">http://arxiv.org/abs/1506.01066</a></span><span>&nbsp;didn&#39;t read abstract; &quot;visualizing and understanding neural networks for nlp&quot;. Should also go in language.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.01983&amp;sa=D&amp;ust=1465142037696000&amp;usg=AFQjCNHIXk8KIDR7BLCMdxoo2B4cn6A8yg">http://arxiv.org/abs/1508.01983</a></span><span>&nbsp;[abs][conf] &quot;Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View Invariance&quot;: looks pretty interesting, worthy of a read. as such, should also go in &quot;theory&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06855&amp;sa=D&amp;ust=1465142037696000&amp;usg=AFQjCNHLWKWeM5yt5RRq5wWQUg0rZRCv-Q">http://arxiv.org/abs/1511.06855</a></span><span>&nbsp;&quot;Discovering Internal Representations from Object-CNNs Using Population Encoding&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.06576&amp;sa=D&amp;ust=1465142037697000&amp;usg=AFQjCNGbncbXNMtyOEhULmrJWPHSnPr2ww">http://arxiv.org/abs/1508.06576</a></span><span>&nbsp;artistic style paper. kinda has debugging implications. also should go in &quot;images&quot; and/or &quot;applications&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1311.2901&amp;sa=D&amp;ust=1465142037697000&amp;usg=AFQjCNEVs1_zoDIlu1tYpV9PrhcywsP2PQ">http://arxiv.org/abs/1311.2901</a></span><span>&nbsp;&quot;visualizing and understanding convolutional networks&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://experiments.mostafa.io/public/ffbpann/&amp;sa=D&amp;ust=1465142037698000&amp;usg=AFQjCNH6btT1x2lqUPaoRBY1izC_SnkU-A">http://experiments.mostafa.io/public/ffbpann/</a></span><span>&nbsp;visualization of a network&#39;s learning - helps with getting an intuition about &quot;the end learns first&quot;. doesn&#39;t help get an intuition how data flows through the network forward or back, since it doesn&#39;t have those values.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.e0r4uo3qvbdc"><span>Classification</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_klzhllb6xcyr-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01121&amp;sa=D&amp;ust=1465142037700000&amp;usg=AFQjCNGT-Mv9c5BDR6Y-2_ik_hEd_q5jyg">http://arxiv.org/abs/1601.01121</a></span><span>&nbsp;- formal model of multi-class classification and is more efficient or something. doesn&#39;t look like a big deal. seems to be listed twice - </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01157&amp;sa=D&amp;ust=1465142037700000&amp;usg=AFQjCNE0CXFAn7f_Q4qAYsIDndeSudoJNQ">http://arxiv.org/abs/1601.01157</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_aui8m1kby916-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03260&amp;sa=D&amp;ust=1465142037701000&amp;usg=AFQjCNG2x7BiL74hY9oK9vrTCbr1Lxx9NQ">http://arxiv.org/abs/1511.03260</a></span><span>&nbsp;heirarchical thing for representing classification between a very large number of classes. this looks useful, I want to do such a thing. </span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.5ysqb9t4jme8"><span>Attention</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_313q3f5x4p2m-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/448kl2/as_far_as_i_know_there_are_no_training_algorithms/czp2lkv?context%3D3&amp;sa=D&amp;ust=1465142037703000&amp;usg=AFQjCNHET3HRs6KWcOj2pGyOm4av-jJrSg">https://www.reddit.com/r/MachineLearning/comments/448kl2/as_far_as_i_know_there_are_no_training_algorithms/czp2lkv?context=3</a></span><span>&nbsp;- looking for better ways for training hard attention.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.ud0if7jtdw9q"><span>Initialization</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_qwp9c5bklrce-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06422&amp;sa=D&amp;ust=1465142037704000&amp;usg=AFQjCNHG4d7G6svJN3fxkT3LrIGwka8Kbg">http://arxiv.org/abs/1511.06422</a></span><span>&nbsp;[abs:4][conf]</span><span>&nbsp;weight initialization that does no worse and can accelerate learning and improve speed. tests on a bunch of different networks.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06856&amp;sa=D&amp;ust=1465142037705000&amp;usg=AFQjCNGH62tEXefpZDX8QduzKn7VPL_dTw">http://arxiv.org/abs/1511.06856</a></span><span>&nbsp;[abs][conf] weight initialization based on properties of your data - intended for convnets. should also go in &quot;convolution&quot;, maaaybe &quot;images&quot; even though it&#39;s not an application to images directly. sounds really cool! is effectively an unsupervised initialization, but is three orders of magnitude faster (which makes sense because it&#39;s probably simple things). </span><span class="c9">very cool!</span></li></ul><h2 class="c5" id="h.dsf646pffj63"><span>Activation functions</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_didibfga6ee5-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07030&amp;sa=D&amp;ust=1465142037707000&amp;usg=AFQjCNG6cDbXP7m7_f-dQ3-fpIlfZGqSiw">http://arxiv.org/abs/1512.07030</a></span><span>&nbsp;new activation function with added parameters and variable curvature, srelu. Technique. Probably a big deal.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07289&amp;sa=D&amp;ust=1465142037707000&amp;usg=AFQjCNFFr18DTIjoxm-M4Pcr_SNyeMwerQ">http://arxiv.org/abs/1511.07289</a></span><span>&nbsp;exponential linear units - </span><span class="c9 c12">big improvement in SOTA </span><span>- looks pretty cool. takes the idea that leaky relu and parameterized relu are trying to apply, and does it better. learns faster on imagenet; 10% classification error with single crop, single model. </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pmg6qjdagav9-0 start"><li class="c6 c7"><span>Idea: how are periodic functions as activation functions? may be very interesting for signal processing. but, will probably screw gradient descent up like mad. what about sin(log(x+1))? what would this do? probably is going to end up useless because if it converges, it will have learned to avoid using it; and if it ever uses it, it will be by accident.</span></li></ul><ul class="c2 lst-kix_pmg6qjdagav9-1 start"><li class="c6 c22"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.wolframalpha.com/input/?i%3Dsin%2528sign%2528x%2529%2B*%2Blog2%2528abs%2528x%2B*%2B10%2529%2B%252B%2B1%2529%2529%2Bfrom%2B-3%2Bto%2B3&amp;sa=D&amp;ust=1465142037709000&amp;usg=AFQjCNFR9gMJKH3jDxVMP7s2iBT7j6EfoA">http://www.wolframalpha.com/input/?i=sin%28sign%28x%29+*+log2%28abs%28x+*+10%29+%2B+1%29%29+from+-3+to+3</a></span><span>&nbsp;</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5 c23" id="h.ni4dst68ox50"><span></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.j53adrh6bcfd"><span></span></h1><h1 class="c5" id="h.mpyiyshr3cn1"><span>Data collection</span></h1><p class="c3"><span class="c18"></span></p><h2 class="c5" id="h.fjj713no8leo"><span>In-training Dataset Labeling</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_z3si3pbri1de-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.00284&amp;sa=D&amp;ust=1465142037711000&amp;usg=AFQjCNHe1pmR93roTOgWVGoubZkUKqGzjw">http://arxiv.org/abs/1504.00284</a></span><span>&nbsp;more information about efficient dataset exploration with human intervention. Should also go in &quot;theory&quot; potentially.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_z3si3pbri1de-0"><li class="c6 c7"><span>Interactive dataset bootstrapping - worked as well as I hoped it would. They probably figured out magic incantations that make it work at all. </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05227&amp;sa=D&amp;ust=1465142037712000&amp;usg=AFQjCNEexScFF04LIjGtrzoL6Qs_tR8npQ">http://arxiv.org/abs/1512.05227</a></span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><h2 class="c5" id="h.i48hsh9fz6gj"><span>Human-compatibility</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_6ykz6yxb3o6x-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05832&amp;sa=D&amp;ust=1465142037714000&amp;usg=AFQjCNGRielwqM9_WMGPGvvBPFPgnicnzw">http://arxiv.org/abs/1512.05832</a></span><span>&nbsp;miri for machine learning: learning the preferences of inconsistent agents. Theory and techniques/miscellaneous</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_6ykz6yxb3o6x-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00741&amp;sa=D&amp;ust=1465142037714000&amp;usg=AFQjCNGeplATAJrufYRkRN9UBWfJDmIkLg">http://arxiv.org/abs/1601.00741</a></span><span>&nbsp;learning human preferences via examples that give directions rather than examples that give end goals. looks cool.</span></li></ul><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5" id="h.xfngvzdms80v"><span>RL</span></h1><p class="c3"><span></span></p><h2 class="c5" id="h.fgsm106i4egy"><span>Misc</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_5p6viq6b9z8b-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01297&amp;sa=D&amp;ust=1465142037716000&amp;usg=AFQjCNFwuyxvh9fiiE9IDdLFx9cY31Y5zg">http://arxiv.org/abs/1601.01297</a></span><span>&nbsp;bayesian reinforcement learner, intended to be used on the output of CV algorithms. might work as a last-layer algorithm. does it give gradients?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_68swn1licdyn-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06569&amp;sa=D&amp;ust=1465142037717000&amp;usg=AFQjCNF800P6gKM2t1GKH1LxbrtVjwwZsg">http://arxiv.org/abs/1601.06569</a></span><span>&nbsp;inverse reinforcement learning where the student agent has some degree of active behaviour,and theoretical stuff about that. Claim to prove that the teacher&#39;s utility function is strictly recoverable in the case of unbounded experiments of environment choice, but then connect that with a claim that the greedy algorithm for environment choice is actually near optimal.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.00331&amp;sa=D&amp;ust=1465142037718000&amp;usg=AFQjCNENzkeBpdSu_068Ol43lOTyubYVSw">http://arxiv.org/abs/1510.00331</a></span><span>&nbsp;thing for maximizing information gain of actions. not deep, but the theory seems interesting to reapply as a mixed objective for q learning or something. </span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.ripmjk8jrldw"><span>Q-learning</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_yh6f66826ysk-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.nervanasys.com/demystifying-deep-reinforcement-learning/&amp;sa=D&amp;ust=1465142037719000&amp;usg=AFQjCNEPan_tHx88Wry31HQxb9zH7Xrstg">http://www.nervanasys.com/demystifying-deep-reinforcement-learning/</a></span><span>&nbsp;blog-format introduction to DQL and mentions several other papers. Worth investigating for the overconfidence problem. Also, google patented DQL? what the fuck were they thinking? ignore and research anyway, hopefully if it goes to court it will be too late.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_kk8oms3v3iyb-0 start"><li class="c6 c7"><span>Q learning with high dimensional actions and as a recommendation system </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.01124&amp;sa=D&amp;ust=1465142037720000&amp;usg=AFQjCNGxS_D-Dj919eLlCDou_0VbMa1LNA">http://arxiv.org/abs/1512.01124</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_gc2knt874en1-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/paper/3964-double-q-learning&amp;sa=D&amp;ust=1465142037721000&amp;usg=AFQjCNHE-citqnFh3vh5jcjMTAh6CdsrUg">http://papers.nips.cc/paper/3964-double-q-learning</a></span><span>&nbsp;q learning value overestimate solution</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_7q22x2z5v5y-0 start"><li class="c6 c7"><span>Isn&#39;t discounting a hack? Seems like the true way to do discounting is to use the uncertainty from the next-state evaluation as the way to represent uncertainty about what to do. With a variational q learner, seems like your target distribution for the action would be some combination of the distributions of the actions on the next step. The sum of their distributions? -&gt; it&#39;s partially a thing about opportunity cost/maximizing investment time. If uncertainty is handled well another way, then it becomes solely about opportunity cost. </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_emy9ap8zeql8-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04574&amp;sa=D&amp;ust=1465142037722000&amp;usg=AFQjCNEFeiE9-IHJNB1uRedGxW_5GbF65w">http://arxiv.org/abs/1601.04574</a></span><span>&nbsp;reinforcement learner on text. spoilers, I want to make this! also in language.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_x5jl3p2g0327-0 start"><li class="c6 c7"><span>see also the generality section.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_gnt40hozc3kv-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05952&amp;sa=D&amp;ust=1465142037723000&amp;usg=AFQjCNG1hecW1b66Hrt8VIPNWIemn8RlRA">http://arxiv.org/abs/1511.05952</a></span><span>&nbsp;prioritized experience replay - not read yet, but looks cool, presumably they came up with a metric for value of an experience. I was thinking the other day that it might be cool to prioritize experiences based on the magnitude of the difference between lowest and highest value actions at the next state. also in &quot;efficiency and theory/data prioritization&quot;</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_gwuatnbmuewh-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.02971&amp;sa=D&amp;ust=1465142037725000&amp;usg=AFQjCNEPw3Mx5SJSMCNER6_pAyUXPf7trg">http://arxiv.org/abs/1509.02971</a></span><span>&nbsp;continuous control with dql - since the neurons output action values normally, you don&#39;t have continuous control. glanced through, don&#39;t remember how they did it.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_gwuatnbmuewh-0"><li class="c6 c7"><span>idea: learning stage at which, given an uncertainty value over the preferred action, choose the most uncertain one?</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04143&amp;sa=D&amp;ust=1465142037726000&amp;usg=AFQjCNF4nY8m5UW1agIXmOONUY2q7S42Sg">http://arxiv.org/abs/1511.04143</a></span><span>&nbsp;[abs][conf] &quot;Deep Reinforcement Learning in Parameterized Action Space&quot; - RoboCup soccor rl, continuous-space stuff. really curious how they did that.</span></li><li class="c6 c7"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06581&amp;sa=D&amp;ust=1465142037726000&amp;usg=AFQjCNEbYNMCHiYgbMl6d3za8aligAsSxA">http://arxiv.org/abs/1511.06581</a></span><span>&nbsp;[abs:4] alternate solution to the overconfidence problem: split the state-value function and policy function. like, duh. lol. &quot;dueling reinforcement learning&quot;. probably has some form of adverserial objective for the two portions. haven&#39;t read.</span></li><li class="c6 c7"><span>idea: rl that generates and runs other networks as a hyperparameter optimization system, then run it on itself. yudkowsky&#39;s afraid of it, so it&#39;s probably a good idea.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf&amp;sa=D&amp;ust=1465142037727000&amp;usg=AFQjCNE6SjUvMq0843PIaNJBFD-NsGp3qA">https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf</a></span><span>&nbsp;deepmind&#39;s recent go paper. </span><span class="c10">IMMENSE IMPROVEMENT IN SOTA. </span><span>they used the state-value function and policy function split. of course. may not be the same math as the split in the paper above.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04636&amp;sa=D&amp;ust=1465142037728000&amp;usg=AFQjCNE-YwwwqStwBx9XxO5ZNpjWyQTZuw">http://arxiv.org/abs/1511.04636</a></span><span>&nbsp;q learning on natural language text games. looks cool. uses embeddings. &quot;deep reinforcement relevance network&quot;. should probably also go in &quot;language&quot;, especially &quot;embeddings&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02672&amp;sa=D&amp;ust=1465142037728000&amp;usg=AFQjCNH-4hhKr8WZCaMCA1cT-pdaLoUmNg">http://arxiv.org/abs/1602.02672</a></span><span>&nbsp;recurrent q networks learning protocols for communicating with other agents. I wonder how it&#39;d work if you first trained them with a language encoder/decoder. works well on tests. looks cool.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.bowj8cgffy6"><span>Useful for RL</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_2mto5juffctz-0 start"><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037730000&amp;usg=AFQjCNEHc3ri1uZud2b5ChzTyRDNRloY-w">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037730000&amp;usg=AFQjCNEHc3ri1uZud2b5ChzTyRDNRloY-w">http://arxiv.org/abs/1603.08983</a></span><span>&nbsp;[abs:5] this is what we&#39;re here for! Alex graves hits it out of the park again, with RNNs that have variable compute time *per output*. This is the shit. Should go in &quot;recurrence&quot;, &quot;attention&quot; (it&#39;s kind of attention over compute time), &quot;generality&quot;, &quot;rl&quot;, &quot;language/language modeling&quot;, &quot;architectures&quot;. Major improvement on hutter prize dataset, apparently. Would be interesting to combine this with some sort of logarithmically decaying implicit memory, or just explicit memory. Maybe one of the recurrent residual things. </span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5" id="h.kwk2e7ewm52z"><span>AGI Safety/reliable reinforcement learners</span></h1><p class="c3"><span></span></p><ul class="c2 lst-kix_ct57w2de70tb-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05849&amp;sa=D&amp;ust=1465142037732000&amp;usg=AFQjCNEXhTBFSfz9ibIJb0yt-uLrZQHUSQ">http://arxiv.org/abs/1512.05849</a></span><span>&nbsp;another &quot;miri for machine learning&quot;: getting some formalism in predicting future ai capabilities. Miscellaneous</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_yhn6vvmwi8p2-0 start"><li class="c6 c7"><span class="c9">my idea</span><span>: neuraltalk sort of an adverserial framing, where the decoder tries to find a description of the mental state. perhaps try to summarize intended actions? no, then it just has to plan too long term. original idea was to give it inputs with labels, and a small decoder, in an attempt to force it to use the RL-under-test&#39;s existing models. warning: world-testing learning can learn crazy things like timing, ex that old GA example. might detect that it&#39;s not running real time or something? but if it never gets to know it&#39;s being siphoned off, then being suspicious of it won&#39;t be enough, it&#39;d have to have planned to lie in this very specific problem framing. which is plausible, if it reads my notes, which it would want to do, assuming it got long enough. so, if this technique will be useful, it must be used continuously in order to catch it before it gets a chance to sneak past it. but since this depends on detecting what kind of mistakes it makes, maybe not useful. what about pulling it into a box, giving it some examples of dishonesty, then searching its mental state for those to see how well they align? depends highly on whether the architecture has an isolated representation learned for actions, which a general learner probably wouldn&#39;t.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_riff65m65tnf-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://users.dsic.upv.es/proy/anynt/measuring.pdf&amp;sa=D&amp;ust=1465142037733000&amp;usg=AFQjCNH9RVNu2x6Nwwxhl-xleqt7npLnHw">http://users.dsic.upv.es/proy/anynt/measuring.pdf</a></span><span>&nbsp;paper on measuring intelligence, intended for identifying an AGI but I suspect this will rate current ML highly. worth checking.</span></li><li class="c6 c7"><span>paul christiano has a bunch of interesting things about ai safety from the perspective of starting with the current state of the art. about damn time! </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://medium.com/ai-control/scalable-ai-control-7db2436feee7%23.u2afx2uhq&amp;sa=D&amp;ust=1465142037734000&amp;usg=AFQjCNF9_ufd8GtPz4KHKagxA1anqN12EA">https://medium.com/ai-control/scalable-ai-control-7db2436feee7#.u2afx2uhq</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://medium.com/ai-control/a-possible-stance-for-ai-control-research-fe9cf717fc1b%23.mc5p50f5d&amp;sa=D&amp;ust=1465142037735000&amp;usg=AFQjCNEBXBSxAO7NRCnAgnmOfAGdncRsvg">https://medium.com/ai-control/a-possible-stance-for-ai-control-research-fe9cf717fc1b#.mc5p50f5d</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://medium.com/ai-control/research-directions-in-ai-control-ef6f666d2062%23.4eoozstsr&amp;sa=D&amp;ust=1465142037735000&amp;usg=AFQjCNHZAnm1ODhCUT6Ra81n5fOpQb0Xzw">https://medium.com/ai-control/research-directions-in-ai-control-ef6f666d2062#.4eoozstsr</a></span><span>&nbsp;</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5" id="h.sul7jf5ame85"><span>Unsupervised</span></h1><h2 class="c5" id="h.1nw5l4p5pdzh"><span>Automatic labeling</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_a9t5z6hxg4oz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06321&amp;sa=D&amp;ust=1465142037737000&amp;usg=AFQjCNEEcu0KZr_bHTePbzgMHbPh9rQvIQ">http://arxiv.org/abs/1511.06321</a></span><span>&nbsp;neural network based clustering, technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_bj6mzrhm69bi-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00022&amp;sa=D&amp;ust=1465142037738000&amp;usg=AFQjCNFSkRia6kWr-W25r-c9pRgmc9SsvQ">http://arxiv.org/abs/1601.00022</a></span><span>&nbsp;image patch and label discovery from news captions. Localization?</span></li></ul><p class="c3"><span class="c18"></span></p><h2 class="c5" id="h.drf1b7a7dlt4"><span>Misc Representation Learning</span></h2><p class="c3"><span class="c18"></span></p><ul class="c2 lst-kix_o5m7acjt2ju1-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.03519&amp;sa=D&amp;ust=1465142037739000&amp;usg=AFQjCNF7dtdzktPGEN8_sjhxrBdhdf4Y7A">http://arxiv.org/abs/1510.03519</a></span><span>&nbsp;&quot;...a model for learning a common representation for V1, V2 and V3 using only the parallel data available between V1V3 and V2V3..&quot; Technique; should also go in &quot;multimodal networks&quot; or &quot;architectures&quot;, next to the other papers on &quot;shared representation&quot;. Also relevant for &quot;knowledge sharing/multi-task&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_kk7o9gi3ij0l-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08806&amp;sa=D&amp;ust=1465142037740000&amp;usg=AFQjCNGe86-fuLyc8YQGOxV3IgSwDsiSFQ">http://arxiv.org/abs/1512.08806</a></span><span>&nbsp;&quot;Common Variable Learning and Invariant Representation Learning using Siamese Neural Networks&quot;; unsupervised technique that looks somewhat different than I&#39;ve seen before, though this might be the same thing I tried to do with music. tries to find common hidden variables between two inputs. technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_i0z4lx8deqnq-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05030&amp;sa=D&amp;ust=1465142037741000&amp;usg=AFQjCNEPRfbHWMOmRdgl4FonT3Nl5P6K-Q">http://arxiv.org/abs/1601.05030</a></span><span>&nbsp;local image similarity detection using train-similar and train-dissimilar as the unsupervised objective (I think).</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_z4z7nvld905a-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00722&amp;sa=D&amp;ust=1465142037741000&amp;usg=AFQjCNEfdyWMqcvJxqxxYabK4HM1UzlfvQ">http://arxiv.org/abs/1601.00722</a></span><span>&nbsp;restricted boltzmann machine for matrices, looks vaguely convnet-ish. seems unlikely to work particularly well, backprop is completely blowing everything else out of the water these days. worth checking though, I suppose.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_zgvzzrmreqeg-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.00830&amp;sa=D&amp;ust=1465142037742000&amp;usg=AFQjCNFPSN0NMHHreJZf447gUsCiWiimWw">http://arxiv.org/abs/1511.00830</a></span><span>&nbsp;autoencoder with special objective to be simple (?), &#39;variational fair autoencoder&#39;. &quot;</span><span class="c1">learning representations that are invariant to certain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_3odao55mezu-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01142&amp;sa=D&amp;ust=1465142037743000&amp;usg=AFQjCNFMFZlKZrc8KRPPYegLpTION1-VHw">http://arxiv.org/abs/1601.01142</a></span><span>&nbsp;non-neural - latent dirilecht allocation thingy.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06448&amp;sa=D&amp;ust=1465142037744000&amp;usg=AFQjCNFD70G75SfgTi1mf9p2aLcEwptxvg">http://arxiv.org/abs/1511.06448</a></span><span>&nbsp;representation learning on EEG data. also in &quot;applications/bio&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.00519&amp;sa=D&amp;ust=1465142037744000&amp;usg=AFQjCNEapE_N521B3O-7FIlTRScAsoQKOw">http://arxiv.org/abs/1509.00519</a></span><span>&nbsp;importance weighted autoencoders - demonstrates failure of VAE to use the network&#39;s full capacity (which kinda makes sense), somehow prioritizes datapoints, not really clear how. notes on it from someone (note that it&#39;s on some random site, not sure how it ended up here: http://www.jianshu.com/p/bb74f58ad1bb)</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.7783&amp;sa=D&amp;ust=1465142037745000&amp;usg=AFQjCNEypszbiMpd4QL3InhKhKs9nqIXSA">http://arxiv.org/abs/1411.7783</a></span><span>&nbsp;original &quot;ladder networks&quot; paper. sounds like they get all their benefit from denoising?</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.1wm460de1xr0"><span>Generative modeling</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_ehe7zglmx7i2-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.01844&amp;sa=D&amp;ust=1465142037746000&amp;usg=AFQjCNGPXo6E2XTYNeAAWKh3LN4ebnTZhQ">http://arxiv.org/abs/1511.01844</a></span><span>&nbsp;analysis of generative papers, and comment that the metrics are incomparable and that one (</span><span class="c1">Parzen window estimates) doesn&#39;t work. notes from whatever his name is: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.evernote.com/shard/s189/sh/db533c55-90ca-4a59-9a4e-4f91bc1209c5/1f0772f58b077e3c6fe77e4802c9851e&amp;sa=D&amp;ust=1465142037747000&amp;usg=AFQjCNE7vkrjJBwU-gNowitQagAwceaKJw">https://www.evernote.com/shard/s189/sh/db533c55-90ca-4a59-9a4e-4f91bc1209c5/1f0772f58b077e3c6fe77e4802c9851e</a></span><span class="c1">&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_2jmrp4ochkmc-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://soumith.ch/eyescream/&amp;sa=D&amp;ust=1465142037748000&amp;usg=AFQjCNGFNxtPO11RTz1CtQsB7bObWxWmvQ">http://soumith.ch/eyescream/</a></span><span>&nbsp;[abs:4] research log including failed experiments! Good pattern to file for research logging. &nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.05751&amp;sa=D&amp;ust=1465142037748000&amp;usg=AFQjCNEv9MN8bxLD9llTM03VXbNSf6gYEA">http://arxiv.org/abs/1506.05751</a></span><span>&nbsp;Improvements on generative adversarial networks. Technique. Proposes a fun toy application - mix and match upscaling models. Also would generalize well to use as superresolution. Should also go in &quot;images/superresolution&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_1xoigud6ruor-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09300&amp;sa=D&amp;ust=1465142037749000&amp;usg=AFQjCNF5gtLATXcfGbpSX3ITR5fnfaUV-A">http://arxiv.org/abs/1512.09300</a></span><span>&nbsp;variational autoencoder with generative adversarial network as the loss function, resulting in directly per feature loss. Technique. Looks interesting. Also in &quot;data efficiency&quot; section.</span></li><li class="c6 c7"><span>Original adversarial network paper: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06434&amp;sa=D&amp;ust=1465142037750000&amp;usg=AFQjCNGy4Ah3EtHorjzRZkzUMoESaY4R9w">http://arxiv.org/abs/1511.06434</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_8k70e8i8rdlm-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04589&amp;sa=D&amp;ust=1465142037751000&amp;usg=AFQjCNGcQbVAXYRYWFnQ-xfvf_sB4AXmLA">http://arxiv.org/abs/1601.04589</a></span><span>&nbsp;generative image model with non-neural model (markov random fields) to stabilize it a bit. looks pretty cool. also in &quot;non-neural&quot;. has a pretty thorough analysis of how their non-neural model combines. </span><span class="c9">very good results.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_79uewz4g8df3-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06421&amp;sa=D&amp;ust=1465142037752000&amp;usg=AFQjCNHLwl1UWmo6DUwjqbAM2Gtxb1Qaxg">http://arxiv.org/abs/1511.06421</a></span><span>&nbsp;manifold traversal - method for meaningfully </span><span class="c9">updating an input to put it in a new class</span><span>, non-erroneously. Technique. Should also go in &quot;images/processing&quot; or some such thing, in addition to &quot;generative&quot;. Also sort of relevant to &quot;classification&quot; and &quot;debugging&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_d2ysoruvczg5-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06038&amp;sa=D&amp;ust=1465142037753000&amp;usg=AFQjCNEQQvlMoYxuHOEwI_2QB7cZQZA-OQ">http://arxiv.org/abs/1511.06038</a></span><span>&nbsp;variational generative language model or something. need to read. also in &quot;language&quot; and &quot;efficiency and theory/bayesian&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_p7d8xi28lme6-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1511.02793v1.pdf&amp;sa=D&amp;ust=1465142037754000&amp;usg=AFQjCNEKi6qba5pONHaaHPSXmNOvNcY-jQ">http://arxiv.org/pdf/1511.02793v1.pdf</a></span><span>&nbsp;image generation from sentences. Application. also in &quot;language/language models&quot; and &quot;images/whole image deep learning&quot;.</span></li><li class="c6 c7"><span>idea: partially unsupervised generative model that takes learned hidden parameters and also a label.</span></li><li class="c6 c7"><span>combination idea: ladder network + dcgan loss function + upsampling recursive network + partially supervised ladder network + provide labels to forward pass on the same neurons that output it.</span></li><li class="c6 c7"><span>another idea: find hard-to-classify images with the high end imagenet models and then </span></li></ul><h2 class="c5" id="h.2545lhiecx9b"><span>Partially supervised training</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_s3fvwoba1t7n-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.02672&amp;sa=D&amp;ust=1465142037756000&amp;usg=AFQjCNFQ-4lYuVegy4BFUm5tcxav0O2SHw">http://arxiv.org/abs/1507.02672</a></span><span>&nbsp;combined supervised/unsupervised learning. pretty interesting. seems like this is a critical step, whether this is the approach taken or not. note: </span><span class="c9 c10">this may be a crazy improvement in SOTA of efficiency</span><span>. this plus bayesian dropout? also, </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://rinuboney.github.io/2016/01/19/ladder-network.html&amp;sa=D&amp;ust=1465142037757000&amp;usg=AFQjCNG-pCNNZUSrRXk3KtmNKDIdLaem6Q">http://rinuboney.github.io/2016/01/19/ladder-network.html</a></span><span>&nbsp;is a tutorial on the same. &quot;ladder networks&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_z8j4l8h7k6b3-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06430&amp;sa=D&amp;ust=1465142037758000&amp;usg=AFQjCNGOSwknyJMQShJuksSEP0RPC8YnTw">http://arxiv.org/abs/1511.06430</a></span><span>&nbsp;</span><span class="c9 c28">references</span><span>&nbsp;the paper above. mentions that the biggest deal is adding noise. soooo, how &#39;bout them bayesian neural networks? seems like this is actually secretly the same technique, not a clever architecture/objective scheme or anything - it&#39;s just the fact that normal neural networks are completely fail at modeling distributions. if you can do it the bayesian way, then this problem should just go away. </span></li></ul><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><p class="c3"><span></span></p><h1 class="c5" id="h.9lm14qdc068k"><span>Sparsity</span></h1><p class="c3"><span></span></p><ul class="c2 lst-kix_d8ik6bmbwuj4-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1410.1165v3.pdf&amp;sa=D&amp;ust=1465142037760000&amp;usg=AFQjCNEhX_UeaCPXXO0GzNV63-vDoZHbuA">http://arxiv.org/pdf/1410.1165v3.pdf</a></span><span>&nbsp;analysis of locally competitive effects of activation functions, visualization, theory</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ucprmx270ar2-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.310.4387%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1465142037761000&amp;usg=AFQjCNG9mdJGhzXq4ml4FvDvdaBaDzXxUg">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.310.4387&amp;rep=rep1&amp;type=pdf</a></span><span>&nbsp; winner-take-all local competition, comparisons to neuroscience, technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_h99428b6laza-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download?doi%3D10.1.1.25.7%26rep%3Drep1%26type%3Dpdf&amp;sa=D&amp;ust=1465142037762000&amp;usg=AFQjCNEvskBARIwAUx-ifr7tDPS16zVqfw">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.25.7&amp;rep=rep1&amp;type=pdf</a></span><span>&nbsp; winner take all as an alternative for activation functions in terms of allowing computation to occur in an otherwise linear network. somewhat old paper, goes into detail about circuits (in the sense of a generalization of neural, not necessarily of electronics and wires) that use this concept. theory</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_5kaugn46jpw6-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00119&amp;sa=D&amp;ust=1465142037763000&amp;usg=AFQjCNFZNyZtzv7cmf2MzEkwZCNnrrWhhw">http://arxiv.org/abs/1601.00119</a></span><span>&nbsp;discriminitive sparsity</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_m46mj27r3fpz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04805&amp;sa=D&amp;ust=1465142037764000&amp;usg=AFQjCNF3rEY_EZQExwCm3x7YUXOklDMbvw">http://arxiv.org/abs/1601.04805</a></span><span>&nbsp;sparse coding for micro expression detection. Also in datasets.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_hh7pij1hmwlh-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00720&amp;sa=D&amp;ust=1465142037764000&amp;usg=AFQjCNE0rUUIcWQ5FtgLOtt84rtlEbPmcw">http://arxiv.org/abs/1601.00720</a></span><span>&nbsp;jeff hawkins/numenta stuff. theory for modeling neuron sparsity - also in q-bio section. probably worth checking out, he doesn&#39;t have a good track record but this looks good.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_nxvbdas3q36w-0 start"><li class="c6 c7"><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08669&amp;sa=D&amp;ust=1465142037765000&amp;usg=AFQjCNFKLJ_93yrhEN3Kje8WKPaFH8OjGg">http://arxiv.org/abs/1512.08669</a></span><span>&nbsp;shallow scene text recognition using sparse coding. also in &quot;images/ocr&quot;. looks non-deep, probably meh.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_z0c6lix981m-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01432&amp;sa=D&amp;ust=1465142037766000&amp;usg=AFQjCNFzORlerMkF1KrLnxbGG0IIn2xYxg">http://arxiv.org/abs/1601.01432</a></span><span>&nbsp;some sparse thing. didn&#39;t even read the title other than to notice &quot;sparse&quot;. classify me.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_2ihanzwz4fgf-0 start"><li class="c6 c7"><span>NeoRL.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06359&amp;sa=D&amp;ust=1465142037767000&amp;usg=AFQjCNF_LHoZQaSHsYvZ1F1B0xhhq__mKg">http://arxiv.org/abs/1511.06359</a></span><span>&nbsp;&quot;FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning and Applications&quot;</span></li></ul><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><h1 class="c5 c23 c30" id="h.6ojct8705m3e"><span></span></h1><h1 class="c5" id="h.8cva673f34sf"><span>Architecture</span></h1><p class="c3"><span></span></p><h2 class="c5" id="h.q82yr2py6ipu"><span>Skip-ahead connections</span></h2><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_axhbb6oyblrk-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/&amp;sa=D&amp;ust=1465142037770000&amp;usg=AFQjCNGDu8ZtHqQFhXalSf_WG5bytCU31Q">http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/</a></span><span>&nbsp;list of architectures for image recognition with visualizations. has links to newer versions of the inception model.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05946&amp;sa=D&amp;ust=1465142037771000&amp;usg=AFQjCNHusJr7gCWQqoAne5fPgPiv4fls2g">http://arxiv.org/abs/1511.05946</a></span><span>&nbsp;&quot;ACDC: A Structured Efficient Linear Layer&quot;, maybe also belongs in model compression; should go in &quot;weight representation&quot; for sure</span></li><li class="c6 c7"><span>Relevant to designing architectures: Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09260&amp;sa=D&amp;ust=1465142037771000&amp;usg=AFQjCNGapv0pap-wVAGwondqGaXy9mBi3A">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09260&amp;sa=D&amp;ust=1465142037772000&amp;usg=AFQjCNGc8fqqEJ8yAjree8BltTMtUd3Ddw">http://arxiv.org/abs/1603.09260</a></span><span>&nbsp;[abs:5] estimate of the number of bits required to represent a neural network; finds it to be *dramatically* lower than the number of parameters. Should go in &quot;theory&quot;, &quot;regularization&quot;, &quot;architectures&quot;, maybe even &quot;basics&quot; because it&#39;s such a big deal.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.93l1m9diyr2p"><span>Very Deep/Vanishing Gradients Problem</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_4ykl6tjv6vdc-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.03385&amp;sa=D&amp;ust=1465142037773000&amp;usg=AFQjCNGNGDfjSDPoUV6dqZen7GGh677v4g">http://arxiv.org/abs/1512.03385</a></span><span>&nbsp;microsoft&#39;s crazy new paper: residual networks (&quot;resnets&quot;). skip-ahead connections that are fast and easy, seem to work perfectly compared to previous attempts at skip-ahead connections. though, how do highway networks work? wouldn&#39;t highway networks work better for unspecified text recognition? massive, insane improvement in sota. how in the world is this so freakishly much better than recurrent networks at depth? is it just because recurrent networks are having to constantly ignore noise, but this just gets to sit and think?</span></li></ul><ul class="c2 lst-kix_4ykl6tjv6vdc-1 start"><li class="c6 c22"><span>references how to deal with vanishing gradients in first few paragraphs</span></li><li class="c6 c22"><span>references theoretical reasoning behind depth being better</span></li><li class="c6 c22"><span>also references other instances of skip-ahead connections, at the first mention of shortcut connections</span></li><li class="c6 c22"><span>in footnote two, mentions that depth being better is actually still an open question! see page 3</span></li><li class="c6 c22"><span>has a bunch of references for how they did initialization, data augmentation, data sampling</span></li><li class="c6 c22"><span>pretrained models: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/KaimingHe/deep-residual-networks&amp;sa=D&amp;ust=1465142037775000&amp;usg=AFQjCNHnL9FqpV3JJ80FgzWTGbQpWSLVWw">https://github.com/KaimingHe/deep-residual-networks</a></span><span>&nbsp;</span></li><li class="c6 c22"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf&amp;sa=D&amp;ust=1465142037776000&amp;usg=AFQjCNETrMbNcUpWHqWZC1kYE5INrzZufQ">http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf</a></span><span>&nbsp;slides about it</span></li></ul><ul class="c2 lst-kix_u82x83buy4jv-0 start"><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09382&amp;sa=D&amp;ust=1465142037777000&amp;usg=AFQjCNGMDfvp9griXGVWU8MGmb5p0ZgaqA">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09382&amp;sa=D&amp;ust=1465142037777000&amp;usg=AFQjCNGMDfvp9griXGVWU8MGmb5p0ZgaqA">http://arxiv.org/abs/1603.09382</a></span><span>&nbsp;[abs:5] dropout over *layers* - sometimes skip layers and treat them as the identity function in resnets. This allows training 1200-layer resnets and still getting good results, and they train much faster. As with dropout, it&#39;s shut off at test time. How does it perform with it on at test time, but averaged? Also, this plus filter dropout seems like it implies some sort of tree-structured dropout for convnets. Pixels, filters, residual blocks? Also, I&#39;m still very curious about using multiple branches of a network, with global competition. Should go in &quot;regularization&quot;, &quot;architectures/very deep&quot;, &quot;vanishing gradients&quot; (which is closely related to &quot;recurrence&quot;, so it should go there as well.)</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.t4svwxozvv4r"><span>Variable Compute</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_edj3r8hgf2z3-0 start"><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037779000&amp;usg=AFQjCNEOVQfi6uLRzRyBEJkNLtXOhjrLFQ">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037780000&amp;usg=AFQjCNFGDahzxn0-2ADZgmhVD2yqb6dKog">http://arxiv.org/abs/1603.08983</a></span><span>&nbsp;[abs:5] this is what we&#39;re here for! Alex graves hits it out of the park again, with RNNs that have variable compute time *per output*. This is the shit. Should go in &quot;recurrence&quot;, &quot;attention&quot; (it&#39;s kind of attention over compute time), &quot;generality&quot;, &quot;rl&quot;, &quot;language/language modeling&quot;, &quot;architectures&quot;. Major improvement on hutter prize dataset, apparently. Would be interesting to combine this with some sort of logarithmically decaying implicit memory, or just explicit memory. Maybe one of the recurrent residual things. </span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5" id="h.m5au0hsd8van"><span>Recurrence</span></h1><p class="c3"><span></span></p><ul class="c2 lst-kix_17is42bhveph-0 start"><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037782000&amp;usg=AFQjCNGbemh_WhDBzDav6M5ONVEcHMUCkA">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037782000&amp;usg=AFQjCNGbemh_WhDBzDav6M5ONVEcHMUCkA">http://arxiv.org/abs/1603.08983</a></span><span>&nbsp;[abs:5] this is what we&#39;re here for! Alex graves hits it out of the park again, with RNNs that have variable compute time *per output*. This is the shit. Should go in &quot;recurrence&quot;, &quot;attention&quot; (it&#39;s kind of attention over compute time), &quot;generality&quot;, &quot;rl&quot;, &quot;language/language modeling&quot;, &quot;architectures&quot;. Major improvement on hutter prize dataset, apparently. Would be interesting to combine this with some sort of logarithmically decaying implicit memory, or just explicit memory. Maybe one of the recurrent residual things. </span></li><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09382&amp;sa=D&amp;ust=1465142037783000&amp;usg=AFQjCNGNP4EVCk3t5NnUzxUdRHRcs7Z33g">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09382&amp;sa=D&amp;ust=1465142037783000&amp;usg=AFQjCNGNP4EVCk3t5NnUzxUdRHRcs7Z33g">http://arxiv.org/abs/1603.09382</a></span><span>&nbsp;[abs:5] dropout over *layers* - sometimes skip layers and treat them as the identity function in resnets. This allows training 1200-layer resnets and still getting good results, and they train much faster. As with dropout, it&#39;s shut off at test time. How does it perform with it on at test time, but averaged? Also, this plus filter dropout seems like it implies some sort of tree-structured dropout for convnets. Pixels, filters, residual blocks? Also, I&#39;m still very curious about using multiple branches of a network, with global competition. Should go in &quot;regularization&quot;, &quot;architectures/very deep&quot;, &quot;vanishing gradients&quot; (which is closely related to &quot;recurrence&quot;, so it should go there as well.)</span></li></ul><p class="c3"><span></span></p><h1 class="c5 c23" id="h.nularwww4lzb"><span></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.r7ty0wtql3a"><span></span></h1><h1 class="c5" id="h.2vaeb9ijh0n"><span>Attention</span></h1><ul class="c2 lst-kix_3fnpvvr0i8os-0 start"><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037785000&amp;usg=AFQjCNH303XP18bhWAbF1N_kFn0TvV5fSA">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037786000&amp;usg=AFQjCNEIinHC8boXkeBEB_HLgZvK6BeoqQ">http://arxiv.org/abs/1603.08983</a></span><span>&nbsp;[abs:5] this is what we&#39;re here for! Alex graves hits it out of the park again, with RNNs that have variable compute time *per output*. This is the shit. Should go in &quot;recurrence&quot;, &quot;attention&quot; (it&#39;s kind of attention over compute time), &quot;generality&quot;, &quot;rl&quot;, &quot;language/language modeling&quot;, &quot;architectures&quot;. Major improvement on hutter prize dataset, apparently. Would be interesting to combine this with some sort of logarithmically decaying implicit memory, or just explicit memory. Maybe one of the recurrent residual things. </span></li></ul><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.3djm05mxwwe2"><span></span></h1><h1 class="c5" id="h.zbb6qf7nzk5z"><span>Memory</span></h1><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_470xpz4q6hoi-0 start"><li class="c6 c7"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1506.07285v3.pdf&amp;sa=D&amp;ust=1465142037788000&amp;usg=AFQjCNEHLs7t81I4v183jKl39mkR7v_psA">http://arxiv.org/pdf/1506.07285v3.pdf</a></span><span>&nbsp;[abs:2] dynamic memory networks, has good results on question answering, not just learning of algorithms, so actually holds some promise. Should also go in &quot;language/text understanding&quot; (part of speech, reference resolution, sentiment analysis), and &quot;symbolic&quot; (babi dataset).</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_x3vcornhe3kv-0 start"><li class="c6 c7"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1503.08895v5.pdf&amp;sa=D&amp;ust=1465142037789000&amp;usg=AFQjCNFo9xgbvf-NYrDo5m3KXEW49_mR7g">http://arxiv.org/pdf/1503.08895v5.pdf</a></span><span>&nbsp;[abs:1] end to end memory networks. Allows multiple computation steps per invocation, just like alex graves&#39; much newer technique. </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_iqbmivscsmun-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02516&amp;sa=D&amp;ust=1465142037789000&amp;usg=AFQjCNGUQbXl4hYOL4lgcGEiK7Y6pVzOfg">http://arxiv.org/abs/1506.02516</a></span><span>&nbsp;another algorithms and memory paper, this one with deques and stacks. from deepmind. Technique - implementation log, quite detailed; should also go in &quot;examples&quot; or &quot;libraries&quot;, and &quot;symbolic&quot;: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/&amp;sa=D&amp;ust=1465142037790000&amp;usg=AFQjCNFn1mVoK3oeOAVPGG7iTL2fE51hVQ">https://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_p0fqpwscumob-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1511.06279v3.pdf&amp;sa=D&amp;ust=1465142037791000&amp;usg=AFQjCNHifTTgA6UpIY5kJBtay4XBwRD2fQ">http://arxiv.org/pdf/1511.06279v3.pdf</a></span><span>&nbsp;funky program structure&hellip; thing from deepmind. Didn&#39;t read closely, very confusing what exactly they&#39;re saying in the abstract, sounds like it&#39;s an rnn with memory and function calls? Excerpt from abstract: &quot;y learning<br>to compose lower-level programs to express higher-level programs, NPI reduces<br>sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs.&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_24cmoxb3qmwl-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08301&amp;sa=D&amp;ust=1465142037791000&amp;usg=AFQjCNEpdH_4n222n0xeXTxFL2y3UjbPag">http://arxiv.org/abs/1512.08301</a></span><span>&nbsp;&quot;not recurrent&quot; form of memory. </span><span class="c9 c12">Significant improvement in sota. </span><span>Much faster, too, apparently. &nbsp;Technique.</span></li></ul><ul class="c2 lst-kix_24cmoxb3qmwl-1 start"><li class="c5 c22"><h5 id="h.pw08k6p04q3" style="display:inline"><span>application: misc</span></h5></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_x2mjmu7myisq-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1601.01272v1.pdf&amp;sa=D&amp;ust=1465142037793000&amp;usg=AFQjCNExlksBN4dnpFe2Q1YhirdLuj5hbA">http://arxiv.org/pdf/1601.01272v1.pdf</a></span><span>&nbsp;&quot;Recurrent memory network&quot;. new memory architecture that works better and is more interpretable. claims to be a big </span><span class="c9 c12">improvement in sota</span><span>.</span></li></ul><ul class="c2 lst-kix_x2mjmu7myisq-1 start"><li class="c5 c22"><h5 id="h.yypf4rsvs20h" style="display:inline"><span>application: language modeling</span></h5></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_398os0jr1iw3-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%25C3%25BCrgen_schmidhuber_ama/cp43tqi&amp;sa=D&amp;ust=1465142037795000&amp;usg=AFQjCNHx8WinaMOygSkMFi3IOdXNhO55tw">https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/cp43tqi</a></span><span>&nbsp;clockwork rnns are only better on synthetic data? perhaps conv over time might still be better? or not&hellip;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_bjneibxfrx3r-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.06442&amp;sa=D&amp;ust=1465142037796000&amp;usg=AFQjCNGfPCnCn0dZuOsx0FCpdVOEA35QnA">http://arxiv.org/abs/1506.06442</a></span><span>&nbsp;[abs:2][r/mlpapers] memory approach for sequence-to-sequence learning. inspired by NTM. looks interesting. haven&#39;t read. appears to learn to query tables in sql or something crazy like that. or maybe that&#39;s their other paper. whatever.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05236&amp;sa=D&amp;ust=1465142037796000&amp;usg=AFQjCNG-_Sd5KqVZuwb-AYTxpCrlKTOZ3w">http://arxiv.org/abs/1511.05236</a></span><span>&nbsp;Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets, should probably also go in &quot;model compression&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.08983&amp;sa=D&amp;ust=1465142037797000&amp;usg=AFQjCNH7UxqLXgt0iahwvNX3W4ItCDcq4w">http://arxiv.org/abs/1510.08983</a></span><span>&nbsp;deeper lstms - but it&#39;s unclear if this is deeper in the depth sense or deeper in the time sense. works better. uses highway networks as the approach toward increasing depth, so basically just more lstm. should also go in &quot;speech recognition&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03771&amp;sa=D&amp;ust=1465142037797000&amp;usg=AFQjCNEFLwShv1JFyzw1SHjjGPJv4HDXHg">http://arxiv.org/abs/1511.03771</a></span><span>&nbsp;trying relu on lstm. apparently works better. should also go in &quot;activation&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/kjw0612/awesome-rnn&amp;sa=D&amp;ust=1465142037798000&amp;usg=AFQjCNGAFEl7tT4Mhr-gvFSImG4hacH-fQ">https://github.com/kjw0612/awesome-rnn</a></span><span>&nbsp;place to get papers for rnn stuff. </span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/441125/highway_networks_are_to_deep_residual_networks/&amp;sa=D&amp;ust=1465142037798000&amp;usg=AFQjCNFuSuYgZaUQ3huxYEN4b-rKXAzGfQ">https://www.reddit.com/r/MachineLearning/comments/441125/highway_networks_are_to_deep_residual_networks/</a></span><span>&nbsp;resnet vs lstm - turns out lstm is like a skip-1 resnet, plus gating; subtract the gating and you have a skip-1 resnet. you still have the same old problem though: how do you skip layers sensibly? worth testing just skipping layers, nothing fancy. I&#39;d expect it to not work as well though.</span></li></ul><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5 c31" id="h.gnka2fu7o02p"><span>META</span></h1><p class="c3"><span></span></p><h2 class="c5" id="h.7vj8rslkx51s"><span>Optimizers / Learning Algorithms</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_vhrrjzyjvp5x-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://people.csail.mit.edu/hmobahi/pubs/rnn_diffusion.pdf&amp;sa=D&amp;ust=1465142037800000&amp;usg=AFQjCNGjnBVj3lsTqtmKCMlES3X6osO4cQ">http://people.csail.mit.edu/hmobahi/pubs/rnn_diffusion.pdf</a></span><span>&nbsp;alternate optimizer which apparently converges faster and implies stuff. or something. looks like it&#39;s based on gaussian something or other, so maybe bayesian stuff yet again. I wonder why the (allegedly) correct method of updating on evidence keeps popping up in how to train neural networks. </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_mh0uhwysa91-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%25C3%25BCrgen_schmidhuber_ama/cp4ecce&amp;sa=D&amp;ust=1465142037801000&amp;usg=AFQjCNGBzR2Yf-JeRSHLljRUS3ssY1kHZw">https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/cp4ecce</a></span><span>&nbsp;train lstm as a learning algorithm itself? schmidthuber doing crazy stuff with things like target prop, but not quite.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_tglcy61nnbu3-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://sebastianruder.com/optimizing-gradient-descent/&amp;sa=D&amp;ust=1465142037802000&amp;usg=AFQjCNHqXKU47MYOc4I88YKXxIvuuWO4Lg">http://sebastianruder.com/optimizing-gradient-descent/</a></span><span>&nbsp;list of optimizers in a blog post about how to make them fast.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_481c98v2pt83-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.05254&amp;sa=D&amp;ust=1465142037803000&amp;usg=AFQjCNEZaXVN_f1MSeGMZgnwQBHlnfz-cw">http://arxiv.org/abs/1506.05254</a></span><span>&nbsp;&quot;</span><span class="c8 c1">Gradient Estimation Using Stochastic Computation Graphs&quot; - not quite clear, but appears to be a way to automatically approximate the gradients of non-differentiable but mostly-smooth thingies. </span></li></ul><p class="c3"><span class="c8 c1"></span></p><ul class="c2 lst-kix_4fwhm2tiefhz-0 start"><li class="c6 c7"><span class="c4 c1 c43"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.01169&amp;sa=D&amp;ust=1465142037804000&amp;usg=AFQjCNFzxFzWjyv_Gkr8tOWLWjZMWk6o9Q">http://arxiv.org/abs/1511.01169</a></span><span class="c1 c8">&nbsp;learning algorithm specifically for training RNNs. appears to be able to model multimodal something or other? is &quot;competitive&quot;, which means &quot;not an improvement&quot;, but diversity is good.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_tltq6x9aaf31-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06481&amp;sa=D&amp;ust=1465142037804000&amp;usg=AFQjCNFuELS5X07W9K2SpGisCBBzRf-YHQ">http://arxiv.org/abs/1511.06481</a></span><span>&nbsp;- &quot;variance reduction in SGD via distributed importance sampling&quot; - also in &quot;efficiency and theory/data prioritization&quot;, &quot;speed/parallelization&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05176&amp;sa=D&amp;ust=1465142037805000&amp;usg=AFQjCNEr_wu8W9x04a_w8oojEcQjEE8fvQ">http://arxiv.org/abs/1511.05176</a></span><span>&nbsp;&quot;unbiased backprop&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/381hnf/experiences_with_target_propagation/&amp;sa=D&amp;ust=1465142037806000&amp;usg=AFQjCNEsAgaXaY0bR_2jURr_B9qqsC2gsw">https://www.reddit.com/r/MachineLearning/comments/381hnf/experiences_with_target_propagation/</a></span><span>&nbsp;- target prop analysis. actually looks more promising than I remembered</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.gl9l64k1cyw4"><span>Hyperparameters techniques</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_vebf56ofbjcp-0 start"><li class="c6 c7"><span>Speed up continuous hyperparameter search: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06727&amp;sa=D&amp;ust=1465142037807000&amp;usg=AFQjCNHCnuSXsHzTtZUgm93hyYpDG3l4yQ">http://arxiv.org/abs/1511.06727</a></span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_spzfuoeekohi-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07502&amp;sa=D&amp;ust=1465142037808000&amp;usg=AFQjCNEGBqorkj6GfhbpdX-ZEOpEtTJB-w">http://arxiv.org/abs/1512.07502</a></span><span>&nbsp;architecture and hyperparameters for video recognition, explored. Technique/details/fine tuning.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_qsdpvqxg1u78-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09251&amp;sa=D&amp;ust=1465142037809000&amp;usg=AFQjCNHI_CncX1YUzM3Xj9AEyqg9x3IjlA">http://arxiv.org/abs/1512.09251</a></span><span>&nbsp;not deep, not neural: function-evaluation-efficient black box optimization with a different technique than Bayesian optimization, called CORBA, which has awkward hyperparameters; proposes a technique that is self adjusting. Technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ydb53brft35n-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00917&amp;sa=D&amp;ust=1465142037810000&amp;usg=AFQjCNERH7isG9e81fU_2PangGypyvefQg">http://arxiv.org/abs/1601.00917</a></span><span>&nbsp;differentiating hyperparameters with an approximate reverse pass - </span><span class="c9">probably a massive </span><span class="c9 c10">improvement in SOTA </span><span class="c9">of hyperparameter optimization</span><span>, claims to be able to train hundreds of thousands of hyperparameters. how does it perform on small hyperparameter vectors compared to the best hyperparameter opt, though?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_5ia1ign2sshz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.05700&amp;sa=D&amp;ust=1465142037811000&amp;usg=AFQjCNEiWEe8buGzGeteOubwSVInGGrCNg">http://arxiv.org/abs/1502.05700</a></span><span>&nbsp;[abs:2] bayesian optimization with neural networks as the model. cool!</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1301.1942&amp;sa=D&amp;ust=1465142037812000&amp;usg=AFQjCNGB6hwATS96VE4y4W_p0pavxlvJyA">http://arxiv.org/abs/1301.1942</a></span><span>&nbsp;billion-dimensional bayesian optimization. does this actually work? this is three years old, why has nobody heard of it?</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/46iium/faster_way_to_optimize_deep_learning_models_with/d05z88g&amp;sa=D&amp;ust=1465142037813000&amp;usg=AFQjCNGLCCUWykcA8sKFL05x93E-yafqCQ">https://www.reddit.com/r/MachineLearning/comments/46iium/faster_way_to_optimize_deep_learning_models_with/d05z88g</a></span><span>&nbsp;&quot;hyperparameters often robust to changes in dataset size&quot; - so optimize hyperparameters with a much smaller dataset</span></li></ul><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><h1 class="c5 c30" id="h.wmea6zouuovy"><span>Knowledge sharing</span></h1><p class="c3"><span></span></p><h2 class="c5" id="h.jsi1njeq9946"><span>Transfer learning</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_crb89j7xw8q2-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05986&amp;sa=D&amp;ust=1465142037815000&amp;usg=AFQjCNFXC8YG1H69AEtCi4TiEVeuUUixBg">http://arxiv.org/abs/1512.05986</a></span><span>&nbsp;comparison of two approaches for doing small dataset classification in medical imaging, transfer vs regularization. Application (duplicated to &quot;bio&quot;)</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_5fj31eh1jc7-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.09033&amp;sa=D&amp;ust=1465142037816000&amp;usg=AFQjCNGPEhNYVNk61mPLlvDA5YK5_mh27Q">http://arxiv.org/abs/1511.09033</a></span><span>&nbsp;theory, analysis, and techniques around transfer learning</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_2nh9kwfbzbik-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05844&amp;sa=D&amp;ust=1465142037817000&amp;usg=AFQjCNEtHFrroh-6OdQsqxhn9-AbOCET6A">http://arxiv.org/abs/1512.05844</a></span><span>&nbsp;transfer learning on &quot; stochastic nets &quot;, technique and references</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_4k4id0a37km3-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06498&amp;sa=D&amp;ust=1465142037818000&amp;usg=AFQjCNFT3oUZzxc-r3Lk2jHdKfIjt3eljg">http://arxiv.org/abs/1512.06498</a></span><span>&nbsp;object recognition for activity recognition respecialization application. should also go in &quot;images&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_x5zq7qqqztpc-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1510.07945v2.pdf&amp;sa=D&amp;ust=1465142037818000&amp;usg=AFQjCNFKclR_i_mr_MX97ZP5NHFhLP91ug">http://arxiv.org/pdf/1510.07945v2.pdf</a></span><span>&nbsp;object tracking, uses task-specialized final layers from a shared representation. looks cool. apparently object tracking is where you get a specific object and must re-locate it in following images. also in &quot;images&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05641&amp;sa=D&amp;ust=1465142037819000&amp;usg=AFQjCNFTCCqsYT_fmgn9FlBGMxZ5XISv5A">http://arxiv.org/abs/1511.05641</a></span><span>&nbsp;[abs][conf] instant bootstrapping of transfer learning by mapping one network directly to another! demonstrated on imagenet via depth-appending inception modules. sounds really cool.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h2 class="c5" id="h.c4ecfbch06oh"><span>Multi-task objectives and sharing</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_s6xk062mdkv5-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.02879&amp;sa=D&amp;ust=1465142037821000&amp;usg=AFQjCNFPDG-4bXTCXaaRv_fDnU_VqxyMsQ">http://arxiv.org/abs/1510.02879</a></span><span>&nbsp;knowledge sharing technique, references reinforce algorithm -&gt; started reading, kind of need to read the reinforce paper</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_w1d9l4m1yg5x-0 start"><li class="c6 c7"><span>Blockout, a model selection technique; promise towards generality, would be good in a reinforcement learner </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05246&amp;sa=D&amp;ust=1465142037822000&amp;usg=AFQjCNG49d3wEfmZMtnkweHwR8XAOMUhuA">http://arxiv.org/abs/1512.05246</a></span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_kz1sfsbi2eng-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06342&amp;sa=D&amp;ust=1465142037823000&amp;usg=AFQjCNHlgEGUXDxrSGVW5yULbMm53-XJog">http://arxiv.org/abs/1511.06342</a></span><span>&nbsp;actor-critic reinforcement learning, trains one model to generalize between multiple games via per game experts. Promise toward generality. Technique. also in &quot;RL&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_2rp5aiyjkdpd-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00400&amp;sa=D&amp;ust=1465142037824000&amp;usg=AFQjCNHQovsNiPeO24U-AtwY4tR0zKojig">http://arxiv.org/abs/1601.00400</a></span><span>&nbsp;self transfer learning for multi task attribute prediction, like classification</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_vrgigcglf3we-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://geometry.allenai.org/&amp;sa=D&amp;ust=1465142037824000&amp;usg=AFQjCNHxEK33ndLM1Odm-w2BSGLhWYeRzw">http://geometry.allenai.org/</a></span><span>&nbsp;high school geometry questions application, end to end. Some promise toward generality. Application, maybe technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_z1vxl485jzzo-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06295&amp;sa=D&amp;ust=1465142037825000&amp;usg=AFQjCNFAovJZk_Ix676bZ64PKWbd1mi_UA">http://arxiv.org/abs/1511.06295</a></span><span>&nbsp;&quot;policy distillation&quot; - training a much smaller network generalizes better than a big one. possible hint about a way to do regularization - might be able to make a much smaller network than the minimum size of initial training. looks like it has a lot in common with the actor-critic stuff. Should also go in &quot;regularization&quot;, &quot;model compression&quot;</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5" id="h.lt3liuj8dukf"><span>Towards generality</span></h1><ul class="c2 lst-kix_iiujyt46s4u7-0 start"><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037827000&amp;usg=AFQjCNHtAu9OdooCoduU4SjGvvhmqqen4g">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037827000&amp;usg=AFQjCNHtAu9OdooCoduU4SjGvvhmqqen4g">http://arxiv.org/abs/1603.08983</a></span><span>&nbsp;[abs:5] this is what we&#39;re here for! Alex graves hits it out of the park again, with RNNs that have variable compute time *per output*. This is the shit. Should go in &quot;recurrence&quot;, &quot;attention&quot; (it&#39;s kind of attention over compute time), &quot;generality&quot;, &quot;rl&quot;, &quot;language/language modeling&quot;, &quot;architectures&quot;. Major improvement on hutter prize dataset, apparently. Would be interesting to combine this with some sort of logarithmically decaying implicit memory, or just explicit memory. Maybe one of the recurrent residual things. </span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.ol1fc5770spt"><span></span></h1><h1 class="c5" id="h.ssasoz3gjmeq"><span>Efficiency and theory</span></h1><p class="c3"><span class="c18"></span></p><h2 class="c5" id="h.dmptjt8ddmfz"><span>Data Prioritization</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_dq6vem8xoqp9-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06481&amp;sa=D&amp;ust=1465142037830000&amp;usg=AFQjCNFRka4j9NBdiGqCThpK6JU1SxthFw">http://arxiv.org/abs/1511.06481</a></span><span>&nbsp;- &quot;variance reduction in SGD via distributed importance sampling&quot; - also in &quot;parallelization&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_scilllbfudea-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05952&amp;sa=D&amp;ust=1465142037831000&amp;usg=AFQjCNGsfQkqoX4R89T3Hnwym5rXsdxYXg">http://arxiv.org/abs/1511.05952</a></span><span>&nbsp;prioritized experience replay - not read yet, but looks cool, presumably they came up with a metric for value of an experience. I was thinking the other day that it might be cool to prioritize experiences based on the magnitude of the difference between lowest and highest value actions at the next state. also in &quot;rl/q learning&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06343&amp;sa=D&amp;ust=1465142037832000&amp;usg=AFQjCNEJliOLHb008UvJ0nXcIkZlYC74ng">http://arxiv.org/abs/1511.06343</a></span><span>&nbsp;&quot;Online Batch Selection for Faster Training of Neural Networks&quot; - </span><span class="c9 c10">looks very cool, and pretty important. 5x speedup in training.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/445aw7/tackling_imbalanced_dataset_deep_learning/&amp;sa=D&amp;ust=1465142037832000&amp;usg=AFQjCNH7oeXphTSp7FkrUIkpfyiUtJlcaw">https://www.reddit.com/r/MachineLearning/comments/445aw7/tackling_imbalanced_dataset_deep_learning/</a></span><span>&nbsp;not a lot of great advice, but a big list of things that didn&#39;t work</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.vky5rg613sb8"><span>Regularization</span></h2><p class="c6"><span>(...regularization that isn&#39;t very clearly connected to the bayesian thing it approximates)</span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_pzunwido5p11-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06757&amp;sa=D&amp;ust=1465142037834000&amp;usg=AFQjCNEwrBE-HOyhiOBpZAPsO6Sf-AXipg">http://arxiv.org/abs/1512.06757</a></span><span>&nbsp;graph connect regularization technique</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06068&amp;sa=D&amp;ust=1465142037834000&amp;usg=AFQjCNEVAQhVfh7_UkVQrYOPTE6SIedShw">http://arxiv.org/abs/1511.06068</a></span><span>&nbsp;[abs] Reducing Overfitting in Deep Networks by Decorrelating Representations - </span><span class="c9 c10">looks very very interesting. </span><span>a new regularization approach that creates distributed representation by adding what is effectively a penalty for exactly that. duh.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06328&amp;sa=D&amp;ust=1465142037835000&amp;usg=AFQjCNGOekYtM_vvHbpHrYfS9rEUNoRi9A">http://arxiv.org/abs/1511.06328</a></span><span>&nbsp;Manifold Regularized Discriminative Neural Networks</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06747&amp;sa=D&amp;ust=1465142037836000&amp;usg=AFQjCNH3lIiFBq0vnNFgttzwh579L9jsUA">http://arxiv.org/abs/1511.06747</a></span><span>&nbsp;[abs][conf] &quot;Data-Dependent Path Normalization in Neural Networks&quot; - &quot;a unified framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions&quot; - not sure if that&#39;s actually useful? batch norm and path-sgd would be good papers to have, though. Should also go in &quot;training algorithms&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://cs.nyu.edu/~wanli/dropc/dropc.pdf&amp;sa=D&amp;ust=1465142037836000&amp;usg=AFQjCNEm77c_HhWzzhvbueLPleIA99eSYA">http://cs.nyu.edu/~wanli/dropc/dropc.pdf</a></span><span>&nbsp;[abs]</span><span>&nbsp;dropconnect - similar to dropout, but for weights. from 2013 - why didn&#39;t it catch on? </span></li><li class="c6 c7"><span>Relevant to regularization: Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09260&amp;sa=D&amp;ust=1465142037837000&amp;usg=AFQjCNHgWrXEdmroPBvOLc5O974LmdrQkA">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09260&amp;sa=D&amp;ust=1465142037838000&amp;usg=AFQjCNE_TGEqhs084LaVn0QSYovh3tw8wA">http://arxiv.org/abs/1603.09260</a></span><span>&nbsp;[abs:5] estimate of the number of bits required to represent a neural network; finds it to be *dramatically* lower than the number of parameters. Should go in &quot;theory&quot;, &quot;regularization&quot;, &quot;architectures&quot;, maybe even &quot;basics&quot; because it&#39;s such a big deal.</span></li><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09382&amp;sa=D&amp;ust=1465142037839000&amp;usg=AFQjCNFw04byQ63rVS2q8VRGHeq8lDALiA">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09382&amp;sa=D&amp;ust=1465142037839000&amp;usg=AFQjCNFw04byQ63rVS2q8VRGHeq8lDALiA">http://arxiv.org/abs/1603.09382</a></span><span>&nbsp;[abs:5] dropout over *layers* - sometimes skip layers and treat them as the identity function in resnets. This allows training 1200-layer resnets and still getting good results, and they train much faster. As with dropout, it&#39;s shut off at test time. How does it perform with it on at test time, but averaged? Also, this plus filter dropout seems like it implies some sort of tree-structured dropout for convnets. Pixels, filters, residual blocks? Also, I&#39;m still very curious about using multiple branches of a network, with global competition. Should go in &quot;regularization&quot;, &quot;architectures/very deep&quot;, &quot;vanishing gradients&quot; (which is closely related to &quot;recurrence&quot;, so it should go there as well.)</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.ff4lngwf8qu0"><span>Bayesian Neural Nets</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_qhrkxnqxxds4-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02557&amp;sa=D&amp;ust=1465142037841000&amp;usg=AFQjCNEqJjhFuBFNXoDK8EHRKnQGdxR0xg">http://arxiv.org/abs/1506.02557</a></span><span>&nbsp;variational dropout technique and theory</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_8wesot3ko53b-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1506.02158v5.pdf&amp;sa=D&amp;ust=1465142037841000&amp;usg=AFQjCNG0iYjug72iiteJH8vUcjyCQ11jQw">http://arxiv.org/pdf/1506.02158v5.pdf</a></span><span>&nbsp;variational convnet for data efficiency/theoretically grounded regularization. </span><span class="c9 c10">major improvement in SOTA</span><span>&nbsp;without using transfer learning. called it from a mile away, of course this was going to be better. but it&#39;s just as efficient?! use this on everything! -&gt; it&#39;s a new way to do dropout. no wonder. this is probably going to get adopted by everyone really soon.</span></li></ul><ul class="c2 lst-kix_8wesot3ko53b-1 start"><li class="c6 c22"><span>Actual technique: </span><span class="c9">&quot;Apply dropout at test time too, and average the runs. And use dropout after every single layer.&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_lyxc6qcmvco2-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.stat.washington.edu/courses/stat527/s13/readings/stat_sci0419(1)128-139.pdf&amp;sa=D&amp;ust=1465142037843000&amp;usg=AFQjCNGsdkHXmkDo2Xkl1WR5e28RQHmHhg">http://www.stat.washington.edu/courses/stat527/s13/readings/stat_sci0419(1)128-139.pdf</a></span><span>&nbsp;a bit old, but an analysis of neural networks from a bayesian perspective</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_1qjopwyk3n75-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07962&amp;sa=D&amp;ust=1465142037844000&amp;usg=AFQjCNH7kku9H82s1uBjt6phoXfuB_vxTw">http://arxiv.org/abs/1512.07962</a></span><span>&nbsp;theoretical stuff about applying mcmc to neural networks. Theory and technique? -&gt; probably not that interesting in light of needing mc dropout</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_60wjig3vq01-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09300&amp;sa=D&amp;ust=1465142037845000&amp;usg=AFQjCNH7NRCQBu8u_419t5N8GDd7wOjBDg">http://arxiv.org/abs/1512.09300</a></span><span>&nbsp;variational autoencoder with generative adversarial network as the loss function, resulting in directly per feature loss. Technique. Looks interesting. Also in &quot;representation learning&quot; section. I wonder how this would do with mc dropout.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_uz5smpj76010-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00670&amp;sa=D&amp;ust=1465142037846000&amp;usg=AFQjCNFUa9sROc00fv2fezNcu_wdEv-NXg">http://arxiv.org/abs/1601.00670</a></span><span>&nbsp;review of variational inference, for statisticians</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ow9ix2mme95u-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.cs.toronto.edu/~graves/nips_2011.pdf&amp;sa=D&amp;ust=1465142037847000&amp;usg=AFQjCNHwshMOuQjI8ZcF-R-K9DX63d5_bg">http://www.cs.toronto.edu/~graves/nips_2011.pdf</a></span><span>&nbsp;older, a general purpose technique for implementing variational inference on a neural network. is it good enough to do automatic variational inference, like one does automatic differentiation? -&gt; probably not, the new technique is much more interesting</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_d3k7hdh35j7a-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09327&amp;sa=D&amp;ust=1465142037848000&amp;usg=AFQjCNHiF1OQjt3156rmxYcv_Z_xV1q_VQ">http://arxiv.org/abs/1512.09327</a></span><span>&nbsp;distributed stochastic (unbiased?) Bayesian inference on neural networks. This looks important. Think they&#39;ll release their code? :p Technique. also in &quot;speed&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_sxaq135mthse-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06038&amp;sa=D&amp;ust=1465142037849000&amp;usg=AFQjCNFbUqJMOJN1S1EwdhbS_88fLX_KmQ">http://arxiv.org/abs/1511.06038</a></span><span>&nbsp;variational generative language model or something. need to read. also in &quot;language&quot; and &quot;generative&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02166&amp;sa=D&amp;ust=1465142037850000&amp;usg=AFQjCNHZ2fINPkD9itHBsFvVS_yR8T_jLQ">http://arxiv.org/abs/1601.02166</a></span><span>&nbsp;estimating what weight distribution prior to use, based on an nn trained on something similar. Used for transfer learning of machine translation, so also goes in &quot;transfer&quot; and &quot;translation&quot;.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.knspraxfupyj"><span>Theory about neural network effectiveness</span></h2><p class="c3"><span class="c18"></span></p><ul class="c2 lst-kix_w14ild1qeoey-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.03965&amp;sa=D&amp;ust=1465142037851000&amp;usg=AFQjCNEOEIQi8l02nNrEaSj3RUoLkmJ3Iw">http://arxiv.org/abs/1512.03965</a></span><span>&nbsp;formal proof that three layers is more efficient than two; implies that they think this generalized to deeper networks. Theory.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_f51q5t2m61i8-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06293&amp;sa=D&amp;ust=1465142037852000&amp;usg=AFQjCNHB8zbH9AEItEyqJt7gGJS0r3igeQ">http://arxiv.org/abs/1512.06293</a></span><span>&nbsp;formal analysis of convnets theory</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_7r3huctovklw-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04920&amp;sa=D&amp;ust=1465142037853000&amp;usg=AFQjCNFUSszXZGjbXbJ1OzHkEhW6s7zAsQ">http://arxiv.org/abs/1601.04920</a></span><span>&nbsp;another formal analysis of convnet theory</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_qlax8iv93hxg-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00013&amp;sa=D&amp;ust=1465142037854000&amp;usg=AFQjCNGFLqO3iAIcICa45hKTJFCK-WttIw">http://arxiv.org/abs/1601.00013</a></span><span>&nbsp;theory about what a single neuron single hidden layer can approximate, apparently any univariate function (??? Can&#39;t possibly be right, audio compression is univariate: the only input variable is time)</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_o4ah8y8am4ct-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.01084&amp;sa=D&amp;ust=1465142037855000&amp;usg=AFQjCNGDIOMq-eWPBB34gx239RMFSrzJpg">http://arxiv.org/abs/1508.01084</a></span><span>&nbsp;convnets as kernel machines, analysis of what that implies</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_vapye4eddf48-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05678&amp;sa=D&amp;ust=1465142037856000&amp;usg=AFQjCNF2wQ8-zt5XCxYia3oEOKYFZt3wmA">http://arxiv.org/abs/1511.05678</a></span><span>&nbsp;relu vs threshold activation (perceptron) formal comparison</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05297&amp;sa=D&amp;ust=1465142037856000&amp;usg=AFQjCNExloWADcJ9kEg5gwG2TikKm-YrRw">http://arxiv.org/abs/1511.05297</a></span><span>&nbsp;On the interplay of network structure and gradient convergence in deep<br> &nbsp;learning</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://research.microsoft.com/apps/video/default.aspx?id%3D259574%26r%3D1&amp;sa=D&amp;ust=1465142037857000&amp;usg=AFQjCNGJqiqQbs6U7Dy2zu42uKXxhig2og">http://research.microsoft.com/apps/video/default.aspx?id=259574&amp;r=1</a></span><span>&nbsp;8:13 references papers about exponential effectiveness of distributed representation</span></li><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09260&amp;sa=D&amp;ust=1465142037858000&amp;usg=AFQjCNFU1jjBPw20di3WOL8r8sMhJ-0U4A">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09260&amp;sa=D&amp;ust=1465142037858000&amp;usg=AFQjCNFU1jjBPw20di3WOL8r8sMhJ-0U4A">http://arxiv.org/abs/1603.09260</a></span><span>&nbsp;[abs:5] estimate of the number of bits required to represent a neural network; finds it to be *dramatically* lower than the number of parameters. Should go in &quot;theory&quot;, &quot;regularization&quot;, &quot;architectures&quot;, maybe even &quot;basics&quot; because it&#39;s such a big deal.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.7keq5uchgeuu"><span>Adverserial Examples Problem</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_nbpnmy8xci8w-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03034&amp;sa=D&amp;ust=1465142037860000&amp;usg=AFQjCNELiys9E3ftcwPbfmdmqskILvmyDQ">http://arxiv.org/abs/1511.03034</a></span><span>&nbsp;</span><span class="c9">solution to the adverserial examples problem</span><span>&nbsp;- adverserial supervised learning, try to find mistake images and then train on those. comes with a much faster way of finding adverserial images.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_g0l7lf51hdyf-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06292&amp;sa=D&amp;ust=1465142037861000&amp;usg=AFQjCNG_2In4ArF6cX5I9fR-EydbhLe6LA">http://arxiv.org/abs/1511.06292</a></span><span>&nbsp;analysis of and possible solution to adversarial examples</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_gnzb8ea0t50l-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.02590&amp;sa=D&amp;ust=1465142037862000&amp;usg=AFQjCNHHvGge95aDzr9B9jLLHuVHHClrPg">http://arxiv.org/abs/1502.02590</a></span><span>&nbsp;analysis of adverserial examples problem demonstrating that it&#39;s not possible to solve fully (called it)</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_cq59e9nktsby-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05122&amp;sa=D&amp;ust=1465142037863000&amp;usg=AFQjCNEI9MsmiatJJa-DrjZTQs1JfRHQ4w">http://arxiv.org/abs/1511.05122</a></span><span>&nbsp;adverserial examples problem but for hidden states instead of outputs</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.6572&amp;sa=D&amp;ust=1465142037863000&amp;usg=AFQjCNGkMNAHB4DvcFihqtPt0VzCp6zXPg">http://arxiv.org/abs/1412.6572</a></span><span>&nbsp;another analysis or something, miri linked this one, not sure if it&#39;s any good</span></li></ul><p class="c3"><span></span></p><h1 class="c5 c23" id="h.rns3upjkt7wt"><span></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.agubnbpo953"><span></span></h1><h1 class="c5" id="h.dzvgxjn5tl9s"><span>Speed</span></h1><h2 class="c5" id="h.yf7u7tv7pgv8"><span>Model compression</span></h2><p class="c3"><span class="c18"></span></p><ul class="c2 lst-kix_62is116n17wq-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06473&amp;sa=D&amp;ust=1465142037865000&amp;usg=AFQjCNH3jL8D24KsfFVpisoUoT43XcKTjA">http://arxiv.org/abs/1512.06473</a></span><span>&nbsp;quantized cnns for mobile devices technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_xav8ikvidc9s-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.00149&amp;sa=D&amp;ust=1465142037866000&amp;usg=AFQjCNHPb293zx13vnSV4Wv8NdxgdV1kRw">http://arxiv.org/abs/1510.00149</a></span><span>&nbsp;deep model compression - one of the early papers on it. &quot;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding&quot;. pretty interesting results.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_383wi8io125v-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09194&amp;sa=D&amp;ust=1465142037867000&amp;usg=AFQjCNH6-ZwBHb4YVfGeiVCaaUceEDrFLA">http://arxiv.org/abs/1512.09194</a></span><span>&nbsp;another mathematically equivalent transform of tensors such that convnets are more computationally efficient. Technique and math theory. uses &nbsp;a &quot;kronecker layer&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_9cwmoyhkmhfb-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08571&amp;sa=D&amp;ust=1465142037868000&amp;usg=AFQjCNHnTn8kxNE4d0_JU9HQNpjsied-8g">http://arxiv.org/abs/1512.08571</a></span><span>&nbsp;pruning of unwanted weights in a more structured way, such that you can actually skip computation and be happy about it. haven&#39;t read yet, but looks interesting. how much computation are they saving? technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_vzf3xt3x8pov-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06569&amp;sa=D&amp;ust=1465142037868000&amp;usg=AFQjCNEk3VsFIZrHbTm83ls5MY6TfjzawQ">http://arxiv.org/abs/1509.06569</a></span><span>&nbsp;tensorizing to tensor train to compress technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_9khvkfghhb3b-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05077&amp;sa=D&amp;ust=1465142037869000&amp;usg=AFQjCNEJfJv-jCHzGZg5beWxtjKbI9rhxQ">http://arxiv.org/abs/1511.05077</a></span><span>&nbsp;learning-time method for throwing away or merging useless neurons. </span><span class="c9 c12">major improvement in model compression SOTA.</span></li></ul><p class="c3"><span class="c9 c12"></span></p><ul class="c2 lst-kix_n8ye7s4j9pok-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06488&amp;sa=D&amp;ust=1465142037870000&amp;usg=AFQjCNGFlKdcwDXmnI9XOyait_K51G3dEg">http://arxiv.org/abs/1511.06488</a></span><span>&nbsp;analysis of the effects of quantization on neural networks. this is probably where the 4 bits number comes from.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_3xkj88gevxle-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.08745&amp;sa=D&amp;ust=1465142037871000&amp;usg=AFQjCNFQz4TKANStADNGWjP51cIGd4Kvtg">http://arxiv.org/abs/1509.08745</a></span><span>&nbsp;learning-time model compression. isn&#39;t much more specific than that.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_solhl6ysg65v-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.08362&amp;sa=D&amp;ust=1465142037872000&amp;usg=AFQjCNHdEAkgC_i79Wlf6RnxzVHAlCx-AQ">http://arxiv.org/abs/1504.08362</a></span><span>&nbsp;- reduced resolution (&quot;perforated&quot; - random locations skipped?) convnets for faster runtime on small devices. small decrease in accuracy.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06393&amp;sa=D&amp;ust=1465142037873000&amp;usg=AFQjCNFcPuvOtKr27RbV-wSESwitPa3Uqg">http://arxiv.org/abs/1511.06393</a></span><span>&nbsp;&quot;Fixed Point Quantization of Deep Convolutional Networks&quot;</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.3fwtohfyukue"><span>Hardware</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_egb7bfw0xx0q-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05552&amp;sa=D&amp;ust=1465142037874000&amp;usg=AFQjCNElLmoOGCgVBj54lvBu4eRtuA0r9Q">http://arxiv.org/abs/1511.05552</a></span><span>&nbsp;implementing RNNs in hardware</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.neti04fevcj6"><span>Parallelization</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_6k5yl8dbqqq2-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06216&amp;sa=D&amp;ust=1465142037876000&amp;usg=AFQjCNGabujscY8orI-qebcHl5eZjWBCPQ">http://arxiv.org/abs/1512.06216</a></span><span>&nbsp;Poseidon gpu cluster parallelism technique</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_5d66udn9quk7-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04033&amp;sa=D&amp;ust=1465142037877000&amp;usg=AFQjCNEHUj7T6fM77rTdFqdOh6ZN0hs2tw">http://arxiv.org/abs/1601.04033</a></span><span>&nbsp;faster asynchronous SGD, should go in &quot;parallelization&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_jourdftw4nrm-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09295&amp;sa=D&amp;ust=1465142037877000&amp;usg=AFQjCNEkYwNJpYUVki5uVfRLXs9WRtorZQ">http://arxiv.org/abs/1512.09295</a></span><span>&nbsp;technical data dump about architecting large scale distributed machine learning. Misc.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_b07y002136mr-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09327&amp;sa=D&amp;ust=1465142037878000&amp;usg=AFQjCNG77MYju1AzVQFKLjLNertTrmcmXQ">http://arxiv.org/abs/1512.09327</a></span><span>&nbsp;distributed stochastic (unbiased?) Bayesian inference on neural networks. This looks important. Think they&#39;ll release their code? :p Technique. also in &quot;efficiency and theory&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_xgsgxyckqw77-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06051&amp;sa=D&amp;ust=1465142037879000&amp;usg=AFQjCNETOV98KJa9O6vzCTQWjE3Fp5bZQg">http://arxiv.org/abs/1511.06051</a></span><span>&nbsp;framework for running neural network training on spark, the distributed processing system. competes with tensorflow?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_m8diz2yltkag-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.07595&amp;sa=D&amp;ust=1465142037880000&amp;usg=AFQjCNEl8MoNHBwlmnZb7sjOmVeIVHnPKA">http://arxiv.org/abs/1507.07595</a></span><span>&nbsp;Distributed Stochastic Variance Reduced Gradient Methods and A Lower Bound for Communication Complexity</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_474t6llskbh7-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06481&amp;sa=D&amp;ust=1465142037881000&amp;usg=AFQjCNH4E0V_tnpycbQClTTKd5FWYV4EyA">http://arxiv.org/abs/1511.06481</a></span><span>&nbsp;- &quot;variance reduction in SGD via distributed importance sampling&quot; - also in &quot;efficiency and theory/data prioritization&quot;, &quot;optimizers&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.00175&amp;sa=D&amp;ust=1465142037881000&amp;usg=AFQjCNHbXKTR_UjXa2dfbGwZ8fOYDOSnMg">http://arxiv.org/abs/1511.00175</a></span><span>&nbsp;&quot;: FireCaffe: near-linear acceleration of deep neural network training on<br> &nbsp;compute clusters&quot;</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.npgoxjjba9ab"><span></span></h1><h1 class="c5" id="h.jvs234lhr5f"><span>Images</span></h1><h2 class="c5" id="h.d8ysl79e6xo8"><span>100% Whole-Image Deep Learning</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_6jabh0roz5s1-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06615&amp;sa=D&amp;ust=1465142037884000&amp;usg=AFQjCNFHKiV1THCKdnF4MR75NNpq979MmQ">http://arxiv.org/abs/1601.06615</a></span><span>&nbsp;survey of computer vision applications of neural networks and cnns specifically, intended for beginners. Recipe style guide for making cnns. Should also go in &quot;basics&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_4yao0efkrd31-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06658&amp;sa=D&amp;ust=1465142037886000&amp;usg=AFQjCNF5x84DDnHdtGwf0-lbxAU2nAZReA">http://arxiv.org/abs/1512.06658</a></span><span>&nbsp;surface and material classification application</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_l47f7qjlu9cu-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06785&amp;sa=D&amp;ust=1465142037887000&amp;usg=AFQjCNHVy-suA0gCYbl5LNTkgXoiMWgxsA">http://arxiv.org/abs/1512.06785</a></span><span>&nbsp;predicting user preferences directly from images on Pinterest application</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_tg2hwffju8t9-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://web.eecs.umich.edu/~jiadeng/paper/RamanathanEtAl_CVPR2015.pdf&amp;sa=D&amp;ust=1465142037888000&amp;usg=AFQjCNHlU3dIZthJbbAR0lfoiriOs6nfDA">http://web.eecs.umich.edu/~jiadeng/paper/RamanathanEtAl_CVPR2015.pdf</a></span><span>&nbsp;application of joint-representation models to generalizing activity recognition. Technique/application</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_uk8k6pzec0np-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04119&amp;sa=D&amp;ust=1465142037888000&amp;usg=AFQjCNEUojUAJC6TmpPfiLl8Iyn-2CiVnw">http://arxiv.org/abs/1511.04119</a></span><span>&nbsp;video</span><span>&nbsp;action classification with attentional RNNs. </span><span class="c9 c28">mentions datasets.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_wq4pisfqksnu-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04590&amp;sa=D&amp;ust=1465142037889000&amp;usg=AFQjCNFjIl3w5HpjI0B-FePwatXXQnhyrw">http://arxiv.org/abs/1511.04590</a></span><span>&nbsp;video and image labeling performance upper bound, finds that the upper bound is higher than expected despite being simple, and that current models fall short of it.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_bq5ybp1zal0b-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04103&amp;sa=D&amp;ust=1465142037890000&amp;usg=AFQjCNHWG_6fCfav51fvsfViN6RuDUAHjQ">http://arxiv.org/abs/1511.04103</a></span><span>&nbsp;cirriculum learning and architecture for deep vision networks based on what is known of the human visual system. looks very interesting. also sort of related to neuromorphic stuff, I guess.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_o14fwownm59e-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06759&amp;sa=D&amp;ust=1465142037891000&amp;usg=AFQjCNHvlGbj7QaabzzF8jM_FBAo2BNP0A">http://arxiv.org/abs/1601.06759</a></span><span>&nbsp;residual 2d rnns for generative image modeling. Also should go in &quot;architecture&quot;, &quot;memory&quot;, &quot;generative&quot;. May be a big sota improvement, especially for things besides what they tried it on. notes on it by someone: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.evernote.com/shard/s189/sh/fdf61a28-f4b6-491b-bef1-f3e148185b18/aba21367d1b3730d9334ed91d3250848&amp;sa=D&amp;ust=1465142037892000&amp;usg=AFQjCNE-Ya4GPYG1ogla18PGNHwz8eAJog">https://www.evernote.com/shard/s189/sh/fdf61a28-f4b6-491b-bef1-f3e148185b18/aba21367d1b3730d9334ed91d3250848</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_p7d8xi28lme6-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1511.02793v1.pdf&amp;sa=D&amp;ust=1465142037892000&amp;usg=AFQjCNG1q5HhFFFklveC2hbSIoSQrYxUdQ">http://arxiv.org/pdf/1511.02793v1.pdf</a></span><span>&nbsp;image generation from sentences. Application. also in &quot;generative&quot;, &quot;language models&quot;.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_6r73oe35erxy-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06432&amp;sa=D&amp;ust=1465142037894000&amp;usg=AFQjCNGnhzQPul8CZirDuKhAXYOkODj6KA">http://arxiv.org/abs/1511.06432</a></span><span>&nbsp;technique for transfer learning from imagenet to video recognition with GRUs. mentions sparsity in the abstract. haven&#39;t read yet. also in &quot;transfer learning&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04891&amp;sa=D&amp;ust=1465142037894000&amp;usg=AFQjCNFHTs7ezU35y2bJNbzUIVXhKv_vUQ">http://arxiv.org/abs/1511.04891</a></span><span>&nbsp;&quot;Sherlock: Modeling Structured Knowledge in Images&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05440&amp;sa=D&amp;ust=1465142037895000&amp;usg=AFQjCNFKNUrAQeDQaXuZyh5TiRI3F4jFPg">http://arxiv.org/abs/1511.05440</a></span><span>&nbsp;&quot;Deep multi-scale video prediction beyond mean square error&quot;, should also go in &quot;generative&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02129&amp;sa=D&amp;ust=1465142037895000&amp;usg=AFQjCNFWVIJSRUZdoauUt-0jCxWynPut8g">http://arxiv.org/abs/1601.02129</a></span><span>&nbsp;temporal action localization. 3d temporal convnets. custom objective function. looks like just an application. </span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.no8lx88bg8"><span>Question answering</span></h2><ul class="c2 lst-kix_6r73oe35erxy-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07394&amp;sa=D&amp;ust=1465142037897000&amp;usg=AFQjCNHQBpeEF4B1yXIDiNUxGu6yR4WEbg">http://arxiv.org/abs/1511.07394</a></span><span>&nbsp;attentional image QA.</span></li></ul><h2 class="c5" id="h.ju34j22b5hzm"><span>Similarity detection</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_isql72ps5u79-0 start"><li class="c6 c7"><span>Similarity detection details, with references to several other similarity detection systems: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05300&amp;sa=D&amp;ust=1465142037898000&amp;usg=AFQjCNG-hsqTf9hKdvLecUlvtxAZ-ixhRA">http://arxiv.org/abs/1512.05300</a></span><span>&nbsp;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02093&amp;sa=D&amp;ust=1465142037899000&amp;usg=AFQjCNGjlJdfdAMXRgwPx4_ai8QhSIafNw">http://arxiv.org/abs/1601.02093</a></span><span>&nbsp;image embedding for lookup. improves on perturbation invariance of the embeddings. mentions a brain inspired thing as inspiration for how to achieve invariance. should go in &quot;CNNs&quot;, &quot;neuromorphic&quot;.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><h2 class="c5" id="h.6zujbp3j9mka"><span>Image Object Segmentation/Pixelwise classification</span></h2><ul class="c2 lst-kix_bhgkuyt4aopz-0 start"><li class="c6 c7"><span class="c29 c28"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08086&amp;sa=D&amp;ust=1465142037900000&amp;usg=AFQjCNHHg6pWBOH0zNCLcC_L-PJVh_Vg4Q">http://arxiv.org/abs/1512.08086</a></span><span class="c28">&nbsp;interpretable cnns. Fine grained classification with explanations. Technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_vt0etbgrzlaw-0 start"><li class="c6 c7"><span class="c28 c29"><a class="c0" href="https://www.google.com/url?q=http://www.cs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&amp;sa=D&amp;ust=1465142037901000&amp;usg=AFQjCNHNHiThhfg9URwqQf1RxErrIw8WJw">http://www.cs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf</a></span><span class="c28">&nbsp;and </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.4038&amp;sa=D&amp;ust=1465142037902000&amp;usg=AFQjCNEdpe2SGeVZpRUI3pFh3cwlDOWFAg">http://arxiv.org/abs/1411.4038</a></span><span class="c28">&nbsp;fully convolutional classification (involves and references deconvolution) technique - should go in &quot;historical&quot; as well</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_l1rt7mvaz29b-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07711&amp;sa=D&amp;ust=1465142037903000&amp;usg=AFQjCNHp7UsEIEJ6LIrvbS0flzw0faadyg">http://arxiv.org/abs/1512.07711</a></span><span>&nbsp;</span><span>even faster alternative to faster r-cnn. Technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_n1766bho68n4-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07729&amp;sa=D&amp;ust=1465142037903000&amp;usg=AFQjCNE0RirjevMhZLwBuFcKkPdPBjGLqQ">http://arxiv.org/abs/1512.07729</a></span><span>&nbsp;another approach to faster object detection like the above and r-cnn. Technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_2s6hp75gzisz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04798&amp;sa=D&amp;ust=1465142037904000&amp;usg=AFQjCNF69jyd66SUX-ZOiEmBV_WgDnnOxA">http://arxiv.org/abs/1601.04798</a></span><span>&nbsp;pixelwise object detection, with a fully conv network to classify pixels and a detection network to output locations. Helps a bit </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_204pjqaavew-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.03159&amp;sa=D&amp;ust=1465142037905000&amp;usg=AFQjCNFrjlbtxSNVbZZKQvN5WQRMOL49Sw">http://arxiv.org/abs/1505.03159</a></span><span>&nbsp;image segmentation application</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_xqfh3tf7tjc0-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://pjreddie.com/darknet/yolo/&amp;sa=D&amp;ust=1465142037906000&amp;usg=AFQjCNHYOCmOPjWwenOUNuy8mexz-GZ6RQ">http://pjreddie.com/darknet/yolo/</a></span><span>&nbsp;bounding box proposal technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_hydge7xfbt8r-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00978&amp;sa=D&amp;ust=1465142037906000&amp;usg=AFQjCNHl_VMbIK4gZXvc7K-T--MidWK7Sg">http://arxiv.org/abs/1601.00978</a></span><span>&nbsp;crater detection application</span></li></ul><ul class="c2 lst-kix_hydge7xfbt8r-1 start"><li class="c6 c22"><span>techniques: just convnets, nothing fancy, looks like</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_894copnhklgz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1510.07945v2.pdf&amp;sa=D&amp;ust=1465142037907000&amp;usg=AFQjCNG5fJ2RcSt6MB82UW6HdFVsj1vRGw">http://arxiv.org/pdf/1510.07945v2.pdf</a></span><span>&nbsp;object tracking, uses task-specialized final layers from a shared representation. looks cool. apparently object tracking is where you get a specific object and must re-locate it in following images. this is a form of knowledge sharing/transfer learning, so it&#39;s also in those categories.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ta3ofdl38yyn-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06260&amp;sa=D&amp;ust=1465142037908000&amp;usg=AFQjCNGrx2_jw12s-fVUcJXC2Z3zmq79lg">http://arxiv.org/abs/1601.06260</a></span><span>&nbsp;person re-identification again and some datasets.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_vsaybtg1hfvm-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.01497&amp;sa=D&amp;ust=1465142037909000&amp;usg=AFQjCNEjHXEkREss8uCFMdMG63VSxLKK6g">http://arxiv.org/abs/1506.01497</a></span><span>&nbsp;fully conv network that outputs objectness scores at each location and bounding box sizes. also has some other hackery to make it go fast. apparently still has multiple runs per image. runs at 5fps, though. </span><span class="c9 c12">looks pretty cool, and claims to be a big improvement in SOTA.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_douubz1tb8an-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06719&amp;sa=D&amp;ust=1465142037910000&amp;usg=AFQjCNFch_Wqq-FZ5w0TrFfqM5MpL-9bAQ">http://arxiv.org/abs/1601.06719</a></span><span>&nbsp;very messy draft, but uses transfer learning from early conv layers to build an object detection thing or something like that to do with pre learned features.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_rr7hjjtid2vg-0 start"><li class="c6 c7"><span>yolo </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://pjreddie.com/darknet/yolo/&amp;sa=D&amp;ust=1465142037910000&amp;usg=AFQjCNEF3sokCbSZ0QucKCOXNcNMY9PcpA">http://pjreddie.com/darknet/yolo/</a></span><span>&nbsp;- another fully conv network that outputs objectness/bounding box scores at each location.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.7il71cdip45x"><span>3D</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_nwi1ck3fibs7-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.06825&amp;sa=D&amp;ust=1465142037912000&amp;usg=AFQjCNHCzqjPkDyDRPeNPAGYJrlsmE7bmA">http://arxiv.org/abs/1506.06825</a></span><span>&nbsp;google street view type thing stereo reprojection/interpolation. dunno what to call it. it&#39;s weird. wtf is a plane sweep volume, anyway? this would be a cool application to reproduce if I can get the dataset they used. application. also in &quot;datasets&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_2zf5bd1mem1t-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00706&amp;sa=D&amp;ust=1465142037912000&amp;usg=AFQjCNHL8b7tP2I_rtZgSzMtl_niCvQ8Pg">http://arxiv.org/abs/1601.00706</a></span><span>&nbsp;single image 3d new view synthesis, solving the difficult prior problem by only doing it for a restricted set of object types.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_saoi6g81wrq2-0 start"><li class="c6 c7"><span>not neural, overview of pose tracking: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01006&amp;sa=D&amp;ust=1465142037913000&amp;usg=AFQjCNESWT_Ifp3RAeXfnERrwoj-tPuu5g">http://arxiv.org/abs/1601.01006</a></span><span>&nbsp;- probably has good datasets and maybe neural papers linked. also in &quot;datasets&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_nf1j1iafbq39-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04119&amp;sa=D&amp;ust=1465142037914000&amp;usg=AFQjCNFrecD7YC02XRHJdDDrsl6s0Y5tIg">http://arxiv.org/abs/1511.04119</a></span><span>&nbsp;occlusion detection dataset and technique. also in &quot;datasets&quot;. </span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01750&amp;sa=D&amp;ust=1465142037915000&amp;usg=AFQjCNGfk_S5bFWkFalcNWTQiH9AyXHG3g">http://arxiv.org/abs/1601.01750</a></span><span>&nbsp;deep error correction for time of flight sensors</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05175&amp;sa=D&amp;ust=1465142037915000&amp;usg=AFQjCNH35VGGIAibDfPtyI6kyVt6UhsWNA">http://arxiv.org/abs/1511.05175</a></span><span>&nbsp;&quot;Convolutional Models for Joint Object Categorization and Pose Estimation&quot;</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.le29gqjpm81t"><span>Faces</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_1a8ua5u4a26p-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04902&amp;sa=D&amp;ust=1465142037916000&amp;usg=AFQjCNGYePrvOdxatYxTKwoaDlclWnoj0A">http://arxiv.org/abs/1601.04902</a></span><span>&nbsp;pupil detection with object detection stuff</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_16ygmg7toocv-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://bamos.github.io/2016/01/19/openface-0.2.0/&amp;sa=D&amp;ust=1465142037917000&amp;usg=AFQjCNGMRRQXw1crfIld-0hehKl5_ArLRQ">http://bamos.github.io/2016/01/19/openface-0.2.0/</a></span><span>&nbsp;project to do open source face recognition. has links to open source face datasets, torch code, and a research log about their optimization. looks like a full application, intended to actually compete, and as such has a lot of tricks to extract. comes with a large pretrained model. also in &quot;datasets&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02487&amp;sa=D&amp;ust=1465142037918000&amp;usg=AFQjCNGWR7dRBVDctlssFbdwBHvnnNCR9A">http://arxiv.org/abs/1601.02487</a></span><span>&nbsp;facial expression recognition improvement. also has a new dataset. should go in &quot;datasets&quot;.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.jtbvllk8i8cb"><span>OCR</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_ltkzlkp2gxkt-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.04395&amp;sa=D&amp;ust=1465142037919000&amp;usg=AFQjCNFsClFXoeWys85aqqfpBgZUzvvCPQ">http://arxiv.org/abs/1506.04395</a></span><span>&nbsp;scene text parsing application</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_ljjwvb506pet-0 start"><li class="c6 c7"><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08669&amp;sa=D&amp;ust=1465142037921000&amp;usg=AFQjCNFTJaz3iQha5aRy1qRPl61tmjXFSA">http://arxiv.org/abs/1512.08669</a></span><span>&nbsp;shallow scene text recognition using sparse coding. also in &quot;sparse coding&quot;. looks non-deep, probably meh.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_z1voh5a7dove-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01100&amp;sa=D&amp;ust=1465142037921000&amp;usg=AFQjCNHYdEOuq3MBXIUYQA4EBDURHqNDlA">http://arxiv.org/abs/1601.01100</a></span><span>&nbsp;convnet + lstm for end-to-end scene text parsing. Does this have attention or not? O_o</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01885&amp;sa=D&amp;ust=1465142037922000&amp;usg=AFQjCNH_aPsZMaVQAzvuZXiAZJnZuEiltA">http://arxiv.org/abs/1601.01885</a></span><span>&nbsp;font detection</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04397&amp;sa=D&amp;ust=1465142037922000&amp;usg=AFQjCNF2ngfffS7fwr63e2ok0naLvIR3xA">http://arxiv.org/abs/1511.04397</a></span><span>&nbsp;Similarity-based Text Recognition by Deeply Supervised Siamese Network. Should also go in &quot;similarity&quot;</span></li><li class="c3 c7"><span></span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h1 class="c5" id="h.vq1xzex5u8pd"><span>Language</span></h1><h2 class="c5" id="h.buurkppm9w2u"><span>Text analysis</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_f3egesa43r6b-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.04945&amp;sa=D&amp;ust=1465142037925000&amp;usg=AFQjCNEFsthWzDv75L77DpiEHlYwrDd9Vg">http://arxiv.org/abs/1508.04945</a></span><span>&nbsp;neural writer identification; application. </span><span class="c9">I want to try this too, this has spoilers!</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_juam6n4619t2-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://blog.dato.com/practical-text-analysis-using-deep-learning&amp;sa=D&amp;ust=1465142037926000&amp;usg=AFQjCNFilFA3kORNQgnFOuKJ2u9TThQIkQ">http://blog.dato.com/practical-text-analysis-using-deep-learning</a></span><span>&nbsp;references writer gender identification and a dataset of a lot of blog posts. might be worth training and then using the manifold traversal technique in the representation learning section for suggesting how someone could sound more like a different class. dataset is &quot;the blog authorship corpus&quot;, is in datasets list as well.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_47kviebmmd71-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01280&amp;sa=D&amp;ust=1465142037927000&amp;usg=AFQjCNF9xSZxzAgkoaKwW-XP2jRb7P36Sg">http://arxiv.org/abs/1601.01280</a></span><span>&nbsp;natural language parsing - convert a sentence into a formal language. works pretty well. not a new technique.</span></li></ul><ul class="c2 lst-kix_47kviebmmd71-1 start"><li class="c6 c22"><span>technique: sequence to sequence with attention</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ltwr0s349me7-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01530&amp;sa=D&amp;ust=1465142037928000&amp;usg=AFQjCNHXSCZlkSHXIQaq62pG6LjhRmJlMg">http://arxiv.org/abs/1601.01530</a></span><span>&nbsp;dependency labeling and stuff - runs through once with an lstm, uses that state vector as the init for another lstm that runs through. kinda like a bidirectional recurrent thingy. looks cool. major improvement in SOTA of something or other. uses an LSTM that gets the actual output label from the previous timestep during training, and then the best guess as the input at runtime. note: this paper has license to use the term &quot;LSTM&quot; because they cite Jurgen Schmidthuber. </span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.7uaays6wpxfr"><span>Talking Computers</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_ulntv5wergyk-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04574&amp;sa=D&amp;ust=1465142037929000&amp;usg=AFQjCNFRoxpy7p8LW7ysY-f1Br0tzRJNyQ">http://arxiv.org/abs/1601.04574</a></span><span>&nbsp;reinforcement learner on text. </span><span class="c9">spoilers, I want to make this!</span><span>&nbsp;also in reinforcement learning.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_o4d3wx7elf58-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06612&amp;sa=D&amp;ust=1465142037931000&amp;usg=AFQjCNEdyKVZpuTazQeZ3Gz6j8N42bw2wg">http://arxiv.org/abs/1512.06612</a></span><span>&nbsp;constrained natural language generation technique and application</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_e2bt8y2ywvrz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05919&amp;sa=D&amp;ust=1465142037931000&amp;usg=AFQjCNG5fzax7C5Uon1HEferswKphVtGwA">http://arxiv.org/abs/1512.05919</a></span><span>&nbsp;gofai essay generation. Theory and miscellaneous.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_xkjgsygnpr2m-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf&amp;sa=D&amp;ust=1465142037932000&amp;usg=AFQjCNHvaYS6eQKPYkLCVA9cCwPE5HhhhQ">http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf</a></span><span>&nbsp;and </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf&amp;sa=D&amp;ust=1465142037933000&amp;usg=AFQjCNEgzLYzQhKz9C9KcwYgifwzzZaYTg">http://papers.nips.cc/paper/5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf</a></span><span>&nbsp;[abs:4]</span><span>&nbsp;multi image story generation. Uses learned consistency checker and a conv-&gt;birnn layout. Does well on artificial metrics, and also on mturk user studies. Technique. Code on github. major improvement in sota of sentence coherency.</span><span>&nbsp;uses blog posts. Should go in &quot;recurrence&quot; as well. Notable paper that I often cite as progress in this task.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_wdjiiwxyq2d1-0 start"><li class="c6 c7"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://googleresearch.blogspot.com/2014/11/a-picture-is-worth-thousand-coherent.html?m%3D1&amp;sa=D&amp;ust=1465142037934000&amp;usg=AFQjCNGdglViw6FKfyEPA1b8NV9aym67eg">http://googleresearch.blogspot.com/2014/11/a-picture-is-worth-thousand-coherent.html?m=1</a></span><span>&nbsp;[abs:0]</span><span>&nbsp;mentions four image captioning datasets and a language quality metric I haven&#39;t heard of called bleu. Blog post is about show and tell paper: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.4555&amp;sa=D&amp;ust=1465142037935000&amp;usg=AFQjCNEjVkPT7kJ0t2jC5VKpGyYnqtoAXQ">http://arxiv.org/abs/1411.4555</a></span><span>&nbsp;- should go in &quot;historical&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_o38l9c2rqk2p-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1506.05869.pdf&amp;sa=D&amp;ust=1465142037936000&amp;usg=AFQjCNEgYJE144gqeu_XB36-1X06Fp4zVw">http://arxiv.org/pdf/1506.05869.pdf</a></span><span>&nbsp;good old conversation bots. these don&#39;t look impressive. it&#39;s a bit better than normal, but not by a lot. it can&#39;t keep a topic. seems like some of the other architectural approaches that worked well, such as giving it time to think about what a consistent thing to say would be, are pretty important. humans get time to process, they don&#39;t just spit words out. also, need orders of magnitude bigger datasets to train from just text - humans see a lot of other things. the shared knowledge between captioning and conversation seems important. and as usual, doesn&#39;t use beam search. how do we model complex distributions of what to say? seems like you just distribute it further and give it more time to ponder, or else you&#39;re back to the good old search problem over recurrent models.</span></li></ul><ul class="c2 lst-kix_o38l9c2rqk2p-1 start"><li class="c6 c22"><span>-&gt; does this imply that the consistency checker paradigm is going to turn out to be a guided search thingy, like beam search is?</span></li><li class="c6 c22"><span>-&gt; it does seem like that. you could implement a consistency checker as a search filter.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ex6gjyozs00-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06931&amp;sa=D&amp;ust=1465142037937000&amp;usg=AFQjCNEYRI7tMFk-GCDJxl9L7Guhmfzgww">http://arxiv.org/abs/1511.06931</a></span><span>&nbsp;dataset for movie/sentence language challenges. also in &quot;datasets&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_mgyoyquoxrm1-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04108&amp;sa=D&amp;ust=1465142037938000&amp;usg=AFQjCNG_UKJU_aIFN1viq1eSvSD_qtywWg">http://arxiv.org/abs/1511.04108</a></span><span>&nbsp;question/answer retrieval using bidirectional LSTMs and something about embeddings of the questions and answers. mentions datasets used.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_k12n8xes8ofj-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.03055&amp;sa=D&amp;ust=1465142037938000&amp;usg=AFQjCNEu7F7SS0Hc0xmRMWtHCrra6Eo4BA">http://arxiv.org/abs/1510.03055</a></span><span>&nbsp;a better objective function for conversation models, based on the observation that the typical objective functions reward safe guesses. &quot;maximum mutual information&quot;. </span><span class="c9 c12">looks very very cool. </span><span>title: A Diversity-Promoting Objective Function for Neural Conversation Models. maybe also should go in &quot;objectives&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01705&amp;sa=D&amp;ust=1465142037939000&amp;usg=AFQjCNFz8TyCJ_dwRbqXzYn-0vSeHS087Q">http://arxiv.org/abs/1601.01705</a></span><span>&nbsp;neural model for question answering that uses composable models. Big step towards general neural cpu!! Should also go in &quot;generality&quot;, &quot;sub-selection&quot;, &quot;rl&quot;. Probably need a question answering section.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.08565&amp;sa=D&amp;ust=1465142037940000&amp;usg=AFQjCNEd1vBN825NRhjuGXnwfmZw--NO8g">http://arxiv.org/abs/1510.08565</a></span><span>&nbsp;attention with intention paper; confusing, not really sure what&#39;s going on. big model that has a lot of complexity in the structure. attentional. probably should go in &quot;attentional&quot; as a category.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.00838&amp;sa=D&amp;ust=1465142037940000&amp;usg=AFQjCNEGQih-DXauiqA7Fm3cZ-zQLimUWA">http://arxiv.org/abs/1509.00838</a></span><span>&nbsp;describing data in natural language with lstms? uses encoder-something-decoder, possibly has a consistency checker. mentions weathergov and robocup datasets, so should probably also go in &quot;datasets&quot;. </span></li><li class="c6 c7"><span>Mar 31, 16:</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06155&amp;sa=D&amp;ust=1465142037941000&amp;usg=AFQjCNE60rnBg8EuhROTPxldi50tLJOUaQ">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06155&amp;sa=D&amp;ust=1465142037941000&amp;usg=AFQjCNE60rnBg8EuhROTPxldi50tLJOUaQ">http://arxiv.org/abs/1603.06155</a></span><span>&nbsp;[abs:5] neural conversation model that embeds the person who&#39;s talking with a personality vector, they call it a &quot;persona&quot;. Dammit, I literally thought of this two days ago. Should go in &quot;language/talking computers&quot;, &quot;embeddings&quot;.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.5zmpmu1vqtm7"><span>Speech</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_ymmukz9l2n7v-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06110&amp;sa=D&amp;ust=1465142037943000&amp;usg=AFQjCNGFB-jtK1ftTLcOJw25lFHMTuH9gA">http://arxiv.org/abs/1512.06110</a></span><span>&nbsp;inflection generation, presumably for speech gen, using char-to-sequence rnns. Good application for neural gpu. Application</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_41ikkbtdotge-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08903&amp;sa=D&amp;ust=1465142037943000&amp;usg=AFQjCNFSs4v156JsJ-D5aFlrrbVUwuhcRQ">http://arxiv.org/abs/1512.08903</a></span><span>&nbsp;hot-word detection with neural networks and connectionist temporal classification. note, this reminds me of a thing from google blogs about decreasing latency of their voice models. also reminds me of another thing about a neural model competing in a guessing game against kenny jennings.</span></li></ul><ul class="c2 lst-kix_41ikkbtdotge-1 start"><li class="c6 c22"><span>as neural network hardware is built for phones, we&#39;ll probably see some really seriously kickass applications of things like this, with real-time detection.</span></li><li class="c6 c22"><span>also, the nsa will finally be able to truly, actually, listen to literally everyone at all times. whee. I wonder if there&#39;s some sort of thing that can be done to convince the nsa to *not* do that? &#39;cause, like, that&#39;s a whole different magnitude of insane privacy violation - most people would literally never say anything private, ever.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_p98dexr65abh-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05389&amp;sa=D&amp;ust=1465142037944000&amp;usg=AFQjCNESMmGIpAuzdQWSp5aoFEqd0uPkwQ">http://arxiv.org/abs/1511.05389</a></span><span>&nbsp;learning to retrieve out-of-vocabulary words - non-neural</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_crt5jd7c2xq0-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06581&amp;sa=D&amp;ust=1465142037945000&amp;usg=AFQjCNHWvT6TkIXhv9qs_dYWOu-6TLCHFQ">http://arxiv.org/abs/1601.06581</a></span><span>&nbsp;ctc rnn for speech to characters, and a per character language model for modeling long term dependency. Works quite well, but I don&#39;t know where speech recognition sota is. Also mentions datasets, which are almost certainly not open access. Interesting for what audio signal they give the rnn, too - is it convolution over time?</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.01032&amp;sa=D&amp;ust=1465142037946000&amp;usg=AFQjCNElZ2dEA3_Pzi8H7SAvCRXtFOGsMA">http://arxiv.org/abs/1510.01032</a></span><span>&nbsp;deep convolutional stuff for making embeddings of speech audio. Involves convolution over time.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06407&amp;sa=D&amp;ust=1465142037947000&amp;usg=AFQjCNGENVb-1RiOtmL9gZEAI-jMqSKx4Q">http://arxiv.org/abs/1511.06407</a></span><span>&nbsp;distance recognition with rnns and multi microphones</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02539&amp;sa=D&amp;ust=1465142037947000&amp;usg=AFQjCNG39Aw-ITxcmUOxfq1TVCnj_ANdmQ">http://arxiv.org/abs/1601.02539</a></span><span>&nbsp;comparisons of gating techniques for using rnns for speech synthesis; should also go in &quot;memory&quot; or &quot;recurrence&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02553&amp;sa=D&amp;ust=1465142037948000&amp;usg=AFQjCNEn_7F2UAVL16fsT4oGjGYLhBh2Dw">http://arxiv.org/abs/1601.02553</a></span><span>&nbsp;speech recognition normalization by using an (unsupervised?) embedding of a noise environment as input to the feed forward network, claims major improvement in sota of noisy speech recognition. Should also go in &quot;unsupervised&quot; or &quot;representation&quot;</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.7p8t3kailgbz"><span>Embeddings</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_6yojqqjec0st-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00893&amp;sa=D&amp;ust=1465142037949000&amp;usg=AFQjCNF4T1ddHrp1lyEEwcyfXEOcsVHMfw">http://arxiv.org/abs/1601.00893</a></span><span>&nbsp;&quot;The Role of Context Types and Dimensionality in Learning Word Embeddings&quot;; looks cool, a bit of theory about what makes good word embeddings - dimensionality, etc - and then proposes a new model.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_rwqhz1uwdird-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01343&amp;sa=D&amp;ust=1465142037950000&amp;usg=AFQjCNGgYKPNKYhhUwAl0oTc4HDdJkF9qg">http://arxiv.org/abs/1601.01343</a></span><span>&nbsp;embeddings for name lookup on things like wikipedia, etc. works well, comes with pretrained vectors, and mentions interesting datasets.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_d4lbyvb2g3j-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.aclweb.org/anthology/D15-1177&amp;sa=D&amp;ust=1465142037951000&amp;usg=AFQjCNGPnOSHOSNijXU-V4isLqI-5_1oBQ">http://www.aclweb.org/anthology/D15-1177</a></span><span>&nbsp;word embedding learning system that is intended to learn embeddings that compose well with each other, to improve the creation of paragraph vectors and such. looks fine. cites multi-sense word embeddings. claims to not depend on dependencies? - arxiv: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1508.02354.pdf&amp;sa=D&amp;ust=1465142037951000&amp;usg=AFQjCNHvKZ-jnUjjAGuNYH55tZUG53z5bg">http://arxiv.org/pdf/1508.02354.pdf</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.kbjtyax3ahb"><span>&quot;Language Models&quot; - next word prediction and etc</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_fk4cszd8thar-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06732&amp;sa=D&amp;ust=1465142037953000&amp;usg=AFQjCNG6ZBnY6Xgreg9QpSrODHrNqLTXLA">http://arxiv.org/abs/1511.06732</a></span><span>&nbsp;a better objective function for language models, based on optimizing the metric used to evaluate language quality? mentions something about generating the whole sequence at once or something, like a good application of the cgru would. should also go in &quot;rl&quot;, apparently. </span><span class="c9 c12">major improvement in sota.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_jys0trw4gd1m-0 start"><li class="c6 c7"><span>Training very large per word language models - not skimmed yet. &quot;blackout&quot;. </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06909&amp;sa=D&amp;ust=1465142037954000&amp;usg=AFQjCNHsVVWkIegtM9Yh69aJ6iIkZJFdWw">http://arxiv.org/abs/1511.06909</a></span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_wq1o6elz9hy4-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00248&amp;sa=D&amp;ust=1465142037955000&amp;usg=AFQjCNG58n19twOUb8axb68ZgBgwlhYnGA">http://arxiv.org/abs/1601.00248</a></span><span>&nbsp;new loss function for &quot;sentence level&quot; language models. Competes with perplexity. Technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_6asjiojyuog6-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03729&amp;sa=D&amp;ust=1465142037956000&amp;usg=AFQjCNEhKfPbK9YEE70MESGmupMxI_lT3g">http://arxiv.org/abs/1511.03729</a></span><span>&nbsp;additional long term context for lstm language models via &quot;late fusion&quot;; technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ukuyvb7r2sb4-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08849&amp;sa=D&amp;ust=1465142037957000&amp;usg=AFQjCNEDcbf9L7d2GWhcsb4e4hxdpx86aA">http://arxiv.org/abs/1512.08849</a></span><span>&nbsp;LSTM for &quot;natural language inference&quot;, new dataset from stanford! dataset/application/technique (also copied to &quot;datasets&quot;)</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_gg06s1qsfn23-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.01006&amp;sa=D&amp;ust=1465142037957000&amp;usg=AFQjCNG6U6LjnJwOYNLxMZwR1OpN_T1m4A">http://arxiv.org/abs/1508.01006</a></span><span>&nbsp;? Some language modeling with rnns thing? They seem to think only cnns have been tried? Application?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_n21asioviqe4-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06038&amp;sa=D&amp;ust=1465142037980000&amp;usg=AFQjCNEw3ciDokYjxFz1CT2Jklk_wKrWfA">http://arxiv.org/abs/1511.06038</a></span><span>&nbsp;variational generative language model or something. need to read. also in &quot;generative&quot; and &quot;bayesian&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_hl5oa0f6tf4f-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.00060&amp;sa=D&amp;ust=1465142037982000&amp;usg=AFQjCNEpxiAvmKSEV9_sLWjtBChp5c2s3A">http://arxiv.org/abs/1511.00060</a></span><span>&nbsp;something something trees something lstms. tree-prediction lstms or something. doesn&#39;t look like it&#39;s actually tree-structured itself, it just predicts tree data? Might be tree structured, though. </span><span class="c9 c12">Major improvement in SOTA.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_p7d8xi28lme6-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06733&amp;sa=D&amp;ust=1465142037983000&amp;usg=AFQjCNGbSTvrQpdagJ-OVEeJQTS6kQ1U2A">http://arxiv.org/abs/1601.06733</a></span><span>&nbsp;language modeling with attentional lstm with memory &quot;tapes&quot;. Also belongs in &quot;memory&quot;. Looks cool. Potential big deal, but it&#39;s probably just another failed approach to memory. Read me.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_p7d8xi28lme6-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1511.02793v1.pdf&amp;sa=D&amp;ust=1465142037985000&amp;usg=AFQjCNEgYXeweSrCTzEEPDHHNROwl7NKKQ">http://arxiv.org/pdf/1511.02793v1.pdf</a></span><span>&nbsp;image generation from sentences. Application. also in &quot;generative&quot;, &quot;images&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://daendinam.github.io/projects/dota-nn.html&amp;sa=D&amp;ust=1465142037986000&amp;usg=AFQjCNG0VZavxmUV1vtyZZxY9ux_ZrlCMQ">https://daendinam.github.io/projects/dota-nn.html</a></span><span>&nbsp;analysis of transfer learning in language models. should also go in &quot;transfer learning&quot;.</span></li><li class="c6 c7"><span>Apr 3, 16</span><span><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037987000&amp;usg=AFQjCNHgSZ_5VAxWD-uAVl7e9czyqkd0JA">&nbsp;</a></span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08983&amp;sa=D&amp;ust=1465142037987000&amp;usg=AFQjCNHgSZ_5VAxWD-uAVl7e9czyqkd0JA">http://arxiv.org/abs/1603.08983</a></span><span>&nbsp;[abs:5] this is what we&#39;re here for! Alex graves hits it out of the park again, with RNNs that have variable compute time *per output*. This is the shit. Should go in &quot;recurrence&quot;, &quot;attention&quot; (it&#39;s kind of attention over compute time), &quot;generality&quot;, &quot;rl&quot;, &quot;language/language modeling&quot;, &quot;architectures&quot;. Major improvement on hutter prize dataset, apparently. Would be interesting to combine this with some sort of logarithmically decaying implicit memory, or just explicit memory. Maybe one of the recurrent residual things. </span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.mw159jot9hf6"><span>Machine Translation</span></h2><p class="c6"><span class="c38">I haven&#39;t read any of these. I don&#39;t really care about machine translation right now. they might be relevant to other things I&#39;m doing, though, so I&#39;m keeping them around with keywords.</span></p><p class="c3"><span class="c38"></span></p><ul class="c2 lst-kix_3fvzyqsgm5ef-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00372&amp;sa=D&amp;ust=1465142037989000&amp;usg=AFQjCNHHaNODlnC7Hu4VduXTq1FlRxAvFg">http://arxiv.org/abs/1601.00372</a></span><span>&nbsp;neural machine translation tweaks, uses a mutual objective, a custom decoder thing, and something else I didn&#39;t understand from the abstract. &quot;Mutual information&quot; comes up, which is cool. Should also go in &quot;objectives&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_o9z4cf2yq4mz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04811&amp;sa=D&amp;ust=1465142037990000&amp;usg=AFQjCNG_fqLHev_xKXqsJHYU3Y4sAW0nUg">http://arxiv.org/abs/1601.04811</a></span><span>&nbsp;neural machine translation with attention - they keep a history of attention so it knows what it&#39;s already done. It helps.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_dj4fsxw52t4l-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00710&amp;sa=D&amp;ust=1465142037990000&amp;usg=AFQjCNGBFLaUFl84xGVYhvZSzDELCQVV0w">http://arxiv.org/abs/1601.00710</a></span><span>&nbsp;more neural machine translation - &quot;maximize the probability of an english sentence given the french and german sentences&quot;. sounds like just normal encoder-decoder training? w/e. claims to improve sota. looks like a pretty meh technique, interesting but not earth-shattering.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_emewkhtp1rfy-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01073&amp;sa=D&amp;ust=1465142037991000&amp;usg=AFQjCNFEp4nEn82e86MPAgxA5XviZiYO9A">http://arxiv.org/abs/1601.01073</a></span><span>&nbsp;neural machine translation with attention that is generic between languages. gets a handle on languages with less data faster.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_gswyhyotsm36-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01085&amp;sa=D&amp;ust=1465142037992000&amp;usg=AFQjCNGo2UHtzS0WF3HhCWGgorCoZkOpyQ">http://arxiv.org/abs/1601.01085</a></span><span>&nbsp;giving attentional neural translation priors found to be useful in non-neural translation. alignment, etc.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02502&amp;sa=D&amp;ust=1465142037992000&amp;usg=AFQjCNGHREIP7_CiGfxKAAY1bFWCBh-bpw">http://arxiv.org/abs/1601.02502</a></span><span>&nbsp;multi-language word embeddings. something something word translation, classification. should also go in &quot;embeddings&quot;.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h1 class="c5 c23 c30" id="h.r00m73ve5ste"><span></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c5 c23 c30" id="h.uo8rfc8o7xny"><span></span></h1><h1 class="c5" id="h.81zsyxnl4oo6"><span>Application</span></h1><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h2 class="c5" id="h.tnxvdbf93ov8"><span>Datasets</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_8fyyex2aqhl9-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08849&amp;sa=D&amp;ust=1465142037998000&amp;usg=AFQjCNFB5w6eW_5N7OJUqIsRX7wT63aEOw">http://arxiv.org/abs/1512.08849</a></span><span>&nbsp;LSTM for &quot;natural language inference&quot;, new dataset from stanford! dataset/application/technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_7apmk6vwlj5r-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm&amp;sa=D&amp;ust=1465142037999000&amp;usg=AFQjCNF-h1ct_qtK4oAL5455tfN6s7urOQ">http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm</a></span><span>&nbsp;dataset of a crapton of blog posts. warning, old data. freely available.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_bcdpfzfy81p5-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://science.sciencemag.org/content/350/6266/1332&amp;sa=D&amp;ust=1465142037999000&amp;usg=AFQjCNHMGNQfzokZ-zvRbn_QbMVup-qPsw">http://science.sciencemag.org/content/350/6266/1332</a></span><span>&nbsp;is the annoyingly-over-linked bayesian program learning paper. but it&#39;d be interesting to see how well a neural network could do on this dataset with bayesian approaches. the compression is going to make it a bit worse, but the architecture will take some work too.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_j24rsp966ll1-0 start"><li class="c6 c7"><span>Dataset wish list:</span></li></ul><ul class="c2 lst-kix_j24rsp966ll1-1 start"><li class="c6 c22"><span>Imagenet</span></li><li class="c6 c22"><span>Ms coco - freely available: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://mscoco.org/dataset/%23overview&amp;sa=D&amp;ust=1465142038001000&amp;usg=AFQjCNHzKhnnp8_JH2Qij1F82l70-sPnvA">http://mscoco.org/dataset/#overview</a></span><span>&nbsp;</span></li><li class="c6 c22"><span>Flickr30k</span></li><li class="c6 c22"><span>MIT places (might be open)</span></li><li class="c6 c22"><span>Young children&#39;s novels - </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02301&amp;sa=D&amp;ust=1465142038002000&amp;usg=AFQjCNEGTSbH7WVHdJw3867JZbo5_R29-A">http://arxiv.org/abs/1511.02301</a></span><span>&nbsp;</span></li><li class="c6 c22"><span>Gutenburg project?</span></li><li class="c6 c22"><span>Simple English wikipedia</span></li><li class="c6 c22"><span>Movies and their transcriptions</span></li><li class="c6 c22"><span>Movie scene label thing</span></li><li class="c6 c22"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://webscope.sandbox.yahoo.com/&amp;sa=D&amp;ust=1465142038003000&amp;usg=AFQjCNGSP3aSUEH8kYWHEib20FeH4cL6cA">http://webscope.sandbox.yahoo.com/</a></span><span>&nbsp;has a bunch of stuff</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_defeme42l2ge-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04805&amp;sa=D&amp;ust=1465142038004000&amp;usg=AFQjCNFoNle8gXfYwy6vh2_wCwHFsTNBnw">http://arxiv.org/abs/1601.04805</a></span><span>&nbsp;micro expression detection datasets, also in sparse coding section</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pfssn45qm13m-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00825&amp;sa=D&amp;ust=1465142038005000&amp;usg=AFQjCNE6yP5Wjrscn62evbk9YyEyHHm--Q">http://arxiv.org/abs/1601.00825</a></span><span>&nbsp;gamifying object localization data collection, should also go in &quot;data collection&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_bj2se7l3xxv-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://ufldl.stanford.edu/housenumbers/&amp;sa=D&amp;ust=1465142038005000&amp;usg=AFQjCNH0845l28xA54kbu_k0p1jnCXq9IA">http://ufldl.stanford.edu/housenumbers/</a></span><span>&nbsp;street view house numbers - would be a good starting point for text recognition, very robust</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_8cma8sbxfy2x-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/420iw9/analyzing_50k_fonts_using_deep_neural_networks/&amp;sa=D&amp;ust=1465142038006000&amp;usg=AFQjCNFembqV06ST5MfJZSq3GjIkXNrxkg">https://www.reddit.com/r/MachineLearning/comments/420iw9/analyzing_50k_fonts_using_deep_neural_networks/</a></span><span>&nbsp;fonts dataset</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_rbhmpzgzmhdz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://blog.kaggle.com/2016/01/19/introducing-kaggle-datasets/&amp;sa=D&amp;ust=1465142038007000&amp;usg=AFQjCNERqvo5VvfDhfiIvrmt4WnRXENSBA">http://blog.kaggle.com/2016/01/19/introducing-kaggle-datasets/</a></span><span>&nbsp;kaggle has some stuff. didn&#39;t read in detail. w/e.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_8g8vlw6j0asg-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00998&amp;sa=D&amp;ust=1465142038008000&amp;usg=AFQjCNEijT5JuwpA86WUfCtu5jkvZi8jdA">http://arxiv.org/abs/1601.00998</a></span><span>&nbsp;human navigation prediction in social environments dataset and application. didn&#39;t read the paper, don&#39;t know what they applied. interesting dataset, though. is it open access?</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_qp3zt329w8t5-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04119&amp;sa=D&amp;ust=1465142038009000&amp;usg=AFQjCNGN55Pu6cbnhNVt8m6-Y4nvn8liQQ">http://arxiv.org/abs/1511.04119</a></span><span>&nbsp;occlusion detection dataset and technique. also in &quot;images/3d&quot;</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_fjl32nwuhu4u-0 start"><li class="c6 c7"><span>not neural, overview of pose tracking: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01006&amp;sa=D&amp;ust=1465142038011000&amp;usg=AFQjCNEFa3uW0s3z3O5ly_UUWLkt8nclLA">http://arxiv.org/abs/1601.01006</a></span><span>&nbsp;- probably has good datasets and maybe neural papers linked. also in &quot;images/3d&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_dowwk88w0dao-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.06825&amp;sa=D&amp;ust=1465142038012000&amp;usg=AFQjCNHyfM-uC7yqOC9Mz_X0DPLEXEXyrQ">http://arxiv.org/abs/1506.06825</a></span><span>&nbsp;google street view type thing stereo reprojection/interpolation. dunno what to call it. it&#39;s weird. wtf is a plane sweep volume, anyway? this would be a cool application to reproduce if I can get the dataset they used. application. also in &quot;images/3d&quot;.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_jzarhjey1jrh-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04119&amp;sa=D&amp;ust=1465142038013000&amp;usg=AFQjCNEZrrZ3ZDjFIUq_nWeU5W8hJHZVPw">http://arxiv.org/abs/1511.04119</a></span><span>&nbsp;action classification with attentional RNNs. also in &quot;images&quot;.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_3de1jg1kuy24-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://bamos.github.io/2016/01/19/openface-0.2.0/&amp;sa=D&amp;ust=1465142038016000&amp;usg=AFQjCNGi_qX7aCkxip7Oh77Qc8pgJTgFHQ">http://bamos.github.io/2016/01/19/openface-0.2.0/</a></span><span>&nbsp;project to do open source face recognition. has links to open source face datasets, torch code, and a research log about their optimization. looks like a full application, intended to actually compete, and as such has a lot of tricks to extract. comes with a large pretrained model. also in &quot;images&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_7qzrf8aug8gu-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06931&amp;sa=D&amp;ust=1465142038018000&amp;usg=AFQjCNGgOnli6xz4ZX6LN9iUb-p0ReoCeg">http://arxiv.org/abs/1511.06931</a></span><span>&nbsp;dataset for movie/sentence language challenges. also in &quot;language&quot;. </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_yhmx20qr5k0e-0 start"><li class="c6 c7"><span>what if you make a predictor for a manually designed completion system, such as YouCompleteMe? You&#39;d be teaching it language, but since it&#39;s formally specified you&#39;d be able to generate as many training examples as you like. Then you train it with much bigger updates towards the results you actually care about, and it starts to predict your behavior too.</span></li><li class="c6 c7"><span>lots of good shit here </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/datasets/top/&amp;sa=D&amp;ust=1465142038021000&amp;usg=AFQjCNEPL7MN9g_ZHjycveTdSFwm5ewAOw">https://www.reddit.com/r/datasets/top/</a></span><span>&nbsp;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01917&amp;sa=D&amp;ust=1465142038024000&amp;usg=AFQjCNGSKLeG74MMz8pgQQdTVa0wQ_D5yw">http://arxiv.org/abs/1601.01917</a></span><span>&nbsp;might be available - song play history; another: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01892&amp;sa=D&amp;ust=1465142038025000&amp;usg=AFQjCNGkXF2JXgp25L7z2cMJE4XEgKVCoQ">http://arxiv.org/abs/1601.01892</a></span><span>&nbsp;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02403&amp;sa=D&amp;ust=1465142038026000&amp;usg=AFQjCNHhfH01fKvPkpm0Cz3SFOKHGhbGpQ">http://arxiv.org/abs/1601.02403</a></span><span>&nbsp;web argument discourse dataset under an open license, with some analysis.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/help/bulk_data&amp;sa=D&amp;ust=1465142038027000&amp;usg=AFQjCNHCpyPu1QDghaD3HWMn0FsZn4XqpA">http://arxiv.org/help/bulk_data</a></span><span>&nbsp;arxiv has their dataset on s3! and it&#39;s fucking massive!</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.bonappetit.com/entertaining-style/trends-news/article/how-ibm-chef-watson-works&amp;sa=D&amp;ust=1465142038028000&amp;usg=AFQjCNH9OapQg8uGQuV4KkRXnsGn46wtcw">http://www.bonappetit.com/entertaining-style/trends-news/article/how-ibm-chef-watson-works</a></span><span>&nbsp;I want this dataset</span></li><li class="c6 c7"><span>idea: train GAN on simplex noise as first bootstrap for terrain gen</span></li><li class="c6 c7"><span>lots of datasets </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/caesar0301/awesome-public-datasets&amp;sa=D&amp;ust=1465142038033000&amp;usg=AFQjCNGe7KrhHC8y7uzeCw67xczisgJxSg">https://github.com/caesar0301/awesome-public-datasets</a></span><span>&nbsp;don&#39;t know if any are good</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://research.facebook.com/research/-babi/&amp;sa=D&amp;ust=1465142038033000&amp;usg=AFQjCNGP7bmdZNWDRDS6chh7yaI_ceQB7Q">https://research.facebook.com/research/-babi/</a></span><span>&nbsp;they have a children&#39;s books dataset!!!!! and movie dialog, and other cool stuff. paper: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.05698&amp;sa=D&amp;ust=1465142038034000&amp;usg=AFQjCNE6GVxC4kE5XNpuTxJWe-IpExa3Sw">http://arxiv.org/abs/1502.05698</a></span><span>&nbsp;[abs][conf]</span></li><li class="c6 c7"><span>Apr 4 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://archive.org/details/stackexchange&amp;sa=D&amp;ust=1465142038035000&amp;usg=AFQjCNF3yG2E_SPe4WgaF9MUu350MMiBlg">https://archive.org/details/stackexchange</a></span><span>&nbsp;[skim:1] database of questions and answers</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.gtrf4vrar4ze"><span>Bio modeling (and bio context/pure bio papers)</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_vjhjbjs9ubca-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05986&amp;sa=D&amp;ust=1465142038036000&amp;usg=AFQjCNG_d6t5ildp2zT-QlLL-TR0gf0PLQ">http://arxiv.org/abs/1512.05986</a></span><span>&nbsp;comparison of two approaches for doing small dataset classification in medical imaging, transfer vs regularization. Application (duplicated to &quot;small datasets&quot;)</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ho1wvdl93240-0 start"><li class="c6 c7"><span>Gene processing, also references irnn, which is new, need to check that out </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05135&amp;sa=D&amp;ust=1465142038037000&amp;usg=AFQjCNHZYfo_oMDzSspGx-pa-jOoP6voig">http://arxiv.org/abs/1512.05135</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_gsefpdxx2m31-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://journals.plos.org/ploscompbiol/article?id%3D10.1371/journal.pcbi.1004295&amp;sa=D&amp;ust=1465142038038000&amp;usg=AFQjCNGhKbH-wR4H_X38ODvgyszIauCTrA">http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004295</a></span><span>&nbsp;not neural, not deep: ga based algorithms for inferring behaviour of dna (I think?). Interesting potential application for a neural network!</span></li></ul><p class="c3"><span></span></p><p class="c3"><span class="c20"></span></p><p class="c6"><span class="c20 c24">these next five from </span><span class="c14 c20"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/401x3o/what_are_the_coolest_ml_applications_in_genomic/&amp;sa=D&amp;ust=1465142038039000&amp;usg=AFQjCNGNZg5lsqQV_-3oMW2qdmOnHyEviQ">https://www.reddit.com/r/MachineLearning/comments/401x3o/what_are_the_coolest_ml_applications_in_genomic/</a></span><span class="c24 c26">&nbsp;</span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_xrce7kijh7l1-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.nature.com/nrg/journal/v16/n6/full/nrg3920.html&amp;sa=D&amp;ust=1465142038040000&amp;usg=AFQjCNGJMJpmbOgAjWk3V4pYLg_N94YQ8g">http://www.nature.com/nrg/journal/v16/n6/full/nrg3920.html</a></span><span>&nbsp;review of uses for machine learning in genetics.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_74gx9bwnn8kt-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload%3Dtrue%26arnumber%3D7347331&amp;sa=D&amp;ust=1465142038041000&amp;usg=AFQjCNFpomiMcDKMZQMBK-eEW0UXjHkDsw">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber=7347331</a></span><span>&nbsp;another review, with datasets.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_r75b0bpeph0o-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.ncbi.nlm.nih.gov/pubmed/25525159&amp;sa=D&amp;ust=1465142038041000&amp;usg=AFQjCNEnqXrhP3TcMiZyQ7R_oIA28vkcYg">http://www.ncbi.nlm.nih.gov/pubmed/25525159</a></span><span>&nbsp;&quot;RNA splicing. The human splicing code reveals new insights into the genetic determinants of disease.&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_b8dlmlw0p2p1-0 start"><li class="c6 c7"><span>&quot;Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning &quot; </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html&amp;sa=D&amp;ust=1465142038042000&amp;usg=AFQjCNG7PX6kGs-L0YOhxeDAnpx7AI6EXg">http://www.nature.com/nbt/journal/v33/n8/full/nbt.3300.html</a></span></li></ul><p class="c3"><span class="c48"></span></p><ul class="c2 lst-kix_8swzovi1ytnr-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.deepgenomics.com/&amp;sa=D&amp;ust=1465142038043000&amp;usg=AFQjCNGBARCT6kcnX6Zy6uZngGVchI_Kdg">http://www.deepgenomics.com/</a></span><span>&nbsp;holy crap they made a startup that&#39;s exactly the right thing</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_58o57c52tmut-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00027&amp;sa=D&amp;ust=1465142038044000&amp;usg=AFQjCNEDkO3kVhjRJjGQ5B9_mSq7yTSRzA">http://arxiv.org/abs/1601.00027</a></span><span>&nbsp;overview of tissue analysis algorithms</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ut5bvwbkq6rt-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.05007&amp;sa=D&amp;ust=1465142038045000&amp;usg=AFQjCNFnmsE_D8fZm5mI4d6QEftufHN4Kg">http://arxiv.org/abs/1505.05007</a></span><span>&nbsp;something something gene expression clustering </span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_jhqr8d48vqqp-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07947&amp;sa=D&amp;ust=1465142038046000&amp;usg=AFQjCNEajgK6grrZHYjj0W64DU4BBun0Jw">http://arxiv.org/abs/1512.07947</a></span><span>&nbsp;not deep, not neural, but data efficient reconstruction of mri data. Technique and application.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_z1olrl2brpzr-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07951&amp;sa=D&amp;ust=1465142038047000&amp;usg=AFQjCNHch1HNAbjAauiQtxFWUPjZNVM9yw">http://arxiv.org/abs/1512.07951</a></span><span>&nbsp;convolution on mri. Application.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_d995nc7613a9-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08457&amp;sa=D&amp;ust=1465142038048000&amp;usg=AFQjCNH8M_g01GgDUsD7-luCmbgpHemkUw">http://arxiv.org/abs/1512.08457</a></span><span>&nbsp;neurobiology theory paper</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_8z696e8bv17-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07041&amp;sa=D&amp;ust=1465142038048000&amp;usg=AFQjCNFTK5pzsmhtzAysWgcS4wFxZCdW-Q">http://arxiv.org/abs/1512.07041</a></span><span>&nbsp;deep learning to detect brain tumors. Application.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_kkphisidn6cz-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05942&amp;sa=D&amp;ust=1465142038049000&amp;usg=AFQjCNG9BWy5VGSCqReSzTsnjLqbSiNRrw">http://arxiv.org/abs/1511.05942</a></span><span>&nbsp;predicting clinical events with neural networks - very very good results!</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_6d5ax9c8k1xa-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03677&amp;sa=D&amp;ust=1465142038050000&amp;usg=AFQjCNFKCfZsiiBiklvpbcCdaPit0IXDaA">http://arxiv.org/abs/1511.03677</a></span><span>&nbsp;predicting diagnoses with lstm networks on irregularly sampled data. </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_i1kls5tk4gt6-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06608&amp;sa=D&amp;ust=1465142038051000&amp;usg=AFQjCNEQSO5NWXAol4m-6-kH7B2m5Erz0g">http://arxiv.org/abs/1601.06608</a></span><span>&nbsp;something about recognizing things in the eye. Looks non neural and they get no mistakes on their test data. Must be an easy kind of image to recognize.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_h4ky6k6cvoue-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06476&amp;sa=D&amp;ust=1465142038052000&amp;usg=AFQjCNE5Muyv6lmP_5eet6PVfSjMT_DO9Q">http://arxiv.org/abs/1601.06476</a></span><span>&nbsp;non neural cancer mutation analysis.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06348&amp;sa=D&amp;ust=1465142038052000&amp;usg=AFQjCNH2mErjLhAJ1Y16isejJFld-xU3nQ">http://arxiv.org/abs/1511.06348</a></span><span>&nbsp;&quot;How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?&quot;, maybe also should go in &quot;data efficiency&quot; or somewhere like that </span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06448&amp;sa=D&amp;ust=1465142038053000&amp;usg=AFQjCNHo-UziUW78__IKOOk-qBvVmCec0w">http://arxiv.org/abs/1511.06448</a></span><span>&nbsp;representation learning on EEG data. also in &quot;unsupervised/representation&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04306&amp;sa=D&amp;ust=1465142038054000&amp;usg=AFQjCNEhCgjHpCkVsidCPHi7-J-p2n4Dyg">http://arxiv.org/abs/1511.04306</a></span><span>&nbsp;deep learning for eeg decoding. should also go in &quot;small datasets&quot; or &quot;data efficiency&quot;.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02197&amp;sa=D&amp;ust=1465142038054000&amp;usg=AFQjCNGj022dULePdqm3-BFvhdRab-Jpiw">http://arxiv.org/abs/1601.02197</a></span><span>&nbsp;emotion recognition in EEGs</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.01865&amp;sa=D&amp;ust=1465142038055000&amp;usg=AFQjCNHlWcBrg_v9J8-KT65yJ6iVk8QRYQ">http://arxiv.org/abs/1511.01865</a></span><span>&nbsp;CNNs for detecting autism</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.v7kbh9jgn44q"><span>Finance</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_gpgigcfykzff-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06228&amp;sa=D&amp;ust=1465142038057000&amp;usg=AFQjCNHlVAY8DLM3h1G14Rpu213g7PryWQ">http://arxiv.org/abs/1512.06228</a></span><span>. - neural networks for bond prices prediction application</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.1tlnq252pxyh"><span>Cars</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_nc0l9nyccr86-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07080&amp;sa=D&amp;ust=1465142038058000&amp;usg=AFQjCNEjrXlGJDwOe2Rp7lrFSjvN47_X6A">http://arxiv.org/abs/1512.07080</a></span><span>&nbsp;vehicle occupant detection for use in smart car safety systems. Application.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_b474o1tvhevy-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01145&amp;sa=D&amp;ust=1465142038059000&amp;usg=AFQjCNFewH-V-sXOrzzuLCB5eHJU1ayNqQ">http://arxiv.org/abs/1601.01145</a></span><span>&nbsp;vehicle classification when driving. techniques unclear, uses augmented features or something. mentions small-dataset work.</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.a4jnlopyarot"><span>Misc applications</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_ly0lmz6fcokr-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00599&amp;sa=D&amp;ust=1465142038060000&amp;usg=AFQjCNFYmz10hDS78lLRXG2qL-pwPko9qw">http://arxiv.org/abs/1601.00599</a></span><span>&nbsp;social event classification - image and text</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_nbuhcgd4ft21-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00770&amp;sa=D&amp;ust=1465142038062000&amp;usg=AFQjCNEnDzUi3QBiiAhtGACjRxns8MlLAQ">http://arxiv.org/abs/1601.00770</a></span><span>&nbsp;some tree structure rnn entity relationship detection thing. looks cool I guess, but it&#39;s not really clear what they&#39;re actually doing. Need to read still.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_cst6lp3lb98n-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06251&amp;sa=D&amp;ust=1465142038063000&amp;usg=AFQjCNF9Rhxc9UHc93sqBToJoayHvlEfwA">http://arxiv.org/abs/1601.06251</a></span><span>&nbsp;something about classifying or generating Persian text.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_xqqiyb3b5ag0-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06248&amp;sa=D&amp;ust=1465142038063000&amp;usg=AFQjCNGyZ40qq7CyJKfBlUcvajkVk4-TNg">http://arxiv.org/abs/1601.06248</a></span><span>&nbsp; bird song recognition, uses both neural and non neural, should also go in &quot;speech&quot;</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01675&amp;sa=D&amp;ust=1465142038064000&amp;usg=AFQjCNEVapUQcbNQujpXzr4PL_jsPvo_6Q">http://arxiv.org/abs/1601.01675</a></span><span>&nbsp;power systems, important for global warming recovery and survival; non neural, just decision forests</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06660&amp;sa=D&amp;ust=1465142038065000&amp;usg=AFQjCNH35foSPuNh_mI798MuEV6dbGNWQA">http://arxiv.org/abs/1511.06660</a></span><span>&nbsp;demographic from metadata prediction for phones</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02376&amp;sa=D&amp;ust=1465142038065000&amp;usg=AFQjCNHn4-R_foK9KLZi2f_df6Knoj0FFw">http://arxiv.org/abs/1601.02376</a></span><span>&nbsp;multi-field learning - predicting user responses to forms and such. Has approaches for properly combining interactions, using different models. looks interesting. should also go in &quot;generality&quot;, maybe. not sure where to put this, but it&#39;s pretty cool.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07111&amp;sa=D&amp;ust=1465142038066000&amp;usg=AFQjCNHa5uPQwmXwJCQgI3M82wDjiQzn7A">http://arxiv.org/abs/1511.07111</a></span><span>&nbsp;robotics thing for generalization. maybe deep?</span></li></ul><p class="c3"><span></span></p><h1 class="c5" id="h.qqbvb5bug2vy"><span>My ideas</span></h1><p class="c3"><span></span></p><p class="c6"><span>Idea for char-cgru: tag fields with learned vectors that represent what they are, adjacently, so it knows where to send things. Still need some sort of global context though, haven&#39;t there been papers recently for global context in cnns?</span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_c2vvnsz0drek-0 start"><li class="c6 c7"><span>variable size memory problem: I think variable-size maxpooling will do well as an input layer. two layers of non-recurrent low-dimensional (128?) cnn over the text, then maxpool maybe with in-vector inhibition - local response normalization? some form of wta within a cell - maybe with a few layers so it isn&#39;t all at once. then once simplified to a single view, use as sentence summary. with any luck, references will be stored as references? hm, don&#39;t want to erase away reference words with this. the error signals will help with that, but it seems likely that references should be resolved on-the-spot. maybe convolutional reference resolution of some kind? perhaps we should do quite a few layers of processing on a single sentence, with read-only memory of some form of context available to each layer in some spatially independent way? maybe each cell gets some sort of attention for resolving references?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_kxpudurk2j6e-0 start"><li class="c6 c7"><span>is there some sort of lookup network you could make that, given a few layers of processing for one vector to figure out what it&#39;s trying to find, resolves references in memory based on some sort of indexable word vector? maybe you could train on wordnet &quot;is an&quot; relationships, so that it predicts possible completions, and then &hellip; no, you still need reinforcement, you can&#39;t evade the need for refining which word based on context, or it&#39;ll get confused when you&#39;re talking about two similar concepts.</span></li></ul><p class="c3"><span></span></p><p class="c6"><span>for now, just variable-size spatial maxpooling.</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_cy2og2rt03zo-0 start"><li class="c6 c7"><span>code completion:</span></li></ul><ul class="c2 lst-kix_u6q708mtjbvx-1 start"><li class="c6 c22"><span>predict what y</span><span>oucompleteme will output - </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/Valloric/ycmd/blob/master/examples/example_client.py&amp;sa=D&amp;ust=1465142038071000&amp;usg=AFQjCNHy5i-hYzU_L2iuY6CNkTBHg5rrJA">https://github.com/Valloric/ycmd/blob/master/examples/example_client.py</a></span></li><li class="c6 c22"><span>predict what was actually there</span></li><li class="c6 c22"><span>predict what change someone would make given a pre-change codebase, a partial change, and a curso</span><span>r location</span></li></ul><p class="c3"><span></span></p><h1 class="c5 c23" id="h.s3bvdh988zuh"><span></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c5 c23" id="h.w22j7jhn6dh7"><span></span></h1><h1 class="c5" id="h.h3p6h2d5ea4z"><span>Misc</span></h1><h2 class="c5" id="h.npvvzxphkp0c"><span>unsure how to classify/where it&#39;s useful</span></h2><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_yv7c7gf6elha-0 start"><li class="c6 c7"><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1409.3970&amp;sa=D&amp;ust=1465142038074000&amp;usg=AFQjCNH1-18EU7KLmQER39uKtsA5tFguWQ">http://arxiv.org/abs/1409.3970</a></span><span>&nbsp;(arXiv admin note: substantial text overlap with arXiv:1305.5306) &quot;A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data&quot; should also go in &quot;language/topic modeling&quot;</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_p2mkzdwo7q6r-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05897&amp;sa=D&amp;ust=1465142038075000&amp;usg=AFQjCNHz9xxRpk1p_yYmRmrQQjyX_jwJvw">http://arxiv.org/abs/1511.05897</a></span><span>&nbsp;adverserial model for representation learning of representations that erase away the influence from a censored variable, such that it cannot be predicted from them. should go in &quot;security&quot;/&quot;privacy&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_mm2j3zumujcl-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02136&amp;sa=D&amp;ust=1465142038076000&amp;usg=AFQjCNGjNd8pzimTAqiUZBYoZhtHG8ao4Q">http://arxiv.org/abs/1511.02136</a></span><span>&nbsp;graph analysis with CNNs. should go in &quot;graphs and trees&quot;</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.k1ivj11rs1r7"><span>Alternate/fringe deep techniques</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_xkmlxx8lhm5d-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05840&amp;sa=D&amp;ust=1465142038078000&amp;usg=AFQjCNHA6IefSKhUTfXWqJIyOs9FDWAovQ">http://arxiv.org/abs/1512.05840</a></span><span>&nbsp;potential non neural deep architecture theory</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_8nsyajlscxcg-0 start"><li class="c6 c7"><span>cireneikual&#39;s thing, NeoRL - stacked recurrent sparse-coding-input-based temporal autoencoders (hey cool you can describe his thing as buzzwords actually-meaningfully)</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_rrs2j0rau6wp-0 start"><li class="c6 c7"><span>Numenta is still trying </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00191&amp;sa=D&amp;ust=1465142038079000&amp;usg=AFQjCNHycYKG_HjrAJq7r7bhhtH6W2If9Q">http://arxiv.org/abs/1601.00191</a></span><span>&nbsp;</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_rwfcya85pd71-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08065&amp;sa=D&amp;ust=1465142038081000&amp;usg=AFQjCNFSdsz29xe95_zVYDrZ1uAsEF3vnA">http://arxiv.org/abs/1512.08065</a></span><span>&nbsp;deep Gaussian processes. Non neural. Technique. may be dramatically more data efficient, probably doesn&#39;t scale for performance reasons.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h2 class="c5" id="h.gdmektcr4m6y"><span>Misc non-ML or non-deep</span></h2><p class="c3"><span class="c18"></span></p><ul class="c2 lst-kix_7b2y41o7s2k3-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.07194&amp;sa=D&amp;ust=1465142038083000&amp;usg=AFQjCNF-yMTzaA1uBbWFh09gtmgv0hmBRA">http://arxiv.org/abs/1506.07194</a></span><span>&nbsp;not neural, introduction to eye movement analysis techniques. Probably a good paper to read to get some stats based cv context. Theory / misc</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_7jzvzpyn9bnk-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.import.io/post/how-to-win-a-kaggle-competition/&amp;sa=D&amp;ust=1465142038084000&amp;usg=AFQjCNGoajR2tgxWH8DIm9WDc_jvH9aaAA">https://www.import.io/post/how-to-win-a-kaggle-competition/</a></span><span>&nbsp;blog post about how to win at kaggle. </span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_ftgmvnn66fx0-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08064&amp;sa=D&amp;ust=1465142038085000&amp;usg=AFQjCNGY17auKAsZDRwHSNbBOJj1KcEPVA">http://arxiv.org/abs/1512.08064</a></span><span>&nbsp;not deep or neural, but mentions giving the model the chance to observe loss on its predictions, which is interesting</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_5x48r7dz2gvp-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08178&amp;sa=D&amp;ust=1465142038086000&amp;usg=AFQjCNG_ztW9qJy6qWNMJYlEG76QEYdPDg">http://arxiv.org/abs/1512.08178</a></span><span>&nbsp;not deep, not neural, not theoretically interesting, but is an interesting application that was mentioned by the page about technical solutions to climate change. Misc.</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_uest4hng52tf-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04943&amp;sa=D&amp;ust=1465142038087000&amp;usg=AFQjCNFrSgrD9AdZJG_IMyknj4hCutnQNA">http://arxiv.org/abs/1601.04943</a></span><span>&nbsp;some random probabilistic programming paper</span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_r8labycqo9zl-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04589&amp;sa=D&amp;ust=1465142038088000&amp;usg=AFQjCNEkdfjbxQxGetED4bXrNaJjC-LD8w">http://arxiv.org/abs/1601.04589</a></span><span>&nbsp;generative image model with non-neural model (markov random fields) to stabilize it a bit. looks pretty cool. also in &quot;generative modeling&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_i5pnhlfopov3-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06245&amp;sa=D&amp;ust=1465142038089000&amp;usg=AFQjCNGGPsTdKID354Ih19a-YUE7sBZeNA">http://arxiv.org/abs/1601.06245</a></span><span>&nbsp;just amusing - book draft on arxiv. Some gofai persuasion thing.</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01887&amp;sa=D&amp;ust=1465142038090000&amp;usg=AFQjCNGIiER3y4F6-k7MqQahQ62toanP3g">http://arxiv.org/abs/1601.01887</a></span><span>&nbsp;research compared to social networks in terms of exponential growth, classification tool for finding research. Hopefully they start a company based on this, the researchversion is totally not going to get used</span></li><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01700&amp;sa=D&amp;ust=1465142038093000&amp;usg=AFQjCNGt7kwhsTxzPZwm0kzvgh8uHDvrlg">http://arxiv.org/abs/1601.01700</a></span><span>&nbsp;statistically detecting true MDPs</span></li></ul><p class="c3"><span></span></p><h2 class="c5" id="h.c110sgu567wf"><span>Neuromorphic</span></h2><p class="c3"><span></span></p><ul class="c2 lst-kix_8hjy0hqh3rvg-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00701&amp;sa=D&amp;ust=1465142038094000&amp;usg=AFQjCNGujSpkYy_2p6MZIR40yr9O-BzTMw">http://arxiv.org/abs/1601.00701</a></span><span>&nbsp;hebbian learning in the brain&hellip; stuff. neuromorphic/bio neurons.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_1ig57j6lcs95-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00909&amp;sa=D&amp;ust=1465142038096000&amp;usg=AFQjCNHkgIroYOwRnooY2jGQH-cqQYt-JA">http://arxiv.org/abs/1601.00909</a></span><span>&nbsp;bayesian inference via sampling in the brain, and proposing low power inference for machine learning. neuromorphic/bio neurons.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_tlb25jkk5mkd-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05875&amp;sa=D&amp;ust=1465142038097000&amp;usg=AFQjCNHjiPtkYvCBjgRQPZU7YRrigJc72Q">http://arxiv.org/abs/1512.05875</a></span><span>&nbsp;example of using neural networks as a model for psychology</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_jbw6p6lajlkh-0 start"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.06944&amp;sa=D&amp;ust=1465142038097000&amp;usg=AFQjCNEBJyaJ_LyYxlsRsCzUJYKBS5hj_Q">http://arxiv.org/abs/1508.06944</a></span><span>&nbsp;model for how human neurons might have working memory.</span></li></ul><hr style="page-break-before:always;display:none;"><p class="c3"><span></span></p><h1 class="c5" id="h.wimcg21ftsty"><span>Inbox</span></h1><h2 class="c5" id="h.d298okv6ki79"><span>Inbox and Uncategorized</span></h2><p class="c3"><span></span></p><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0 start"><li class="c3 c7"><span></span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07851&amp;sa=D&amp;ust=1465142038100000&amp;usg=AFQjCNFQxa_92FLsxdg7dMqI6PBQE09M7Q">http://arxiv.org/abs/1512.07851</a></span><span>&nbsp;not neural; for use with small data and little compute resources. Predicting user app use. For the perspective.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07748&amp;sa=D&amp;ust=1465142038101000&amp;usg=AFQjCNFYDA6_LTxjJCtuJuSeJzMOherJrg">http://arxiv.org/abs/1512.07748</a></span><span>&nbsp;not neural; computer listening system. For the references.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02565&amp;sa=D&amp;ust=1465142038102000&amp;usg=AFQjCNEUA9PX1GVrXPe8a0sWw1sNqk_qDA">http://arxiv.org/abs/1506.02565</a></span><span>&nbsp;neural model selection with Bayesian svms(?) Technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07336&amp;sa=D&amp;ust=1465142038103000&amp;usg=AFQjCNEClFPWM2P5OiEwiiuHRsh3pIv6tg">http://arxiv.org/abs/1512.07336</a></span><span>&nbsp;? Non neural combined with neural for interpretability ? Technique ?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07344&amp;sa=D&amp;ust=1465142038104000&amp;usg=AFQjCNFlXdnJ3Pc9z-mRYGKyGVNVopal-g">http://arxiv.org/abs/1512.07344</a></span><span>&nbsp;new deconv generative modeling that actually works! Technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07143&amp;sa=D&amp;ust=1465142038105000&amp;usg=AFQjCNF593lzdREL32i9ourDhIFZ_r8TsQ">http://arxiv.org/abs/1512.07143</a></span><span>&nbsp;wearable video analysis with cnns. Application.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06974&amp;sa=D&amp;ust=1465142038106000&amp;usg=AFQjCNHj3CO3uBoc132MmBo_RMkCwlCQSA">http://arxiv.org/abs/1512.06974</a></span><span>&nbsp;neural, but the abstract doesn&#39;t say so. Debiasing human-centric data labels. Application/technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07108&amp;sa=D&amp;ust=1465142038106000&amp;usg=AFQjCNFeC6-gGQOU4Z9--7r8ZCSr34ie4w">http://arxiv.org/abs/1512.07108</a></span><span>&nbsp;survey of recent progress on cnns. Meta-analysis of technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07155&amp;sa=D&amp;ust=1465142038107000&amp;usg=AFQjCNHVvV38k4RwKA9_Sc3AJAxdzx5CtA">http://arxiv.org/abs/1512.07155</a></span><span>&nbsp;dataset collection of actions, action classification. Dataset and application.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07972&amp;sa=D&amp;ust=1465142038108000&amp;usg=AFQjCNGu5XdWpo7OXVz_ebqaH0tJOTaIRA">http://arxiv.org/abs/1511.07972</a></span><span>&nbsp;[skim abs:1]</span><span>&nbsp;not directly neural, is about embeddings. Compares different kinds of embeddings to brain functions. Neuroscience, theory, and technique. Should go in &quot;neuromorphic&quot;, &quot;memory&quot;, &quot;representations&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05742&amp;sa=D&amp;ust=1465142038109000&amp;usg=AFQjCNHS5wRAYx3ixGBA84b61pH7rmSR7w">http://arxiv.org/abs/1512.05742</a></span><span>&nbsp;survey of datasets relevant for dialog and presumably speech; details</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.07583&amp;sa=D&amp;ust=1465142038110000&amp;usg=AFQjCNGqb5aFHJNj8WBXw1bLIoWCtfWgiQ">http://arxiv.org/abs/1507.07583</a></span><span>&nbsp;comparison of cnns to random forests, theory?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.cvpapers.com/index.html&amp;sa=D&amp;ust=1465142038111000&amp;usg=AFQjCNG6Hazh-UxCBYGJRhfNtNv0mOGUKQ">http://www.cvpapers.com/index.html</a></span><span>&nbsp;journal list. should go in &quot;indexes&quot;</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08183&amp;sa=D&amp;ust=1465142038112000&amp;usg=AFQjCNGWmn578Nut_W52hLMDW6515A3gAA">http://arxiv.org/abs/1512.08183</a></span><span>&nbsp;document vectors. Cites/references paragraph vectors. Technique and demo application.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08422&amp;sa=D&amp;ust=1465142038113000&amp;usg=AFQjCNHuSoyJej514_pLthNryJYVsBz64w">http://arxiv.org/abs/1512.08422</a></span><span>&nbsp;tree based convolution, sentence contradiction detection, large improvement in sota on that task, application and technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07928&amp;sa=D&amp;ust=1465142038114000&amp;usg=AFQjCNE5Krptr6FBooB86wWhFof5jaJbUw">http://arxiv.org/abs/1512.07928</a></span><span>&nbsp;weakly supervised segmentation - train on classification with attention and some sort of encoder/decoder, retrain on segmentation. Technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1503.08909.pdf&amp;sa=D&amp;ust=1465142038114000&amp;usg=AFQjCNHoxeIh4iO8EO3eyINkvdgTugwyTg">http://arxiv.org/pdf/1503.08909.pdf</a></span><span>&nbsp;video classification - according to ben, highest public accuracy for video classification</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08133&amp;sa=D&amp;ust=1465142038115000&amp;usg=AFQjCNFvcLmfs4dZMqarxdDZCBJJfXIhEg">http://arxiv.org/abs/1512.08133</a></span><span>&nbsp;theory and reasoning about allowing a model to abstain from making any prediction, presumably by having an &quot;I don&#39;t know&quot; prediction. Theory and maybe technique.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07982&amp;sa=D&amp;ust=1465142038116000&amp;usg=AFQjCNEsg3BaECNu4RSl6v3oRtj9Vpsy-A">http://arxiv.org/abs/1512.07982</a></span><span>&nbsp;theory, proposed technique for format, and presumably good references, regarding multi label classification. Theory?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08512&amp;sa=D&amp;ust=1465142038117000&amp;usg=AFQjCNFjJmV4Do9gYA3lPDNdY82SNglwYw">http://arxiv.org/abs/1512.08512</a></span><span>&nbsp;learned sound synthesis of material impact sounds from videos - very cool application! I want this dataset, would be wonderful for use in games! Application</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04176&amp;sa=D&amp;ust=1465142038118000&amp;usg=AFQjCNEvhs52154i9inO7GvB0c1wo9RJYw">http://arxiv.org/abs/1511.04176</a></span><span>&nbsp;&quot;sequence to sequence learning for ocr&quot;; application</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06927&amp;sa=D&amp;ust=1465142038119000&amp;usg=AFQjCNHXfOw1YyUlmBOu_Ii7Rf-_iNjWkA">http://arxiv.org/abs/1512.06927</a></span><span>&nbsp;&quot;multimodal deep learning&quot;, new framework</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08949&amp;sa=D&amp;ust=1465142038120000&amp;usg=AFQjCNHbP2dpzNh7S8naAHDlEFDJw8dRsQ">http://arxiv.org/abs/1512.08949</a></span><span>&nbsp;non-neural, non-deep; for dataset gathering, especially of crowdsourced sort data: data-efficient algorithm for getting an ordering out of noisy pairwise comparisons, most likely references older ways to do the same. technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09023&amp;sa=D&amp;ust=1465142038121000&amp;usg=AFQjCNE0xH1FuzZNJf6Ujvp-GjSnPYWpJA">http://arxiv.org/abs/1512.09023</a></span><span>&nbsp;non-neural, non-deep, non-theory, just caught my eye: clustering publications by network analysis</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08525&amp;sa=D&amp;ust=1465142038121000&amp;usg=AFQjCNHMn7GH1_qmdpg-4GmbmprswdpS4A">http://arxiv.org/abs/1512.08525</a></span><span>&nbsp;candidate for bayesian approach to structured learning: bayesian networks that, you know, actually scale. how does this perform head-to-head with a current nn? and how does it perform head-to-head with a bayesian nn, such as the variational ones that are more data efficient? Should go in &quot;bayesian&quot;.</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09272&amp;sa=D&amp;ust=1465142038122000&amp;usg=AFQjCNE1GVIor9FJsIOkjuapQTpG24hAig">http://arxiv.org/abs/1512.09272</a></span><span>&nbsp;local image descriptor learning, Siamese networks, triplet networks. Some technique, mostly application?</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09170&amp;sa=D&amp;ust=1465142038123000&amp;usg=AFQjCNFV4cseyhlAiTkyoYGiKHjLh6LoYw">http://arxiv.org/abs/1512.09170</a></span><span>&nbsp;convex optimization weigh only statistical querys, whatever those are. Mentions being able to train perceptrons more efficiently. Technique</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.03167&amp;sa=D&amp;ust=1465142038124000&amp;usg=AFQjCNEXvUiQvBrCW_nsSLvJHOAvDN9n8g">http://arxiv.org/abs/1503.03167</a></span><span>&nbsp;learned scene inference and scene renderer, with optimization criteria to be interpretable. Should go in &quot;objectives&quot;, &quot;images/3d&quot;.</span></li><li class="c3 c7"><span></span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www-personal.umich.edu/~reedscot/nips2015.pdf&amp;sa=D&amp;ust=1465142038125000&amp;usg=AFQjCNEPsaIlCmault6P0dpMNP_g-hEmvw">http://www-personal.umich.edu/~reedscot/nips2015.pdf</a></span><span>&nbsp;[abs:2] image analogy generation via embedding space that allows for math to produce similar images, similarly to the embedding tricks demonstrated with word2vec-learned embeddings. Looks pretty cool. Should go in &quot;images/embedding&quot;, &quot;embeddings&quot;/&quot;representations&quot;/&quot;representation learning&quot; (but it&#39;s not unsupervised, is it?)</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://archive.org/details/Redwood_Center_2015_11_04_Sergey_Levine_and_Chelsea_Finn&amp;sa=D&amp;ust=1465142038126000&amp;usg=AFQjCNFrk7EgSKjPBkyYYxLqJqjlf1YK3g">https://archive.org/details/Redwood_Center_2015_11_04_Sergey_Levine_and_Chelsea_Finn</a></span><span>&nbsp;ucb robotic reinforcement learning to perform everyday tasks. No paper? Just video and abstract? &nbsp; ???</span></li></ul><p class="c3"><span></span></p><ul class="c2 lst-kix_pjyqz9f44g8s-0"><li class="c6 c7"><span>rbf activation functions seem interesting for audio modeling </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://frnsys.com/ai_notes/machine_learning/neural_nets.html&amp;sa=D&amp;ust=1465142038128000&amp;usg=AFQjCNEfFkR6h87M4vcSLTUgxAk91XVjbg">http://frnsys.com/ai_notes/machine_learning/neural_nets.html</a></span></li></ul><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h2 class="c5" id="h.yl7u1m94myey"><span>Inbox 2</span></h2><p class="c3"><span></span></p><p class="c6"><span>earlier this year stuff:</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08756&amp;sa=D&amp;ust=1465142038131000&amp;usg=AFQjCNGt7zPzWO0bwgW8IPxFh6PoORgZiQ">http://arxiv.org/abs/1512.08756</a></span><span>&nbsp;feed forward with attention works better for tasks that don&#39;t care about the order</span></p><p class="c3"><span></span></p><p class="c6"><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08562&amp;sa=D&amp;ust=1465142038132000&amp;usg=AFQjCNHZjj0e-g5w63AcyYPG4GVcaD7X5A">http://arxiv.org/abs/1512.08562</a></span><span>&nbsp;g-learning - regularization in early learning of q learners, to prevent unreasonable overconfidence</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05666&amp;sa=D&amp;ust=1465142038133000&amp;usg=AFQjCNFvrGyIofC9PDL240gaUGuXhBBcGw">http://arxiv.org/abs/1511.05666</a></span><span>&nbsp;superresolution with a &quot;gibbs distribution&quot; and convolutions, somehow. should go in &quot;generative&quot; and &quot;images&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07053&amp;sa=D&amp;ust=1465142038134000&amp;usg=AFQjCNEnHGmwRnXLDl-7H1nCLo81vGrGgQ">http://arxiv.org/abs/1511.07053</a></span><span>&nbsp;four-directional recurrence over an image for object segmentation. should go in &quot;images&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html&amp;sa=D&amp;ust=1465142038135000&amp;usg=AFQjCNGdqql5t1VD8EuaZHFDjGEJnJp86A">http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html</a></span><span>&nbsp;&quot;what my deep model doesn&#39;t know&quot;, uncertainty estimates from deep models</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/junhyukoh/deep-reinforcement-learning-papers&amp;sa=D&amp;ust=1465142038136000&amp;usg=AFQjCNG5PDmca7ijZCPKJH7IzLSSrXkgBw">https://github.com/junhyukoh/deep-reinforcement-learning-papers</a></span><span>&nbsp;</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04747&amp;sa=D&amp;ust=1465142038138000&amp;usg=AFQjCNHuBe1wOFESs42EOJbRQYPCzFT22w">http://arxiv.org/abs/1511.04747</a></span><span>&nbsp;learning representations of affect from speech, explores input encodings. - should go in &quot;representation learning&quot;, &quot;speech&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.04681&amp;sa=D&amp;ust=1465142038138000&amp;usg=AFQjCNF6-FwROSoRM4XDqdwsOZCWhX2cag">http://arxiv.org/abs/1502.04681</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04906&amp;sa=D&amp;ust=1465142038139000&amp;usg=AFQjCNFYu_VRw-w7Ircw9WNS7n4eqa3OtA">http://arxiv.org/abs/1511.04906</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06485&amp;sa=D&amp;ust=1465142038140000&amp;usg=AFQjCNGO__OaDweU8Eze6fNFLnEm7sv_Eg">http://arxiv.org/abs/1511.06485</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02954&amp;sa=D&amp;ust=1465142038140000&amp;usg=AFQjCNEYpjR8fMnWnPSGe4dGM6OO5w_R-A">http://arxiv.org/abs/1511.02954</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.08347&amp;sa=D&amp;ust=1465142038140000&amp;usg=AFQjCNEjKR2SWyXDVRK-0qRYRQdZrkEBxg">http://arxiv.org/abs/1512.08347</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04590&amp;sa=D&amp;ust=1465142038141000&amp;usg=AFQjCNGTUQL3Ynd8l-PjaPs8v2GOAtyRIA">http://arxiv.org/abs/1511.04590</a></span><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1408.0848&amp;sa=D&amp;ust=1465142038141000&amp;usg=AFQjCNFyc3IATAcrA2KtT4iXQxS9t0S1jA">http://arxiv.org/abs/1408.0848</a></span><span>&nbsp;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07404&amp;sa=D&amp;ust=1465142038142000&amp;usg=AFQjCNGOJaCEQpRqVV8QiEK2hvf-H-8yCw">http://arxiv.org/abs/1511.07404</a></span><span>&nbsp;cnns for billiards</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06104&amp;sa=D&amp;ust=1465142038143000&amp;usg=AFQjCNHTkPTQMeOvP1KNNxnkCgFqldG2sw">http://arxiv.org/abs/1511.06104</a></span><span>&nbsp;[abs:2] semi-supervised learning based on an algorithm I&#39;m not familiar with (but have heard of - &quot;expectation maximization&quot;), that apparently constructs a graph and uses it to train based on the unlabeled data. Should go in &quot;semi-supervised learning&quot;, &quot;graphs&quot;. </span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.01746&amp;sa=D&amp;ust=1465142038144000&amp;usg=AFQjCNEeNvOuljvWLWiFQ-o06pNJWhr0Dw">http://arxiv.org/abs/1508.01746</a></span><span>&nbsp;[abs:1] dataset of speech imitation attacks. Should go in &quot;security&quot;, &quot;language/speech&quot;, &quot;datasets&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Some updated papers: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.08967&amp;sa=D&amp;ust=1465142038145000&amp;usg=AFQjCNEwenGBRuIzd91PrHeSmI9Oakxn5w">http://arxiv.org/abs/1509.08967</a></span><span>&nbsp;multilingual, </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.5319&amp;sa=D&amp;ust=1465142038146000&amp;usg=AFQjCNGbLWf5YKX1gimwiDaazHdulJlQnw">http://arxiv.org/abs/1411.5319</a></span><span>&nbsp;apparel, </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.05689&amp;sa=D&amp;ust=1465142038146000&amp;usg=AFQjCNF8jSB_JAG3LWGdhAJmsgmY_2Picg">http://arxiv.org/abs/1502.05689</a></span><span>&nbsp;human, </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.06840&amp;sa=D&amp;ust=1465142038146000&amp;usg=AFQjCNEAOA7TSTItrSK3oeR_fzpI-hQuvA">http://arxiv.org/abs/1506.06840</a></span><span>&nbsp;variance, </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.00736&amp;sa=D&amp;ust=1465142038147000&amp;usg=AFQjCNHrnw9Vo2Pa1MnomSk6KDwYsgIGNw">http://arxiv.org/abs/1511.00736</a></span><span>&nbsp;protein, </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.09337&amp;sa=D&amp;ust=1465142038147000&amp;usg=AFQjCNG7YIQUeWnCYH8emeYiuYPs8a7mhw">http://arxiv.org/abs/1511.09337</a></span><span>&nbsp;multiclass, </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.01322&amp;sa=D&amp;ust=1465142038148000&amp;usg=AFQjCNHAD-IegF7Lj5v4tk5_o7_mWREJAQ">http://arxiv.org/abs/1512.01322</a></span><span>&nbsp;integers,</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.04652&amp;sa=D&amp;ust=1465142038149000&amp;usg=AFQjCNEth1IoLhACRntdB5Un0utGxF_iJg">http://arxiv.org/abs/1512.04652</a></span><span>&nbsp;[abs:1] cancer detection paper. Should go in &quot;bio&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06744&amp;sa=D&amp;ust=1465142038149000&amp;usg=AFQjCNGxpT-rPeuEPC1NW0WZwPZKIwhNFQ">http://arxiv.org/abs/1511.06744</a></span><span>&nbsp;learn convolutional filters as compositions of other convolutional filters - interesting! compression within a layer. should go in &quot;sparsity&quot;, &quot;convolution&quot;, &quot;model compression&quot;, &quot;compressed representation&quot;/&quot;hidden representation&quot;/&quot;architecture&quot;/whatever it is I&#39;m calling that. claims equivalent performance with many fewer parameters.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06435&amp;sa=D&amp;ust=1465142038150000&amp;usg=AFQjCNHP1jHOnoGnRcZTX8xXWoWF0Muefw">http://arxiv.org/abs/1511.06435</a></span><span>&nbsp;comparison of frameworks - theano, torch, etc</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00740&amp;sa=D&amp;ust=1465142038151000&amp;usg=AFQjCNH630wiNXQ2VT-5khwBwzHQPf3qWw">http://arxiv.org/abs/1601.00740</a></span><span>&nbsp;action prediction for cars - most of the components of self-driving cars and then try to predict actions.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/426q0t/robot_control_with_distributed_deep_reinforcement/cz8d223&amp;sa=D&amp;ust=1465142038152000&amp;usg=AFQjCNEr6OqBPFdl7r1Hy4EzNUaHJOLzxg">https://www.reddit.com/r/MachineLearning/comments/426q0t/robot_control_with_distributed_deep_reinforcement/cz8d223</a></span><span>&nbsp;robot control demo of reinforcement learning. might have some insights as to crafting reward functions, looks boring otherwise.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html%2343494641522d3130&amp;sa=D&amp;ust=1465142038153000&amp;usg=AFQjCNFMtW9HdxVWYjeuqXoNbmfiB7JTzQ">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130</a></span><span>&nbsp;current SOTA numbers on various datasets. misleading, because what you really want to know is how big of a difference a technique makes, not what its absolute effectiveness is on a dataset. lots of good papers here, though, worth going through to find the good ones. most of these are pretty mainstream, big-splash papers.</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6"><span>Learning Dense Convolutional Embeddings for Semantic Segmentation - maybe language? </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04377&amp;sa=D&amp;ust=1465142038154000&amp;usg=AFQjCNEipmX_ylSpk1DdbNlPGim87HjlOA">http://arxiv.org/abs/1511.04377</a></span><span>&nbsp;</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06437&amp;sa=D&amp;ust=1465142038155000&amp;usg=AFQjCNHrnegMhekk13S7OsjtkBer-FHPsg">http://arxiv.org/abs/1511.06437</a></span><span>&nbsp;&quot;a convnet for non-maximum suppression&quot;</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06739&amp;sa=D&amp;ust=1465142038155000&amp;usg=AFQjCNEZ6NLfQuWuIZRx4hNpECstSyCTww">http://arxiv.org/abs/1511.06739</a></span><span>&nbsp;Superpixel Convolutional Networks using Bilateral Inceptions</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06281&amp;sa=D&amp;ust=1465142038156000&amp;usg=AFQjCNGrB2YMxQ_7ZQZBXSsgz8VpRsbS1w">http://arxiv.org/abs/1511.06281</a></span><span>&nbsp;Density Modeling of Images using a Generalized Normalization</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06297&amp;sa=D&amp;ust=1465142038156000&amp;usg=AFQjCNFBsI5sQLGHd_xIutbqEJyMjqAXbQ">http://arxiv.org/abs/1511.06297</a></span><span>&nbsp;Conditional Computation in Neural Networks for faster models - should go in &quot;model compression&quot;, maybe? &quot;Model sub selection&quot;?</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06456&amp;sa=D&amp;ust=1465142038157000&amp;usg=AFQjCNFF_X2ca3po9u-dGzPR_a1qxZnWDg">http://arxiv.org/abs/1511.06456</a></span><span>&nbsp;how to optimize task loss such as bleu, rather than surrogate loss such as cross-entropy. probably need a new category under architecture called loss. Should go in &quot;objectives&quot;. Big improvement in sota.</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1511.06350v2.pdf&amp;sa=D&amp;ust=1465142038157000&amp;usg=AFQjCNGKfMtl7T-hlfRYysC3UfrUXcITJw">http://arxiv.org/pdf/1511.06350v2.pdf</a></span><span>&nbsp;&quot;structured energy prediction networks&quot;. something about minimizing the energy of something at inference time. they mention multi-label classification as an application. by fast pattern matching, looks like it&#39;s one of those really cool papers that don&#39;t work. Should go in &quot;classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Cannibalize and categorize/label this list </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/robertsdionne/neural-network-papers&amp;sa=D&amp;ust=1465142038158000&amp;usg=AFQjCNHEm4YeJfeG1XQ63dK3lbmjh480fA">https://github.com/robertsdionne/neural-network-papers</a></span><span>&nbsp;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.arxiv-sanity.com/&amp;sa=D&amp;ust=1465142038159000&amp;usg=AFQjCNHan9hLuqKVHevrCGdlExBbkwUNDw">http://www.arxiv-sanity.com/</a></span><span>&nbsp;- go through for stuff</span></p><p class="c6"><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://recommend-papers.org/&amp;sa=D&amp;ust=1465142038160000&amp;usg=AFQjCNG6oftwYQYqTzaKwGq0TPXEZZLJ2Q">https://recommend-papers.org/</a></span><span>&nbsp;- go through for stuff</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://cs.stanford.edu/people/karpathy/nips2015/&amp;sa=D&amp;ust=1465142038161000&amp;usg=AFQjCNECUq43gpO8yLaRi5Sfb9iQ1ZFtBg">http://cs.stanford.edu/people/karpathy/nips2015/</a></span><span>&nbsp;- go through for stuff</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/karpathy/nipspreview/blob/master/lda.py&amp;sa=D&amp;ust=1465142038161000&amp;usg=AFQjCNHG-QN3iW4V1RIdCzTs-YkT-nefaQ">https://github.com/karpathy/nipspreview/blob/master/lda.py</a></span><span>&nbsp;- implementation of lda, don&#39;t care</span></p><p class="c3"><span></span></p><p class="c6"><span>2012-?? </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://nlp.stanford.edu/pubs/HuangACL12.pdf&amp;sa=D&amp;ust=1465142038162000&amp;usg=AFQjCNFXqZSq1HxeYQkUpjE3FcPC8zL_9w">http://nlp.stanford.edu/pubs/HuangACL12.pdf</a></span><span>&nbsp;word vector thing for global context and multiple meanings. Comes with a dataset and pretrained vectors. Should go in &quot;word embeddings&quot;.</span></p><p class="c6"><span>2015-11 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.01526&amp;sa=D&amp;ust=1465142038163000&amp;usg=AFQjCNHYYDnH5EFMDZYgC_S8M6txcB5NuA">http://arxiv.org/abs/1507.01526</a></span><span>&nbsp;grid lstm - should go in &quot;memory&quot;. Looks like it&#39;s basically cgru, but with a different framing. Curious how it got good results on Wikipedia, though, the problem framing they used is probably interesting.</span></p><p class="c6"><span>2015-11 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.09263&amp;sa=D&amp;ust=1465142038164000&amp;usg=AFQjCNGxdy5LM_y3rBSBA32Ea3eiqbmvYw">http://arxiv.org/abs/1511.09263</a></span><span>&nbsp;scalable feature selection - non neural thing for picking features out of ridiculous amounts of data</span></p><p class="c6"><span>2015-11 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07938&amp;sa=D&amp;ust=1465142038164000&amp;usg=AFQjCNENVHF3fywMOOy7jReJyRHImuiwZw">http://arxiv.org/abs/1511.07938</a></span><span>&nbsp;temporal convolution for diagnosis - should go in &quot;bio applications&quot; and maybe &quot;time series&quot;</span></p><p class="c6"><span>2015-11 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07401&amp;sa=D&amp;ust=1465142038165000&amp;usg=AFQjCNGY-s3X824PJ-bL7OIW3aSIFsO-NQ">http://arxiv.org/abs/1511.07401</a></span><span>&nbsp;&quot;mazebox, a sandbox for learning games&quot; - should go in &quot;rl&quot;</span></p><p class="c6"><span>2015-11 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06964&amp;sa=D&amp;ust=1465142038166000&amp;usg=AFQjCNGw9HhZWNMH-BfbO6-KgR62jcCOZA">http://arxiv.org/abs/1511.06964</a></span><span>&nbsp;Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and Denoising Autoencoders, should go in &quot;semi-supervised&quot;</span></p><p class="c6"><span>2015-11 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06485&amp;sa=D&amp;ust=1465142038167000&amp;usg=AFQjCNHUWCFGwGqAnmtPmuLRia8sJAXreA">http://arxiv.org/abs/1511.06485</a></span><span>&nbsp;Trivializing The Energy Landscape Of Deep Networks - regularization model that removes local minima via a technique used in physics. Should go in &quot;regularization&quot;.</span></p><p class="c6"><span>2015-11 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06429&amp;sa=D&amp;ust=1465142038168000&amp;usg=AFQjCNESifnCwI8wEYhj3SMiB7rD5CqtBA">http://arxiv.org/abs/1511.06429</a></span><span>&nbsp;</span><span class="c1">Contextual learning subsumes a variety of related approaches, e.g. multi-task learning and learning using privileged information. Our contributions are (i) a new perspective that connects these previously isolated approaches, (ii) insights about how these methods incorporate useful priors by implementing different patterns, (iii) a simple way to apply them to novel problems, as well as (iv) a systematic experimental evaluation of these patterns in two supervised learning tasks. Should go in &quot;regularization&quot;, &quot;unsupervised&quot;, and &quot;knowledge sharing&quot;.</span></p><p class="c6"><span class="c1">2015-11 </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06361&amp;sa=D&amp;ust=1465142038168000&amp;usg=AFQjCNGj5GTme-B3BdkKrzp_K7VahaRxVQ">http://arxiv.org/abs/1511.06361</a></span><span class="c1">&nbsp;Order-Embeddings of Images and Language, something to do with learning ordered embeddings or something, looks boring. Should go in &quot;language&quot; and &quot;images&quot; I guess.</span></p><p class="c6"><span class="c1">2011-?? </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://repository.upenn.edu/cgi/viewcontent.cgi?article%3D1716%26context%3Dcis_papers&amp;sa=D&amp;ust=1465142038169000&amp;usg=AFQjCNFbePG16kw_P1AzZ0cXr1ApTQtU8w">http://repository.upenn.edu/cgi/viewcontent.cgi?article=1716&amp;context=cis_papers</a></span><span class="c1">&nbsp;Multi-View Learning of Word Embeddings via CCA, should go in &quot;word embeddings&quot;</span></p><p class="c6"><span class="c1">2012-?? </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://nlp.stanford.edu/pubs/HuangACL12.pdf&amp;sa=D&amp;ust=1465142038170000&amp;usg=AFQjCNHcvEiyW5Ewa9BBmyEhRhUhdK6-Gg">http://nlp.stanford.edu/pubs/HuangACL12.pdf</a></span><span class="c1">&nbsp;GLoVE Improving Word Representations via Global Context and Multiple Word Prototypes, should go in &quot;word embeddings&quot;</span></p><p class="c6"><span class="c1">2014-?? </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/viewFile/8168/8838&amp;sa=D&amp;ust=1465142038171000&amp;usg=AFQjCNGLPyk_QfT_KUK_xjC38uYGNvydbQ">http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/viewFile/8168/8838</a></span><span class="c1">&nbsp;Pre-Trained Multi-View Word Embedding Using Two-side Neural Network, should go in &quot;word embeddings&quot;</span></p><p class="c6"><span class="c1">2015=?? </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://lms.comp.nus.edu.sg/sites/default/files/publication-attachments/liuyang_chua.pdf&amp;sa=D&amp;ust=1465142038171000&amp;usg=AFQjCNFqZPUPwT6yfLcCP-r9RNuId-O7Jg">http://lms.comp.nus.edu.sg/sites/default/files/publication-attachments/liuyang_chua.pdf</a></span><span class="c1">&nbsp;Topical Word Embeddings - alt link: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_twe.pdf&amp;sa=D&amp;ust=1465142038172000&amp;usg=AFQjCNFc842DXs2OMmFvXmHhYEQUy74jlg">http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2015_twe.pdf</a></span><span class="c1">&nbsp;should go in &quot;word embeddings&quot;</span></p><p class="c6"><span class="c1">2014-07 </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/&amp;sa=D&amp;ust=1465142038173000&amp;usg=AFQjCNHHvEQDhOfZfDm54WDvYatDqmnnjQ">http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/</a></span><span class="c1">&nbsp;some debugging or theory thing, should go in &quot;theory&quot; and &quot;debugging&quot;</span></p><p class="c6"><span class="c1">2011-?? </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/paper/4193-multi-view-learning-of-word-embeddings-via-cca.pdf&amp;sa=D&amp;ust=1465142038174000&amp;usg=AFQjCNEjVucjuUabYKmMx8aNiJPMtPjfUw">http://papers.nips.cc/paper/4193-multi-view-learning-of-word-embeddings-via-cca.pdf</a></span><span class="c1">&nbsp;title in link, should go in &quot;word embeddings&quot;</span></p><p class="c6"><span class="c1">2014-?? </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.aclweb.org/anthology/C14-1016&amp;sa=D&amp;ust=1465142038175000&amp;usg=AFQjCNHEBxCU0Cj_aoY2PNnznoLwlqQKkA">http://www.aclweb.org/anthology/C14-1016</a></span><span class="c1">&nbsp;A Probabilistic Model for Learning Multi-Prototype Word Embeddings, should go in &quot;word embeddings&quot;</span></p><p class="c6"><span class="c1">2015-11 </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07838&amp;sa=D&amp;ust=1465142038175000&amp;usg=AFQjCNGbXgrJItj94rLvPmeq3tVNfhurqw">http://arxiv.org/abs/1511.07838</a></span><span class="c1">&nbsp;Dynamic Capacity Networks - should probably go in &quot;model compression&quot;, &quot;speed&quot;, or &quot;regularization&quot;. very very closely related to spatial transformers.</span></p><p class="c6"><span class="c1">2014-12 </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.4210&amp;sa=D&amp;ust=1465142038176000&amp;usg=AFQjCNGKc8wcAns-NN2XhVOhr3ywb0P11g">http://arxiv.org/abs/1412.4210</a></span><span class="c1">&nbsp;Learning Precise Spike Train to Spike Train Transformations in Multilayer Feedforward Neuronal Networks - should go in &quot;Neuromorphic&quot;</span></p><p class="c6"><span class="c1">2015-11 </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.08400&amp;sa=D&amp;ust=1465142038177000&amp;usg=AFQjCNFQvzUGo7SQ6YY41iIN8pDGhcmk3A">http://arxiv.org/abs/1511.08400</a></span><span class="c1">&nbsp;regularizing rnns by stabilizing activations (note: by david krueger :) - should go in &quot;regularization&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><ul class="c2 lst-kix_iigck7esqel5-0 start"><li class="c6 c7"><span class="c1">idea: annotate this document with cite counts automatically every so often. Should go in &quot;my ideas&quot;</span></li><li class="c6 c7"><span class="c1">idea: annotate this document with dates automatically when easily detected. Should go in &quot;my ideas&quot;</span></li><li class="c6 c7"><span class="c1">idea: auto-copy things in this document based on some simple syntax (that </span><span class="c1">isn&#39;t</span><span class="c1">&nbsp;the stupid natural language I&#39;m using now). Should go in &quot;my ideas&quot;</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.08700&amp;sa=D&amp;ust=1465142038180000&amp;usg=AFQjCNGXbNgJjkkLOdbbbjK2n01MIRgDsQ">http://arxiv.org/abs/1506.08700</a></span><span class="c1">&nbsp;dropout as data augmentation - analysis of dropout where they view it as data augmentation and then use it to do exactly that, and then test it on a no-dropout network, where it performs similarly. They also get a model of it such that they suggest better versions of it that are equally fast but preform better. Should go in &quot;theory&quot; and &quot;regularization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/ChristosChristofidis/awesome-deep-learning&amp;sa=D&amp;ust=1465142038181000&amp;usg=AFQjCNHQpz1M4WXTDAE80U5bU-xa3-U7UQ">https://github.com/ChristosChristofidis/awesome-deep-learning</a></span><span class="c1">&nbsp;misc somewhat fan-ish crap. some looks interesting. no, wait, actually quite a few cool papers.</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/kjw0612/awesome-deep-vision&amp;sa=D&amp;ust=1465142038182000&amp;usg=AFQjCNFVkmB1aVO5yl5UwSipbvFXkHLV6g">https://github.com/kjw0612/awesome-deep-vision</a></span><span class="c1">&nbsp;similar, for CV.</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/josephmisiti/awesome-machine-learning&amp;sa=D&amp;ust=1465142038182000&amp;usg=AFQjCNEDPTizKUWGf8WAymgj54YoNLaMYw">https://github.com/josephmisiti/awesome-machine-learning</a></span><span class="c1">&nbsp;a list of ml libraries</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/edobashira/speech-language-processing&amp;sa=D&amp;ust=1465142038183000&amp;usg=AFQjCNG7urtiaEIAD0__hKD47WmVAtpZGA">https://github.com/edobashira/speech-language-processing</a></span><span class="c1">&nbsp;speech recognition field overview from a fan perspective</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://blog.evorithmics.org/2016/01/31/when-will-evolution-outperform-local-search/&amp;sa=D&amp;ust=1465142038184000&amp;usg=AFQjCNFxCsIdA6Q9LxiN1DnQDsErTGEy3w">http://blog.evorithmics.org/2016/01/31/when-will-evolution-outperform-local-search/</a></span><span class="c1">&nbsp;formal analysis of GAs and why they work</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://gitxiv.com/posts/JHuYmxTxtuNHKiyFQ/are-elephants-bigger-than-butterflies-reasoning-about-sizes&amp;sa=D&amp;ust=1465142038185000&amp;usg=AFQjCNHII6Q8oysPHehkZ1ER42DquA2eEw">http://gitxiv.com/posts/JHuYmxTxtuNHKiyFQ/are-elephants-bigger-than-butterflies-reasoning-about-sizes</a></span><span class="c1">&nbsp;object size in images something or other. predicts relative size from an image. has a dataset for it. should go in &quot;whole-image deep learning&quot; and &quot;datasets&quot;.</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06071&amp;sa=D&amp;ust=1465142038186000&amp;usg=AFQjCNFw1rhtFb4HRZpl951D-nSyYtYkwg">http://arxiv.org/abs/1601.06071</a></span><span class="c1">&nbsp;single-bit boolean-op neural networks - should go in &quot;model compression&quot;.</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/paper/5865-efficient-exact-gradient-update-for-training-deep-networks-with-very-large-sparse-targets.pdf&amp;sa=D&amp;ust=1465142038187000&amp;usg=AFQjCNGhveWoVIOxxmrrqq49bXVjCidORQ">http://papers.nips.cc/paper/5865-efficient-exact-gradient-update-for-training-deep-networks-with-very-large-sparse-targets.pdf</a></span><span class="c1">&nbsp;speedup for training networks with huge, single-activation-target output spaces. very relevant to training networks that interface with databases by id. Note: how do you run them, though? I&#39;ve been thinking about indexing&hellip; should go in &quot;model compression&quot;, I suppose. also, since I want to use it for it, in &quot;memory&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3Dgi4Zf59_IcU&amp;sa=D&amp;ust=1465142038187000&amp;usg=AFQjCNF6h5wecmiApJ9PRjrKeDVBPVzThg">https://www.youtube.com/watch?v=gi4Zf59_IcU</a></span><span class="c1">&nbsp;presentation on a memory approach and how it relates to AGI. Tomas Mikolov.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.00363&amp;sa=D&amp;ust=1465142038188000&amp;usg=AFQjCNF4eclsK4lT4rEnoifSCUCaNFWShA">http://arxiv.org/abs/1511.00363</a></span><span class="c1">&nbsp;binaryconnect: both binary forward and backward passes, but floating-point gradients. ! - should go in &quot;model compression&quot;, &quot;hardware&quot;, &quot;learning algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.0247&amp;sa=D&amp;ust=1465142038189000&amp;usg=AFQjCNHGRP-Dbc6Zvdy5r1TQC_EiAm0Iow">http://arxiv.org/abs/1411.0247</a></span><span class="c1">&nbsp;an alternative to target prop for learned training without backprop, using feedback connections. should go in &quot;training algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1511.06807.pdf&amp;sa=D&amp;ust=1465142038191000&amp;usg=AFQjCNFF1npyxRwAlGy5Q-yZDJQ278hP4A">http://arxiv.org/pdf/1511.06807.pdf</a></span><span class="c1">&nbsp;google gradient noise paper. should go in &quot;data efficiency&quot; or &quot;regularization&quot;. includes some stuff about what other hyperparameters do.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://people.ee.duke.edu/~lcarin/sgnht-4.pdf&amp;sa=D&amp;ust=1465142038192000&amp;usg=AFQjCNElrMyxFymY8ugvjrmEflNcp1l-rQ">http://people.ee.duke.edu/~lcarin/sgnht-4.pdf</a></span><span class="c1">&nbsp;not focused on neural, but mentions trying an mnist autoencoder; looks like an interesting speedup of using noise to estimate bayesian methods. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/42nnpe/why_do_i_never_see_dropout_applied_in/&amp;sa=D&amp;ust=1465142038193000&amp;usg=AFQjCNGlRP_erASw7y2Tj-L0YlYSKnhISw">https://www.reddit.com/r/MachineLearning/comments/42nnpe/why_do_i_never_see_dropout_applied_in/</a></span><span class="c1">&nbsp;discussion about dropout need in conv layers. mentions mc dropout paper. mentions that batch norm can remove need for dropout (wat).</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01783&amp;sa=D&amp;ust=1465142038194000&amp;usg=AFQjCNGuoX4YIEP7PIrelp5J3LeUKUJ9oQ">http://arxiv.org/abs/1602.01783</a></span><span class="c1">&nbsp;asynchronous RL - acts as a regularizer, they try a bunch of different rl methods including some I haven&#39;t heard much about called &quot;actor-critic&quot; that&#39;s able to do continuous motor control, and one successfully plays a 3d game. should go in &quot;images/3d&quot;, &quot;rl&quot;, and &quot;regularization&quot;. </span><span class="c9 c12 c1">Looks like a big-deal paper.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02025v3&amp;sa=D&amp;ust=1465142038195000&amp;usg=AFQjCNER3cNi_d10UOyKUgUc-r9BEng5qw">http://arxiv.org/abs/1506.02025v3</a></span><span class="c1">&nbsp;spatial transformer networks. should go in &quot;regularization&quot;, &quot;images&quot;. maybe also in like, &quot;conv&quot;? or &quot;architecture&quot;? maybe &quot;pooling&quot;. nah, &quot;architecture&quot;. eh, whtever. torch blog post about them: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://torch.ch/blog/2015/09/07/spatial_transformers.html&amp;sa=D&amp;ust=1465142038196000&amp;usg=AFQjCNH6zHXdv-gImCNpRN6nmrVaJHlNKQ">http://torch.ch/blog/2015/09/07/spatial_transformers.html</a></span><span class="c1">&nbsp;- blog post comes with a german street signs dataset, so maybe also &quot;datasets&quot;. -&gt; looks like they&#39;re not what I thought they were, it finds a global representation first and summarizes that to a localization output that zooms in on the thing. it also allows swapping out the grid generator for something with a few more parameters, such as a higher-resolution grid generator (:P). but now I wonder: what if you had a conv layer as your maxpooling layer, but the conv layer had to output a pixel to select? as a way of implementing something like winner takes all: the one the conv layer thinks won takes all. use a conv layer of 3x3, stride 2, similar to a typical maxpool, but with a conv network with nine channels: one for each pixel. or, as a grid generator with more stops, by outputting directions and magnitudes rather than grid positions.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02745&amp;sa=D&amp;ust=1465142038197000&amp;usg=AFQjCNF3hPk7uINe3-5Cdfjcbl9w-sAbUg">http://arxiv.org/abs/1601.02745</a></span><span class="c1">&nbsp;tensor product decomposition reasoning and apparently some application. Should go in &quot;model compression&quot; and probably also &quot;speed&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02789&amp;sa=D&amp;ust=1465142038197000&amp;usg=AFQjCNGfVLjdN05NqihtHhum20JHbPzT1g">http://arxiv.org/abs/1601.02789</a></span><span class="c1">&nbsp;comparison of text quality metrics like Bleu to human quality ratings. Should go in &quot;language&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02828&amp;sa=D&amp;ust=1465142038198000&amp;usg=AFQjCNGslXr5LByLgi0MX9iUwu_kxVmi9Q">http://arxiv.org/abs/1601.02828</a></span><span class="c1">&nbsp;something about credit assignment or unsupervised learning or something. Should go in &quot;unsupervised&quot; and &quot;credit assignment&quot;. Possibly a learned learning algorithm. </span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02852&amp;sa=D&amp;ust=1465142038200000&amp;usg=AFQjCNGI3LwYo5ekEbPqIX1NzfZ2-Oivvg">http://arxiv.org/abs/1601.02852</a></span><span class="c1">&nbsp;human attention estimation, looks like maybe an attentional model with the attention also trained to follow human attention? might not be neural, though. looks boring. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02919&amp;sa=D&amp;ust=1465142038201000&amp;usg=AFQjCNFrpYzmErquFzUGVb-69Ze4C9hL5Q">http://arxiv.org/abs/1601.02919</a></span><span class="c1">&nbsp;looks pretty cool - based on connecting convnets to existing texture analysis methods, they bring over other observations from texture analysis, and claim to greatly speed up convnets. mentions using an energy measure at the end - oh, they&#39;re averaging each of the filter columns in the last layer, and using that as a texture summary. in a full-fledged convnet, they do this in early layers of the net, and then pass it forward as part of the input to a fully connected layer, which also gets the shape summary at the end of the convnet. Seems like this has a lot in common with the artistic style approach. However, it seems to only work for texture analysis. figures. might be good in a generative model of texture. not sure where this should go.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02970&amp;sa=D&amp;ust=1465142038202000&amp;usg=AFQjCNGOUA_nfu2vnq5wPY3Cyg1GCjEkAw">http://arxiv.org/abs/1601.02970</a></span><span class="c1">&nbsp;comparison of convnets to the human visual system. concludes that they&#39;re not a good model unless trained on natural images, in which case they are. Should go in &quot;neuromorphic&quot; and &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02733&amp;sa=D&amp;ust=1465142038202000&amp;usg=AFQjCNGinvARe-WFDxy5zpvhAtGBkrOn8w">http://arxiv.org/abs/1601.02733</a></span><span class="c1">&nbsp;some weird autoencoder thing about non-negative weight constraints or something. should go in &quot;unsupervised&quot;, &quot;weights&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02705&amp;sa=D&amp;ust=1465142038203000&amp;usg=AFQjCNFBiYANNrvUymQMxRgRzWu3A6VKRA">http://arxiv.org/abs/1601.02705</a></span><span class="c1">&nbsp;</span><span class="c9 c12 c1">absolutely crazy world-interaction paper</span><span class="c1">&nbsp;- they successfully train a robot to interact with new objects. uses a point cloud volume representation as input to something or other. I wonder if the dataset is public? should go in &quot;rl&quot;, &quot;3d&quot;, and I guess in &quot;robotics&quot; if I have that in here.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02644&amp;sa=D&amp;ust=1465142038204000&amp;usg=AFQjCNHuzB-SbbQUoGwLlDRUriVvwLuCBw">http://arxiv.org/abs/1601.02644</a></span><span class="c1">&nbsp;non-neural, &quot;3D Gaze Estimation from 2D Pupil Positions on Monocular Head-Mounted Eye Trackers&quot;. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.02913&amp;sa=D&amp;ust=1465142038205000&amp;usg=AFQjCNFyglY6AU02X33ioNmRBDjN2w5-UQ">http://arxiv.org/abs/1601.02913</a></span><span class="c1">&nbsp;auto-generated subclasses (I think) for a classification problem with multimodal classes. Looks cool. should go in &quot;classification&quot; and maybe &quot;architecture learning&quot;. mentions yahoo dataset.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.08198&amp;sa=D&amp;ust=1465142038206000&amp;usg=AFQjCNF0pQD6UQnHdoyL5JtsQAb4dj8CkA">http://arxiv.org/abs/1511.08198</a></span><span class="c1">&nbsp;paraphrasic (paraphrase-invariant?) sentence embeddings, using a model that, like word2vec, is small in order to scale. Looks very cool. comes with a dataset. should go in &quot;datasets&quot;, &quot;word embeddings&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04401&amp;sa=D&amp;ust=1465142038207000&amp;usg=AFQjCNHAM79Y0OXf5zMCqjUlYNImhDwP1A">http://arxiv.org/abs/1511.04401</a></span><span class="c1">&nbsp;aligning symbols between sequences of two types - say, language and video feed? - looks cool. Might help with being able to learn word grounding from larger datasets, such as movie captions. should go in &quot;language&quot;, &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06085&amp;sa=D&amp;ust=1465142038207000&amp;usg=AFQjCNGusdtJ6WqJyvNVENmDdFpaxm78xw">http://arxiv.org/abs/1511.06085</a></span><span class="c1">&nbsp;autoencoder-based incremental compression of images. Looks pretty simple, but outdoes typical jpeg by 10%! should go in &quot;unsupervised&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06114&amp;sa=D&amp;ust=1465142038208000&amp;usg=AFQjCNHpmj6nM0uEY3hUZROykcD3_RSrmQ">http://arxiv.org/abs/1511.06114</a></span><span class="c1">&nbsp;generalization via sequence-to-sequence with different encoders/decoders. Looks like it worked exactly as well as I thought it would - it increased performance quite a bit. should go in &quot;generality&quot;, &quot;sequence&quot;, &quot;language&quot;, &quot;images&quot;, &quot;unsupervised&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06444&amp;sa=D&amp;ust=1465142038209000&amp;usg=AFQjCNFNkBkT51e2T7X70yj5fZ7Qq8tmCw">http://arxiv.org/abs/1511.06444</a></span><span class="c1">&nbsp;analysis of halting time of optimizers on different problems. Has the result that halting time follows one of only a few distributions depending on the task. Should go in &quot;learning algorithms and optimizers&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.04630&amp;sa=D&amp;ust=1465142038210000&amp;usg=AFQjCNG-gmC4FMnmMmAI3o94ca8vUUuBrA">http://arxiv.org/abs/1505.04630</a></span><span class="c1">&nbsp;Doing training-a-smaller-network model compression by training an rnn on the outputs of a feed-forward net. Should go in &quot;memory&quot;, &quot;model compression&quot;, and &quot;transfer learning&quot;. also maybe &quot;regularization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1206.5533.pdf&amp;sa=D&amp;ust=1465142038211000&amp;usg=AFQjCNEJ3NTSw6tJw1-NucMAwVLM7niUog">http://arxiv.org/pdf/1206.5533.pdf</a></span><span class="c1">&nbsp;&quot;practical recommendations for training deep networks&quot;. from 2012, but interesting for comparison.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1206.1106.pdf&amp;sa=D&amp;ust=1465142038211000&amp;usg=AFQjCNE6vyZJDEaEPhkmaHnQeA1XHivmTA">http://arxiv.org/pdf/1206.1106.pdf</a></span><span class="c1">&nbsp;sgd variant that doesn&#39;t need learning rates and wildly beats vanilla SGD with even a little bit of a mistuned learning rate. Also beats adagrad. Has no hyperparameters. Note: might vanilla SGD/nesterov/rms/adagrad outdo this with powerful hyperparameter tuning? should go in &quot;hyperparameters&quot; and &quot;learning algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.04069&amp;sa=D&amp;ust=1465142038212000&amp;usg=AFQjCNFei1mp2zEEBl9oE8H4mp4yHg1uIQ">http://arxiv.org/abs/1503.04069</a></span><span class="c1">&nbsp;analysis of lstm learning, pulling apart the lstm to investigate which parts are important and what hyperparameters are actually useful. proposes a bunch of architectures but then shoots them down and finds lstm outperforms them. Should go in &quot;memory&quot;, or &quot;recurrence&quot;, and &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1511.05497v1.pdf&amp;sa=D&amp;ust=1465142038213000&amp;usg=AFQjCNFuB_GFu1GT2D_DKv0HLgYckz9-pA">http://arxiv.org/pdf/1511.05497v1.pdf</a></span><span class="c1">&nbsp;learning the architecture of deep networks via a special activation function that signals the optimizer to change the architecture (...I think). should go in &quot;learning algorithms&quot;, and &quot;architecture learning&quot; or &quot;hyperparameters&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/&amp;sa=D&amp;ust=1465142038214000&amp;usg=AFQjCNG9iFFsngBROpM4smDHck2W5uYldA">http://blog.claymcleod.io/2016/02/06/Parametric-Activation-Pools-greatly-increase-performance-and-consistency-in-ConvNets/</a></span><span class="c1">&nbsp;preliminary research on using multiple activation functions and blending between them. Looks cool, but confusing and not clear whether it actually means anything. should go in &quot;activation functions&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44bxdj/scrn_vs_lstm/czp4hqr&amp;sa=D&amp;ust=1465142038216000&amp;usg=AFQjCNFnZfV6V_QIyGuikuu5FcMc_PMOPw">https://www.reddit.com/r/MachineLearning/comments/44bxdj/scrn_vs_lstm/czp4hqr</a></span><span class="c1">&nbsp;apparently-intuitive explanation of LSTMs. Doesn&#39;t look super interesting beyond that, but a good intro I suppose. Should go in &quot;basics&quot;. also </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://imgur.com/gallery/vaNahKE/new&amp;sa=D&amp;ust=1465142038217000&amp;usg=AFQjCNHBFH7QmDTRfpUmzymD8hYUp5IogA">http://imgur.com/gallery/vaNahKE/new</a></span><span class="c1">&nbsp;as a demo of vanishing gradients in rnns. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44elbp/fantastic_explanation_of_the_lstm_architecture_in/czpx6qo&amp;sa=D&amp;ust=1465142038218000&amp;usg=AFQjCNG3m-O_U5Hxf7cE56NTQK8z6snffw">https://www.reddit.com/r/MachineLearning/comments/44elbp/fantastic_explanation_of_the_lstm_architecture_in/czpx6qo</a></span><span class="c1">&nbsp;Another explanation of LSTMs, claims to be better. should go in &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44ld5c/interesting_papers_on_learning_automatically/&amp;sa=D&amp;ust=1465142038219000&amp;usg=AFQjCNGYKX4J3Y22p65pFyqEAL2TvIyuRg">https://www.reddit.com/r/MachineLearning/comments/44ld5c/interesting_papers_on_learning_automatically/</a></span><span class="c1">&nbsp;links to architecture-learning stuff. should go in &quot;architecture learning&quot; or &quot;hyperparameters&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://research.microsoft.com/apps/video/?id%3D259564&amp;sa=D&amp;ust=1465142038220000&amp;usg=AFQjCNF0YH3otjsi38oh3SGVt0ySppyGDQ">http://research.microsoft.com/apps/video/?id=259564</a></span><span class="c1">&nbsp;video tutorial on using distributed training using tensorflow. should go in &quot;parallelization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf&amp;sa=D&amp;ust=1465142038221000&amp;usg=AFQjCNFnJni5-sATdGuVGfOzS3h_MIC4gw">http://cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf</a></span><span class="c1">&nbsp;model that doesn&#39;t pay much attention to syntax, and gets good results with a smaller bag-of-words-ish model. used for classification and regression. should go in &quot;language&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.scholarpedia.org/article/Temporal_difference_learning&amp;sa=D&amp;ust=1465142038222000&amp;usg=AFQjCNFvlKoJO2PW6kt4XPWsxxLaOj6TcQ">http://www.scholarpedia.org/article/Temporal_difference_learning</a></span><span class="c1">&nbsp;looks like q learning is a subset of temporal difference learning - it uses it to learn the value function, sort of. Can also be thought of as trying to get the right answer as soon as possible. Should go in &quot;reinforcement learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40808.pdf&amp;sa=D&amp;ust=1465142038223000&amp;usg=AFQjCNEsX_Qh2QdQQ3mkCddIbgb18xmHgg">http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40808.pdf</a></span><span class="c1">&nbsp;exploration of what learning rates work well for speech recognition. goes in &quot;hyperparameters&quot; I guess. no interesting suggestions though.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44mzfh/implementing_dynamic_memory_networks_in_theano/&amp;sa=D&amp;ust=1465142038224000&amp;usg=AFQjCNH4UHsibGUPw1dYgYXAc6Op0PNVUg">https://www.reddit.com/r/MachineLearning/comments/44mzfh/implementing_dynamic_memory_networks_in_theano/</a></span><span class="c1">&nbsp;implementation of a thing that works on bAbI in theano. looks cool. should go in &quot;memory&quot; and &quot;examples&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1411.2738v3.pdf&amp;sa=D&amp;ust=1465142038225000&amp;usg=AFQjCNEKIugYSVMDX8UPo8XTCOh6eWFQGw">http://arxiv.org/pdf/1411.2738v3.pdf</a></span><span class="c1">&nbsp;investigating word2vec internals. should go in &quot;embeddings&quot; and &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03288&amp;sa=D&amp;ust=1465142038225000&amp;usg=AFQjCNHiOac__U9dlRiDPBCdu3XU2WPdYg">http://arxiv.org/abs/1601.03288</a></span><span class="c1">&nbsp;training a classifier on its own test set output without asking a human. Analysis of when this works. Should go in &quot;data efficiency&quot; or &quot;semi supervised&quot;. I wonder if you could do this where the training monitors it to find a threshold is of its confidence above which it&#39;s always or nearly always right, and then use only that confidence threshold to generate more training examples. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05644&amp;sa=D&amp;ust=1465142038226000&amp;usg=AFQjCNGaR3-OlkR64vOJC9dFPLrD-IcaMA">http://arxiv.org/abs/1511.05644</a></span><span class="c1">&nbsp;looks very cool - someone plugged a DCGAN adversary into an autoencoder and it worked. However, looks like they might not have plugged it in at the end? not sure&hellip;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03313&amp;sa=D&amp;ust=1465142038227000&amp;usg=AFQjCNEwUUfWzi44J2XgE1MU3-LZxd2Kmw">http://arxiv.org/abs/1601.03313</a></span><span class="c1">&nbsp;automatic generation of political speeches based on a combination of a topic model for consistency and an RNN for actual sentence generation. Looks very interesting, cool approach to the consistency model - I wonder if this is a good reference for what to subsume for . should go in &quot;language/talking computers&quot; and &quot;language/topic models&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03317&amp;sa=D&amp;ust=1465142038228000&amp;usg=AFQjCNEy0-lTOD2LxiJqrXfnoVvVn86dQA">http://arxiv.org/abs/1601.03317</a></span><span class="c1">&nbsp;translation improvement by improving alignment of input streams or something. improves by 2 bleu points. don&#39;t know what that means. should go in &quot;machine translation&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03055&amp;sa=D&amp;ust=1465142038228000&amp;usg=AFQjCNFyj2PBxfj5ETD_egDVsPSaxY9srg">http://arxiv.org/abs/1601.03055</a></span><span class="c1">&nbsp;approach for smoothing over potentially missing tags on user created image label dataset. uses a shallow technique, &quot;subspace clustering&quot;, and cnns somehow. should go in &quot;images&quot; somewhere.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03128&amp;sa=D&amp;ust=1465142038229000&amp;usg=AFQjCNH_EnCvKMHTEHQy0UE1JeEOnLmkww">http://arxiv.org/abs/1601.03128</a></span><span class="c1">&nbsp;non-neural, non-deep, but uses cnn features? approach for scene text recognition (surprising if this can get good results without deep stuff, but I suppose text recognition has worked for a while). should go in &quot;ocr&quot;. mentions datasets &quot;Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word&quot; so maybe also in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">misc stuff: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03239&amp;sa=D&amp;ust=1465142038230000&amp;usg=AFQjCNGRc3DMA9H9Xmc-xKgRSx2_4lE8IQ">http://arxiv.org/abs/1601.03239</a></span><span class="c1">&nbsp;- arms race in image forensics vs image falsification. should go in &quot;misc&quot; probably, maybe also &quot;images&quot;. Also &quot;security&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03295&amp;sa=D&amp;ust=1465142038231000&amp;usg=AFQjCNFmiRpnHGPxfym12uFT2RsJ0-4cyQ">http://arxiv.org/abs/1601.03295</a></span><span class="c1">&nbsp;classifying and looking up images that appear in patent documents, based on topic models or something. should go in &quot;applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03333&amp;sa=D&amp;ust=1465142038231000&amp;usg=AFQjCNF9nvs1_x6fszIPryQ9DBzp0TbG9g">http://arxiv.org/abs/1601.03333</a></span><span class="c1">&nbsp;research in biometric access control by using eye movements as an additional signal. apparently gets really good results. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03375&amp;sa=D&amp;ust=1465142038232000&amp;usg=AFQjCNHw5I6IOE59Exsr12HbowrBDvnveg">http://arxiv.org/abs/1601.03375</a></span><span class="c1">&nbsp;segmenting the spinal cord in xrays or something. should go in &quot;bio&quot;. &nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03073&amp;sa=D&amp;ust=1465142038233000&amp;usg=AFQjCNFdOf4I07O5SBQAhmVeMTotyf_Gxg">http://arxiv.org/abs/1601.03073</a></span><span class="c1">&nbsp;formal model of exploration vs exploitation using information maximization (I think. they say &quot;infomax&quot; which is probably the same thing). should go in &quot;rl&quot; and &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.03293&amp;sa=D&amp;ust=1465142038234000&amp;usg=AFQjCNHXEkfKes7cE4sLyc0uH-hY-dV1Iw">http://arxiv.org/abs/1504.03293</a></span><span class="c1">&nbsp;improvements in object detection by refining in the localization step via bayesian optimization. Looks fine. Also looks a bit old. mentions using structured loss, which is cool. should go in &quot;images/object detection&quot; or whatever I called it.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.08350&amp;sa=D&amp;ust=1465142038235000&amp;usg=AFQjCNGntiw-iWeYfmLEa2QbMCJctg0p1g">http://arxiv.org/abs/1506.08350</a></span><span class="c1">&nbsp;accelerated/stabilized SGD via including global gradient context. this has been tried before, but their contribution is a way to do it faster. (note: isn&#39;t this what momentum does for you?) should go in &quot;training algorithms&quot; or &quot;optimizers&quot; if the latter exists</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06644&amp;sa=D&amp;ust=1465142038236000&amp;usg=AFQjCNEfCXN_w0Z2kZk3WyaeEsRUNwfMxQ">http://arxiv.org/abs/1511.06644</a></span><span class="c1">&nbsp;variation of gaussian processes that has memory, like an RNN. may be interesting to pull ideas for how to do an rnn; I suspect that this has excessive-simplicity problems, but may be interesting for smooth prediction sort of stuff. Would probably work well as a final layer, replacing lstm, for some sorts of things. should go in &quot;memory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02215&amp;sa=D&amp;ust=1465142038237000&amp;usg=AFQjCNGaE-j-sPwizuND3MX5ubkLPfAXxw">http://arxiv.org/abs/1602.02215</a></span><span class="c1">&nbsp;an approach using some sort of matrix thing I don&#39;t understand to find missing examples or something like that. &quot;swivel&quot;. made a big splash on reddit: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44u9nb/_/&amp;sa=D&amp;ust=1465142038237000&amp;usg=AFQjCNE0pKUpEbr2LmNNrYb0BqEt87vkWw">https://www.reddit.com/r/MachineLearning/comments/44u9nb/_/</a></span><span class="c1">. should go in &quot;embeddings&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.01240&amp;sa=D&amp;ust=1465142038238000&amp;usg=AFQjCNH3WeDjFq9FO295arCfZ-l-XDJo7A">http://arxiv.org/abs/1509.01240</a></span><span class="c1">&nbsp;formal analysis of training time of SGD, finding that reducing training time is a big deal for generalization. made a big splash on reddit: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44u927/_/&amp;sa=D&amp;ust=1465142038238000&amp;usg=AFQjCNF30thLcVYu-2y3UtnoSu-CSEkWPA">https://www.reddit.com/r/MachineLearning/comments/44u927/_/</a></span><span class="c1">. should go in &quot;optimizers&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://mtyka.github.io/deepdream/2016/02/05/bilateral-class-vis.html&amp;sa=D&amp;ust=1465142038240000&amp;usg=AFQjCNH0JqGQLGV22gnLOral-FkNsUlrmw">http://mtyka.github.io/deepdream/2016/02/05/bilateral-class-vis.html</a></span><span class="c1">&nbsp;new variation to deep dream that produces AMAZING results. </span><span class="c12 c1">Huge improvement in SOTA on something, but I don&#39;t fucking know what. </span><span class="c1">produces images that actually look like they contain a given class. Should go in &quot;regularization&quot; and &quot;debugging&quot;, I think. maybe also &quot;generative&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02261&amp;sa=D&amp;ust=1465142038241000&amp;usg=AFQjCNHUMIyd4ng8W20V_9K5SpXoXd4Hgw">http://arxiv.org/abs/1602.02261</a></span><span class="c1">&nbsp;awesome new dataset generator (&quot;webnav&quot;) and dataset (&quot;wikinav&quot;) for web navigation as a planning/natural language understanding/RL task. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44rrzm/pretrained_speech_text_nns/&amp;sa=D&amp;ust=1465142038241000&amp;usg=AFQjCNGXNV5j74Cf5Dh0yJbyI2DkJOHPAQ">https://www.reddit.com/r/MachineLearning/comments/44rrzm/pretrained_speech_text_nns/</a></span><span class="c1">&nbsp;speech recognition models that are public. answer: not many. should go in &quot;speech recognition&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/44swb0/looking_to_write_a_natural_language_query/&amp;sa=D&amp;ust=1465142038242000&amp;usg=AFQjCNFXDOMcfAnk3S_359vuiUWLzot0vA">https://www.reddit.com/r/MachineLearning/comments/44swb0/looking_to_write_a_natural_language_query/</a></span><span class="c1">&nbsp;generating database queries from natural language. very interesting. not sure where this should go. maybe &quot;language parsing&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.lab41.org/nine-datasets-for-investigating-recommender-systems/&amp;sa=D&amp;ust=1465142038243000&amp;usg=AFQjCNGDUV09tdMLP7-VTOr42c_iAWW7Ew">http://www.lab41.org/nine-datasets-for-investigating-recommender-systems/</a></span><span class="c1">&nbsp;recommender systems datasets. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03411&amp;sa=D&amp;ust=1465142038244000&amp;usg=AFQjCNEr9x2-C2vqacpPpaQY6SLOzLOz5Q">http://arxiv.org/abs/1601.03411</a></span><span class="c1">&nbsp;non-miri ai safety paper that cites miri. these exist!? analysis of non-halting functions something something. looks cool. should go in &quot;agi safety&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03651&amp;sa=D&amp;ust=1465142038246000&amp;usg=AFQjCNGwvrESC5RlSOrSzsLNV4ZN4moYJA">http://arxiv.org/abs/1601.03651</a></span><span class="c1">&nbsp;&quot;relation classification&quot; with dnns. should go in &quot;applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03531&amp;sa=D&amp;ust=1465142038246000&amp;usg=AFQjCNEW2XlU0DquaXKJgJRAiEOL0lUK6w">http://arxiv.org/abs/1601.03531</a></span><span class="c1">&nbsp;tissue classification or something. not neural. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03642&amp;sa=D&amp;ust=1465142038247000&amp;usg=AFQjCNE5nYyIe2Zn1XDsxMTzA7rNwx3Gng">http://arxiv.org/abs/1601.03642</a></span><span class="c1">&nbsp;intro to the current work on making creative machines for beginners. should go in &quot;intro&quot; or &quot;initial&quot; or whatever I called it. &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03679&amp;sa=D&amp;ust=1465142038248000&amp;usg=AFQjCNH3g7mGxHVAxAifP9d3C5LlriKV_Q">http://arxiv.org/abs/1601.03679</a></span><span class="c1">&nbsp;unclear if it&#39;s neural, but they try to train something to classify events in videos as classes that it hasn&#39;t seen examples of, based on similar classes. or something. looks cool. should go in &quot;video&quot; and &quot;classification&quot; and maybe &quot;data efficiency&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03466&amp;sa=D&amp;ust=1465142038249000&amp;usg=AFQjCNGOrEDhCJdsbb4Irai4xfPGdNAq5w">http://arxiv.org/abs/1601.03466</a></span><span class="c1">&nbsp;something about preserving privacy in distributed ml training. mentions differential privacy. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03478&amp;sa=D&amp;ust=1465142038250000&amp;usg=AFQjCNHMRFbxMvS56-DBy0MHML2PjTMK3g">http://arxiv.org/abs/1601.03478</a></span><span class="c1">&nbsp;simpler model for bidirectional image lookup from text, text lookup from image, and maybe generating them as well. should go in &quot;images/captioning&quot; and &quot;language&quot; somewhere.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03649&amp;sa=D&amp;ust=1465142038251000&amp;usg=AFQjCNFPJRtli3L4Aqy89yAbKGoIsoeQQw">http://arxiv.org/abs/1601.03649</a></span><span class="c1">&nbsp;neuromorphic models - spiking NNs and precise spike timing as an encoding. should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.03710&amp;sa=D&amp;ust=1465142038251000&amp;usg=AFQjCNGyyOb4SKG10ZoC2MpnnNux_ypLbQ">http://arxiv.org/abs/1510.03710</a></span><span class="c1">&nbsp;dialog state tracking application. should go in &quot;applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.02017&amp;sa=D&amp;ust=1465142038252000&amp;usg=AFQjCNGIBWpgUwef44oUjTnH9k4iQiNH4g">http://arxiv.org/abs/1512.02017</a></span><span class="c1">&nbsp;exploration of different ways of visualizing what image models think of an image. looks like it includes deep dream as one of the things it considers. should go in &quot;debugging&quot;. - arXiv admin note: text overlap with arXiv:1412.0035; Comments: A substantially extended version of </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf&amp;sa=D&amp;ust=1465142038253000&amp;usg=AFQjCNG21FrRORalbStDX7k5Qr1bQ33Ppg">http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06381&amp;sa=D&amp;ust=1465142038253000&amp;usg=AFQjCNFiKPVTJyey2HrRkLHf9_u8j8PMyA">http://arxiv.org/abs/1511.06381</a></span><span class="c1">&nbsp;&quot;Manifold Regularized Deep Neural Networks using Adversarial Examples&quot;. minimize difference between internal representation of adverserial examples and normal images. Probably pairs well with the other (is it a duplicate?) adverserial-training framing of this problem I have in here. should go in &quot;adverserial examples&quot;, which is in &quot;images&quot;, iirc.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://scholar.google.com/scholar?cluster%3D14180974105121584086%26hl%3Den%26as_sdt%3D0,5%26sciodt%3D0,5&amp;sa=D&amp;ust=1465142038255000&amp;usg=AFQjCNEte849xqh7p-GsKOeW087f7lZkww">https://scholar.google.com/scholar?cluster=14180974105121584086&amp;hl=en&amp;as_sdt=0,5&amp;sciodt=0,5</a></span><span class="c1">&nbsp;differentiable raytracer. &nbsp;Should go in &quot;images&quot;, &quot;non neural &quot;. See also </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://optics.synopsys.com/codev/pdfs/DifferentialRayTracing.pdf&amp;sa=D&amp;ust=1465142038255000&amp;usg=AFQjCNGTZoWGjguYCcIghaBfbcS0v5K2uQ">https://optics.synopsys.com/codev/pdfs/DifferentialRayTracing.pdf</a></span><span class="c1">&nbsp;and </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/lahwran/reversible-raytracer&amp;sa=D&amp;ust=1465142038256000&amp;usg=AFQjCNEn2P0o36fdfk0vHS_ODYpuAeqhtA">https://github.com/lahwran/reversible-raytracer</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03764&amp;sa=D&amp;ust=1465142038257000&amp;usg=AFQjCNGgVrXCCcYz-uqBcIsp2Lvbl405vw">http://arxiv.org/abs/1601.03764</a></span><span class="c1">&nbsp;word sense disambiguation/handling polysemy. analysis of the word vectors generated by things like word2vec for polysemic words. should go in &quot;word embeddings&quot;. looks pretty cool. uses sparse coding to recover the different word embeddings (!), so should also go in &quot;sparse coding&quot;. </span><span class="c9 c12 c1">big deal for word polysemy, I think.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03896&amp;sa=D&amp;ust=1465142038257000&amp;usg=AFQjCNHyMm5AsvyjiYnbwUPE7PvUaRxsHA">http://arxiv.org/abs/1601.03896</a></span><span class="c1">&nbsp;overview of various approaches to solving the image description problem, datasets, metrics, etc. should go in &quot;overviews&quot; or something, something like &quot;basics&quot; except maybe not basic?, and &quot;images/captioning&quot; and/or &quot;language/description&quot;, maybe also &quot;objectives&quot; (did I call it &quot;loss&quot;?) and &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03916&amp;sa=D&amp;ust=1465142038258000&amp;usg=AFQjCNFHTusO4azUccZQzMKEmdV6Vzvqbw">http://arxiv.org/abs/1601.03916</a></span><span class="c1">&nbsp;neural machine translation using some sort of transfer learning from image captioning. should go in &quot;machine translation&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1601.03890v2.pdf&amp;sa=D&amp;ust=1465142038259000&amp;usg=AFQjCNHeV_a3W6F9huYNNqk6mJeupCq_gA">http://arxiv.org/pdf/1601.03890v2.pdf</a></span><span class="c1">&nbsp;depth inference via matching stereo images. Looks non-neural, but potentially useful as a reference for 3d stuff. should go in &quot;images/3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03945&amp;sa=D&amp;ust=1465142038260000&amp;usg=AFQjCNHcJxqzR8FvbdMKfb0RkCGI5SRKLg">http://arxiv.org/abs/1601.03945</a></span><span class="c1">&nbsp;non-neural deep feature extraction via some graphical model thingy. should go in &quot;alternate deep&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://kevinchai.net/datasets&amp;sa=D&amp;ust=1465142038261000&amp;usg=AFQjCNHeVpNzhtKLIssw1w_n23gp73LJkw">http://kevinchai.net/datasets</a></span><span class="c1">&nbsp;index of indexes of open datasets; came up in search for an interesting one, &quot;duc2014&quot;. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.00685&amp;sa=D&amp;ust=1465142038262000&amp;usg=AFQjCNE4J75lzBorGBU1A4ABrsmxKBNz8A">http://arxiv.org/abs/1509.00685</a></span><span class="c1">&nbsp;text summarization. </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/carpedm20/neural-summary-tensorflow&amp;sa=D&amp;ust=1465142038262000&amp;usg=AFQjCNHHs2EpPfC7DtFenp5WBzHo6RNBCg">https://github.com/carpedm20/neural-summary-tensorflow</a></span><span class="c1">&nbsp;implementation of it. </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://people.seas.harvard.edu/~srush/emnlp2015_slides.pdf&amp;sa=D&amp;ust=1465142038262000&amp;usg=AFQjCNEr-Y7Yz_IB1ynn-wsRKAPgWGkDFA">http://people.seas.harvard.edu/~srush/emnlp2015_slides.pdf</a></span><span class="c1">&nbsp;slides about it. looks like it might come with pretrained models, based on the duc2014 dataset. should go in &quot;language/text understanding&quot;, &quot;examples&quot;, and &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://open.blogs.nytimes.com/2009/01/12/fatten-up-your-corpus/?_php%3Dtrue%26_type%3Dblogs%26_r%3D0&amp;sa=D&amp;ust=1465142038263000&amp;usg=AFQjCNF4whCM7OdJsKZE-wAmWBebVku5HA">http://open.blogs.nytimes.com/2009/01/12/fatten-up-your-corpus/?_php=true&amp;_type=blogs&amp;_r=0</a></span><span class="c1">&nbsp;new york times historical data corpus - also, annotations from google about entity salience: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://googleresearch.blogspot.ca/2014/08/teaching-machines-to-read-between-lines.html&amp;sa=D&amp;ust=1465142038264000&amp;usg=AFQjCNFInp1vON99eBaiH2mZ833vEGIoqA">http://googleresearch.blogspot.ca/2014/08/teaching-machines-to-read-between-lines.html</a></span><span class="c1">&nbsp;should go in &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/viorik/ConvLSTM&amp;sa=D&amp;ust=1465142038265000&amp;usg=AFQjCNG5jAOdToAgv7x45yCWPSX8cxbOYQ">https://github.com/viorik/ConvLSTM</a></span><span class="c1">&nbsp;video thing - classify me in more detail; has an open dataset: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://mi.eng.cam.ac.uk/~vp344/&amp;sa=D&amp;ust=1465142038265000&amp;usg=AFQjCNGoehFvV0i23n3Zw_cWE2XF1tJCbw">http://mi.eng.cam.ac.uk/~vp344/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.iclr.cc/doku.php?id%3Diclr2016:main%23accepted_papers_conference_track&amp;sa=D&amp;ust=1465142038266000&amp;usg=AFQjCNHu_WftTWMoOQtyCezZsGa5H6BSIQ">http://www.iclr.cc/doku.php?id=iclr2016:main#accepted_papers_conference_track</a></span><span class="c1">&nbsp;iclr papers.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1404.2188v1.pdf&amp;sa=D&amp;ust=1465142038267000&amp;usg=AFQjCNHPsZZyRbkN5pZIwJ98UoQmH9fKGQ">http://arxiv.org/pdf/1404.2188v1.pdf</a></span><span class="c1">&nbsp;convnets as language parsers. maxpooling as dynamic tree selection. looks like this solves the variable length sentence problem!! should go in &quot;my ideas&quot;, &quot;language/parsing&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://web.archive.org/web/20150910032430/http://www.ri.cmu.edu/pub_files/2015/1/Venkatraman.pdf&amp;sa=D&amp;ust=1465142038268000&amp;usg=AFQjCNGegarDSopXQoJjSVX2koBSQvCZDw">https://web.archive.org/web/20150910032430/http://www.ri.cmu.edu/pub_files/2015/1/Venkatraman.pdf</a></span><span class="c1">&nbsp;multi-step prediction theory and performance improvement. see also the facebook go paper which uses multi-step prediction regularization. should go in &quot;memory&quot;, and also some other category. maybe &quot;recurrence&quot;? &quot;time series&quot;?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03032&amp;sa=D&amp;ust=1465142038269000&amp;usg=AFQjCNFmsNvVpoQU_Y0GETvzFDSyW2kDSg">http://arxiv.org/abs/1602.03032</a></span><span class="c1">&nbsp;[abs][sanity] memory approach that has unbounded (?) associative memory. not sure how &quot;associative&quot; differs from the attentional approaches used in things like ntms, but it sounds cool. </span><span class="c9 c1">doesn&#39;t need any extra parameters. </span><span class="c1">should go in &quot;memory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1602.03616v1.pdf&amp;sa=D&amp;ust=1465142038270000&amp;usg=AFQjCNFEBhRnzyr3ZDR6YiBuGUVEQ5xfZg">http://arxiv.org/pdf/1602.03616v1.pdf</a></span><span class="c1">&nbsp;multimodal deep dream - it&#39;s obviously silly to tell a network to maximize an activation of a class that has multiple large modalities. this takes that into account. didn&#39;t read how. good results visually. would work well with the blurring technique for regularization. should go in &quot;debugging&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/454ksm/tutorial_on_deconvolution/&amp;sa=D&amp;ust=1465142038270000&amp;usg=AFQjCNH33L2IaqVk5MtFN1com8PHRw9hxA">https://www.reddit.com/r/MachineLearning/comments/454ksm/tutorial_on_deconvolution/</a></span><span class="c1">&nbsp;some explanation of why deconvolution is also just convolution. I didn&#39;t quite get it, but it sounds cool. should go in &quot;generative&quot; and &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.deeplearningbook.org/&amp;sa=D&amp;ust=1465142038271000&amp;usg=AFQjCNG_fRM80HA_ijtBOL9MaOdDyVK7IA">http://www.deeplearningbook.org/</a></span><span class="c1">&nbsp;book preprint about deep learning, </span><span class="c9 c1">by yoshua and friends</span><span class="c1">. should go in &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1602.03483.pdf&amp;sa=D&amp;ust=1465142038272000&amp;usg=AFQjCNELgyP2VL5n3kwH6RdLTfhyOrMx-g">http://arxiv.org/pdf/1602.03483.pdf</a></span><span class="c1">&nbsp;unsupervised/representation learning of sentences. looks like it doesn&#39;t find anything remarkable. kinda useful to have around I guess. should go in &quot;language&quot; and &quot;unsupervised&quot;. </span><span class="c9 c1">they try a bunch of models - a good index of language learning models.</span></p><p class="c3"><span class="c9 c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/&amp;sa=D&amp;ust=1465142038273000&amp;usg=AFQjCNEJl_y4T49zu3UCnwE80ORzJAsumQ">http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/</a></span><span class="c1">&nbsp;music model that seems to be quite a bit better than other attempts. looks like it uses convolution over time. I&#39;ll bet you could do convolution over time using a striding mode in torch&hellip;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://nuit-blanche.blogspot.com/2015/03/sparse-coding-20-years-later.html&amp;sa=D&amp;ust=1465142038274000&amp;usg=AFQjCNH5ozD-6GwrrqOE-s1ocUuVC3Gl7g">http://nuit-blanche.blogspot.com/2015/03/sparse-coding-20-years-later.html</a></span><span class="c1">&nbsp;the state of sparse coding. should go in &quot;sparsity&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://tensortalk.com/&amp;sa=D&amp;ust=1465142038275000&amp;usg=AFQjCNF6Jtv6tBARv7x8YQggPMG1jbTu1A">https://tensortalk.com/</a></span><span class="c1">&nbsp;alternative to r/machinelearning that gets less activity but interesting diversity. should go in &quot;basics&quot; somewhere. has nice tags with some cool stuff, like &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/nvbn/thefuck&amp;sa=D&amp;ust=1465142038275000&amp;usg=AFQjCNHwDfTtfMBiMCClsHZLnNyrxY5W4A">https://github.com/nvbn/thefuck</a></span><span class="c1">&nbsp;amusing training target, maybe. should go in &quot;datasets&quot; or maybe &quot;applications&quot; or &hellip; something.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://svail.github.io/mandarin/&amp;sa=D&amp;ust=1465142038276000&amp;usg=AFQjCNF9utzXtIreODhcyyKW3c9pIuxBQw">http://svail.github.io/mandarin/</a></span><span class="c1">&nbsp;baidu speech recognition research logs. they outperform humans on that noise profile. doesn&#39;t come with a dataset. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.wildml.com/deep-learning-glossary/&amp;sa=D&amp;ust=1465142038277000&amp;usg=AFQjCNE4YzWI3vEkFrktLDTQnAQLSIunww">http://www.wildml.com/deep-learning-glossary/</a></span><span class="c1">&nbsp;a ton of useful definitions. should go in &quot;basics&quot; probably? maybe also anywhere that it mentions.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1602.02410v1.pdf&amp;sa=D&amp;ust=1465142038278000&amp;usg=AFQjCNEzErFKDsYfOl_C4DseSU230BqqKw">http://arxiv.org/pdf/1602.02410v1.pdf</a></span><span class="c1">&nbsp;massive language model training that use all the bells and whistles and </span><span class="c9 c1 c10">is a crazy immense improvement in SOTA</span><span class="c1">. includes a better memory technique. should go in &quot;multi-modal models&quot; (which needs to be a new category), &quot;language&quot;, &quot;memory&quot;, &quot;parallelization&quot;. uses a char-rnn, a char-cnn-rnn, and maybe a deconv char-cnn? also 20x less model parameters, half the perplexity.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://da-data.blogspot.com/2016/02/cleaning-imagenet-dataset-collected.html&amp;sa=D&amp;ust=1465142038279000&amp;usg=AFQjCNFa5DMeMEjSAjhPQ7GCgOlWa5Qoww">http://da-data.blogspot.com/2016/02/cleaning-imagenet-dataset-collected.html</a></span><span class="c1">&nbsp;imagenet has problems - should go in &quot;datasets&quot;. I need to get the cleaned data.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/zer0n/deepframeworks/blob/master/README.md&amp;sa=D&amp;ust=1465142038279000&amp;usg=AFQjCNHdiAufUX9IzZUIJX7LY9FgcFEUFQ">https://github.com/zer0n/deepframeworks/blob/master/README.md</a></span><span class="c1">&nbsp;comparison of different frameworks in quite a bit of detail. Backs up my conclusions about torch being the best of the pack right now, cntk may be good too, but primarily for computer-parallel stuff. should go in &quot;libraries&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/tensorflow/skflow&amp;sa=D&amp;ust=1465142038280000&amp;usg=AFQjCNHIMUxZQLzftM1Co0mlYa3-6Rjf8w">https://github.com/tensorflow/skflow</a></span><span class="c1">&nbsp;tensorflow, &quot;tensorflow-- edition&quot;; should go in &quot;libraries&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://medium.com/@NathanBenaich/investing-in-artificial-intelligence-a-vc-perspective-afaf6adc82ea%23.z0vo6rt3i&amp;sa=D&amp;ust=1465142038281000&amp;usg=AFQjCNGKN8NaReN5PvuclxJch09tnxfq-Q">https://medium.com/@NathanBenaich/investing-in-artificial-intelligence-a-vc-perspective-afaf6adc82ea#.z0vo6rt3i</a></span><span class="c1">&nbsp;misc information on the state of investment in ml. should go in &quot;misc&quot;. summary: there&#39;s a bubble, author wants it to continue.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://tensortalk.com/?cat%3Ddatasets&amp;sa=D&amp;ust=1465142038282000&amp;usg=AFQjCNHd-G-HQS5rAHshZieNK7QzBQvZyg">https://tensortalk.com/?cat=dataset</a></span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://tensortalk.com/?cat%3Ddatasets&amp;sa=D&amp;ust=1465142038282000&amp;usg=AFQjCNHd-G-HQS5rAHshZieNK7QzBQvZyg">s</a></span><span class="c1">&nbsp;some datasets. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/shuzi/insuranceQA&amp;sa=D&amp;ust=1465142038283000&amp;usg=AFQjCNGxKg6DCzbWx1PbBt5O-NdKRTX7sA">https://github.com/shuzi/insuranceQA</a></span><span class="c1">&nbsp;a copy of the insuranceqa dataset (forked to me &#39;cause I don&#39;t trust them to leave it up). should go in &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.03167&amp;sa=D&amp;ust=1465142038283000&amp;usg=AFQjCNG2A8xYF-HJn62Rh998Ow4XbB20Aw">http://arxiv.org/abs/1502.03167</a></span><span class="c1">&nbsp;the actual batch norm paper. need to read a bit to figure out what categories it might go in besides &quot;regularization&quot;. possibly theory. notable because it trains many times faster (14x in tests) and [writing timed out] -</span><span class="c9 c12 c1">&nbsp;huge things in 2015</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.4280&amp;sa=D&amp;ust=1465142038284000&amp;usg=AFQjCNHhGRdvMO2WNMraGbWTWvvOQk8MBg">http://arxiv.org/abs/1411.4280</a></span><span class="c1">&nbsp;localization with some sort of fully conv network that has multiple branches. interesting contribution is spatial dropout, which is a much better transformation of dropout into the conv world: kill a whole feature map, since the feature map is effectively the tiling of one neuron from a fully connected layer. should go in at least &quot;regularization&quot; because of this, but also probably other places. found because torch implements this as SpatialDropout.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03220&amp;sa=D&amp;ust=1465142038285000&amp;usg=AFQjCNHxn7IgIRhiTWmM6-XMEcY9NTXaOA">http://arxiv.org/abs/1602.03220</a></span><span class="c1">&nbsp;some sort of regularization for generative models via a discriminative model. looks like it&#39;s probably a similar idea to generative adverserial networks - stick something on the end that tries to figure out what the generative network was thinking, and maximize that, or something. fairly obvious extension to the GAN idea, I think. should go in &quot;regularization&quot;, &quot;generative&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04491&amp;sa=D&amp;ust=1465142038286000&amp;usg=AFQjCNFp2S3r33VYBYeGAhbDAewA6CoSSA">http://arxiv.org/abs/1511.04491</a></span><span class="c1">&nbsp;recursive cnn (ie, same cnn layer repeated?) for large-scale generative modeling/superresolution of images. Looks related to the thing that soumith was involved in, eyescream. uses skip connections and something - possibly related to q learning? - for passing training data back along the recursive network. should go in &quot;generative&quot;, &quot;recursive&quot;, &quot;skip connections&quot;, maybe &quot;rl&quot;, maybe &quot;memory&quot; (because vanishing gradients).</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04587&amp;sa=D&amp;ust=1465142038287000&amp;usg=AFQjCNHP5JO4bSY13MSsObcrbQTT1oN7ew">http://arxiv.org/abs/1511.04587</a></span><span class="c1">&nbsp;another superresolution method, this time using a very deep network. mentions learning residuals, but also only using 20 layers (wat? why stop there?). also mentions very high learning rates - 10,000 times larger than SRCNN, which is probably another superresolution approach. same people as 1511.04491. should go in &quot;generative&quot;, &quot;skip connections&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06464&amp;sa=D&amp;ust=1465142038287000&amp;usg=AFQjCNHkT_Sw4Rs6Zwrv2-6wnpSaQWwijQ">http://arxiv.org/abs/1511.06464</a></span><span class="c1">&nbsp;unitary evolution rnns - uses a unitary matrix, which apparently has some reversibility properties, as the weights for an rnn. should go in &quot;memory&quot;, &quot;architecture&quot; (or something? where should we put interesting fundamental tweaks?), &quot;weights&quot;, and/or &quot;recurrence&quot;. comes with code and a reddit discussion: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/3uk2q5/151106464_unitary_evolution_recurrent_neural/&amp;sa=D&amp;ust=1465142038288000&amp;usg=AFQjCNFmAx8oQURzgww9N_Pvm5pZ9YczbQ">https://www.reddit.com/r/MachineLearning/comments/3uk2q5/151106464_unitary_evolution_recurrent_neural/</a></span><span class="c1">&nbsp;- mentions that the learning curves are &quot;shockingly good&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://jvns.ca/blog/2016/01/02/winning-the-bias-variance-tradeoff/&amp;sa=D&amp;ust=1465142038289000&amp;usg=AFQjCNEgk9mCppN8eQdkNUOIsD_nMQI0gQ">http://jvns.ca/blog/2016/01/02/winning-the-bias-variance-tradeoff/</a></span><span class="c1">&nbsp;machine learning theory: variance of the model relative to what subset of the training data it saw, as a way of determining the true quality of the model. mentions regularization existing as a way to improve the model and there being formalized stuff about it, but she doesn&#39;t understand it. should go in &quot;theory&quot;, though it&#39;s not specific to NNs, it&#39;s about learning models in general.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.04069&amp;sa=D&amp;ust=1465142038289000&amp;usg=AFQjCNEDX43T21RFA8sc17VHxcGnrbeFeg">http://arxiv.org/abs/1503.04069</a></span><span class="c1">&nbsp;J&uuml;rgen Schmidhuber and friends explore some alternatives to LSTM, find them all to be infinitely inferior, and formally prove that nothing exists besides LSTMs. they also analyze what parts of lstm seem to work as well as they do, and make a claim about this result. should go in &quot;recurrence&quot; or &quot;memory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.00853&amp;sa=D&amp;ust=1465142038290000&amp;usg=AFQjCNGK8zJSDToSKUm5TGio6NF5Ojo8gA">http://arxiv.org/abs/1505.00853</a></span><span class="c1">&nbsp;reproductions of all the variations of relu, plus a new one, &quot;random relu&quot;. </span><span class="c9 c1">&quot;Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU&quot;. </span><span class="c1">should go in &quot;activation functions&quot;, and &quot;theory&quot;, maybe also &quot;sparsity&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/45b35v/difference_between_adding_layers_and_adding/czwv771?context%3D3&amp;sa=D&amp;ust=1465142038291000&amp;usg=AFQjCNFI4r2ZlPOqAxTYJ2CuLDPGFPDknw">https://www.reddit.com/r/MachineLearning/comments/45b35v/difference_between_adding_layers_and_adding/czwv771?context=3</a></span><span class="c1">&nbsp;-&gt; &quot;Look at &quot;Do deep networks really need to be deep?&quot; by Ba &amp; Caruana. It was basically &quot;Hey look, model compression [Rich Caruana&#39;s favourite topic] also applies to shrinking deep nets to shallow nets&quot;, following shortly on the heels of the Geoff Hinton &quot;distillation&quot;/&quot;dark knowledge&quot; work for compressing an ensemble of neural nets into one.&quot; -&gt; relevant to &quot;model compression&quot; and &quot;regularization&quot;, possibly also &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://research.microsoft.com/apps/video/default.aspx?id%3D259218%26r%3D1&amp;sa=D&amp;ust=1465142038293000&amp;usg=AFQjCNHV6OISJ9W28_8CVgFV0LQN_if0Xw">http://research.microsoft.com/apps/video/default.aspx?id=259218&amp;r=1</a></span><span class="c1">&nbsp;overview of bayes in ml: smaller datasets, uncertainty, new state of the art results, plugging deep learning into other bayesian techniques. should go in &quot;theory/bayesian&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://medium.com/@samim/assisted-drawing-7b26c81daf2d%23.1b53ryido&amp;sa=D&amp;ust=1465142038293000&amp;usg=AFQjCNF1UMtYBqsgjg7NunVkUlJKltAAvg">https://medium.com/@samim/assisted-drawing-7b26c81daf2d#.1b53ryido</a></span><span class="c1">&nbsp;some interesting demos of approaches to using ml and such to make drawing easier. &quot;applications/misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thetalkingmachines.com/blog/&amp;sa=D&amp;ust=1465142038294000&amp;usg=AFQjCNH-WGbjf8BqhDqzJiblTsWgHqfAig">http://www.thetalkingmachines.com/blog/</a></span><span class="c1">&nbsp;actually cool. should go in &quot;basics&quot; maybe? not really sure</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://dswalter.github.io/blog/best-things-of-2015/&amp;sa=D&amp;ust=1465142038295000&amp;usg=AFQjCNFFv228l2Qt0YIoXWl3O9l9fnVwGQ">https://dswalter.github.io/blog/best-things-of-2015/</a></span><span class="c1">&nbsp;overview of some things he liked from 2015; links subsumed into this document. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://yanirseroussi.com/2015/11/23/the-hardest-parts-of-data-science/&amp;sa=D&amp;ust=1465142038296000&amp;usg=AFQjCNHaoTJ3Gp4UeYqa-yqKpdZ0KHPVIA">http://yanirseroussi.com/2015/11/23/the-hardest-parts-of-data-science/</a></span><span class="c1">&nbsp;summary of which parts of &quot;data science&quot; are actually hard, &quot;model fitting&quot; not considered one of them, while data collection and problem definition/success metrics are. Interesting, as there are definitely datasets that are beyond us right now. maybe that&#39;s just another way of framing the same thing, though. should go in like &quot;business&quot; or &quot;basics&quot; or something.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://treycausey.com/software_dev_skills.html&amp;sa=D&amp;ust=1465142038297000&amp;usg=AFQjCNFvrzWjzmILdsYyVnHL04Cr8K4XFQ">http://treycausey.com/software_dev_skills.html</a></span><span class="c1">&nbsp;summary of often-missed skills for people who learn data science first. also has some good links to programming stuff. should go in &quot;basics&quot;, or maybe not live in this document at all(?).</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.01563&amp;sa=D&amp;ust=1465142038298000&amp;usg=AFQjCNFksgRI_RKX7sTJWvXrYvlerZOjog">http://arxiv.org/abs/1512.01563</a></span><span class="c1">&nbsp;this is either &quot;the arcade learning environment is easier than it looks&quot; (media&#39;s gonna pick this one), or &quot;dqns are overcomplicated&quot;. they do something that sounds vaguely like model shrinking and find that they can still compete with dqn on atari just fine. should go in &quot;rl&quot;, and maybe &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.01710&amp;sa=D&amp;ust=1465142038298000&amp;usg=AFQjCNEo7pj9KjjtPpKWeEXKTJIF95KfcA">http://arxiv.org/abs/1502.01710</a></span><span class="c1">&nbsp;convnets on text per-character work really well; not really news, but it&#39;s a paper, so it&#39;ll have details. should go in &quot;language/text understanding&quot;. this is a major techniques paper, and anything that isn&#39;t using it is going to perform far worse. like, say, char-rnn. the &quot;exploring the limits of language modeling paper&quot; google put out takes advantage of this, and absolutely blows everything else out of the water because of it.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/mesnilgr/is13&amp;sa=D&amp;ust=1465142038299000&amp;usg=AFQjCNFTOdbuBV92dNeAWLWVYj8seZ6sQA">https://github.com/mesnilgr/is13</a></span><span class="c1">&nbsp;old-ish thing, from 2013, about slot-filling in natural language understanding stuff. looks like it&#39;s mainly interesting for the dataset. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://vis-www.cs.umass.edu/lfw/%23download&amp;sa=D&amp;ust=1465142038300000&amp;usg=AFQjCNE47FdwUlxP-nMvuBY7v7nTEAbl_A">http://vis-www.cs.umass.edu/lfw/#download</a></span><span class="c1">&nbsp;face dataset, freely available. should go in &quot;datasets&quot;. looks like this may be one of the major ones people have been using for gans.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/aleju/gan-reverser&amp;sa=D&amp;ust=1465142038301000&amp;usg=AFQjCNHb3xLoVDlBDvBLBcn9aX-L_iLFQA">https://github.com/aleju/gan-reverser</a></span><span class="c1">&nbsp;sort of a backwards autoencoder: autoencoding a representation, and the hidden state is the generated image that the adverserial network provides direction to create. should go in &quot;generative&quot;. produces interesting results, but not amazing ones. an interesting result is that the hidden representations don&#39;t seem to be very semantically interesting. might be interesting to combine whatever the idea is in that blog post about single-shot multiple-new-view-synthesis with this. implemented in theano, so maybe also should go in &quot;examples&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/aleju/face-generator&amp;sa=D&amp;ust=1465142038301000&amp;usg=AFQjCNELV_ndsYEQD_rqtaN9XVTIckVeUw">https://github.com/aleju/face-generator</a></span><span class="c1">&nbsp;generative adverserial networks implementation on faces. should go in &quot;examples&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/danfischetti/shape-encoder&amp;sa=D&amp;ust=1465142038302000&amp;usg=AFQjCNHKIip30OLnOHw0NAszXEfMmxzL5w">https://github.com/danfischetti/shape-encoder</a></span><span class="c1">&nbsp;</span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://danfischetti.github.io/jekyll/update/2016/01/07/convolutional-shape-encoder.html&amp;sa=D&amp;ust=1465142038302000&amp;usg=AFQjCNGlgR_SjNrOzwWluMF7rum6IsDSkQ">http://danfischetti.github.io/jekyll/update/2016/01/07/convolutional-shape-encoder.html</a></span><span class="c1">&nbsp;single-shot multiple-new-view-synthesis implementation, by generating a tensor that encodes the shape in such a way that it can be rotated. seems a bit silly, why not just encode it to a voxelization and then render that? w/e, though. makes for a much better autoencoder when it has to actually do something. should go in &quot;recurrence&quot;/&quot;memory&quot; (uses a cgru to great effect), &quot;3d&quot;, &quot;generative&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/skaae/lasagne-draw&amp;sa=D&amp;ust=1465142038303000&amp;usg=AFQjCNGL8u91dVgwi6zandr7sGe9ZQ4nTQ">https://github.com/skaae/lasagne-draw</a></span><span class="c1">&nbsp;DRAW generative network implementation. DRAW being a recurrent network is interesting for how to make incrementally refining networks of various kinds, including other generative networks; so, should go in &quot;generative&quot;, &quot;recurrence&quot;, &quot;architecture&quot;. maybe also &quot;attention&quot;. thought: the adverserial portion of a GAN is pretty generic, what happens when you stick it on this? draw paper: 1502.04623</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.00955&amp;sa=D&amp;ust=1465142038304000&amp;usg=AFQjCNEKY7bgQ7SBafOSoU-a1LxuYqDwgA">http://arxiv.org/abs/1507.00955</a></span><span class="c1">&nbsp;twitter sentiment analysis intro, with both ml and naive bayes; finds that naive bayes is better? wat? what about the stanford sentiment analysis thing with recursive networks? whatever. should go in &quot;language/sentiment analysis&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://acompa.net/on-regularization.html&amp;sa=D&amp;ust=1465142038305000&amp;usg=AFQjCNFkTxaNgGqgLznw-cP9mOS8sfk-Mg">http://acompa.net/on-regularization.html</a></span><span class="c1">&nbsp;simple intro to why regularization is even a thing, but skips and of the difficult theory stuff about why and just handwaves. should go in &quot;regularization&quot; or &quot;basics&quot;, probably &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://imgur.com/a/Hqolp&amp;sa=D&amp;ust=1465142038306000&amp;usg=AFQjCNHeVDIwHhoZftcHFzB0l5kzCkcwhA">http://imgur.com/a/Hqolp</a></span><span class="c1">&nbsp;visualization of why everyone is using RMSprop - it outperforms or matches ada- variants any time they work, and works more reliably than they do. should go in &quot;optimizers&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/45fa9o/why_montezuma_revenge_doesnt_work_in_deepmind/czxs72h&amp;sa=D&amp;ust=1465142038307000&amp;usg=AFQjCNGmSev3iDpCcBHlEt1RUy6K7vMpkg">https://www.reddit.com/r/MachineLearning/comments/45fa9o/why_montezuma_revenge_doesnt_work_in_deepmind/czxs72h</a></span><span class="c1">&nbsp;gwern comments on deepmind having done work with exploration: &quot;they use, IIRC, an autoencoder to map the screen onto a smaller dimension representing relevant game state&quot;. should go in &quot;RL&quot;. where do I find these papers?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03609&amp;sa=D&amp;ust=1465142038308000&amp;usg=AFQjCNFGJZP3guefqw0vAHbf8nfziXcIGw">http://arxiv.org/abs/1602.03609</a></span><span class="c1">&nbsp;unclear application of some sort of attention to question answer selection (ie, watson; this is from the watson team). they do something to encourage a similar representation for the question and answer, or something like that. should go in &quot;language/talking machines&quot; and &quot;attention&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/gdb/kaggle%23resources-ive-been-learning-from&amp;sa=D&amp;ust=1465142038309000&amp;usg=AFQjCNEUlQLcQh_q26b_fMjFZEVk-MV3qw">https://github.com/gdb/kaggle#resources-ive-been-learning-from</a></span><span class="c1">&nbsp;very early-in-learning resources; should go in &quot;basics&quot;. pretty good overview, good suggestions. I more or less endorse the reasoning behind the comments on the resources, and recommend anyone reading my &quot;basics&quot; section use that sort of &quot;cherry pick the good stuff&quot; learning.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/ChristosChristofidis/awesome-deep-learning&amp;sa=D&amp;ust=1465142038310000&amp;usg=AFQjCNEay3EsUzJezTvfKX2DVhe362k8Qg">https://github.com/ChristosChristofidis/awesome-deep-learning</a></span><span class="c1">&nbsp;big list of a ton of stuff, but the most useful part is the dataset list, so should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.jeremydjacksonphd.com/?cat%3D7&amp;sa=D&amp;ust=1465142038311000&amp;usg=AFQjCNHIyAmLVQ2jsrr79DnwrvPdDHngRA">http://www.jeremydjacksonphd.com/?cat=7</a></span><span class="c1">&nbsp;more misc interesting datasets/tutorials/frameworks. should go in &quot;datasets&quot; and &quot;basics&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07122&amp;sa=D&amp;ust=1465142038311000&amp;usg=AFQjCNFEqcHx87n4uvMIb0glfvwMS8yWFQ">http://arxiv.org/abs/1511.07122</a></span><span class="c1">&nbsp;based on the idea that convolutions are overly optimized for classification, restructures convolutional filters to allow &quot;dilating&quot; - haven&#39;t read yet, not sure what this means, but it&#39;s about multi-scale stuff, so maybe something about changing how big the conv filter is. should go in &quot;per-pixel classification&quot;, &quot;architecture&quot;; will probably also be very interesting for language.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://dswalter.github.io/blog/overfitting-regularization-hyperparameters/&amp;sa=D&amp;ust=1465142038312000&amp;usg=AFQjCNF3qJaTCdMOpcLWJezEOl4UzoscAg">https://dswalter.github.io/blog/overfitting-regularization-hyperparameters/</a></span><span class="c1">&nbsp;another interesting &quot;basics of regularization&quot; blog post, should go in &quot;basics&quot;. mentions what &quot;cross-validation&quot; is: a bunch of train/test sets, compare them to each other! interesting.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://mitpress.mit.edu/books/machine-learning-0&amp;sa=D&amp;ust=1465142038313000&amp;usg=AFQjCNGX9kLraF8AiZm7BYtys-LMONIPWg">https://mitpress.mit.edu/books/machine-learning-0</a></span><span class="c1">&nbsp;&quot;a probabalistic perspective of machine learning&quot; (or something, I saw the title 10 seconds ago and already probably forgot it). looks cool. costs money. should go in &quot;basics&quot; or &quot;theory&quot;. w/e.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/iamaziz/PyDataset&amp;sa=D&amp;ust=1465142038314000&amp;usg=AFQjCNFObYeKHVOPbmbEUBwj9-rtCHcGCQ">https://github.com/iamaziz/PyDataset</a></span><span class="c1">&nbsp;combination of a dataset index and a bunch of dataset-specific tools for downloading them. sorta ends up being a package manager for datasets, but they don&#39;t set out to create that and so they kinda mess it up a bit. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1502.03044&amp;sa=D&amp;ust=1465142038314000&amp;usg=AFQjCNHDyelYDL0lrtRezLNAu3t1pN9Tyw">http://arxiv.org/pdf/1502.03044</a></span><span class="c1">&nbsp;good old &quot;show, attend, and tell&quot;. attentional image captioning. should go in &quot;attention&quot; and &quot;images/captioning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1410.5401v2.pdf&amp;sa=D&amp;ust=1465142038315000&amp;usg=AFQjCNH_-HUHHX_7oMzHR-FbWlhM3QipOw">http://arxiv.org/pdf/1410.5401v2.pdf</a></span><span class="c1">&nbsp;neural turing machines. included for completeness, now considered a failure. should go in &quot;memory&quot; and maybe &quot;attention&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1410.3916.pdf&amp;sa=D&amp;ust=1465142038316000&amp;usg=AFQjCNHCT2Z4fMeNR2brgTUI-II0K-_4Bg">http://arxiv.org/pdf/1410.3916.pdf</a></span><span class="c1">&nbsp;memory networks. included for completeness, now considered a failure. should go in &quot;memory&quot; and maybe &quot;attention&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1506.07503.pdf&amp;sa=D&amp;ust=1465142038317000&amp;usg=AFQjCNE4sO48yFs2oBikAvFdJdkNEARxsw">http://arxiv.org/pdf/1506.07503.pdf</a></span><span class="c1">&nbsp;&quot;attention-based models for speech recognition&quot;. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.03134&amp;sa=D&amp;ust=1465142038317000&amp;usg=AFQjCNGcSV8mQ2-qcLzbSkvgdv8Dfu-S9g">http://arxiv.org/abs/1506.03134</a></span><span class="c1">&nbsp;&quot;pointer networks&quot; - ie, what I was originally imagining as a way to solve memory. works on learning some pretty difficult algorithms! though they&#39;re still algorithms where partial solutions give results, of course. I&#39;m curious how it works on reasoning - this is the sort of architecture I&#39;d really expect to work well in reasoning, because your memory is unbounded but it&#39;s up to the controller to remember where it put things (but as usual, haven&#39;t read it, not sure that&#39;s how it works). should go in &quot;memory&quot;. implementation: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/vshallc/PtrNets&amp;sa=D&amp;ust=1465142038318000&amp;usg=AFQjCNFpKhKA8QRVUUnXRKIs3EOhSZ4vbA">https://github.com/vshallc/PtrNets</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1503.01007.pdf&amp;sa=D&amp;ust=1465142038319000&amp;usg=AFQjCNGqaFZCP26rXaOIesH2beUjIs_fsw">http://arxiv.org/pdf/1503.01007.pdf</a></span><span class="c1">&nbsp;stack-augmented recurrent nets. should go in &quot;memory&quot;. has been improved upon.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02075&amp;sa=D&amp;ust=1465142038321000&amp;usg=AFQjCNHb9a1wPGFOc8d2Da_G_xvxfDXveA">http://arxiv.org/abs/1506.02075</a></span><span class="c1">&nbsp;&quot;large scale simple question answering with memory networks&quot;. iirc they didn&#39;t have very good results, read this one a while ago. should go in &quot;memory&quot; and &quot;language/question answering&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/slides/session2/evolving-neural-turing-risi.pdf&amp;sa=D&amp;ust=1465142038322000&amp;usg=AFQjCNGbSN3kmVu9E1VajBewYHq8BddxCw">http://www.thespermwhale.com/jaseweston/ram/slides/session2/evolving-neural-turing-risi.pdf</a></span><span class="c1">&nbsp;- </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf&amp;sa=D&amp;ust=1465142038322000&amp;usg=AFQjCNFoafS9p2OWq2FrqhYGalHpKHFpSw">http://www.thespermwhale.com/jaseweston/ram/papers/paper_6.pdf</a></span><span class="c1">&nbsp;lightning talk from nips recurrence, attention, and memory workshop: NEAT actually works really, really, really well on NTMs for tasks simple enough that it works at all. Sorta makes sense, NTMs and NEAT are both very well suited for more discrete tasks. but this means it might be able to learn the really hard algorithms that most gradient-descent approaches fail on; matches with the intuition that gradient descent is going to fail on any serious reasoning problems. might be interesting as the controller for a system that is mostly gd-based world-modeling. should go in &quot;memory&quot; and &quot;learning algorithms&quot;, maybe also &quot;rl&quot; because of relevance even though it&#39;s not mentioned.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/slides/session2/MOZER_RAM_WORKSHOP.pptx&amp;sa=D&amp;ust=1465142038324000&amp;usg=AFQjCNHb0Vg1yMKC4FKVvk81bWBWM4fEHA">http://www.thespermwhale.com/jaseweston/ram/slides/session2/MOZER_RAM_WORKSHOP.pptx</a></span><span class="c1">&nbsp;- </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://docs.google.com/presentation/d/1CGd9ELDfJ2SO_6FAIUfEAQ_l737DEwihriHIQErbTuQ/edit?usp%3Dsharing&amp;sa=D&amp;ust=1465142038324000&amp;usg=AFQjCNEmnk67-6fJp-A2kHreIjENtjzgsw">https://docs.google.com/presentation/d/1CGd9ELDfJ2SO_6FAIUfEAQ_l737DEwihriHIQErbTuQ/edit?usp=sharing</a></span><span class="c1">&nbsp;(my google docs copy so it&#39;s viewable online) - an exploration of the spaced-repetition research as an inspiration in the &quot;try to have the same problems as humans&quot; mindset. pretty interesting, conclusions are that multiple memory scales are important and that the longer-term memory should be written to when the nearer memory fails to predict something and then the data is re-acquired. sounds a hell of a lot like clockwork rnns. should go in &quot;recurrence&quot; or &quot;memory&quot;. lightning talk from nips 2015 RAM workshop.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/papers/paper_16.pdf&amp;sa=D&amp;ust=1465142038325000&amp;usg=AFQjCNGRwJgq4prpIuT1BWknSbrFfoTzxg">http://www.thespermwhale.com/jaseweston/ram/papers/paper_16.pdf</a></span><span class="c1">&nbsp;- </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/slides/session2/bosc-Learning%2520to%2520learn%2520neural%2520networks.pdf&amp;sa=D&amp;ust=1465142038326000&amp;usg=AFQjCNE2PN78TwZ957MQKS2btWLwW6T94Q">http://www.thespermwhale.com/jaseweston/ram/slides/session2/bosc-Learning%20to%20learn%20neural%20networks.pdf</a></span><span class="c1">&nbsp;- another approach to training one nn with another: use the hidden states of an rnn as the model parameters. this greatly limits what it can do, but it means it can generalize a lot better. should go in &quot;training algorithms&quot;/&quot;learning algorithms&quot;. idea: this is just the rnn&#39;s hidden states, but what if you use a cgru with the same width but only like 10 channels? also, if you give it a tiered abstraction system - say, deconv with some sort of local WTA and larger conv filters instead of maxpooling, or something - actually that sounds really interesting in general - then it can learn to propagate gradients across layers when it can take shortcuts if it does so.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.00965&amp;sa=D&amp;ust=1465142038327000&amp;usg=AFQjCNGI8-N5-iB1YClxfWUCOT7ttfu7sg">http://arxiv.org/abs/1512.00965</a></span><span class="c1">&nbsp;&quot;it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory.&quot; - looks cool, same group and approach as arXiv:1506.06442 and arXiv:1508.05008. should go in &quot;memory&quot; and &quot;language/parsing&quot;. </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/slides/session2/neural-reason.pdf&amp;sa=D&amp;ust=1465142038327000&amp;usg=AFQjCNF2I05shrnsY3d6RWhGRCnWhb0P6A">http://www.thespermwhale.com/jaseweston/ram/slides/session2/neural-reason.pdf</a></span><span class="c1">&nbsp;is an overview of their approach from the NIPS 2015 RAM workshop.</span><span class="c9 c12 c1">&nbsp;claims VERY VERY GOOD RESULTS ON QUESTION ANSWERING, huge improvement in SOTA on bAbI.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://dblp.uni-trier.de/pers/hd/l/Lu:Zhengdong&amp;sa=D&amp;ust=1465142038328000&amp;usg=AFQjCNGviXeizUscmXfX-K8PGX8FvsUwAw">http://dblp.uni-trier.de/pers/hd/l/Lu:Zhengdong</a></span><span class="c1">&nbsp;author who has a ton of papers on language understanding and architectures to do it. should go in &quot;language&quot;. one of the folks involved in the &quot;DEEPMEMORY&quot; thing.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/slides/session2/struct-memory-Wei%2520Zhang.pdf&amp;sa=D&amp;ust=1465142038329000&amp;usg=AFQjCNHEONk_JUZqGTwbFFb71E6acrjgcA">http://www.thespermwhale.com/jaseweston/ram/slides/session2/struct-memory-Wei%20Zhang.pdf</a></span><span class="c1">&nbsp;- </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/papers/paper_20.pdf&amp;sa=D&amp;ust=1465142038329000&amp;usg=AFQjCNHv1OXcbMa2lxBY452S6qG4gIilnw">http://www.thespermwhale.com/jaseweston/ram/papers/paper_20.pdf</a></span><span class="c1">&nbsp;- explored neural turing machines a bit, found that they just needed more layers, as in a multi-layer LSTM. whoda thunk, more layers works better. they try three ways of making it deeper, they get varying results on how well they work on different tasks. odd, why? but maybe NTMs are interesting after all, then. should go in &quot;memory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.thespermwhale.com/jaseweston/ram/&amp;sa=D&amp;ust=1465142038330000&amp;usg=AFQjCNHftGFFMG2IKH-MpoB5EYREXqdifQ">http://www.thespermwhale.com/jaseweston/ram/</a></span><span class="c1">&nbsp;RAM workshop papers. didn&#39;t add all of them. should go in &quot;memory&quot; and &quot;attention&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06392&amp;sa=D&amp;ust=1465142038331000&amp;usg=AFQjCNEyoeitp0yZSbdMPPvKerhAAga73g">http://arxiv.org/abs/1511.06392</a></span><span class="c1">&nbsp;neural random access machine; apparently a *form* of NTM that allows dereferencing pointers. I remember reading this and being confused, because iirc it outputs discrete ops, and didn&#39;t perform very well. but maybe I misremembered, or read the wrong thing? should go in &quot;memory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf&amp;sa=D&amp;ust=1465142038332000&amp;usg=AFQjCNE54wGuFTjDKniKimbugj3VtN0AcQ">http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf</a></span><span class="c1">&nbsp;yoshua bengio &quot;ai&quot; review from 2012. should go in &quot;reviews&quot; or &quot;basics&quot;. &quot;learning deep architectures for AI&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.iro.umontreal.ca/~bengioy/papers/YB-tricks.pdf&amp;sa=D&amp;ust=1465142038333000&amp;usg=AFQjCNFlciD8hJCg3ghKtza18hsHVJ_zrQ">http://www.iro.umontreal.ca/~bengioy/papers/YB-tricks.pdf</a></span><span class="c1">&nbsp;another review from yoshua from 2012. he likes his reviews. should go in &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&amp;sa=D&amp;ust=1465142038334000&amp;usg=AFQjCNGLMqjzGGapGsOR-9605_cyzbdyXA">http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></span><span class="c1">&nbsp;bengio/hinton/lecun review of deep learning, from 2015</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.cs.toronto.edu/~hinton/absps/distillation.pdf&amp;sa=D&amp;ust=1465142038335000&amp;usg=AFQjCNEylrvxr9YQCUg-iVMkDkLduYmIpQ">http://www.cs.toronto.edu/~hinton/absps/distillation.pdf</a></span><span class="c1">&nbsp;hinton&#39;s &quot;dark knowledge distillation&quot; thing. haven&#39;t read. summary of abstract: train a single network on the predictions of an ensemble. effectively model compression of ensembles. should go in &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1502.02367v3.pdf&amp;sa=D&amp;ust=1465142038336000&amp;usg=AFQjCNHDIlvdeQfINi_TdRIUIR5_yKnjUQ">http://arxiv.org/pdf/1502.02367v3.pdf</a></span><span class="c1">&nbsp;alternative to clockwork rnn: make the network learn the timescale to do the same thing at! should go in &quot;recurrence&quot; or &quot;memory&quot;. fits in well with the analysis about spaced repetition.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf&amp;sa=D&amp;ust=1465142038336000&amp;usg=AFQjCNHSwIdh_iJR1CQr19-FPyiKA5GAXg">http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf</a></span><span class="c1">&nbsp;bidirectional RNNS: turns out, shit&#39;s old, yo. should go in &quot;recurrence&quot;. warning, probably not a good source of SOTA.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1506.02078v1.pdf&amp;sa=D&amp;ust=1465142038337000&amp;usg=AFQjCNGSDhx1fI4cwRE0PWMsUD_iPRB14Q">http://arxiv.org/pdf/1506.02078v1.pdf</a></span><span class="c1">&nbsp;debugging and analysis paper about rnns, in the context of char-rnn. this is the paper to go with karpathy&#39;s blog post, basically. should go in &quot;recurrence&quot; and &quot;debugging&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&amp;sa=D&amp;ust=1465142038338000&amp;usg=AFQjCNFQz0Cpm-jJhTd4ltBUAp3K9TkkHQ">http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf</a></span><span class="c1">&nbsp;the crazy exploration of rnn architectures with a GA, which basically found &quot;we&#39;re way past the diminishing returns point of rnn basic architecture optimization already&quot;. which makes sense, the problems seem to be along different dimensions - like the need for longer term skip connections, like clockwork or gfrnn. should go in &quot;recurrence&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1507.01273v1.pdf&amp;sa=D&amp;ust=1465142038338000&amp;usg=AFQjCNH4hOAj8Xv5vwA-wl20LYbGusMzfw">http://arxiv.org/pdf/1507.01273v1.pdf</a></span><span class="c1">&nbsp;[abs:3]</span><span class="c1">&nbsp;very strange approach to RNNs, for reinforcement learning: optimize the ideal hidden state trajectory and a policy to get it there independently. (wat? how do even?) should go in &quot;RL&quot; and &quot;recurrence&quot; and &quot;memory&quot;, maybe also &quot;weights&quot;. Sounds very interesting.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1505.01861v1.pdf&amp;sa=D&amp;ust=1465142038340000&amp;usg=AFQjCNG4EMS_2oB_xX-RpEBO8pFEsGowRw">http://arxiv.org/pdf/1505.01861v1.pdf</a></span><span class="c1">&nbsp;video description via a join embedding. should go in &quot;images/captioning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05897&amp;sa=D&amp;ust=1465142038340000&amp;usg=AFQjCNGH1bXdevHvRkFQJyMCsuX9a0Tu0A">http://arxiv.org/abs/1602.05897</a></span><span class="c1">&nbsp;some thing about how nns work, &quot;dual kernel&quot; something or other. should go in &quot;theory&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05314&amp;sa=D&amp;ust=1465142038341000&amp;usg=AFQjCNHT3Ag9vmXMYR71eBMGQ4uuEJV9ug">http://arxiv.org/abs/1602.05314</a></span><span class="c1">&nbsp;geolocation from pixels, without specific place names or such. should go in &quot;images/applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05561&amp;sa=D&amp;ust=1465142038342000&amp;usg=AFQjCNGtiEuXYS3A6ImC0dHcw9sL_vx3aQ">http://arxiv.org/abs/1602.05561</a></span><span class="c1">&nbsp;formal nesting-detection technique; effectively gzip++, looks like. should go in &quot;non-neural&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03943&amp;sa=D&amp;ust=1465142038343000&amp;usg=AFQjCNEmbpEJ77XFjJZddZ-oZd7kEeogVw">http://arxiv.org/abs/1602.03943</a></span><span class="c1">&nbsp;much faster second-order training technique &hellip; for linear classifiers. should go in &quot;training algorithms&quot;. reddit discussion: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/45wpaw/second_order_stochastic_optimization_in_linear/&amp;sa=D&amp;ust=1465142038344000&amp;usg=AFQjCNGelGnME-VDCnCmwJEc0S-TtGkpDg">https://www.reddit.com/r/MachineLearning/comments/45wpaw/second_order_stochastic_optimization_in_linear/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/45wie8/anyone_else_feeling_overwhelmed_by_the_fast_pace/&amp;sa=D&amp;ust=1465142038345000&amp;usg=AFQjCNG76TybA_N-dBl7RpDnQ3GXGRhPdw">https://www.reddit.com/r/MachineLearning/comments/45wie8/anyone_else_feeling_overwhelmed_by_the_fast_pace/</a></span><span class="c1">&nbsp;apparently I should slow down in reading papers :(</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.00210&amp;sa=D&amp;ust=1465142038346000&amp;usg=AFQjCNH1xLzZhXwp_AHlPcrPHRy-O6icwA">http://arxiv.org/abs/1507.00210</a></span><span class="c1">&nbsp;some learning speedup by restricting the kinds of internal representations - I&#39;ve heard about this a few times, but it looks confusing. &quot;natural neural networks&quot;. apparently it&#39;s the next batch norm, says reddit. should go in &quot;training algorithms&quot; maybe, or &quot;regularization&quot;. reddit discussion: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/3bw1xr/150700210_natural_neural_networks/&amp;sa=D&amp;ust=1465142038347000&amp;usg=AFQjCNExCHpUr0YoKdHYedVXg9WkFUP-ZA">https://www.reddit.com/r/MachineLearning/comments/3bw1xr/150700210_natural_neural_networks/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/45qpxf/is_there_a_good_text_to_speech_trianingset/&amp;sa=D&amp;ust=1465142038347000&amp;usg=AFQjCNGOmaqkKsD-pWd7eL_tJHwfA_od2g">https://www.reddit.com/r/MachineLearning/comments/45qpxf/is_there_a_good_text_to_speech_trianingset/</a></span><span class="c1">&nbsp;various datasets for tts - there are some free ones. should go in &quot;datasets&quot; and/or &quot;speech&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02282&amp;sa=D&amp;ust=1465142038348000&amp;usg=AFQjCNGZ4EFrbZ9Bxi5mnXfG9uEK0TrpiQ">http://arxiv.org/abs/1602.02282</a></span><span class="c1">&nbsp;variational ladder networks, apparently they can go deeper than previous autoencoders. should go in &quot;bayesian&quot; and &quot;unsupervised&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02481&amp;sa=D&amp;ust=1465142038349000&amp;usg=AFQjCNHKaa3KGAarhZ7d7dYEpIj6s890yA">http://arxiv.org/abs/1602.02481</a></span><span class="c1">&nbsp;PUBLIC DOMAIN dataset of 3d object scans! yessssss should go in &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/&amp;sa=D&amp;ust=1465142038350000&amp;usg=AFQjCNGBPvip1cdUTOr_ueYrtyfCXk0Fdw">https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/</a></span><span class="c1">&nbsp;some q/a about how to debug nns - </span><span class="c1">&nbsp;should go in &quot;debugging&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/46apdw/good_examples_of_deep_learning_versus_traditional/&amp;sa=D&amp;ust=1465142038351000&amp;usg=AFQjCNFd0Y2sEJ2LvmL9odSfy5wP5P2Trg">https://www.reddit.com/r/MachineLearning/comments/46apdw/good_examples_of_deep_learning_versus_traditional/</a></span><span class="c1">&nbsp;deep learning vs traditional vision - should go in &quot;non-neural&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.nature.com/articles/sdata201610&amp;sa=D&amp;ust=1465142038352000&amp;usg=AFQjCNHPEypUzlA9bGj3Gns554JcGE6Dyg">http://www.nature.com/articles/sdata201610</a></span><span class="c1">&nbsp;genomic data ml should go in &quot;bio&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://scs.ryerson.ca/~aharley/vis/conv/&amp;sa=D&amp;ust=1465142038353000&amp;usg=AFQjCNHb3WJNQREHzsOD29htBBC5T-u19w">http://scs.ryerson.ca/~aharley/vis/conv/</a></span><span class="c1">&nbsp;cool visualization of an mnist net - should go in &quot;basics&quot; or &quot;debugging&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/469rtc/removing_features_greatly_increases_performance/&amp;sa=D&amp;ust=1465142038355000&amp;usg=AFQjCNGsMTG3oEKcXA5A8BNi4ViOVwaLww">https://www.reddit.com/r/MachineLearning/comments/469rtc/removing_features_greatly_increases_performance/</a></span><span class="c1">&nbsp;if removing a feature input greatly increases performance, why might that be? -&gt; probably feature magnitude. should go in &quot;debugging&quot; and/or &quot;theory&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05110&amp;sa=D&amp;ust=1465142038356000&amp;usg=AFQjCNE5S__-_3PgK-xHiwFuRLu8ZxdLRg">http://arxiv.org/abs/1602.05110</a></span><span class="c1">&nbsp;DRAW + GAN! totally called it. again, I implemented something by googling it. yay. I should actually do things. should go in &quot;generative&quot;, &quot;recurrent&quot;, &quot;images&quot;. see also reddit discussion </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/469gtc/generating_images_with_recurrent_adversarial/&amp;sa=D&amp;ust=1465142038357000&amp;usg=AFQjCNFhwhDJt60vtlKKSw62nzWBLj0HDA">https://www.reddit.com/r/MachineLearning/comments/469gtc/generating_images_with_recurrent_adversarial/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/467mf8/why_deep_reinforcement_learning_is_not_a_thing_in/&amp;sa=D&amp;ust=1465142038358000&amp;usg=AFQjCNGbFg3CLj-jt5KoBIZtkM6bZ2zqLg">https://www.reddit.com/r/MachineLearning/comments/467mf8/why_deep_reinforcement_learning_is_not_a_thing_in/</a></span><span class="c1">&nbsp;reddit-based overview of RL in robots. should go in &quot;RL&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/465w29/deep_learning_for_depth_map_estimation_from/&amp;sa=D&amp;ust=1465142038359000&amp;usg=AFQjCNHPrzTbWi1Vgp-LggTUgwYLYJFFnA">https://www.reddit.com/r/MachineLearning/comments/465w29/deep_learning_for_depth_map_estimation_from/</a></span><span class="c1">&nbsp;depth map generation based on stereo images; references papers that do this, and has a blender tool for generating random 3d scenes, which would be useful for the noisy-data-gen-for-ocr thing I was thinking about. should go in &quot;images/3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04485&amp;sa=D&amp;ust=1465142038360000&amp;usg=AFQjCNGzooaarf-Q2i44Uhw11frTQecdCA">http://arxiv.org/abs/1602.04485</a></span><span class="c1">&nbsp;a possible *proof* about depth in NNs being more powerful. should go in &quot;theory&quot;. see also reddit discussion, which mentions very old papers that have similar results: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/4669nn/benefits_of_depth_in_neural_networks_160204485/&amp;sa=D&amp;ust=1465142038360000&amp;usg=AFQjCNFYIsjIrFB9_f8ZYKO5SiEXFlOHFw">https://www.reddit.com/r/MachineLearning/comments/4669nn/benefits_of_depth_in_neural_networks_160204485/</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/467cfh/open_pretrained_models_for_speech_recognition/&amp;sa=D&amp;ust=1465142038361000&amp;usg=AFQjCNENH7OEYrwf-KY9UjfWpyPqseMqWw">https://www.reddit.com/r/MachineLearning/comments/467cfh/open_pretrained_models_for_speech_recognition/</a></span><span class="c1">&nbsp;pretrained models for speech. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.projectoxford.ai&amp;sa=D&amp;ust=1465142038362000&amp;usg=AFQjCNEmZjmlXVi7GBV_lXv_wwXWX-wHUw">https://www.projectoxford.ai</a></span><span class="c1">&nbsp;some cool ms api demos, good for just using their models for personal use. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03809&amp;sa=D&amp;ust=1465142038363000&amp;usg=AFQjCNHiycaYqaXIm3WR4mw_sHQHiFxFtw">http://arxiv.org/abs/1601.03809</a></span><span class="c1">&nbsp;application to detecting hardware faults or something. &quot;misc&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04037&amp;sa=D&amp;ust=1465142038363000&amp;usg=AFQjCNEGpEiMdI95HSN7q_3bIaVntzx_YQ">http://arxiv.org/abs/1601.04037</a></span><span class="c1">&nbsp;robot motion planning in uncertain environments - GOFAI. should go in &quot;non-neural&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.03797&amp;sa=D&amp;ust=1465142038364000&amp;usg=AFQjCNFhg0tNIir4SrVYrRyLIj79FfjMgw">http://arxiv.org/abs/1601.03797</a></span><span class="c1">&nbsp;active-learning-based data cleaning system. should go in &quot;data collection&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.04434&amp;sa=D&amp;ust=1465142038365000&amp;usg=AFQjCNH1yA8PIcGRWBAxAmkCJmqqsv9fDA">http://arxiv.org/abs/1502.04434</a></span><span class="c1">&nbsp;alternate learning algorithm that apparently handles things like adverserial cases via a modification to backprop. should go in &quot;training algorithms&quot; or &quot;regularization&quot; or &quot;adverserial examples problem&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05440&amp;sa=D&amp;ust=1465142038366000&amp;usg=AFQjCNElPF6nINMP-qPGLAPyn1yf9BM55Q">http://arxiv.org/abs/1511.05440</a></span><span class="c1">&nbsp;practical considerations of training a next frame predictor. instead of MSE, uses adverserial training on multiple scales of output, and some sort of thing about the image gradient (wat?). should go in &quot;images&quot;. -&gt; already in images</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04149&amp;sa=D&amp;ust=1465142038366000&amp;usg=AFQjCNFAvGDeXO7ati0nmBcPdFaUfEoAAA">http://arxiv.org/abs/1601.04149</a></span><span class="c1">&nbsp;much faster image reconstruction system that improves the appearance of a jpeg image using a sparse coding thing or something. claims to be 30x faster than normal deep stuff. should go in &quot;sparsity&quot;, looks related to cirenaikual&#39;s thing.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04153&amp;sa=D&amp;ust=1465142038367000&amp;usg=AFQjCNGO8Svi1jkKXUYO83ppchd2NQw3Yg">http://arxiv.org/abs/1601.04153</a></span><span class="c1">&nbsp;misc stuff to do classification of very small image regions. should go in &quot;images&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04155&amp;sa=D&amp;ust=1465142038368000&amp;usg=AFQjCNGKCTW6itpVGkZNvCm02_5mGAYUpA">http://arxiv.org/abs/1601.04155</a></span><span class="c1">&nbsp;Brain-Inspired Deep Networks for Image Aesthetics Assessment - should go in &quot;neuromorphic&quot;. looks pretty boring, unlikely to be backprop-based. didn&#39;t read though.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04568&amp;sa=D&amp;ust=1465142038369000&amp;usg=AFQjCNFkrizvb7IWMJyZBzJnyJcOthwNpw">http://arxiv.org/abs/1601.04568</a></span><span class="c1">&nbsp;incremental refinement to neuralstyle. should go in &quot;applications/art&quot; or something.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04187&amp;sa=D&amp;ust=1465142038370000&amp;usg=AFQjCNED8rB_Ia3y9ecWAEKS6Yu17S2IRg">http://arxiv.org/abs/1601.04187</a></span><span class="c1">&nbsp;convert rnns to spiking neural networks, for running on spiking ASICs. should go in &quot;model compression&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04114&amp;sa=D&amp;ust=1465142038370000&amp;usg=AFQjCNF6SDtqqQiPdVS8zIBaBN_mBJ2-Kg">http://arxiv.org/abs/1601.04114</a></span><span class="c1">&nbsp;&quot;training recurrent neural networks by diffusion&quot; - they somehow blur the cost landscape, and thereby get a much smoother gradient to descend down. should go in &quot;regularization&quot; and/or &quot;training algorithms&quot;. they say it&#39;s equivalent to an infinite number of trails of injecting a noise distribution and that this makes it faster. They also say it&#39;s like reverse-order layerwise pretraining, which is kinda a thing that it already does&hellip;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04126&amp;sa=D&amp;ust=1465142038371000&amp;usg=AFQjCNHjVWQtuf8szLQks_lositfty5nsg">http://arxiv.org/abs/1601.04126</a></span><span class="c1">&nbsp;another miri for ml: &quot;</span><span class="c8 c1">Engineering Safety in Machine Learning</span><span class="c1">&quot;. focused on situations where you need formal risk analyses, rather than just risk minimization. should go in &quot;rl/safety&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04183&amp;sa=D&amp;ust=1465142038372000&amp;usg=AFQjCNGR40WyrOGRRz7O1YJ3n7lUx0mnsw">http://arxiv.org/abs/1601.04183</a></span><span class="c1">&nbsp;another neuromorphic model compression thing. should go in &quot;model compression&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.05192&amp;sa=D&amp;ust=1465142038373000&amp;usg=AFQjCNGclaMoxsjntiLzolIEv3QXOeVZEw">http://arxiv.org/abs/1505.05192</a></span><span class="c1">&nbsp;unsupervised learning by training a convnet to predict the relative location of two image patches. should go in &quot;images&quot;, &quot;unsupervised&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06664&amp;sa=D&amp;ust=1465142038374000&amp;usg=AFQjCNHp0P7rABVaVzNidSd-kPuvVJEfZA">http://arxiv.org/abs/1509.06664</a></span><span class="c1">&nbsp;neural solution to the problem of determining if one sentence is reasonable to follow another (&quot;entailment&quot;). should go in &quot;language&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.06922&amp;sa=D&amp;ust=1465142038375000&amp;usg=AFQjCNGMrH0HzfUFf8kUo1ojLSCz9l9zdg">http://arxiv.org/abs/1502.06922</a></span><span class="c1">&nbsp;sentence embedding with an lstm, unsupervised based on some clickthrough dataset or something. should go in &quot;language/embeddings&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05297&amp;sa=D&amp;ust=1465142038376000&amp;usg=AFQjCNHTuhu323PZZ45lPumPVk-578yMnw">http://arxiv.org/abs/1511.05297</a></span><span class="c1">&nbsp;a theory of model size and learning rate relative to input noise and stuff. should go in &quot;theory&quot; and &quot;hyperparameters&quot;. this looks awesome!</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02554&amp;sa=D&amp;ust=1465142038377000&amp;usg=AFQjCNEkRKzvIO4zuRfgmP423M9X8nRMjQ">http://arxiv.org/abs/1511.02554</a></span><span class="c1">&nbsp;relu nets for genome something. should go in &quot;bio&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06391&amp;sa=D&amp;ust=1465142038378000&amp;usg=AFQjCNEiNYtvFo8GdkEBmJJxWHejpfI5rw">http://arxiv.org/abs/1511.06391</a></span><span class="c1">&nbsp;empirical stuff on how to order a set when using seq2seq. should go in &quot;recurrence&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05263&amp;sa=D&amp;ust=1465142038378000&amp;usg=AFQjCNHZxUObAdjhE-4eCQSxy0VfOI85JQ">http://arxiv.org/abs/1511.05263</a></span><span class="c1">&nbsp;review of recommender systems. should go in &quot;basics&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05432&amp;sa=D&amp;ust=1465142038379000&amp;usg=AFQjCNGkvM60ZNBy9mHRcNopx3lWAS1egQ">http://arxiv.org/abs/1511.05432</a></span><span class="c1">&nbsp;improvement to the way an NN is trained, such that adverserial examples become much less of a thing, appears to be a &quot;train on perturbed examples&quot; thing.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06418&amp;sa=D&amp;ust=1465142038380000&amp;usg=AFQjCNFmFHlutwHHZt9qqz2MJ4yE9tPDmw">http://arxiv.org/abs/1511.06418</a></span><span class="c1">&nbsp;weird autoencoder technique that allows multiple object representations. apparently this is called the &quot;binding problem&quot;. should go in &quot;representation learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.02011&amp;sa=D&amp;ust=1465142038381000&amp;usg=AFQjCNEyO0SnJBI0701hy3EPWF-xFskizg">http://arxiv.org/abs/1512.02011</a></span><span class="c1">&nbsp;some work on how to do discounting in a DQL dynamically. this is very interesting, I didn&#39;t read it yet, but I&#39;ve been having similar thoughts - there&#39;s probably some way to guess at opportunity cost, and then the only other thing it&#39;s useful for is estimating uncertainty, which is better done with a more explicit model of uncertainty, such as mc dropout - should go in &quot;rl&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06380&amp;sa=D&amp;ust=1465142038381000&amp;usg=AFQjCNHQYEYnxgnYd20ONmNFiZMuop-gyg">http://arxiv.org/abs/1511.06380</a></span><span class="c1">&nbsp;temporal autoencoder w/recurrence for video. works about as well as you&#39;d expect a temporal autoencoder to: quite well. should go in &quot;images&quot; and &quot;unsupervised&quot;. maybe also &quot;recurrence&quot;, uses it but doesn&#39;t really contribute to it.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05347&amp;sa=D&amp;ust=1465142038382000&amp;usg=AFQjCNGLqezbbZCgEj8wYfN9zmPQN_tW1Q">http://arxiv.org/abs/1601.05347</a></span><span class="c1">&nbsp;more nsa crap: mapping faces between thermal and optical data. should go in &quot;applications/misc&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05150&amp;sa=D&amp;ust=1465142038383000&amp;usg=AFQjCNFR45YwQEyBJ8sz2g4VqOQ-UaauFg">http://arxiv.org/abs/1601.05150</a></span><span class="c1">&nbsp;some work on how to balance classes for finetuning a detection model. mentions that you often have a long tail distribution of class data. should go in &quot;images&quot; and maybe &quot;data efficiency&quot; or something.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.04295&amp;sa=D&amp;ust=1465142038384000&amp;usg=AFQjCNGGI-8tMFtzpalug_YIrUKwWsWkfg">http://arxiv.org/abs/1512.04295</a></span><span class="c1">&nbsp;new hardware to run neural networks - claims huge improvements in power efficiency. danger. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06394&amp;sa=D&amp;ust=1465142038385000&amp;usg=AFQjCNGR9Jc2JjVKPpNOQJw7GvKb__De2A">http://arxiv.org/abs/1511.06394</a></span><span class="c1">&nbsp;&quot;geodesics of learned representations&quot;, should go in &quot;theory&quot;</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07131&amp;sa=D&amp;ust=1465142038385000&amp;usg=AFQjCNGwjEmKTpONhYUdWBqpCQvtxXQnOw">http://arxiv.org/abs/1511.07131</a></span><span class="c1">&nbsp;&quot;DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization&quot;, should go in &quot;images&quot;</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/bwallace/ACL-2014-irony&amp;sa=D&amp;ust=1465142038386000&amp;usg=AFQjCNFOpQHgFzT7WaYtaPtzjihtwALAeg">https://github.com/bwallace/ACL-2014-irony</a></span><span class="c1">&nbsp;irony dataset. should go in &quot;datasets&quot;</span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/46twvh/question_on_nn_weight_initialization_based_on/&amp;sa=D&amp;ust=1465142038386000&amp;usg=AFQjCNHZDQ8f-7JVbVTnMVuKTqCpLAWTDg">https://www.reddit.com/r/MachineLearning/comments/46twvh/question_on_nn_weight_initialization_based_on/</a></span><span class="c1">&nbsp;weight initialization matters for preventing vanishing gradients. should go one of &quot;theory&quot;, &quot;architecture/weights&quot;. maybe also would be nice to have a &quot;vanishing gradients section&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.01852&amp;sa=D&amp;ust=1465142038387000&amp;usg=AFQjCNGFKAM0qxzPibli6uhyoBcHBLjtkQ">http://arxiv.org/abs/1502.01852</a></span><span class="c1">&nbsp;PReLU - also mentions initialization. should go in &quot;activation functions&quot;, &quot;initialization&quot;. claims to be the first to surpass human level - isn&#39;t.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://beta.openreview.net/group?id%3DICLR.cc%252F2016%252Fworkshop&amp;sa=D&amp;ust=1465142038388000&amp;usg=AFQjCNGmdClkINVQji0flMg9YwuKWCNtog">http://beta.openreview.net/group?id=ICLR.cc%2F2016%2Fworkshop</a></span><span class="c1">&nbsp;huge open review site. most of these seem to be on the arxiv anyway, though. need to go through and match them up with arxiv papers - I expect I&#39;ll have 40% of them or so.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05447&amp;sa=D&amp;ust=1465142038389000&amp;usg=AFQjCNE6PonV8hd7OyFfW00J-EQWRG4lUg">http://arxiv.org/abs/1601.05447</a></span><span class="c1">&nbsp;object detection in video, over time. should go in &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05511&amp;sa=D&amp;ust=1465142038390000&amp;usg=AFQjCNFs5yuO8DdVLZpDEjpu4zwh9i37CQ">http://arxiv.org/abs/1601.05511</a></span><span class="c1">&nbsp;rgb+depth datasets overview. should go in &quot;datasets&quot;, &quot;3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05610&amp;sa=D&amp;ust=1465142038391000&amp;usg=AFQjCNFEj0zxOa-lO_N8wowHl1mBrZMWPA">http://arxiv.org/abs/1601.05610</a></span><span class="c1">&nbsp;application of scene text reading car license plates, uses conv+lstm. should go in &quot;images/ocr&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05654&amp;sa=D&amp;ust=1465142038392000&amp;usg=AFQjCNEMzMjj2Tau-mIpbVAJIqEWN443sg">http://arxiv.org/abs/1601.05654</a></span><span class="c1">&nbsp;something something visualizing latent properties of time series data? not sure what&#39;s going on here. should go in &quot;misc&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05175&amp;sa=D&amp;ust=1465142038393000&amp;usg=AFQjCNFNNPQUkxi77qvTXayoHu-S7aHXDg">http://arxiv.org/abs/1511.05175</a></span><span class="c1">&nbsp;splits off classifying objects and estimating their pose, or something. also works through how the hidden representations work, so should go in &quot;debugging&quot; and &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02438&amp;sa=D&amp;ust=1465142038393000&amp;usg=AFQjCNEB48zt388K7XglrYdnaKq5UDHEig">http://arxiv.org/abs/1506.02438</a></span><span class="c1">&nbsp;learning system that uses policy gradients (what are those) to train robots to walk using direct kinematic control (cool) in one to two weeks of equivalent training time (super cool). should go in &quot;RL&quot;, still model-free but it&#39;s pretty cool. from peter abeel&#39;s group.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03719&amp;sa=D&amp;ust=1465142038394000&amp;usg=AFQjCNEgktV1Gu6cPOynaQG_y_jQTUq3_w">http://arxiv.org/abs/1511.03719</a></span><span class="c1">&nbsp;yann lecun tries having a &quot;none of the above&quot; option. a little different from the medical imaging one where it was &quot;refrain from guessing&quot;. it works better. what a surprise. a little stupid that nobody tried this sooner, this is very very obvious. should go in &quot;images&quot;, maybe in &quot;classification&quot; or &quot;architecture&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.05936&amp;sa=D&amp;ust=1465142038395000&amp;usg=AFQjCNFscry2szOTL7ddUGnSMAMjkE4G8A">http://arxiv.org/abs/1601.05936</a></span><span class="c1">&nbsp;something about using a combination of sparse coding, neural networks, and hidden markov models for speech. should go in &quot;sparsity&quot;, &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06087&amp;sa=D&amp;ust=1465142038396000&amp;usg=AFQjCNEIJKb2lu89jMOQw56dnA2qvXzQ4w">http://arxiv.org/abs/1601.06087</a></span><span class="c1">&nbsp;motion estimation between two images with a convnet, uses some sort of unsupervised approach based on bootstrapping off of the existing model and then doing&hellip; something. they say it performs comparably. whatever. should go in &quot;video&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">&nbsp;</span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07386&amp;sa=D&amp;ust=1465142038397000&amp;usg=AFQjCNEDJ53K3hgvdOuatR-cBPX9YiiSmQ">http://arxiv.org/abs/1511.07386</a></span><span class="c1">&nbsp;&quot;pushing the boundaries of boundary detection&quot;. paper title bein&#39; smart. looks cool, appears to be another &quot;now pour it into one pot&quot; paper like &quot;exploring the limits of language modeling&quot;. (is there going to be another paper, &quot;exploring the art of alliteration&quot;?) should go in &quot;images/detection&quot;. nearly hits human performance. noting that boundary detection is actually pretty annoyingly hard, I often have difficulty with it myself.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04621&amp;sa=D&amp;ust=1465142038398000&amp;usg=AFQjCNFVh9TvD0WRaElmyCepX3fa_4VMZA">http://arxiv.org/abs/1602.04621</a></span><span class="c1">&nbsp;&quot;deep exploration via bootstrapped dqn&quot; - this looks *very* interesting. I suspect it&#39;s still not very principled, but since I have my own ideas about how to do this, I&#39;m not going to read it until I&#39;ve tried mine - spoilers. I suspect it&#39;s based on adding random noise to the value function, but it&#39;s unclear - they say &quot;random value functions&quot;, but I&#39;m not really sure what that means.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">wordset: open dictionary thing. not quite as awesome as oxford english for training nns, probably. I think the implication is that we desperately need to give them something to talk about, because it&#39;s a hard problem. should go in &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment&amp;sa=D&amp;ust=1465142038399000&amp;usg=AFQjCNFJX03qbdiIs8ctsSPWrYqHGvFKlA">https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment</a></span><span class="c1">&nbsp;absolutely insane amount of text. should go in &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://blog.shakirm.com/2016/02/learning-in-brains-and-machines-1/&amp;sa=D&amp;ust=1465142038401000&amp;usg=AFQjCNECdhZhQ9al5KazDMqCjGm_y5CbsA">http://blog.shakirm.com/2016/02/learning-in-brains-and-machines-1/</a></span><span class="c1">&nbsp;some thoughts about how RL might work in the brain; should go in &quot;rl&quot; and &quot;neuromorphic&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06291&amp;sa=D&amp;ust=1465142038402000&amp;usg=AFQjCNFcGYV4NVXN6YwORfre3H3myINkKA">http://arxiv.org/abs/1602.06291</a></span><span class="c1">&nbsp;lstm given global context about what it&#39;s doing, I think at each timestep. should go in &quot;recurrence&quot; and &quot;language&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/paper/4136-tiled-convolutional-neural-networks.pdf&amp;sa=D&amp;ust=1465142038403000&amp;usg=AFQjCNFJOnXrvsEcF0Qq4vmu9v1GWeYQ6w">http://papers.nips.cc/paper/4136-tiled-convolutional-neural-networks.pdf</a></span><span class="c1">&nbsp;not sure how old this is, but it&#39;s a paper about learned local weight sharing in a convnet-like structure. should go in &quot;convolution&quot;/&quot;architecture/weight sharing&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02697&amp;sa=D&amp;ust=1465142038404000&amp;usg=AFQjCNG5ErZuEQH6Qd6ezuEWa2Yd8YJSCQ">http://arxiv.org/abs/1602.02697</a></span><span class="c1">&nbsp;adverserial examples paper: they still work, and you can make adverserial examples that work on anything! also useful for hiding from facebook, until such time as they figure it out. should go in &quot;adverserial examples problem&quot;. written as a security attack.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07332&amp;sa=D&amp;ust=1465142038405000&amp;usg=AFQjCNEumks5QfPyQjNWtn9UCFabLDrKTw">http://arxiv.org/abs/1602.07332</a></span><span class="c1">&nbsp;and </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://visualgenome.org/&amp;sa=D&amp;ust=1465142038406000&amp;usg=AFQjCNGVAnwfNiV-3kMuLgM4cTQykp7KIA">https://visualgenome.org/</a></span><span class="c1">&nbsp;visual genome: awesome dataset of images, and relationships between objects in the image. should go in &quot;images&quot; maybe and &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.7062&amp;sa=D&amp;ust=1465142038407000&amp;usg=AFQjCNHkzjNQ9U3MIOwNszjzVgJ-CR_a2g">http://arxiv.org/abs/1412.7062</a></span><span class="c1">&nbsp;image segmentation with convnets, with added conditional random fields. still not sure what they are, but they&#39;ve shown up in a few places. two years old! probably seriously beaten. should go in &quot;images/segmentation&quot;. associated implementation: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://bitbucket.org/deeplab/deeplab-public/&amp;sa=D&amp;ust=1465142038408000&amp;usg=AFQjCNFphPZuN1TjHHuFWJMRi2Zb9Ud-SQ">https://bitbucket.org/deeplab/deeplab-public/</a></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.cc.gatech.edu/~riedl/pubs/aaai-ethics16.pdf&amp;sa=D&amp;ust=1465142038409000&amp;usg=AFQjCNH-x6tawVuzwV7IXNp6ECje092ajA">http://www.cc.gatech.edu/~riedl/pubs/aaai-ethics16.pdf</a></span><span class="c1">&nbsp;ai safety thing by teaching the system with stories. should go in &quot;rl/safety&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06823&amp;sa=D&amp;ust=1465142038409000&amp;usg=AFQjCNFYnGvqAGcTIzxSVEu7RBKNmmvy9g">http://arxiv.org/abs/1601.06823</a></span><span class="c1">&nbsp;image-attention rnn which is constrained to look at something new each timestep. should go in &quot;attention&quot;, &quot;images&quot;, &quot;recurrence&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06892&amp;sa=D&amp;ust=1465142038410000&amp;usg=AFQjCNFMdnb-Rb8dGVZoi94nyN--fyE2Gg">http://arxiv.org/abs/1601.06892</a></span><span class="c1">&nbsp;using a cnn to reconstruct images from compressive sensing. should go in &quot;images&quot;, maybe &quot;unsupervised&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06950&amp;sa=D&amp;ust=1465142038411000&amp;usg=AFQjCNHMAJ4-cO1GhAd1f0GzMbMGn6mAFg">http://arxiv.org/abs/1601.06950</a></span><span class="c1">&nbsp;loss function for 3d scene reconstruction error. should go in &quot;3d&quot;, &quot;loss functions&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07014&amp;sa=D&amp;ust=1465142038412000&amp;usg=AFQjCNFh3lqME7J8Jd6oeWkjC9O8jt1gww">http://arxiv.org/abs/1601.07014</a></span><span class="c1">&nbsp;cnns for segmentation in ultrasound. should go in &quot;bio&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07140&amp;sa=D&amp;ust=1465142038413000&amp;usg=AFQjCNHgnbMZPyzc9DQaeQ4ozAJ4YJGFGw">http://arxiv.org/abs/1601.07140</a></span><span class="c1">&nbsp;ms coco now has a text detection/ocr dataset, this is the paper. should go in &quot;images/ocr&quot;, &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06815&amp;sa=D&amp;ust=1465142038414000&amp;usg=AFQjCNEljUFQaXfxkKZKA7hHk7tV3bFcmA">http://arxiv.org/abs/1601.06815</a></span><span class="c1">&nbsp;FFTs to train convnets more quickly. isn&#39;t this old hat? whatever. should go in &quot;performance&quot;, &quot;convolution&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.01320&amp;sa=D&amp;ust=1465142038415000&amp;usg=AFQjCNFjQu8yinXFGP8a4yt9SvzFEGUKwA">http://arxiv.org/abs/1512.01320</a></span><span class="c1">&nbsp;dataset of dense transformations to objects - many samples along different dimensions of transformations - and analysis of how convnets are invariant to these. should go in &quot;datasets&quot;, &quot;images&quot;, &quot;3d&quot;, &quot;convolution&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02274&amp;sa=D&amp;ust=1465142038416000&amp;usg=AFQjCNFjjhB6G65haqXdMq_mPE4bcvma0w">http://arxiv.org/abs/1511.02274</a></span><span class="c1">&nbsp;multi layer question answering with attention. looks like a feedforward interpretation of recurrence or something. should go in &quot;attention&quot;, &quot;language/qa&quot;. meh.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06410&amp;sa=D&amp;ust=1465142038417000&amp;usg=AFQjCNFprBY8Ru9EI-TZdPLf4dNqKGonUQ">http://arxiv.org/abs/1511.06410</a></span><span class="c1">&nbsp;facebook&#39;s go paper, with multi-step prediction; competes with pure mcts. should go in &quot;go&quot; and &quot;images&quot;, and &quot;long-term prediction&quot; or &quot;regularization&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.09426&amp;sa=D&amp;ust=1465142038418000&amp;usg=AFQjCNEM8Spg4qmMNzgc9Hbp0KATjE-dYg">http://arxiv.org/abs/1511.09426</a></span><span class="c1">&nbsp;neuromorphic dimensionality reduction, should go in &quot;neuromorphic&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07215&amp;sa=D&amp;ust=1465142038419000&amp;usg=AFQjCNE-jS-SyZwlid0B4YIBCnfRlYicvg">http://arxiv.org/abs/1601.07215</a></span><span class="c1">&nbsp;speech synth that uses rnns to postfilter the generated stuff. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07255&amp;sa=D&amp;ust=1465142038419000&amp;usg=AFQjCNFah4VHJebDl2_lp0z093XipecWnw">http://arxiv.org/abs/1601.07255</a></span><span class="c1">&nbsp;another person similarity detection paper. should go in &quot;images/similarity&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07265&amp;sa=D&amp;ust=1465142038420000&amp;usg=AFQjCNEMpxA8dSDEbzEG6iQ80-ozORoo5Q">http://arxiv.org/abs/1601.07265</a></span><span class="c1">&nbsp;prediction of future paths of objects. comes with large datasets. should go in &quot;video&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07532&amp;sa=D&amp;ust=1465142038421000&amp;usg=AFQjCNE7sXTgfVJxD7CfBvJA5OAD-6DLLg">http://arxiv.org/abs/1601.07532</a></span><span class="c1">&nbsp;discovering optical flow with convnets. should go in &quot;video&quot;, &quot;unsupervised&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07213&amp;sa=D&amp;ust=1465142038422000&amp;usg=AFQjCNGu24dmYHQDFucp_cFSBD1kbAehgw">http://arxiv.org/abs/1601.07213</a></span><span class="c1">&nbsp;solution to the adverserial examples problem with a new training algorithm/regularization that generalizes the problem a lot, without specifically fighting against the adverserial examples problem. claims to work well. should go in &quot;adverserial examples&quot;, &quot;regularization&quot;, &quot;training algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07381&amp;sa=D&amp;ust=1465142038423000&amp;usg=AFQjCNF0uhdJCf2C6MdGozst0LA7wf-z6w">http://arxiv.org/abs/1601.07381</a></span><span class="c1">&nbsp;something something echo state networks. not sure what those are, should go in &quot;neuromorphic&quot; and/or &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.02763&amp;sa=D&amp;ust=1465142038423000&amp;usg=AFQjCNG7NZcvBFQCWuIqwgAWIOXmNSQY0w">http://arxiv.org/abs/1504.02763</a></span><span class="c1">&nbsp;cost functions for classification problems that optimize for rejecting unclassifiable data points. should go in &quot;cost functions&quot;, &quot;classification&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04737&amp;sa=D&amp;ust=1465142038424000&amp;usg=AFQjCNHRTDav-0k6R51uei6RETEZV5BKFA">http://arxiv.org/abs/1601.04737</a></span><span class="c1">&nbsp;and </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04738&amp;sa=D&amp;ust=1465142038425000&amp;usg=AFQjCNFSc_h2yfjBSadoxO-OgsS2NRCiGw">http://arxiv.org/abs/1601.04738</a></span><span class="c1">&nbsp;-</span><span class="c1">&nbsp;newtonian methods thing. should go in &quot;optimization&quot;/&quot;optimizers&quot;/&quot;training algorithms&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07576&amp;sa=D&amp;ust=1465142038425000&amp;usg=AFQjCNHwDFhTn2xeu13TO_inONOYJsKezQ">http://arxiv.org/abs/1601.07576</a></span><span class="c1">&nbsp;mid-layer supervision or something for convnets, claims a big increase in SOTA on mit places and something else. should go in &quot;images&quot;, &quot;regularization&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07649&amp;sa=D&amp;ust=1465142038426000&amp;usg=AFQjCNHDeUb4uMatQqjrC31X8cHcgY6K4Q">http://arxiv.org/abs/1601.07649</a></span><span class="c1">&nbsp;something about deep conditional random fields. should go in &quot;alternate deep&quot;, and they do depth estimation with it, so &quot;3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07661&amp;sa=D&amp;ust=1465142038427000&amp;usg=AFQjCNE7u_Gxhn8_k4fOzWALBUHVBluRBA">http://arxiv.org/abs/1601.07661</a></span><span class="c1">&nbsp;application of convnets to removing haze from an image. should go in &quot;images&quot;. proposes an alternate activation function, should go in &quot;activation functions&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07883&amp;sa=D&amp;ust=1465142038428000&amp;usg=AFQjCNHkGDUmTRC4wttIjauNhNxzTOabFw">http://arxiv.org/abs/1601.07883</a></span><span class="c1">&nbsp;overview of history of cnns for classification, some stuff about face recognition, some open problems. should go in &quot;basics&quot; and &quot;images/faces&quot; I guess?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07621&amp;sa=D&amp;ust=1465142038429000&amp;usg=AFQjCNF0_WAQRxZREYKGCqMWLOQDRcrI5g">http://arxiv.org/abs/1601.07621</a></span><span class="c1">&nbsp;some sort of application of convnets to physics data. should go in &quot;applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07950&amp;sa=D&amp;ust=1465142038429000&amp;usg=AFQjCNEIqs2f2617g8C0DpXCBJiN2IpFfg">http://arxiv.org/abs/1601.07950</a></span><span class="c1">&nbsp;face alignment detection using some sort of fully convolutional network. significantly outperforms other face detection stuff. should go in &quot;images/faces&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.07977&amp;sa=D&amp;ust=1465142038430000&amp;usg=AFQjCNFFFvPGy3f1NEOBnFwJEdW1D33Uew">http://arxiv.org/abs/1601.07977</a></span><span class="c1">&nbsp;some sort of transfer learning of dictionary representation (what is that) from convnets. notes that the features generalize quite a bit. should go in &quot;transfer learning&quot;, &quot;alternate deep methods&quot;, &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.08188&amp;sa=D&amp;ust=1465142038431000&amp;usg=AFQjCNF3FdJKNymsfhUjX1mWPVWe3F8nSg">http://arxiv.org/abs/1601.08188</a></span><span class="c1">&nbsp;speech recognition from just lip reading, with lstms. should go in &quot;language/speech&quot;, &quot;images&quot;, &quot;recurrence&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">idea: use the improvement over a baseline *as* the loss, to force something to outperform trivial priors. should go in &quot;language/speech&quot;, &quot;images/captioning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.pnas.org/content/113/1/200.full.pdf&amp;sa=D&amp;ust=1465142038432000&amp;usg=AFQjCNHQvz3O8kLYDQWzvqCCNXu6j8x9kg">http://www.pnas.org/content/113/1/200.full.pdf</a></span><span class="c1">&nbsp;dopamine encodes difference between expected and actual reward, what a surprise - hence risk aversion? should go in &quot;neuromorphic&quot; and/or &quot;brains&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00367&amp;sa=D&amp;ust=1465142038433000&amp;usg=AFQjCNE9O0F68UGpALJlQqJJjXL70YFDRQ">http://arxiv.org/abs/1602.00367</a></span><span class="c1">&nbsp;document classification with convolution over characters and lstm. should go in &quot;language/classification&quot;, &quot;classification&quot;, &quot;convolution&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00426&amp;sa=D&amp;ust=1465142038434000&amp;usg=AFQjCNHPT-JCsDuhsIzvrE0-gljA_LyyoQ">http://arxiv.org/abs/1602.00426</a></span><span class="c1">&nbsp;speech feature detection with unsupervised nns, might be fully connected. should go in &quot;unsupervised&quot;, &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00020&amp;sa=D&amp;ust=1465142038435000&amp;usg=AFQjCNF_ocRb1bPMRr8whNmc3FT_ae8nlw">http://arxiv.org/abs/1602.00020</a></span><span class="c1">&nbsp;convnets for detecting spine fractures. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00134&amp;sa=D&amp;ust=1465142038436000&amp;usg=AFQjCNEpJs0_O72VBC2rIIg3TneRI2B_kA">http://arxiv.org/abs/1602.00134</a></span><span class="c1">&nbsp;pose detection - just for humans, or for objects too? - using both a traditional pose system and an augmentation with convnets. should go in &quot;images/3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00172&amp;sa=D&amp;ust=1465142038436000&amp;usg=AFQjCNHajezsUHZLO2uiBaskv75ukPVi5A">http://arxiv.org/abs/1602.00172</a></span><span class="c1">&nbsp;facial expression detection, especially smiles, with convnets; uses some sort of architecture/hyperparameter search. claims that this is because they use a gpu, lol. mentions a dataset of facial expressions. should go in &quot;datasets&quot;, &quot;images/faces&quot;, maybe &quot;hyperparameter search&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00206&amp;sa=D&amp;ust=1465142038437000&amp;usg=AFQjCNFhk5XPc1W8vHiqw3fi4CFG287tug">http://arxiv.org/abs/1602.00206</a></span><span class="c1">&nbsp;some sort of image id generation using unsupervised learning, using both autoencoders and RBMs. should go in &quot;unsupervised&quot;. rbms making a comeback?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00224&amp;sa=D&amp;ust=1465142038438000&amp;usg=AFQjCNHZn9Z113Z_ml06je81r6rWr7P_ww">http://arxiv.org/abs/1602.00224</a></span><span class="c1">&nbsp;new pooling method, inspired by/for temporal pooling of 3d filters over video, I think. should go in &quot;convolution/pooling&quot; and/or &quot;pooling&quot;, &quot;video&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00577&amp;sa=D&amp;ust=1465142038439000&amp;usg=AFQjCNGqDxI-6N8s_34Jd3nZ6pzkVoPZoQ">http://arxiv.org/abs/1602.00577</a></span><span class="c1">&nbsp;saliency detection using something like deep dream where they descend along the input gradient to find the output. should go in &quot;images/per-pixel classification&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00203&amp;sa=D&amp;ust=1465142038439000&amp;usg=AFQjCNEmrXeWfmpboNnnSv3h7l5IZfDhZQ">http://arxiv.org/abs/1602.00203</a></span><span class="c1">&nbsp;deep dictionary learning. should go in &quot;alternate deep techniques&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00223&amp;sa=D&amp;ust=1465142038440000&amp;usg=AFQjCNF6_DNyATYUOOwW9ppfZjHk9ueQTw">http://arxiv.org/abs/1602.00223</a></span><span class="c1">&nbsp;&quot;Variance-Reduced Second-Order Methods&quot;; I don&#39;t know much about this, but people keep saying second order methods would be cool if they worked, so I&#39;m saving it. should go in &quot;optimizers&quot;/&quot;training algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00575&amp;sa=D&amp;ust=1465142038441000&amp;usg=AFQjCNH0oqdx19b0ieX7U9V8eN01yYIjSg">http://arxiv.org/abs/1602.00575</a></span><span class="c1">&nbsp;classification with rejection as a crowdsourcing task. should go in &quot;data collection&quot; and/or &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00032&amp;sa=D&amp;ust=1465142038442000&amp;usg=AFQjCNH5I9E6arNZ4KzhqTtFkE1imcza8A">http://arxiv.org/abs/1602.00032</a></span><span class="c1">&nbsp;robotics: using a convnet to predict available actions in an environment. should go in &quot;robots&quot; or &quot;rl&quot; or something, and &quot;images&quot;. mentions two publicly available indoor scene datasets, so should also go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00357&amp;sa=D&amp;ust=1465142038443000&amp;usg=AFQjCNEcKp5hFN3Cho9vymxe2C1hiQjcyA">http://arxiv.org/abs/1602.00357</a></span><span class="c1">&nbsp;deep learning to predict patient status and expected clinical events, should go in &quot;bio&quot;, and maybe be sent to my dad.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.2738&amp;sa=D&amp;ust=1465142038443000&amp;usg=AFQjCNF0Y2V8TKBeNzUt552fhCyKW5rudw">http://arxiv.org/abs/1411.2738</a></span><span class="c1">&nbsp;detailed explanation of how word2vec works; should go in &quot;language/embeddings&quot; and &quot;debugging&quot; and/or &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05099&amp;sa=D&amp;ust=1465142038444000&amp;usg=AFQjCNGmeks5rmlAhEQnhUI1GXJtjg0nGw">http://arxiv.org/abs/1511.05099</a></span><span class="c1">&nbsp;answering binary visual questions, instead of language-based ones, to prevent guessing based on probable word use. should go in &quot;language/simplicity problem&quot;, &quot;images/qa&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.02291&amp;sa=D&amp;ust=1465142038445000&amp;usg=AFQjCNFXU0zbf_6XQUY30hCwuizQrqb5qg">http://arxiv.org/abs/1503.02291</a></span><span class="c1">&nbsp;objective that tolerates more reasonable dimensions of edit distance than MSE, so you don&#39;t need adverserial networks to get a good cost function. should go in &quot;objectives&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00753&amp;sa=D&amp;ust=1465142038445000&amp;usg=AFQjCNG2uDh_C9gWDd3WGBlYoY4jBN3YQA">http://arxiv.org/abs/1602.00753</a></span><span class="c1">&nbsp;some methods for predicting size of an object given an image of it, and a relative-size dataset for competing with this paper. should go in &quot;datasets&quot;, &quot;3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00749&amp;sa=D&amp;ust=1465142038446000&amp;usg=AFQjCNEc1G5CGY1ELeeMp2SRFhVCfVzsog">http://arxiv.org/abs/1602.00749</a></span><span class="c1">&nbsp;action prediction using rgb/depth data, using both hand-designed skeleton models and convnets. uses some bayesian and HMM classifiers from the features the convnet learns. should go in &quot;images/action classification&quot;, &quot;images/3d&quot;, maybe some category about also using bayesian techniques.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00828&amp;sa=D&amp;ust=1465142038447000&amp;usg=AFQjCNG4sPvAaXfnyUK9bNYArerI-icu2Q">http://arxiv.org/abs/1602.00828</a></span><span class="c1">&nbsp;action classification by first converting any view into an abstract view-invariant space, and then classifying from that. very relevant to my interests! should go in &quot;images/3d&quot;, &quot;representation learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00985&amp;sa=D&amp;ust=1465142038448000&amp;usg=AFQjCNEM1jZFOhmZjanL8vKMtOz8Y2X9JA">http://arxiv.org/abs/1602.00985</a></span><span class="c1">&nbsp;classification of human mental state with machine learning on eeg data. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00991&amp;sa=D&amp;ust=1465142038448000&amp;usg=AFQjCNHZolTC4jAuuC6feR06i56bv9zIFQ">http://arxiv.org/abs/1602.00991</a></span><span class="c1">&nbsp;object tracking in a partially observable world using RNNs. should go in &quot;images/video&quot;, &quot;recurrence&quot;. has a video that demos it, looks like they used synthetic data. comes with the code.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01024&amp;sa=D&amp;ust=1465142038449000&amp;usg=AFQjCNErD9p526AoqpQcL0Q0exBaYxWtfQ">http://arxiv.org/abs/1602.01024</a></span><span class="c1">&nbsp;another view invariance paper, compares a bunch of representation learning schemes for it. should go in &quot;representation learning&quot;, &quot;images/3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00709&amp;sa=D&amp;ust=1465142038450000&amp;usg=AFQjCNHDvyUCTA--G9GxOvtaRW0QyegZPQ">http://arxiv.org/abs/1602.00709</a></span><span class="c1">&nbsp;some quantum nn thing. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">David Krueger: This should be sufficient for most SL problems: 1. tune your learning rate 2. use batch normalization 3. use Adam 4. also, use gradient clipping for RNNs - should go in &quot;basics&quot;, &quot;learning algorithms&quot;, &quot;hyperparameters&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">David Krueger: you do still need to tune LR for [AdaDelta or AdaSecant or AdaGrad] methods I think, although they should be less sensitive. ;; Adam is *much* more standard than any of those methods ATM. Well tuned SGD momentum does still get the best results on, e.g. ImageNet, last I heard, but if you aren&#39;t concerned with that last .5% or whatever, Adam can work significantly faster and the default params seem to do a good job for most problems. ;; I don&#39;t do much LR tuning. I would go for .01, .1, .001 as the first three to try usually (for SGD momentum). - should go in &quot;basics&quot;, &quot;learning algorithms&quot;, &quot;hyperparameters&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07868&amp;sa=D&amp;ust=1465142038453000&amp;usg=AFQjCNEKCPuUBxsE9qVwkWWJ6NxI9ynjMQ">http://arxiv.org/abs/1602.07868</a></span><span class="c1">&nbsp;&quot;weight normalization&quot;, another reparameterization of weights to make them keep a level. should go in &quot;architecture&quot;, &quot;recurrence&quot;, &quot;regularization&quot;, &quot;weights&quot;. from the openai folks, will probably get talked about a lot, especially since it looks quite a bit easier to implement than unitary evolution rnns. inspired by batch norm, doesn&#39;t actually have much to do with it.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html&amp;sa=D&amp;ust=1465142038454000&amp;usg=AFQjCNGE2-iKJ6c5MdwRd071Ik7m_yLflQ">http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html</a></span><span class="c1">&nbsp;tips and tricks for ml. should go in &quot;basics&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://deeplearning4j.org/trainingtricks&amp;sa=D&amp;ust=1465142038454000&amp;usg=AFQjCNEETPChTvyeV5X8QJxJEUypaUz2yA">http://deeplearning4j.org/trainingtricks</a></span><span class="c1">&nbsp;suggestions for training. looks fairly recent. should go in &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.deeplearningbook.org/contents/guidelines.html&amp;sa=D&amp;ust=1465142038455000&amp;usg=AFQjCNGfLghxVSuGiAUeDF5NX2IbYkXTlQ">http://www.deeplearningbook.org/contents/guidelines.html</a></span><span class="c1">&nbsp;specific section of bengio&#39;s book focused on what it takes to train. should go in &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01168&amp;sa=D&amp;ust=1465142038457000&amp;usg=AFQjCNG1P0nr7ef8nr0Dr56MtO51HP0D1g">http://arxiv.org/abs/1602.01168</a></span><span class="c1">&nbsp;regularization where you add a term to loss based on how consistent hidden units are about which label they activate for. should go in &quot;regularization&quot; and &quot;classification&quot;. seems kinda weird, but maybe does something useful.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01255&amp;sa=D&amp;ust=1465142038457000&amp;usg=AFQjCNHPJKXEQ1hNwWN6mmWx8T8HgN5t2Q">http://arxiv.org/abs/1602.01255</a></span><span class="c1">&nbsp;multi-scale cnn that explicitly encourages the network to learn some scale-invariant features. should go in &quot;convolution&quot;, &quot;images&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01164&amp;sa=D&amp;ust=1465142038458000&amp;usg=AFQjCNGHlSG_iC1RbtJBZAGWsvM_nsxeAw">http://arxiv.org/abs/1602.01164</a></span><span class="c1">&nbsp;alternate loss function to compete with MSE, some formal bound between them, and some other thing. &quot;Single-Solution Hypervolume Maximization and its use for Improving Generalization of Neural Networks&quot;. should go in &quot;loss functions&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07360&amp;sa=D&amp;ust=1465142038459000&amp;usg=AFQjCNGSnqcSLgMlywJW-QGhRNRsy7VwYg">http://arxiv.org/abs/1602.07360</a></span><span class="c1">&nbsp;model compression paper, that compares itself to alexnet (??) with fewer parameters, and then applies model compression. &quot;squeezenet&quot;. should go in &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://mxnet.readthedocs.org/en/latest/tutorial/smart_device.html&amp;sa=D&amp;ust=1465142038460000&amp;usg=AFQjCNHl8rM3TJSBWbyq4XgGHURv8Xct-Q">https://mxnet.readthedocs.org/en/latest/tutorial/smart_device.html</a></span><span class="c1">&nbsp;concrete tutorial on using model compression. should go in &quot;model compression&quot;, &quot;examples&quot;/&quot;libraries&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://rll.berkeley.edu/gps/index.html&amp;sa=D&amp;ust=1465142038460000&amp;usg=AFQjCNH1f38seF3_3VdOgVBMb-8zcB4bXA">http://rll.berkeley.edu/gps/index.html</a></span><span class="c1">&nbsp;berkeley RL stuff as a software package, should go in &quot;RL&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07416&amp;sa=D&amp;ust=1465142038461000&amp;usg=AFQjCNEdz0IXFuNZ8FJHBA0a3vtwIv58Ig">http://arxiv.org/abs/1602.07416</a></span><span class="c1">&nbsp;generative model thing I didn&#39;t quite follow, with some sort of memory. should go in &quot;generative&quot; and &quot;memory&quot;. </span><span class="c1 c21">This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. The whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://willwhitney.github.io/understanding-visual-concepts/&amp;sa=D&amp;ust=1465142038462000&amp;usg=AFQjCNFgW20ZAFdEYYUHNBtXAY7ItiGGSA">http://willwhitney.github.io/understanding-visual-concepts/</a></span><span class="c1">&nbsp;and </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06822&amp;sa=D&amp;ust=1465142038463000&amp;usg=AFQjCNGYynZGthzrlt9nkuWoLmmbDlJnIA">http://arxiv.org/abs/1602.06822</a></span><span class="c1">&nbsp;- building a predictive autoencoder with some sort of interesting sparsification constraint or generative portion or something. should go in &quot;sparsity&quot;, &quot;generative&quot;/&quot;autoencoders&quot;, &quot;images&quot;, &quot;rl&quot; - this looks very relevant to my world modeling ideas. they also mention another: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/47kn8b/understanding_visual_concepts_with_continuation/d0dxxpw&amp;sa=D&amp;ust=1465142038463000&amp;usg=AFQjCNEdIzvQrU-1naXWZW2tnRnE8aafBg">https://www.reddit.com/r/MachineLearning/comments/47kn8b/understanding_visual_concepts_with_continuation/d0dxxpw</a></span><span class="c1">&nbsp;some discussion about what this and </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.08750&amp;sa=D&amp;ust=1465142038464000&amp;usg=AFQjCNF3iUmZNreQFHddSn-caGDbFTX__g">1507.08750</a></span><span class="c1">&nbsp;can learn, </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/47kn8b/understanding_visual_concepts_with_continuation/d0e24au&amp;sa=D&amp;ust=1465142038464000&amp;usg=AFQjCNEGKjt8sfMzXlZOMalyAo4U9rwjpQ">https://www.reddit.com/r/MachineLearning/comments/47kn8b/understanding_visual_concepts_with_continuation/d0e24au</a></span><span class="c1">&nbsp;some discussion about how they test out what it&#39;s learned.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction&amp;sa=D&amp;ust=1465142038465000&amp;usg=AFQjCNHoRUa2gaRDZtm1oWBtqtFjH51zNg">https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction</a></span><span class="c1">&nbsp;and </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.08750&amp;sa=D&amp;ust=1465142038466000&amp;usg=AFQjCNF-FrjjbI0rVqYVIIQ1Sv2tasZ9PA">http://arxiv.org/abs/1507.08750</a></span><span class="c1">&nbsp;- action-conditional exploration in DQNs - looks like they did almost exactly what I was thinking. should go in &quot;autoencoders&quot;, &quot;images&quot;, &quot;rl&quot;, &quot;recurrence&quot;. They did EXACTLY what I was thinking, with a recurrent state-predictor. damn. guess it works, though. once again, doing science by hearing that other people independently generated and tested my ideas. woo hoo.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03218v2&amp;sa=D&amp;ust=1465142038467000&amp;usg=AFQjCNFjb1YK4eBDonICbrWTFBD5pN1VhQ">http://arxiv.org/abs/1602.03218v2</a></span><span class="c1">&nbsp;[abs:4][sanity,reddit] trainable binary trees. uses a memory system that is tree-structured. should go in &quot;memory&quot;. algorithm learning, yay. Might be really cool though, got referenced on reddit as a way to build watson.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07576&amp;sa=D&amp;ust=1465142038467000&amp;usg=AFQjCNHVN2UoFwVT65Ahdacc4-qYALmrMw">http://arxiv.org/abs/1602.07576</a></span><span class="c1">&nbsp;convolution that varies in dimensions besides position: also rotation, reflection, etc. should go in &quot;convolution&quot;, &quot;images&quot;, one of &quot;regularization&quot; or &quot;parameter reduction&quot;. as is to be expected, helps quite a bit.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://colah.github.io/posts/2015-09-NN-Types-FP/&amp;sa=D&amp;ust=1465142038468000&amp;usg=AFQjCNHwFztiB6i2tUAFTvt-qdg744UYDw">http://colah.github.io/posts/2015-09-NN-Types-FP/</a></span><span class="c1">&nbsp;interpreting complicated nns as approximate differentiable programming. uh, yes, of course, duh, this is so excessively obvious it&#39;s a little bit stupid - you literally implement it by doing this. should go in &quot;theory&quot;, though.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.cs.jhu.edu/~jason/tutorials/variational.html&amp;sa=D&amp;ust=1465142038469000&amp;usg=AFQjCNEUE70_Qr1QBUZhStw8P0UcMFkXgg">https://www.cs.jhu.edu/~jason/tutorials/variational.html</a></span><span class="c1">&nbsp;overview of how variational inference works. should go in &quot;basics&quot; and/or &quot;theory/bayesian&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06709&amp;sa=D&amp;ust=1465142038470000&amp;usg=AFQjCNGZuTg6iLH6UUjcyXVZ6Wyv2g2C2Q">http://arxiv.org/abs/1602.06709</a></span><span class="c1">&nbsp;synchronous stochastic gradient descent - HOLY FUCKING CHRIST THEY HAVE A 90X SPEEDUP. takes no shortcuts like asgd does - just parallelizes the whole thing. should go in &quot;parallelization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07261&amp;sa=D&amp;ust=1465142038470000&amp;usg=AFQjCNGUW7xS43prWJzoIUoVgP2KFLol9Q">http://arxiv.org/abs/1602.07261</a></span><span class="c1">&nbsp;analysis of residual and inception architectures - should go in &quot;architectures&quot;. speculation: inception architectures seem a little stupid, because you could just as well get what they get via things like rotation and multi-scale invariance.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01321&amp;sa=D&amp;ust=1465142038471000&amp;usg=AFQjCNG3IF7fMaMYYIAzodpDCKb4BQPeIg">http://arxiv.org/abs/1602.01321</a></span><span class="c1">&nbsp;activation function that smoothly varies between several basic functions: logarithmic, linear, and exponential. should go in &quot;activation functions&quot;. this is the sort of thing I was looking for.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01407&amp;sa=D&amp;ust=1465142038472000&amp;usg=AFQjCNEwM66tq6AFkDTE4K4mv2tGmj88MQ">http://arxiv.org/abs/1602.01407</a></span><span class="c1">&nbsp;training algorithm and architecture that uses &quot;Kronecker-factored approximate Fisher matrix&quot; representations of the convolutions. much more update efficient - 10 to 20 times fewer iterations. should go in &quot;update efficiency&quot; or &quot;speed&quot;, &quot;training algorithms&quot;, &quot;architecture/representations&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.04086&amp;sa=D&amp;ust=1465142038473000&amp;usg=AFQjCNG02HaAoa1zF-dejunFilwhH_AWXQ">http://arxiv.org/abs/1512.04086</a></span><span class="c1">&nbsp;some sort of transfer learning thing where they train a siamese network to estimate class similarity and then use that for actual classification via an SVM. should go in &quot;classification&quot;, &quot;wat&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.08495&amp;sa=D&amp;ust=1465142038473000&amp;usg=AFQjCNGlhNYIklY2o_IlgKYQyJpdJMTxkQ">http://arxiv.org/abs/1511.08495</a></span><span class="c1">&nbsp;low rank temporal difference learning something I don&#39;t understand something something whatever. should go in &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01576&amp;sa=D&amp;ust=1465142038474000&amp;usg=AFQjCNEoDXMAuiHSBrgdDD_6Tqu3T7a4tQ">http://arxiv.org/abs/1602.01576</a></span><span class="c1">&nbsp;large language models with some sort of advancement to heirarchical softmax or something. should go in &quot;language/language models&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01517&amp;sa=D&amp;ust=1465142038475000&amp;usg=AFQjCNHwvYHTVJP-InyX929qu6oqU_4Bqg">http://arxiv.org/abs/1602.01517</a></span><span class="c1">&nbsp;fine tuning convnets and then using their features for SVMs for doing &quot;remote sensing&quot;, whatever the fuck that is. apparently new state of the art on their datasets. should go in &quot;classification&quot;, &quot;transfer learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01528&amp;sa=D&amp;ust=1465142038476000&amp;usg=AFQjCNHlyEkGiN-wRDv9RB1TlFsAnHr0xA">http://arxiv.org/abs/1602.01528</a></span><span class="c1">&nbsp;very high speed implementation of nns on apparently a specialized chip, should go in &quot;speed&quot; and &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/47qjfz/neural_net_crf_in_torch/&amp;sa=D&amp;ust=1465142038477000&amp;usg=AFQjCNESiIQOYyihRYEjdWBSZe38JzBjAQ">https://www.reddit.com/r/MachineLearning/comments/47qjfz/neural_net_crf_in_torch/</a></span><span class="c1">&nbsp;question about implementing crf/conditional random fields in torch. should go in &quot;libraries&quot;, &quot;conditional random fields&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06279&amp;sa=D&amp;ust=1465142038477000&amp;usg=AFQjCNF9xEu4YPcveD-LssFamtJ7P50vQw">http://arxiv.org/abs/1511.06279</a></span><span class="c1">&nbsp;[abs:2] recommended twice - neural programmer-interpreter. should go in &quot;memory&quot;, &quot;towards generality&quot;, &quot;training algorithms&quot; - trains its own representations of programs. then uses this to solve whatever actual problem you want, and gets to decide how long to think about it, to a limit.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01625&amp;sa=D&amp;ust=1465142038478000&amp;usg=AFQjCNEirGWbWReFoS1OgH_8Fh2fzru0yw">http://arxiv.org/abs/1602.01625</a></span><span class="c1">&nbsp;network that trains multiple objectives at once, they call it &quot;self transfer learning&quot;, for transferring knowledge from classification to object localization. should go in &quot;images&quot;, &quot;transfer learning&quot;, &quot;weight sharing&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01728&amp;sa=D&amp;ust=1465142038479000&amp;usg=AFQjCNE3Lw9Rb8Bz_tRlJaxC5tyt8nyTYA">http://arxiv.org/abs/1602.01728</a></span><span class="c1">&nbsp;some sort of transfer learning from cnns to salience detection. &quot;NeRD&quot;. should go in &quot;images&quot;, &quot;transfer learning&quot;. mentions using stochastic nns?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01827&amp;sa=D&amp;ust=1465142038479000&amp;usg=AFQjCNGgO9MvOmAY-A9JyB2PiSU2x45EWQ">http://arxiv.org/abs/1602.01827</a></span><span class="c1">&nbsp;face recognition and face attribute prediction by using the high level features for recognition and the intermediate ones for attribute prediction. they find that the deeper features don&#39;t work as well for attribute prediction - should have been residual? should go in &quot;architectures&quot;, &quot;images/faces&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01580&amp;sa=D&amp;ust=1465142038480000&amp;usg=AFQjCNHu8J8p7037z3N0QDXOE3KJz3QA7Q">http://arxiv.org/abs/1602.01580</a></span><span class="c1">&nbsp;supervised reinforcement learning by discovering the optimal path with foreknowledge and then training on it, with an rnn. and some sort of near-future prediction. should go in &quot;rl&quot;. mentions autonomous vehicles.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01690&amp;sa=D&amp;ust=1465142038481000&amp;usg=AFQjCNHQk0Y4nFWT8J6TYN1WFNCr7hUzug">http://arxiv.org/abs/1602.01690</a></span><span class="c1">&nbsp;average loss vs maximal loss as an objective. gives a function that has derivatives for only the data point with maximal loss, reports that this can fit the training set better, and does help with generalization, not just overfitting. should go in &quot;loss functions&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01510&amp;sa=D&amp;ust=1465142038482000&amp;usg=AFQjCNESQu2sddDimUpMoQEphYPnkHyF2w">http://arxiv.org/abs/1602.01510</a></span><span class="c1">&nbsp;sparse spiking cnns on imagenet and cifar - gets reasonable results. should go in &quot;sparsity&quot;, &quot;spiking&quot;/&quot;binary&quot;, &quot;generative&quot;/&quot;autoencoders&quot;. uses an autoencoder to learn features, and then a supervised layer to classify.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01616&amp;sa=D&amp;ust=1465142038482000&amp;usg=AFQjCNEvWdh_tWPYhKK5onGth4mF7eInrA">http://arxiv.org/abs/1602.01616</a></span><span class="c1">&nbsp;very efficient fpga implementation of NNs - quarter the performance of gpu, for only five watts. uses three-bit weights, and uses appropriate training algorithms. should go in &quot;speed&quot;/&quot;efficiency&quot;/&quot;hardware&quot;, &quot;model compression&quot;. maybe &quot;weights&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.08909&amp;sa=D&amp;ust=1465142038483000&amp;usg=AFQjCNFNrMOkkAujzcR2PhSqyCWR-G-LqA">http://arxiv.org/abs/1506.08909</a></span><span class="c1">&nbsp;#ubuntu irc logs as a corpus. lol. should go in &quot;datasets&quot;. I have a better dataset like this though, lol. *way* better.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.05067&amp;sa=D&amp;ust=1465142038484000&amp;usg=AFQjCNG6HI5cFzzhLG2-D8jW7sLled6YZA">http://arxiv.org/abs/1510.05067</a></span><span class="c1">&nbsp;another bio backprop via asymmetric feedback weights. w/e. should go in &quot;neuromorphic&quot;, &quot;training algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01929&amp;sa=D&amp;ust=1465142038485000&amp;usg=AFQjCNHjWmHnYu9u6fBYz10pddY9sYVa1Q">http://arxiv.org/abs/1602.01929</a></span><span class="c1">&nbsp;language detection application. should go in &quot;language/misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02068&amp;sa=D&amp;ust=1465142038486000&amp;usg=AFQjCNHVJ4MhmlmxwgnyUWvyfyxpwSSvGQ">http://arxiv.org/abs/1602.02068</a></span><span class="c1">&nbsp;&quot;sparsemax&quot; - &quot;able to output sparse probabilities&quot;. haven&#39;t read, looks interesting; when used for selecting what to apply attention to in an attention problem, gets tighter attention bounds. should go in &quot;activation functions&quot;, &quot;sparsity&quot;, &quot;attention&quot;. looks cool.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01890&amp;sa=D&amp;ust=1465142038487000&amp;usg=AFQjCNE6piB9yIw93g8e0Mji13VaRFb1Rw">http://arxiv.org/abs/1602.01890</a></span><span class="c1">&nbsp;weird retrieval-based object tracking system. comes with object tracking dataset. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01895&amp;sa=D&amp;ust=1465142038487000&amp;usg=AFQjCNHk0uWRLE_USrzHgu_36Ixs5WFg7g">http://arxiv.org/abs/1602.01895</a></span><span class="c1">&nbsp;improvement in image captioning via gating what features the image sees. sounds like it&#39;s basically attention. looks hella boring. should go in &quot;images/captioning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.01921&amp;sa=D&amp;ust=1465142038488000&amp;usg=AFQjCNHKOvlae9SSLtNzycm5GRk4FhGmQQ">http://arxiv.org/abs/1602.01921</a></span><span class="c1">&nbsp;not super clear; some sort of action categorization thing, mentions &quot;directed objects&quot; which I suspect are objects under operation in an action. uses multi-timescale rnn; this is what is really interesting. should go in &quot;memory&quot;, &quot;recurrence&quot;, next to clockwork rnns.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02130&amp;sa=D&amp;ust=1465142038489000&amp;usg=AFQjCNETcyDDYCWl8QhXKEkTsTwliZMW0A">http://arxiv.org/abs/1602.02130</a></span><span class="c1">&nbsp;brain section segmentation with fully convolutional networks. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02123&amp;sa=D&amp;ust=1465142038490000&amp;usg=AFQjCNGoH_omL1v7Jys0_8QGh5E8jBGRvg">http://arxiv.org/abs/1602.02123</a></span><span class="c1">&nbsp;conditional random fields for sequence classification. should go in &quot;conditional random fields&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02136&amp;sa=D&amp;ust=1465142038490000&amp;usg=AFQjCNG84hSyq8ZuKKxmQoGMTOP-hIl5Fw">http://arxiv.org/abs/1602.02136</a></span><span class="c1">&nbsp;another update-efficiency thing that revisits samples sometimes in gradient descent. should go in &quot;training algorithms&quot;, &quot;update efficiency&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02009&amp;sa=D&amp;ust=1465142038491000&amp;usg=AFQjCNGbgXJgkBEbO76hwf_RmAko-bTkfg">http://arxiv.org/abs/1602.02009</a></span><span class="c1">&nbsp;some stuff about whether spiking neurons are a good idea for hardware. should go in &quot;spiking&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.01337&amp;sa=D&amp;ust=1465142038492000&amp;usg=AFQjCNGeOK3k_PzHYbc38M_z76Y8yr_7dQ">http://arxiv.org/abs/1512.01337</a></span><span class="c1">&nbsp;some sort of question-answering thing that uses attention over a learned embedding thing, also mentions generative stuff. not really clear where the improvement actually is, or how generative comes in. should go in &quot;language/qa&quot;, and I guess &quot;generative&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02202&amp;sa=D&amp;ust=1465142038493000&amp;usg=AFQjCNG6zx6OKRSS9jZf7iASOSFZ8IK_dA">http://arxiv.org/abs/1602.02202</a></span><span class="c1">&nbsp;another second-order learning technique. mentions &quot;sketching&quot;, and oja&#39;s rule. whatever. should go in &quot;training algorithms/second order&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02218&amp;sa=D&amp;ust=1465142038494000&amp;usg=AFQjCNF64dzqFSSbzdR9esyPs0JXRpkORA">http://arxiv.org/abs/1602.02218</a></span><span class="c1">&nbsp;something about &quot;strongly typed nets&quot;. looks cool, but wat?? should go in &quot;architecture&quot;, maybe &quot;theory&quot;. could also be considered &quot;regularization&quot;, I guess.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02220&amp;sa=D&amp;ust=1465142038495000&amp;usg=AFQjCNEJ77iyVGWDBdZiyLybgHvt_UbhGw">http://arxiv.org/abs/1602.02220</a></span><span class="c1">&nbsp;fancy dropout that adjusts dropout rates based on second-order values, not clear how it adjusts it from my lack of knowledge. should go in &quot;second order&quot;, &quot;regularization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02283&amp;sa=D&amp;ust=1465142038496000&amp;usg=AFQjCNFbg0jZMDtcEBWxms4AjC2LTsEMQA">http://arxiv.org/abs/1602.02283</a></span><span class="c1">&nbsp;another prioritizing-data-samples technique, looks similar to ilya&#39;s, also claims a huge improvement. should go in &quot;training algorithms&quot;, &quot;update efficiency&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02383&amp;sa=D&amp;ust=1465142038496000&amp;usg=AFQjCNHZ7cbAjWepeoNT9D2NrGqvsVHN6g">http://arxiv.org/abs/1602.02383</a></span><span class="c1">&nbsp;some work on encouraging disentangling representation in representation learning; looks interesting, I suspect it&#39;s an objective. should go in &quot;unsupervised/representation learning&quot;, &quot;objectives&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.02777&amp;sa=D&amp;ust=1465142038497000&amp;usg=AFQjCNFObNOqxBGksRPkv1HueCFxwDrm3w">http://arxiv.org/abs/1510.02777</a></span><span class="c1">&nbsp;comparison of a non-neural thing to backprop. should go in &quot;neuromorphic&quot;, &quot;bio-plausible&quot;. partly because I cbf to figure out what it means. &quot;Early Inference in Energy-Based Models Approximates Back-Propagation&quot;. one of yoshua bengio&#39;s insane papers.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05946&amp;sa=D&amp;ust=1465142038498000&amp;usg=AFQjCNHUhyNAKBrvKo46EPDuaDwkGybJmg">http://arxiv.org/abs/1511.05946</a></span><span class="c1">&nbsp;more efficient fully connected layer via some sort of approximate representation, &quot;ACDC&quot;. should go in &quot;performance&quot; or &quot;hidden representations&quot; or whatever I called it, maybe it&#39;s under &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02656&amp;sa=D&amp;ust=1465142038498000&amp;usg=AFQjCNHI5DcMbo8cDN3YIrqYqCXPeE9gyg">http://arxiv.org/abs/1602.02656</a></span><span class="c1">&nbsp;using LSTMs for improving the quality of synthetic voices. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02761&amp;sa=D&amp;ust=1465142038499000&amp;usg=AFQjCNHM4aLr4YcTEZzGpX1SOHtfdzFDSA">http://arxiv.org/abs/1506.02761</a></span><span class="c1">&nbsp;much more data efficient word-embedding learning algorithm that is also faster. should go in &quot;language/embeddings&quot;. uses &quot;ranking&quot;, not sure how literal that is in the typical language sense.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.03877&amp;sa=D&amp;ust=1465142038500000&amp;usg=AFQjCNH-Og76NturS-Yty4sOs2jWpKvDhw">http://arxiv.org/abs/1509.03877</a></span><span class="c1">&nbsp;some sort of combination of CNNs and RNNs. not really clear how it&#39;s combined from the abstract. claims to be using the RNN to represent spatial something or other, which the CNN already does? should go in &quot;convolution&quot; and &quot;recurrence&quot; anyway, though. also &quot;images&quot; because that&#39;s what they test it on and they say it does well.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02311&amp;sa=D&amp;ust=1465142038501000&amp;usg=AFQjCNHK-MMFTWz0acXj0NWG-b-l_pMzog">http://arxiv.org/abs/1602.02311</a></span><span class="c1">&nbsp;a new objective for variational inference, &quot;R&eacute;nyi Divergence&quot;. they try it on bayesian neural networks. should go in &quot;bayesian neural networks&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02355&amp;sa=D&amp;ust=1465142038502000&amp;usg=AFQjCNHiCHwsLFiPd-V4G7SX2b7ur3qASg">http://arxiv.org/abs/1602.02355</a></span><span class="c1">&nbsp;another gradient-approximation-for-hyperparameters paper. claims it&#39;s &quot;highly competitive&quot;. should go in &quot;hyperparameters&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">&nbsp;</span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02523&amp;sa=D&amp;ust=1465142038503000&amp;usg=AFQjCNGJUn-pyBrHwJAPjWEuae0u5n5XHw">http://arxiv.org/abs/1602.02523</a></span><span class="c1">&nbsp;non-neural reinforcement learning in continuous-state partially observable mdps. should go in &quot;rl&quot;, &quot;non-neural&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02389&amp;sa=D&amp;ust=1465142038504000&amp;usg=AFQjCNEo-eSl0i0sxEjWlDVGNlCR7uBcrA">http://arxiv.org/abs/1602.02389</a></span><span class="c1">&nbsp;model of why nns perform so well, based on a theoretical model they call &quot;ensemble robustness&quot;. not tested analytically, but they test it empirically and it seems to hold up ok. should go in &quot;theory&quot;. they also use it to find a better semi-supervised learner - should go in &quot;semi-supervised&quot;. cool!</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02505&amp;sa=D&amp;ust=1465142038505000&amp;usg=AFQjCNFTeatR2WKnIAfeJJET9EBFzOGMgg">http://arxiv.org/abs/1602.02505</a></span><span class="c1">&nbsp;binarized nns paper; binary weights, binary activations. high precision gradients. small potatoes. should go in &quot;binary&quot;/&quot;spiking&quot;, &quot;regularization&quot;, &quot;hardware&quot;, &quot;weights&quot;..</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02644&amp;sa=D&amp;ust=1465142038506000&amp;usg=AFQjCNFmRP2v84f986Jg17wHadai2w1hBg">http://arxiv.org/abs/1602.02644</a></span><span class="c1">&nbsp;similar idea to adverserial networks - toss another network on the end as your loss function. however, this doesn&#39;t seem to be the same adverserial framing as DCGAN - instead, it looks like they use a pretrained net and minimize representation difference. I suspect this will work better, primarily because a pretrained net will know a lot more about natural images. should go in &quot;loss functions&quot; next to dcgan (is it in loss functions?) and in something else related to that. it&#39;s pretty relevant for generative networks, so &quot;generative&quot;. also &quot;images&quot;, maybe &quot;images/prediction&quot; or something. &nbsp;</span><span class="c9 c12 c1">-&gt; whoa this works shittons better.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02658&amp;sa=D&amp;ust=1465142038507000&amp;usg=AFQjCNHWABBzNe_lBQbuXbqGZHGV8axcdg">http://arxiv.org/abs/1602.02658</a></span><span class="c1">&nbsp;theoretical analysis of why dqn works so well. should go in &quot;rl&quot;, &quot;theory&quot;. looks very interesting. also involves why convnets work; might not even have anything that unique to DQN. I wonder if they come up with the worldmodel/value/search split.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">note: &quot;theory&quot; and &quot;debugging&quot; should almost certainly be merged.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">note: regarding worldmodel/value/search, there&#39;s also the question of search over the worldmodel. and value function. I guess basically I&#39;d want everything to be recurrent and get to think for a while. you also want everything to have subnetworks, as in the sub-model selection section, so it can learn esoteric things like math. I think. basically my assumption is that eventually you have to stop using the same distributed representation for everything, because it just isn&#39;t distributed along anything like the same dimensions. on the other hand, people seem to get a lot of bang for their buck out of reusing physical models for seriously abstract things. should go in &quot;my ideas&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02660&amp;sa=D&amp;ust=1465142038509000&amp;usg=AFQjCNFjUDBPVHmaoiJEtfDv9pyvSZ4Ivg">http://arxiv.org/abs/1602.02660</a></span><span class="c1">&nbsp;another paper about introducing more learnable invariance into CNNs. this one is just rotation invariance, done by adding a layer type. didn&#39;t read, not sure how that works. should go in &quot;convolution&quot; and &quot;images&quot;, maybe also &quot;regularization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02685&amp;sa=D&amp;ust=1465142038510000&amp;usg=AFQjCNF0L-OQ8KZDjDGTrQLfd4_33AFivA">http://arxiv.org/abs/1602.02685</a></span><span class="c1">&nbsp;another clinical event prediction paper, this one using rnns for event prediction with both varying and static inputs. they also try predicting good intervention next actions, and get better results with feedforward. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02332&amp;sa=D&amp;ust=1465142038511000&amp;usg=AFQjCNFxQBx-8kmIm17ovbZOTKhVNgBTiw">http://arxiv.org/abs/1602.02332</a></span><span class="c1">&nbsp;some sort of generative text mining thing. not sure if it&#39;s neural; mentions sparsity, I suspect it&#39;s deep sparse generative networks and then clustering on hidden representations. mentions kaggle, which makes me suspicious that it might be lacking on rigor. should go in &quot;generative&quot;, &quot;sparsity&quot;, &quot;language/generative&quot; or &quot;language/prediction&quot; or whatever I called it. I guess I called it language models, but this isn&#39;t a language model.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.00677&amp;sa=D&amp;ust=1465142038512000&amp;usg=AFQjCNG2DbQBdv8NJY1MhzvAmGjzhgIoJQ">http://arxiv.org/abs/1507.00677</a></span><span class="c1">&nbsp;regularization that works against the adverserial examples problem, by maximizing local smoothness. looks pretty cool, possibly superseded by the other regularization approach I have sitting; note that this doesn&#39;t actually require adverserial examples to be generated. should go in &quot;regularization&quot;, &quot;adverserial examples problem&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02373&amp;sa=D&amp;ust=1465142038513000&amp;usg=AFQjCNGV-GRInI12hz7ZB5k2zPGwAm6aag">http://arxiv.org/abs/1602.02373</a></span><span class="c1">&nbsp;lstm with no initial linear layer when doing word embedding. compares lstm to convolution as &quot;area pooling&quot;; sure, seems fine. should go in &quot;recurrence&quot;, &quot;language/embeddings&quot; or &quot;language models&quot;. </span><span class="c9 c1 c12">claims big results with this.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02181&amp;sa=D&amp;ust=1465142038514000&amp;usg=AFQjCNH8I54Z2uWWjIYEy8eI4zz2VjXLRw">http://arxiv.org/abs/1602.02181</a></span><span class="c1">&nbsp;active learning in the context of performing searches or something. looks cool. should go in &quot;data acquisition&quot;/&quot;active learning&quot;, and also they use it for &quot;language/sentiment&quot; and &quot;image&quot; &quot;classification&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02285&amp;sa=D&amp;ust=1465142038514000&amp;usg=AFQjCNHj2YgZjmgKVccWuL5BQD4bH2qqkg">http://arxiv.org/abs/1602.02285</a></span><span class="c1">&nbsp;some thing using rbms for crowdsourcing. wat? should go in &quot;data acquisition&quot;/&quot;active learning&quot; because I suspect it&#39;s related, and &quot;unsupervised/rbm&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06561v2&amp;sa=D&amp;ust=1465142038515000&amp;usg=AFQjCNFFtVwg4_woLWamxbFLOGVwXqI0Lg">http://arxiv.org/abs/1602.06561v2</a></span><span class="c1">&nbsp;&quot;deep learning in finance&quot;. abstract isn&#39;t very specific, but it&#39;s not trading or anything. should go in &quot;finance&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07188v1&amp;sa=D&amp;ust=1465142038516000&amp;usg=AFQjCNEuIm7S9JOf5T1wY3IdVpReScaapA">http://arxiv.org/abs/1602.07188v1</a></span><span class="c1">&nbsp;further investigation into the &quot;artistic style&quot; thing. should go in &quot;generative&quot;, I guess. also, they apparently use an adverserial portion. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07125v1&amp;sa=D&amp;ust=1465142038517000&amp;usg=AFQjCNExFIxwGM8vLtwWV7PT3XrB0qcyMQ">http://arxiv.org/abs/1602.07125v1</a></span><span class="c1">&nbsp;first application of deep learning to a task: car type recognition. surprising nobody, blows manual feature engineering out of the fucking water. should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07291v1&amp;sa=D&amp;ust=1465142038518000&amp;usg=AFQjCNGJqnPrD8p9XLqv4KNLSfZHCl0B9g">http://arxiv.org/abs/1602.07291v1</a></span><span class="c1">&nbsp;some ibm speech recognition thing. mentions a speech dataset. overview of combined application. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06662&amp;sa=D&amp;ust=1465142038519000&amp;usg=AFQjCNF-N64wemdeQHRvBT5u91yC0DaXHA">http://arxiv.org/abs/1602.06662</a></span><span class="c1">&nbsp;theory work on what rnns learn (lecun), and why the unitary/etc stuff works well. should go in &quot;recurrence&quot;, &quot;theory&quot;, &quot;parameter shape priors, weights, and init&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04650&amp;sa=D&amp;ust=1465142038520000&amp;usg=AFQjCNEYSIYMctUeMaSBJd3F_8XNwQ-qEQ">http://arxiv.org/abs/1601.04650</a></span><span class="c1">&nbsp;legit statistical theory work on high dimensional inference. over my head. should go in &quot;theory&quot;. mentions bayesian stuff.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04834&amp;sa=D&amp;ust=1465142038521000&amp;usg=AFQjCNGpUwYZfW5sF6sOdmKZ4UhJTnJh6Q">http://arxiv.org/abs/1511.04834</a></span><span class="c1">&nbsp;[abs] &quot;neural programmer&quot;; looks like a precursor to the neural gpu. should go in &quot;memory&quot;, maybe also &quot;homoiconicity&quot;/&quot;programming&quot;. ilya was involved.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02867&amp;sa=D&amp;ust=1465142038522000&amp;usg=AFQjCNEtp0jzIDcCXW_MwMiGi1kXFIZw5A">http://arxiv.org/abs/1602.02867</a></span><span class="c1">&nbsp;[abs] here we go! planning within the neural model. should go in &quot;rl&quot;, maybe also &quot;planning&quot;/&quot;search&quot;. performs well on difficult map-planning datasets!!! as RL!</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress&amp;sa=D&amp;ust=1465142038523000&amp;usg=AFQjCNHeUlLs25pRQ3BgZvurLHDGrPPUoA">http://www.milesbrundage.com/blog-posts/alphago-and-ai-progress</a></span><span class="c1">&nbsp;[abs] some commentary about how to measure ai progress, by the same guy who had the other ai progress paper. should go in &quot;rl/safety&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1104.2373v4&amp;sa=D&amp;ust=1465142038523000&amp;usg=AFQjCNHAxSOOsaEk2PzHIV1jERvVCZdXwA">http://arxiv.org/abs/1104.2373v4</a></span><span class="c1">&nbsp;[abs] another approach to the gradient problem: vary your batch size, reducing it when you need larger updates and increasing it when you need refinement. should go in &quot;regularization&quot;, &quot;prioritization&quot;, &quot;training algorithms&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://cs231n.github.io/neural-networks-3/&amp;sa=D&amp;ust=1465142038524000&amp;usg=AFQjCNGUoGg-15aBbYkU8lL1EQhbldmWvw">http://cs231n.github.io/neural-networks-3/</a></span><span class="c1">&nbsp;[skimmed] page from cs231n (could you have guessed?) that gives an overview of training tricks. should go in &quot;tricks&quot;/&quot;basics&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html&amp;sa=D&amp;ust=1465142038525000&amp;usg=AFQjCNHOOCuj64XFYxZHcvBQIyKWYwDM4g">http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html</a></span><span class="c1">&nbsp;[read]</span><span class="c1">&nbsp;out of date, doesn&#39;t know about resnets and training 1000-layer nets, but overview of training tricks and intuition behinds nns, by none other than ilya sutskever. should go in &quot;theory&quot; and &quot;basics&quot;. </span></p><ul class="c2 lst-kix_rcp8sez4svjd-0 start"><li class="c6 c7"><span class="c9 c1">disagreement with something he says:</span><span class="c1">&nbsp;</span><span class="c1 c21">&quot;And if human neurons turn out to be noisy (for example), which means that many human neurons are required to implement a single real-valued operation that can be done using just one artificial neuron&quot; </span><span class="c1">&nbsp;- when noise is added to anns, they get better, not worse. especially if you make them bigger as you add noise. you can shrink them afterward, but it helps a ton with training. see also regularization, especially mc dropout.</span></li><li class="c6 c7"><span class="c9 c1">another interesting thing:</span><span class="c1">&nbsp;he mentions that training nns is np-hard in the general case (unsurprising); so, what kinds of datasets can nns not learn, or will have a very hard time learning? (prediction: things that hinge on np-hard problems of their own, such as finding good-enough solutions to TSP quickly.)</span></li><li class="c6 c7"><span class="c9 c1">another thing: </span><span class="c1">he mentions that there&#39;s apparently a theorem as to how many training examples you need to train a model class (architecture) with some number of bits of possible model configurations (parameters). this provides a much better model of why regularization works than anything else I&#39;ve heard of, and matches my intuition *very* well.</span></li><li class="c6 c7"><span class="c9 c1">summary of suggestions:</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-1 start"><li class="c6 c22"><span class="c1">center your data as a preprocessing and keep its magnitudes fairly small.</span></li><li class="c6 c22"><span class="c1">use the smallest minibatch that maxes out your compute resources.</span></li><li class="c6 c22"><span class="c1">divide the gradient by the minibatch size (note: I&#39;ve heard it&#39;s more complicated than that, something about square roots).</span></li><li class="c6 c22"><span class="c1">start with the learning rate 0.1, it usually works; start lowering the learning rate when the validation error starts to increase or stops decreasing, to taste (in other words, try shit, I dunno).</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-2 start"><li class="c6 c17"><span class="c1">he suggests </span><span class="c1 c21">&quot;if you see that you stopped making progress on the validation set, divide the LR by 2 (or by 5), and keep going. Eventually, the LR will become very small, at which point you will stop your training.&quot;</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-1"><li class="c6 c22"><span class="c1">monitor the ratio between the update norm (gradient norm) and the weight norm. This ratio should be at around 10^-3. If it is much smaller then learning will probably be too slow, and if it is much larger then learning will be unstable and will probably fail.</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-2 start"><li class="c6 c17"><span class="c1">note: norm, most likely he means specifically the l2 norm; often called absolute value for vectors, is just the length of the vector relative to the 0 point</span></li><li class="c6 c17"><span class="c1">note: there are adaptive learning rate algorithms. they usually converge must faster to a network that works not-quite-as-well as vanilla sgd would have found.</span></li><li class="c6 c17"><span class="c1">note: updating based on the size of the gradient is actually a bit mistaken in a non-convex world! see the &quot;learning algorithms&quot; section to see SOTA on how to do better. summary: it&#39;s actually really hard, but rmsprop can sometimes be cool.</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-1"><li class="c6 c22"><span class="c1">weight initialization:</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-2 start"><li class="c6 c17"><span class="c1">lazy version: 0.02 * randn(num_params)</span></li><li class="c6 c17"><span class="c1">if that doesn&#39;t work: init_scale / sqrt(layer_width) * randn</span></li><li class="c6 c17"><span class="c1">weight initialization is apparently the first thing to look at funny when things don&#39;t work well. just use sane defaults for learning rates and such, but weight init makes the biggest difference.</span></li><li class="c6 c17"><span class="c1">see also &quot;architecture/weights&quot; or whatever I end up calling it. a bunch of stuff there to either make the weight init problem not exist in the first place or to solve it well. there might also be stuff in &quot;recurrence&quot; and &quot;architecture&quot;; in particular, residual nets work reliably with much simpler init.</span></li><li class="c6 c17"><span class="c9 c1">specific thing for rnns/lstms:</span><span class="c9 c1 c21">&nbsp;</span><span class="c1 c21">&quot;[if] you want to train them on problems with very long range dependencies, you should initialize the biases of the forget gates of the LSTMs to large values. By default, the forget gates are the sigmoids of their total input, and when the weights are small, the forget gate is set to 0.5, which is adequate for some but not all problems. This is the one non-obvious caveat about the initialization of the LSTM.&quot;</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-1"><li class="c6 c22"><span class="c1 c21">&quot;If you are training RNNs or LSTMs, use a hard constraint over the norm of the gradient (remember that the gradient has been divided by batch size). Something like 15 or 5 works well in practice in my own experiments. Take your gradient, divide it by the size of the minibatch, and check if its norm exceeds 15 (or 5). If it does, then shrink it until it is 15 (or 5)&quot;.</span></li><li class="c6 c22"><span class="c1">do gradient checking if you implement your own gradients. see the cs231n page in &quot;tricks&quot; on how to do this.</span></li><li class="c6 c22"><span class="c1 c21">&quot;Data augmentation: be creative, and find ways to algorithmically increase the number of training cases that are in your disposal.&quot; .. &quot;Data augmentation is an art (unless you&rsquo;re dealing with images). Use common sense.&quot;</span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-2 start"><li class="c6 c17"><span class="c1 c21">&quot;If you have images, then you should translate and rotate them</span></li><li class="c6 c17"><span class="c1 c21">&quot;if you have speech, you should combine clean speech with all types of random noise, etc.&quot;</span></li><li class="c6 c17"><span class="c1">note: effectively, this is one of the big places you encode priors about what you want out of the data, by trying to add noise to anything you don&#39;t want or don&#39;t know. don&#39;t care about magnitude? multiply by random scalars! (or actually, for that one, normalize.) not sure how precise your samples are? the noise you add (naively) encodes your priors about </span></li></ul><ul class="c2 lst-kix_rcp8sez4svjd-1"><li class="c6 c22"><span class="c1">use dropout. ilya says &quot;don&#39;t forget to shut it off&quot;, I say &quot;don&#39;t forget to (keep it on and) sample it multiple times&quot; - see &quot;mc dropout&quot; in the bayesian neural networks section. he also mentions you need to tune the dropout rate&hellip; but doesn&#39;t say how to do that. apparently also dropout nets don&#39;t need early stopping, so don&#39;t worry about the validation error increasing.</span></li><li class="c6 c22"><span class="c1">he then recommends ensembling. sure, whatever, do that, fine. I say just use dropout harder and make the net proportionally wider. tune the number of bits representable by active units to be small enough to match the number of examples those units will see, then average over 1/(1-dropout_prob) runs of the net. you can also get a nice confidence interval this way.</span></li></ul><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.html&amp;sa=D&amp;ust=1465142038536000&amp;usg=AFQjCNFkHw_AvWlmlmI4HG4ssgMcpn97Bw">http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.html</a></span><span class="c1">&nbsp;- [skimmed] activitynet, a new dataset for activity recognition. is it open access? should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.03365&amp;sa=D&amp;ust=1465142038537000&amp;usg=AFQjCNGvAGzNIzjyNwKdrt3E4r240yLh7w">http://arxiv.org/abs/1506.03365</a></span><span class="c1">&nbsp;[skimmed]</span><span class="c1">&nbsp;new images dataset (locations, looks like?) that uses unsupervised modeling of the image, clustering of the representation, and then asks for the label from a human. looks interesting, but the paper is of very low quality. should go in &quot;data collection&quot; and &quot;datasets&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www5.in.tum.de/persons/august/guided_research.pdf&amp;sa=D&amp;ust=1465142038538000&amp;usg=AFQjCNHoPXVsRbFDxtTFjbiHBcbngVziPg">http://www5.in.tum.de/persons/august/guided_research.pdf</a></span><span class="c1">&nbsp;[skimmed] comparison of dropout/lwta/maxout/etc. from 2013. should go in &quot;activation&quot; and &quot;regularization&quot; and &quot;neuromorphic&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1409.2752.pdf&amp;sa=D&amp;ust=1465142038538000&amp;usg=AFQjCNEsChD64M6FVuiY2F31JZbLPdUKAg">http://arxiv.org/pdf/1409.2752.pdf</a></span><span class="c1">&nbsp;[skimmed] some old crap thing about lwta autoencoders. looks like a cool idea, but they only gave it one layer of decoder!? or something. anyway, it might work really well. should go in &quot;unsupervised/autoencoders&quot;, &quot;sparsity&quot;. (should sparsity go in &quot;regularization&quot;?)</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://people.idsia.ch/~juergen/nips2013.pdf&amp;sa=D&amp;ust=1465142038539000&amp;usg=AFQjCNHxEtgwyxvZt8yACoR9qUCL9yCLjg">http://people.idsia.ch/~juergen/nips2013.pdf</a></span><span class="c1">&nbsp;original local winner takes all paper. should go in &quot;sparsity&quot;, &quot;regularization&quot;. a bit old. came after maxpooling, that&#39;s interesting. note: is almost identical to maxpooling, except that you also select a different weight set depending on which neuron won. that part is interesting. looks like it&#39;s useful for multimodal stuff, which means assumptions I had about using it for heavily multimodal networks do in fact hold up.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.ncbi.nlm.nih.gov/pubmed/25986752&amp;sa=D&amp;ust=1465142038540000&amp;usg=AFQjCNFjhnfWYvfif2IEN-_rQ9GT_VfWiw">http://www.ncbi.nlm.nih.gov/pubmed/25986752</a></span><span class="c1">&nbsp;neuroscience inspiration for neural programmer-interpreter thing from deepmind. note: deepmind&#39;s thing is pretty seriously stupid. it&#39;s discrete. why. anyway, should go in &quot;neuromorphic&quot; and &quot;memory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06455&amp;sa=D&amp;ust=1465142038541000&amp;usg=AFQjCNHL_c8-QK1UgtSqZWYLKUiecbvAXg">http://arxiv.org/abs/1511.06455</a></span><span class="c1">&nbsp;[abs][conf] deep unsupervised learning (VAEs?), deep gaussian optimization, by making gaussian processes deep. sounds cool. should go in &quot;bayesian neural networks&quot;, &quot;unsupervised&quot;, &quot;gaussian processes&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.03009&amp;sa=D&amp;ust=1465142038542000&amp;usg=AFQjCNEHlZYe6Sfpr3qG9Xju5iVYlesmlQ">http://arxiv.org/abs/1510.03009</a></span><span class="c1">&nbsp;[abs][conf] older binary-stuff training approach - sign changes instead of multiplication, bit shifts instead of some other thing. should go in &quot;binary&quot;/&quot;spiking&quot;, &quot;hardware&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06067&amp;sa=D&amp;ust=1465142038543000&amp;usg=AFQjCNFh7tNCYcHdxfgXUOJSnlgnT7wz0Q">http://arxiv.org/abs/1511.06067</a></span><span class="c1">&nbsp;[abs][conf] low-rank tensor decomposition - train-time model compression that actually makes a difference. should go in &quot;model compression&quot;, &quot;performance&quot;, &quot;weights&quot;. </span><span class="c1 c9">this looks like a big deal.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03643&amp;sa=D&amp;ust=1465142038544000&amp;usg=AFQjCNEoroGR9yC5DtH2FhMwOw3RFA_8cA">http://arxiv.org/abs/1511.03643</a></span><span class="c1">&nbsp;[abs][conf] generalization of hinton&#39;s model distillation paper. sounds vaguely interesting. should go in &quot;transfer learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05879&amp;sa=D&amp;ust=1465142038545000&amp;usg=AFQjCNE2P-nUPb9Gkrz88P7N1Kj9FoLzGw">http://arxiv.org/abs/1511.05879</a></span><span class="c1">&nbsp;[abs][conf] claims, but I can&#39;t evaluate the claim, that they significantly improve SOTA on object recognition by mixing in ideas from traditional object search. should go in &quot;images/detection&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.05011&amp;sa=D&amp;ust=1465142038546000&amp;usg=AFQjCNGhcRVPFV71dFZbhI3lU5Sup86Anw">http://arxiv.org/abs/1506.05011</a></span><span class="c1">&nbsp;[abs][conf] use of bayesian models to manage crowdsourcing. should go in &quot;data collection&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06390&amp;sa=D&amp;ust=1465142038547000&amp;usg=AFQjCNGejXwHaT2pH1PR1YgvlxABxOGHmw">http://arxiv.org/abs/1511.06390</a></span><span class="c1">&nbsp;[abs][conf] adverserial supervised learning with the output as the target and the adversery as the input. looks cool. interesting how they flip it around. should go in &quot;regularization&quot;, &quot;adverserial examples problem&quot;, &quot;generative&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02301&amp;sa=D&amp;ust=1465142038548000&amp;usg=AFQjCNHNnyuprzNM_Lcrnpn6raKRzIbEIQ">http://arxiv.org/abs/1511.02301</a></span><span class="c1">&nbsp;[abs][conf] dataset of children&#39;s books and analysis of language models using it. I WANT THIS SHIT!!!! this is a critical step in the curriculum learning process that allows humans to understand meaning! they also maybe introduce an objective that improves focus on content words. as expected, non-content words are too easy, etc etc. should go in &quot;language/language models&quot;, &quot;datasets&quot; (is already there).</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05176&amp;sa=D&amp;ust=1465142038549000&amp;usg=AFQjCNH9OBAiJLfzpBPRgQcbYu1AxRhZsQ">http://arxiv.org/abs/1511.05176</a></span><span class="c1">&nbsp;[abs][conf] not really sure what is useful about this yet. graphical models, stochastic networks. &quot;muprop&quot;. they train stochastic networks. ???</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04581&amp;sa=D&amp;ust=1465142038550000&amp;usg=AFQjCNFMRGrrgyRZoOiNzpfk2-VIhCgp9g">http://arxiv.org/abs/1511.04581</a></span><span class="c1">&nbsp;[abs][conf] model of model performance for generative networks. should go in &quot;generative&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06530&amp;sa=D&amp;ust=1465142038551000&amp;usg=AFQjCNEOiIEWozEX9rhE9fp_x5dhhJgvPg">http://arxiv.org/abs/1511.06530</a></span><span class="c1">&nbsp;[abs][conf] one of the core model compression papers. &quot;Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications&quot;. uses a multi-step thing with low-rank tensor decomposition and then fine tuning. should go in &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06939&amp;sa=D&amp;ust=1465142038552000&amp;usg=AFQjCNGb3mLJgSlOlRWVVNsQZTiIPo_kgg">http://arxiv.org/abs/1511.06939</a></span><span class="c1">&nbsp;[abs][conf] rnns as recommenders. note: this looks very similar to my idea of using rnns as online learning. should go in &quot;online learning&quot;, &quot;recurrence&quot;, &quot;applications/recommendation&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06499&amp;sa=D&amp;ust=1465142038552000&amp;usg=AFQjCNEVfTveX_NZFGx3vbLCcejJjoXBBw">http://arxiv.org/abs/1511.06499</a></span><span class="c1">&nbsp;[abs][conf] &quot;variational gaussian processes&quot;. should go in &quot;gaussian processes&quot;, &quot;bayesian neural networks&quot;. not exactly sure what they did, but they get much better results out of DRAW when they swap some part of it for this.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06426&amp;sa=D&amp;ust=1465142038553000&amp;usg=AFQjCNHUMprlqTdS3fv1LW51GTeBmQIxNQ">http://arxiv.org/abs/1511.06426</a></span><span class="c1">&nbsp;[abs][conf] tensor product representation modeling of the bAbI problem, performs very well, and they analyze the dataset to find ways to make a harder one. should go in &quot;language/question answering&quot; I guess, or maybe &quot;symbolic tasks&quot;?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05042&amp;sa=D&amp;ust=1465142038554000&amp;usg=AFQjCNG_HZLQV39Vsx2Q3az3k6c25ot1Dw">http://arxiv.org/abs/1511.05042</a></span><span class="c1">&nbsp;[abs][conf] softmax is not theoretically justified (!) and this compares it to an alternative (that is also not theoretically justified, but has better bounds?) called spherical loss. should go in &quot;loss functions&quot; or &quot;activation functions&quot; or if it exists, &quot;output functions&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04773&amp;sa=D&amp;ust=1465142038555000&amp;usg=AFQjCNFD4nKsrQd_XG1h2cbEOyA9pP0EoA">http://arxiv.org/abs/1511.04773</a></span><span class="c1">&nbsp;[abs][conf] some speech thing, kcca. should go in &quot;speech&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04707&amp;sa=D&amp;ust=1465142038556000&amp;usg=AFQjCNGvngZhYi8MUC7Xudr4FMlG78KXHA">http://arxiv.org/abs/1511.04707</a></span><span class="c1">&nbsp;[abs][conf] alternative to softmax (I think?) as the output for a classification: linear discriminant analysis as the output. should go in &quot;activation functions&quot; or &quot;loss functions&quot; or whatever. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06018&amp;sa=D&amp;ust=1465142038557000&amp;usg=AFQjCNGA0k1T7pwYLYE5g0Qf0TZuO98_TQ">http://arxiv.org/abs/1511.06018</a></span><span class="c1">&nbsp;[abs][conf] segment-rnns: embed a segment with a birnn, then use that embedding as the input to an rnn that outputs segment labels. alternative to CTC, apparently works much better. should go in &quot;speech&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05622&amp;sa=D&amp;ust=1465142038558000&amp;usg=AFQjCNGKat3_Ukpc7ZKesbGlfCMybFZ0nQ">http://arxiv.org/abs/1511.05622</a></span><span class="c1">&nbsp;[abs][conf] weird, can&#39;t evaluate from the abstract, deletion-summarized abstract: stochastic binary variables in neural networks. can predict more than expected value of output Y given input X. predicts a distribution of outputs Y which is useful when an average is not necessarily a valid answer. particularly relevant to inverse problems. traditional sigmoid belief networks are hard to train. new family of networks called linearizing belief nets or LBNs: decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. the linear pathways efficiently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efficiently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset. should go in &quot;generative&quot;, &quot;bayesian&quot;, &quot;stochastic&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04561&amp;sa=D&amp;ust=1465142038559000&amp;usg=AFQjCNECOgAZ7K0N0a3yj4tYCBzklj2bRQ">http://arxiv.org/abs/1511.04561</a></span><span class="c1">&nbsp;[abs][conf] 8 bit compressed representations of the relevant data when doing parallel SGD. should go in &quot;parallelization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07069&amp;sa=D&amp;ust=1465142038560000&amp;usg=AFQjCNFiaGeDlsCw8CVEi_Vr1opgvcTBuA">http://arxiv.org/abs/1511.07069</a></span><span class="c1">&nbsp;[abs][conf] regularization for cnns where the labels are noisy. uses some sort of funky regularizer to do some sort of mutual context thing or something. should go in &quot;regularization&quot;, &quot;semi-supervised&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05493&amp;sa=D&amp;ust=1465142038561000&amp;usg=AFQjCNHEMCfu5gD5nI9BZkfMg53uqij3Iw">http://arxiv.org/abs/1511.05493</a></span><span class="c1">&nbsp;[abs][conf] graph-structured inputs for recurrent things. should go in &quot;recurrence&quot;. they test on babi.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.7676&amp;sa=D&amp;ust=1465142038562000&amp;usg=AFQjCNEub_Z7W3fPoA3OUBNCGCoU3Cg-DA">http://arxiv.org/abs/1411.7676</a></span><span class="c1">&nbsp;[abs][conf] analysis of the minimum sufficient representation for some things. finds interesting things about why to use pooling/clamping/etc. should go in &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.08130v2&amp;sa=D&amp;ust=1465142038562000&amp;usg=AFQjCNFM8F46EUAwvSh8hzpsXkOgr9JFZw">http://arxiv.org/abs/1511.08130v2</a></span><span class="c1">&nbsp;[abs][sanity] &quot;A Roadmap towards Machine Intelligence&quot; by tomas mikolov. &nbsp;should go in &quot;safety&quot;, &quot;language&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1406.5298&amp;sa=D&amp;ust=1465142038563000&amp;usg=AFQjCNHjp5O_8_SO361zZUKjaSonizn5kQ">http://arxiv.org/abs/1406.5298</a></span><span class="c1">&nbsp;[abs] &quot;semi-supervised learning with deep generative models&quot;. should go in &quot;generative&quot;, &quot;semi-supervised&quot;. a bit old.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.lab41.org/one-more-reason-not-to-be-scared-of-deep-learning/&amp;sa=D&amp;ust=1465142038564000&amp;usg=AFQjCNH79YgBHSsRtYfsnDtrAlEz4FWAoA">http://www.lab41.org/one-more-reason-not-to-be-scared-of-deep-learning/</a></span><span class="c1">&nbsp;claim that it doesn&#39;t need as much data as you think. probably the modern regularization techniques are to blame for this. should go in &quot;basics&quot; or something.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/48a1zk/_/&amp;sa=D&amp;ust=1465142038565000&amp;usg=AFQjCNEr1sov2B_7AGjwiwUjFrNtGmP4TA">https://www.reddit.com/r/MachineLearning/comments/48a1zk/_/</a></span><span class="c1">&nbsp;why they use the frequency formats they do for speech. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://academic.research.microsoft.com/&amp;sa=D&amp;ust=1465142038566000&amp;usg=AFQjCNFt7oBurMdXNOrA8ImERSoLBImi3A">http://academic.research.microsoft.com/</a></span><span class="c1">&nbsp;microsoft academic search engine. should go in &quot;utilities&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://medium.com/innoarchitech-innovation-architecture-technology/machine-learning-an-in-depth-non-technical-guide-part-3-b7a5d3911d3b%23.cnbp6mxyx&amp;sa=D&amp;ust=1465142038567000&amp;usg=AFQjCNGuySJgglLteHLcLH9nO9JS30JBhQ">https://medium.com/innoarchitech-innovation-architecture-technology/machine-learning-an-in-depth-non-technical-guide-part-3-b7a5d3911d3b#.cnbp6mxyx</a></span><span class="c1">&nbsp;an overview of deep learning, without the math. should go in &quot;basics&quot; I guess?? read something else first.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08194&amp;sa=D&amp;ust=1465142038567000&amp;usg=AFQjCNF_S5FKcV4OTHro_lEI-iZUx-I_rA">http://arxiv.org/abs/1602.08194</a></span><span class="c1">&nbsp;[abs] some sort of sub-sampling thing. &quot;Scalable and Sustainable Deep Learning via Randomized Hashing&quot;. should go in &quot;parallelization&quot;, &quot;performance&quot;, and &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08210&amp;sa=D&amp;ust=1465142038568000&amp;usg=AFQjCNFa7Ed85nijWjZts-4Mk2lntrO1yw">http://arxiv.org/abs/1602.08210</a></span><span class="c1">&nbsp;[abs] recurrence depth dimensions analysis - forward, temporal, skip. finds that skip connections are good. nobody is surprised. in the distance, jurgen schmidhuber is crying. should go in &quot;recurrence&quot;. &quot;Architectural Complexity Measures of Recurrent Neural Networks&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.pnas.org/content/early/2016/02/09/1513198113.full.pdf&amp;sa=D&amp;ust=1465142038569000&amp;usg=AFQjCNEVN_rFFgC-cRJmAxFNkTFrhSjGSw">http://www.pnas.org/content/early/2016/02/09/1513198113.full.pdf</a></span><span class="c1">&nbsp;[abs] analysis of the minimum recognizeable images for humans. should go in &quot;images&quot; and &quot;neuromorphic&quot; I guess.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08952&amp;sa=D&amp;ust=1465142038570000&amp;usg=AFQjCNG1pS-lBLHXJOlZHivGhxetkpRndA">http://arxiv.org/abs/1602.08952</a></span><span class="c1">&nbsp;[abs] analysis of rnns based on what they learn when used as image captioning models. should go in &quot;debugging&quot;, &quot;recurrence&quot;. also interesting because it brings up the &quot;don&#39;t care about syntax pls&quot; thing. once again, adverserial networks for text and/or prioritizing based on competition are both interesting.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_main.pdf&amp;sa=D&amp;ust=1465142038570000&amp;usg=AFQjCNGDwBELGPCFUzo6a2f_Lzt1J6i1cg">http://research.microsoft.com/pubs/255952/ICCV15_DeepNDF_main.pdf</a></span><span class="c1">&nbsp;[abs] decision forests as the output classifier in an NN. they&#39;re apparently much better at classification given a vector, so they may kick this out of the park. not as flexible, though. should go in &quot;classification&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://cs231n.stanford.edu/slides/winter1516_lecture12.pdf&amp;sa=D&amp;ust=1465142038571000&amp;usg=AFQjCNFbe_FFCrExPV_QR-AWf04Zti69xw">http://cs231n.stanford.edu/slides/winter1516_lecture12.pdf</a></span><span class="c1">&nbsp;[fast skim] lecture on which libraries are which. should go in &quot;basics/libraries&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1">ftp://</span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://ftp.idsia.ch/pub/juergen/icml2006.pdf&amp;sa=D&amp;ust=1465142038572000&amp;usg=AFQjCNEDtswAZK7iAg1Qds4hjKHFWX4Jxw">ftp.idsia.ch/pub/juergen/icml2006.pdf</a></span><span class="c1">&nbsp;[abs] original CTC paper. should go in &quot;recurrence&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3D4h0uC9FPVMQ&amp;sa=D&amp;ust=1465142038573000&amp;usg=AFQjCNFBoDHpX8o60aH0zbbQEaokzoSLfg">https://www.youtube.com/watch?v=4h0uC9FPVMQ</a></span><span class="c1">&nbsp;[title] two minute papers on getting started with machine learning. should go in &quot;basics&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://esc.fnwi.uva.nl/thesis/centraal/files/f1497350858.pdf&amp;sa=D&amp;ust=1465142038574000&amp;usg=AFQjCNFqLT4fvdQtr7hfqJ77RGBjabgv6A">https://esc.fnwi.uva.nl/thesis/centraal/files/f1497350858.pdf</a></span><span class="c1">&nbsp;[skim] mess of a thesis - introduces neural networks from 2011, even though it knows about things like deepdream; misunderstands newer neural networks as being autoencoders (uh, lol, no, deepdream was originally done on feedforward nets). </span><span class="c9 c1">the main topic is convolution over time - goes over how to structure a convolutional network to do convolution over time. they don&#39;t use pooling, but this will be very helpful when I want to do it. </span><span class="c1">they get bad results, but I think that&#39;s because they&#39;re idiots and are using vanilla autoencoders - what are you even doing seriously? don&#39;t make it predict further in the future with the same weights. should go in &quot;convolution&quot;, &quot;unsupervised&quot;, &quot;architectures/timeseries&quot;/&quot;recurrence&quot;. they also use it for action selection in RL, so should also go in &quot;RL&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1207.0580.pdf&amp;sa=D&amp;ust=1465142038575000&amp;usg=AFQjCNGqtuflEuk8SZWoNyTMAsdOxrAUcw">http://arxiv.org/pdf/1207.0580.pdf</a></span><span class="c1">&nbsp;[read] original dropout paper. should go in &quot;regularization&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.03229&amp;sa=D&amp;ust=1465142038576000&amp;usg=AFQjCNFquyjNQNZq-vPAMSnFeE2Rm8UyBA">http://arxiv.org/abs/1505.03229</a></span><span class="c1">&nbsp;[abs] suspicious looking paper about &quot;augmented pattern classification&quot; - something about how to do data augmentation. should go in &quot;regularization&quot; or &quot;data augmentation&quot; I guess. gives a best-ever result on mnist, if the authors are to be believed.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.nervanasys.com/winograd/&amp;sa=D&amp;ust=1465142038577000&amp;usg=AFQjCNGLcvV6dV2qzn66_dIKaZyP10Glkg">http://www.nervanasys.com/winograd/</a></span><span class="c1">&nbsp;[skim] apparently &quot;winograd&quot; is much faster than fft for convolution if the convolutions are 3x3. should go in &quot;convolution&quot;, &quot;speed&quot;. paper: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.09308&amp;sa=D&amp;ust=1465142038578000&amp;usg=AFQjCNG6u_XBgD-3DhGmvdRiwtNgg3ydmw">http://arxiv.org/abs/1509.09308</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://lmb.informatik.uni-freiburg.de/people/dosovits/code.html&amp;sa=D&amp;ust=1465142038578000&amp;usg=AFQjCNG7J3bafRAoCYfubBgCHujC-seSjg">http://lmb.informatik.uni-freiburg.de/people/dosovits/code.html</a></span><span class="c1">&nbsp;[title] pretrained models for some papers about inverting convolution. the papers are </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://lmb.informatik.uni-freiburg.de/Publications/2014/DB14a/&amp;sa=D&amp;ust=1465142038579000&amp;usg=AFQjCNHYuf7spD4lsbjeN5Z4OURTCZxq_Q">http://lmb.informatik.uni-freiburg.de/Publications/2014/DB14a/</a></span><span class="c1">&nbsp;and arXiv:1506.02753</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/&amp;sa=D&amp;ust=1465142038580000&amp;usg=AFQjCNFtdi9p_NVejdjGBMAwMuXQfuIr1g">http://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/</a></span><span class="c1">&nbsp;and </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.06852&amp;sa=D&amp;ust=1465142038580000&amp;usg=AFQjCNGVa6iiEBC6IUZly2kJDSIUZkzM0g">http://arxiv.org/abs/1504.06852</a></span><span class="c1">&nbsp;[recommended papers list] FlowNet: Learning Optical Flow with Convolutional Networks - another optical flow thing. should go in &quot;images/video&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://people.idsia.ch/~ciresan/data/cvpr2012.pdf&amp;sa=D&amp;ust=1465142038581000&amp;usg=AFQjCNHmQ3_M7MSlU9bLv_CHSEfsfXAQ1w">http://people.idsia.ch/~ciresan/data/cvpr2012.pdf</a></span><span class="c1">&nbsp;[skim] larger-scale winner takes all, with a bunch of subnetworks, across which winner takes all is run. sounds awesome, exactly the sort of thing I&#39;d want to do if I were building a brain and get to spend the energy to run the entire thing every timestep if I want. should go in &quot;model sub-selection&quot; and &quot;WTA&quot;. a bit old, though. the specific purpose is probably beaten.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01431&amp;sa=D&amp;ust=1465142038582000&amp;usg=AFQjCNFEkeCbezJhekD2TMde0vN1GAaJ6w">http://arxiv.org/abs/1603.01431</a></span><span class="c1">&nbsp;[abs+reddit] alternative to batch normalization that takes advantage of the gaussian distribution of activations pre-relu (wat!? interesting&hellip;) and is apparently much more complex but faster to calculate. reddit thread: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/49cvr8/_/&amp;sa=D&amp;ust=1465142038582000&amp;usg=AFQjCNHX633zH-06__Rwn1o8uKd5rlZZSg">https://www.reddit.com/r/MachineLearning/comments/49cvr8/_/</a></span><span class="c1">&nbsp;should go in &quot;regularization&quot;, &quot;speed&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.7580&amp;sa=D&amp;ust=1465142038583000&amp;usg=AFQjCNFtAdOWnNmPb8U8xUhAH45xSdhzvw">http://arxiv.org/abs/1412.7580</a></span><span class="c1">&nbsp;[abs] analysis of the performance of fft convolutions, they introduce a faster one, which became fbfft and cudnn v4. should go in &quot;speed&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01417&amp;sa=D&amp;ust=1465142038584000&amp;usg=AFQjCNEJkqiMvRnaQFKA2fDqW_zO4ZVd3g">http://arxiv.org/abs/1603.01417</a></span><span class="c1">&nbsp;[abs] incremental improvement of question answering attentional thingies. should go in &quot;language/qa&quot;, &quot;recurrence&quot;, &quot;attention&quot;, &quot;memory&quot;. hn thread: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://news.ycombinator.com/item?id%3D11237125&amp;sa=D&amp;ust=1465142038585000&amp;usg=AFQjCNGbXNF4J7Ix5DPj1mMv79rdF5RSmA">https://news.ycombinator.com/item?id=11237125</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00391&amp;sa=D&amp;ust=1465142038585000&amp;usg=AFQjCNGZVAGrTNAhGoeNiTnSU3GzE5SR_w">http://arxiv.org/abs/1603.00391</a></span><span class="c1">&nbsp;[abs] activation functions that are noisy in zero-gradient areas. cool. mentions that it works best when training seems to be the most difficult/curriculum learning is needed. that sounds cooler. should go in &quot;activation&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/deepmind/rc-data/&amp;sa=D&amp;ust=1465142038586000&amp;usg=AFQjCNHnG9CGU0OmpdS5OBTrYTAyCsb9bw">https://github.com/deepmind/rc-data/</a></span><span class="c1">&nbsp;deepmind reading comprehension dataset. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.03340&amp;sa=D&amp;ust=1465142038587000&amp;usg=AFQjCNE_H_XxUOO59RBOMvfJNf_T3XgJCw">http://arxiv.org/abs/1506.03340</a></span><span class="c1">&nbsp;[abs] paper and basic method to go with deepmind reading comprehension dataset. should go in &quot;datasets&quot; and &quot;language/qa&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.lab41.org/anything2vec/&amp;sa=D&amp;ust=1465142038588000&amp;usg=AFQjCNEnP8zlXJ0_vKzHsI03OEYeRmPu9A">http://www.lab41.org/anything2vec/</a></span><span class="c1">&nbsp;[skim] overview of embeddings. should go in &quot;language/embeddings&quot; I guess. maybe &quot;basics&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.07680&amp;sa=D&amp;ust=1465142038588000&amp;usg=AFQjCNEQkKyHlDoj69_ar-JHmovOZZhvfQ">http://arxiv.org/abs/1507.07680</a></span><span class="c1">&nbsp;[abs] gradient estimation for RNNs without going backwards in time or storing activation history. should go in &quot;training algorithms&quot;, &quot;speed&quot;, &quot;recurrence&quot;. they name it &quot;NoBackTrack&quot;. notes from that one guy: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.xlgps.com/article/189052.html&amp;sa=D&amp;ust=1465142038589000&amp;usg=AFQjCNHhnjWpVYrO-oxscqll8eNlpzGxLA">http://www.xlgps.com/article/189052.html</a></span><span class="c1">&nbsp;- he doesn&#39;t think it&#39;s ready to use, but he thinks it&#39;s interesting.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00954&amp;sa=D&amp;ust=1465142038590000&amp;usg=AFQjCNHpdGoXnEv5QNWRwhLnTALsKPRUMQ">http://arxiv.org/abs/1603.00954</a></span><span class="c1">&nbsp;[abs] theoretically modeled approach for training RNNs, uses tensor decomposition stuff and has learning guarantees (whee). should go in &quot;recurrence&quot;, &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08671&amp;sa=D&amp;ust=1465142038590000&amp;usg=AFQjCNHUuOCkK7BvVTGUD36nKCphXe6ZQA">http://arxiv.org/abs/1602.08671</a></span><span class="c1">&nbsp;[abs] new version of the NTM with what looks like a continuous space for memory. This is what we need! should go in &quot;memory&quot;. video of training read/write patterns: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://fat.gfycat.com/MedicalKeyGalapagostortoise.webm&amp;sa=D&amp;ust=1465142038591000&amp;usg=AFQjCNFchhwEk1htvMoZilRLuUiuhKfj_Q">https://fat.gfycat.com/MedicalKeyGalapagostortoise.webm</a></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/awentzonline/image-analogies&amp;sa=D&amp;ust=1465142038592000&amp;usg=AFQjCNFd_ZrABoiWzVKa3UNTZMsDMb3Vtw">https://github.com/awentzonline/image-analogies</a></span><span class="c1">&nbsp;implementation of neural analogies thing. amusing. uses a pretrained model from classification. should go in &quot;transfer learning&quot;, &quot;basics/examples&quot;. see also semantic style transfer paper.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01768&amp;sa=D&amp;ust=1465142038593000&amp;usg=AFQjCNETJOAo4sJi0t_5qNtjX_5oOGQYfw">http://arxiv.org/abs/1603.01768</a></span><span class="c1">&nbsp;[abs][reddit] &quot;Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork&quot;. augmented generative thing that uses per pixel class annotations to allow using them to paint. should go in &quot;generative&quot;, &quot;applications/art&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03379&amp;sa=D&amp;ust=1465142038593000&amp;usg=AFQjCNE2wSrQGqp1CNP2shPC8yUjtXdJZw">http://arxiv.org/abs/1602.03379</a></span><span class="c1">&nbsp;[abs] spiking nn thing. should go in &quot;neuromorphic&quot; or &quot;neuroscience&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03264&amp;sa=D&amp;ust=1465142038594000&amp;usg=AFQjCNHcVvkLxIvtw0qiOsv4v5WHavM5_Q">http://arxiv.org/abs/1602.03264</a></span><span class="c1">&nbsp;[abs] some sort of more traditional generative interpretation of convnets, interpreting them as a &quot;generative random field model&quot;. should go in &quot;generative&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.01911&amp;sa=D&amp;ust=1465142038595000&amp;usg=AFQjCNEvCBYsdwu2u1-3g8k5B-sxUP58dg">http://arxiv.org/abs/1506.01911</a></span><span class="c1">&nbsp;[abs] temporal convolutions and recurrence for gesture recognition. don&#39;t use temporal pooling, they say. should go in &quot;images/video&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.00852&amp;sa=D&amp;ust=1465142038596000&amp;usg=AFQjCNHaltt0Me1e1oIwy1mX9IlTenM7iQ">http://arxiv.org/abs/1506.00852</a></span><span class="c1">&nbsp;[abs] misc interesting paper: student peer review doesn&#39;t have exploitable structure to find better approximations of target review values. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04210&amp;sa=D&amp;ust=1465142038597000&amp;usg=AFQjCNFZipG2_fVbRVAAAKEribIbKuIsKA">http://arxiv.org/abs/1511.04210</a></span><span class="c1">&nbsp;[abs] theoretical analysis of neural network loss structure, &quot;On the Quality of the Initial Basin in Overspecified Neural Networks&quot;. finds that you really need that overspecification or you&#39;re going nowhere. should go in &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03506&amp;sa=D&amp;ust=1465142038597000&amp;usg=AFQjCNE3NJnAJ5kRAZLS1SsPMtqwQyd2SQ">http://arxiv.org/abs/1602.03506</a></span><span class="c1">&nbsp;[abs] safety paper from russel, fhi, tegmark. should go in &quot;safety&quot;. is about projects on safety.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03426&amp;sa=D&amp;ust=1465142038599000&amp;usg=AFQjCNEZooAoaQn_b9GYbPTtFy6kq8zP0g">http://arxiv.org/abs/1602.03426</a></span><span class="c1">&nbsp;[abs] overview of sarcasm detection (which is an easy problem). should go in &quot;language&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03483&amp;sa=D&amp;ust=1465142038599000&amp;usg=AFQjCNEjWWSb29EnvAR9Hx7OALTW5tkrog">http://arxiv.org/abs/1602.03483</a></span><span class="c1">&nbsp;[abs] overview of representation learning of sentences. doesn&#39;t seem to find anything interesting. should go in &quot;representation learning&quot; and &quot;language/language models&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03346&amp;sa=D&amp;ust=1465142038600000&amp;usg=AFQjCNHyphaoPdPPTLmDu3ZgFcYQ3t337g">http://arxiv.org/abs/1602.03346</a></span><span class="c1">&nbsp;[abs] joint spatial and temporal localization and classification of actions in video. should go in &quot;images/video&quot;. also contributes a massive new dataset for the purpose, &quot;NASA&quot;, should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03409&amp;sa=D&amp;ust=1465142038601000&amp;usg=AFQjCNEvwgJJ7Np7dRCEigTeI09a2Gh2MA">http://arxiv.org/abs/1602.03409</a></span><span class="c1">&nbsp;[abs] more work on transfer learning for medical imaging, because it&#39;s really hard to get labeled datasets remotely close to the size of imagenet. should go in &quot;bio&quot;, &quot;transfer learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03418&amp;sa=D&amp;ust=1465142038602000&amp;usg=AFQjCNF330eKB_69XvR7gqm_ytNwg2l1fg">http://arxiv.org/abs/1602.03418</a></span><span class="c1">&nbsp;[abs] triplet face identification thing. should go in &quot;images/faces&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02822&amp;sa=D&amp;ust=1465142038603000&amp;usg=AFQjCNGtiJcG-LJosIdd-B8fxE80x9C4kQ">http://arxiv.org/abs/1602.02822</a></span><span class="c1">&nbsp;[confused abs] &quot;Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes On Second Order Statistics&quot; should go in &quot;sparsity&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02865&amp;sa=D&amp;ust=1465142038604000&amp;usg=AFQjCNG1eiG0-y1FxsVN1K3inbvqjP083A">http://arxiv.org/abs/1602.02865</a></span><span class="c1">&nbsp;[abs] dramatic improvement in generalization to new domains of convnets by evaluating how &quot;typical&quot; images are in the training set, and prioritizing based on that. should go in &quot;regularization&quot;, &quot;images&quot;, &quot;prioritization&quot;, &quot;generality&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03012&amp;sa=D&amp;ust=1465142038605000&amp;usg=AFQjCNGh3r4DoIYxDEWVmd2NgepKqcIMJQ">http://arxiv.org/abs/1602.03012</a></span><span class="c1">&nbsp;[abs] recognition of actions in surgery workflows. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02823&amp;sa=D&amp;ust=1465142038606000&amp;usg=AFQjCNFCmBWxl454DmrQQakCyiZeTVeBmQ">http://arxiv.org/abs/1602.02823</a></span><span class="c1">&nbsp;[abs] analysis of why you need nesterov momentum - primarily because of bad starting points. should go in &quot;initialization&quot;, &quot;optimizers&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03001&amp;sa=D&amp;ust=1465142038606000&amp;usg=AFQjCNFMkLsOE8gkZGzJq8ZBNu6hinjlIA">http://arxiv.org/abs/1602.03001</a></span><span class="c1">&nbsp;[abs] </span><span class="c9 c1 c32">COOL!!! </span><span class="c1">application of summarization to code - predict the method name from the code. doesn&#39;t work amazingly, but works pretty damn well. should go in &quot;applications&quot;, maybe &quot;language/summarization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.03488&amp;sa=D&amp;ust=1465142038607000&amp;usg=AFQjCNGWhJTrn4jUKyTWwSlvJcNLS7R65A">http://arxiv.org/abs/1503.03488</a></span><span class="c1">&nbsp;replaced by </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03822&amp;sa=D&amp;ust=1465142038608000&amp;usg=AFQjCNG3YZJJ6wwOzMzQee-DS8bVYPmYJQ">http://arxiv.org/abs/1602.03822</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.4178&amp;sa=D&amp;ust=1465142038609000&amp;usg=AFQjCNEnaDe2ZW_qqP3izc14SwEGJtKWNQ">http://arxiv.org/abs/1412.4178</a></span><span class="c1">&nbsp;replaced by </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03822&amp;sa=D&amp;ust=1465142038609000&amp;usg=AFQjCNHOA4oDzGGkiRxJukzKdkQX-4SThg">http://arxiv.org/abs/1602.03822</a></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1501.07227&amp;sa=D&amp;ust=1465142038610000&amp;usg=AFQjCNGC4h82iltiJW709e7lyyENrs3kDQ">http://arxiv.org/abs/1501.07227</a></span><span class="c1">&nbsp;replaced by </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03822&amp;sa=D&amp;ust=1465142038611000&amp;usg=AFQjCNE24EZu-KAZJwWLTWEkcvYSnEaKFQ">http://arxiv.org/abs/1602.03822</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03822&amp;sa=D&amp;ust=1465142038611000&amp;usg=AFQjCNE24EZu-KAZJwWLTWEkcvYSnEaKFQ">http://arxiv.org/abs/1602.03822</a></span><span class="c1">&nbsp;[abs] unsupervised clustering and such by finding some sort of dimensionality reduction thing I have no idea how this differs from the normal thing everyone was already doing but I guess it&#39;s great. should go in &quot;clustering&quot; I guess.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03814&amp;sa=D&amp;ust=1465142038612000&amp;usg=AFQjCNEQyLzkrDVMYiEvl3z3hLcXbLFuSA">http://arxiv.org/abs/1602.03814</a></span><span class="c1">&nbsp;[abs] paper on human-robot interaction. should go in &quot;safety&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03534&amp;sa=D&amp;ust=1465142038613000&amp;usg=AFQjCNE6uC62j6L56C7yXU5DYrJw5wXo6A">http://arxiv.org/abs/1602.03534</a></span><span class="c1">&nbsp;[abs] generalization thing. does something to represent the difference between domains, and then uses that as an optimization term. should go in &quot;generality&quot;, &quot;regularization&quot;, &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.09300&amp;sa=D&amp;ust=1465142038614000&amp;usg=AFQjCNH7a6Av3uVZriP_7peR0FmkwJulVw">http://arxiv.org/abs/1512.09300</a></span><span class="c1">&nbsp;[abs] extension of GANs by first training a GAN, then using its discriminator on a VAE. should go in &quot;representation learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.01774&amp;sa=D&amp;ust=1465142038615000&amp;usg=AFQjCNH7r5eq0lFh3NlGkL8DiCCkSE3HSw">http://arxiv.org/abs/1508.01774</a></span><span class="c1">&nbsp;[abs] real time transcription of piano music to sheet music! cool! should go in &quot;applications/audio&quot; and maybe also &quot;speech&quot; &#39;cause it rides on that a lot. I wanted to do this aaaages ago.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04019&amp;sa=D&amp;ust=1465142038616000&amp;usg=AFQjCNEFqiYxwSit8htCJ2wOuJa3Phu3BQ">http://arxiv.org/abs/1602.04019</a></span><span class="c1">&nbsp;[abs] analysis of AI safety from the perspective of brain energy. under the assumption that you can do a lot better than the algorithmic approaches used by the brain, finds that you can do a lot better than the power consumption of the brain. I find the assumption to be highly suspect. should go in &quot;safety&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03930&amp;sa=D&amp;ust=1465142038616000&amp;usg=AFQjCNG3pI59DYKJPVa3x-fXog-9CVVaPQ">http://arxiv.org/abs/1602.03930</a></span><span class="c1">&nbsp;[abs] semantic segmentation with deconvolution. should go in &quot;images/per-pixel prediction&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.03935&amp;sa=D&amp;ust=1465142038617000&amp;usg=AFQjCNHcy79yfVyr_yXDJ1RucZXr5xY1bA">http://arxiv.org/abs/1602.03935</a></span><span class="c1">&nbsp;[abs] &quot;Face Attribute Prediction Using Off-The-Shelf Deep Learning Networks&quot;. apparently traditional face systems are overcomplicated compared to the nn approach. whoda thunk. should go in &quot;images/faces&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04105&amp;sa=D&amp;ust=1465142038618000&amp;usg=AFQjCNFv1as0cgNyzti-UkJQgFPfuBTBpw">http://arxiv.org/abs/1602.04105</a></span><span class="c1">&nbsp;[abs] new application of neural networks to a classification problem. blows expert features out of the water. yet another field is revolutionized. should go in &quot;applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04062&amp;sa=D&amp;ust=1465142038619000&amp;usg=AFQjCNGnHjwJuAU5XApI2rVb6AZpfTXvtg">http://arxiv.org/abs/1602.04062</a></span><span class="c1">&nbsp;[abs] using reinforcement learning to entirely automatically tune hyperparameters!! about damn fucking time! should go in &quot;hyperparameters&quot;, &quot;rl&quot;. they also do some interesting investigation into what are good strategies based on the learned behaviors.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04133&amp;sa=D&amp;ust=1465142038620000&amp;usg=AFQjCNF0LvF7oiid-cC8ciy5pjY9fbOFLw">http://arxiv.org/abs/1602.04133</a></span><span class="c1">&nbsp;[abs] more progress in using deep gaussian processes as deep bayesian neural networks. should go in &quot;gaussian processes&quot;, &quot;bayesian neural networks&quot;. very interesting, I wonder if this will take over in a year or two?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04868&amp;sa=D&amp;ust=1465142038620000&amp;usg=AFQjCNGZswdULKoXH2alkLUlLd1zkaWh0g">http://arxiv.org/abs/1602.04868</a></span><span class="c1">&nbsp;[abs] real time face recognition on mobile devices. should go in &quot;speed&quot;, &quot;images/faces&quot;. they implement it on mobile gpus.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.09065&amp;sa=D&amp;ust=1465142038621000&amp;usg=AFQjCNHZQapfZbwQikPU-0xanMI0tt6Jmg">http://arxiv.org/abs/1602.09065</a></span><span class="c1">&nbsp;[abs] dataset for sign language gesture recognition, and some pose recognition experiments on it. should go in &quot;images/pose&quot; and maybe &quot;language/speech&quot; and &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08771&amp;sa=D&amp;ust=1465142038622000&amp;usg=AFQjCNEGtR6FYa2hGJtXOYu0qsse68_rrA">http://arxiv.org/abs/1602.08771</a></span><span class="c1">&nbsp;[abs] non-neural: &quot;we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use.&quot; should go in &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.09118&amp;sa=D&amp;ust=1465142038623000&amp;usg=AFQjCNGAjUPFsxtaa5JmZn0l4BnRpuoaRA">http://arxiv.org/abs/1602.09118</a></span><span class="c1">&nbsp;[abs] based on the claim that nn RL doesn&#39;t have very good bounds towards making a strictly improving policy, they put together a thing using an average of &quot;divergence&quot; (divergence of what?) to make what they then name &quot;Easy Monotonic Policy Iteration&quot;. should go in &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08556&amp;sa=D&amp;ust=1465142038623000&amp;usg=AFQjCNGBR098k5-Uii2ulS3J1BKPZPywnQ">http://arxiv.org/abs/1602.08556</a></span><span class="c1">&nbsp;[abs] some hardware thing exploring how lossy they can make RAM before it becomes a problem. should go in &quot;hardware&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08557&amp;sa=D&amp;ust=1465142038624000&amp;usg=AFQjCNHwV5FDuMsbmyzwAkH2nuIuVgE-qA">http://arxiv.org/abs/1602.08557</a></span><span class="c1">&nbsp;[abs] multiplication-free NN thing. unclear exactly what they did or whether this is another &quot;spiking&quot; paper. should go in &quot;model compression&quot;, &quot;performance&quot;/&quot;speed&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08486&amp;sa=D&amp;ust=1465142038625000&amp;usg=AFQjCNEl2v_mvuUrHu_bjffxsQF4ZYnh5w">http://arxiv.org/abs/1602.08486</a></span><span class="c1">&nbsp;[abs] model of perceptual neuron learning in humans. designed for visual, but correctly predicts auditory too. cool. should go in &quot;sparsity&quot;, &quot;neuroscience&quot;, &quot;neuromorphic&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08734&amp;sa=D&amp;ust=1465142038626000&amp;usg=AFQjCNFRufIAAmem-6l-fzq-ygBKsbfF3w">http://arxiv.org/abs/1602.08734</a></span><span class="c1">&nbsp;[abs] deep variational sparse autoencoder. does some stuff to make that work better. probably cool. should go in &quot;bayesian neural networks&quot;, &quot;unsupervised/generative&quot;, &quot;sparsity&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.00098&amp;sa=D&amp;ust=1465142038626000&amp;usg=AFQjCNErGdxW94-GkLBI88svElopkFKYRg">http://arxiv.org/abs/1510.00098</a></span><span class="c1">&nbsp;[abs] bootstrapping from low quality data for poverty mapping: from sattellite data, predict nighttime lights based on daytime structure, then learn on povery survey data. gets very high quality results, but that generalize. should go in &quot;applications/ea&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02632&amp;sa=D&amp;ust=1465142038627000&amp;usg=AFQjCNFNcS4Z3h2K7pfbvffacIBGA-ZWTw">http://arxiv.org/abs/1506.02632</a></span><span class="c1">&nbsp;[abs] &quot;Cumulative prospect theory&quot;, a model of human decision making, used in RL instead of expected value, and they find it can work well. interesting. should go in &quot;RL&quot;. not neural.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07962&amp;sa=D&amp;ust=1465142038628000&amp;usg=AFQjCNFVsZR6ZxI_3d6ARNQsuFJ0os3ttA">http://arxiv.org/abs/1512.07962</a></span><span class="c1">&nbsp;[abs] &quot;Stochastic gradient Markov chain Monte Carlo&quot;, for neural networks. claims to converge to the global optimum (!). is effectively a gradient noise schedule, if I understand correctly - but it&#39;s based off of simulated annealing, which is pretty cool. should go in &quot;training algorithms&quot; and &quot;bayesian neural networks&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00423&amp;sa=D&amp;ust=1465142038628000&amp;usg=AFQjCNGtdgkcZpqdf_3C-SvDyyAJJNp68w">http://arxiv.org/abs/1603.00423</a></span><span class="c1">&nbsp;[abs] recursive-shaped lstms work better than recursive-shaped rnns. what a surprise. should go in &quot;recurrence/recursive&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00223&amp;sa=D&amp;ust=1465142038629000&amp;usg=AFQjCNH5utKia44S0Y61EJi1xL3ZXc304g">http://arxiv.org/abs/1603.00223</a></span><span class="c1">&nbsp;[abs] conditional random field for segmentation and rnn for classification of speech, using no other training signals like language modeling. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00375&amp;sa=D&amp;ust=1465142038630000&amp;usg=AFQjCNFvGmIb2Ui-_ucQF-BeYhVRNI2xcQ">http://arxiv.org/abs/1603.00375</a></span><span class="c1">&nbsp;[abs] new SOTA in recursive tree parsing of english (and chinese). &quot;recursive combination of recurrent neural network encoders&quot;. should go in &quot;recurrence/recursive&quot;, &quot;language/parsing&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00124&amp;sa=D&amp;ust=1465142038631000&amp;usg=AFQjCNE7Ts1MyLYETbcWnv2JVIcvlTNMag">http://arxiv.org/abs/1603.00124</a></span><span class="c1">&nbsp;[abs] meaningfully better new SOTA in pedestrian detection. combines CNNs and hand-engineered features. should go in &quot;images/detection&quot;, and if I made &quot;applications/cars&quot; then that too.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00128&amp;sa=D&amp;ust=1465142038632000&amp;usg=AFQjCNGxZS09Rexmc0ZLALGpGP1Uwg0meA">http://arxiv.org/abs/1603.00128</a></span><span class="c1">&nbsp;[abs] &quot;cascaded subpatch networks&quot; - looks like it&#39;s just 3x3 conv, 1x1 conv. might be more complex than that. works well on cifar-10, they mistakenly say they&#39;re sota. should go in &quot;convolution&quot;, &quot;architecture&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00275&amp;sa=D&amp;ust=1465142038633000&amp;usg=AFQjCNGRsO0pknYZnyoYuqT4azNErHZb6w">http://arxiv.org/abs/1603.00275</a></span><span class="c1">&nbsp;[abs] &quot;Gland Segmentation in Colon Histology Images: The GlaS Challenge Contest&quot; - should go in &quot;bio&quot;, maybe also &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00438&amp;sa=D&amp;ust=1465142038633000&amp;usg=AFQjCNEkXZ4sSLs86hmtE2dSG-H19w0s-w">http://arxiv.org/abs/1603.00438</a></span><span class="c1">&nbsp;[abs] some sort of patch lookup retrieval thing using convolutional kernel thingies. should go in &quot;images/retrieval&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00106&amp;sa=D&amp;ust=1465142038634000&amp;usg=AFQjCNEc-NgHV0AZjBPGvOOgVyt6hx-KcA">http://arxiv.org/abs/1603.00106</a></span><span class="c1">&nbsp;[abs] forecasting disease outbreaks using word vectors or something. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00448&amp;sa=D&amp;ust=1465142038635000&amp;usg=AFQjCNGyj6kR9mZmSPk_qo1BqYVmeum2tw">http://arxiv.org/abs/1603.00448</a></span><span class="c1">&nbsp;[abs] some stuff on inverse optimal control via imitation learning. &quot;Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization&quot;. should go in &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00162&amp;sa=D&amp;ust=1465142038636000&amp;usg=AFQjCNE5BWAxbpy_B1IBlnja7bSN3AxC2g">http://arxiv.org/abs/1603.00162</a></span><span class="c1">&nbsp;[abs] some theory work on comparing &quot;convolutional arithmetic circuits&quot; to convolutional relu+maxpool nets. finds that relu+maxpool is universal, relu+averagepool isn&#39;t, and also finds that arithmetic circuits have better depth efficiency. should go in &quot;theory&quot;, &quot;architecture&quot;, maybe &quot;activation&quot;. uses tensor decomposition to do it, so maybe also &quot;weight representation&quot; or something.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.03273&amp;sa=D&amp;ust=1465142038637000&amp;usg=AFQjCNGKlPatZ5Qwydj-PkWmoJVB3_KWlA">http://arxiv.org/abs/1502.03273</a></span><span class="c1">&nbsp;[abs] image denoising with sparse representations. should go in &quot;sparsity&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.04760&amp;sa=D&amp;ust=1465142038638000&amp;usg=AFQjCNFONY53zkC-NfCkdTvVXFw4VUfM_Q">http://arxiv.org/abs/1507.04760</a></span><span class="c1">&nbsp;[abs] non-neural driver gaze estimation. should go in &quot;misc&quot; or &quot;cars&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06113&amp;sa=D&amp;ust=1465142038639000&amp;usg=AFQjCNEJHSIsCnzRpOasOzYEUdHhYPSpKw">http://arxiv.org/abs/1509.06113</a></span><span class="c1">&nbsp;[abs] robot control using an autoencoder to learn to represent object locations, and then simple linear policies in the hidden state. should go in &quot;rl&quot;, &quot;representation learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00772&amp;sa=D&amp;ust=1465142038639000&amp;usg=AFQjCNGRmaKtFsoGC0SHysdnwS6HBptDwQ">http://arxiv.org/abs/1603.00772</a></span><span class="c1">&nbsp;[abs] for massive heirarchical classification: &quot;propose two efficient data driven filter based approaches for hierarchical structure modification: (i) Flattening (local and global) approach that identifies and removes inconsistent nodes present within the hierarchy and (ii) Rewiring approach modifies parent-child relationships to improve the classification performance of learned models.&quot; should go in &quot;classification&quot;. note: wouldn&#39;t decision forests do this job much better?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00810&amp;sa=D&amp;ust=1465142038640000&amp;usg=AFQjCNEuavizS2VQccfBgSWeA4MRNwBjLA">http://arxiv.org/abs/1603.00810</a></span><span class="c1">&nbsp;[abs] neural machine translation with per-character inputs. should go in &quot;language/translation&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00489&amp;sa=D&amp;ust=1465142038641000&amp;usg=AFQjCNHTTGNvTeY5kYU0_P2SYRQ4w9PAUQ">http://arxiv.org/abs/1603.00489</a></span><span class="c1">&nbsp;[abs] localization using classification networks. outdoes non-transfer-learning based approaches by a lot. should go in &quot;transfer learning&quot;, &quot;images/detection&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00502&amp;sa=D&amp;ust=1465142038642000&amp;usg=AFQjCNGDlDt-CZCBbqvLE0O3U2Y9cf7Ihw">http://arxiv.org/abs/1603.00502</a></span><span class="c1">&nbsp;[abs] sped up detection by &quot;100%&quot; (2x? 100x?). &quot;Keypoint Density-based Region Proposal&quot;. should go in &quot;images/detection&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00550&amp;sa=D&amp;ust=1465142038642000&amp;usg=AFQjCNFXAZCL_Vv8oggtN5GfCazSI2Bt_w">http://arxiv.org/abs/1603.00550</a></span><span class="c1">&nbsp;[abs] &quot;zero-shot learning&quot; by using descriptions of classes without examples and doing some sort of regularization with them. interesting. should go in &quot;classification&quot;, &quot;generalization&quot;, maybe &quot;transfer learning&quot;, &quot;regularization&quot;. sounds like it works really really well, and this is a really cool topic.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00831&amp;sa=D&amp;ust=1465142038643000&amp;usg=AFQjCNHBMLqAwmXdywzJCBYn3kdyYmulhA">http://arxiv.org/abs/1603.00831</a></span><span class="c1">&nbsp;[abs] object tracking dataset - carefully labeled pedestrian tracking dataset. should go in &quot;images/people&quot; or &quot;images/tracking&quot; or &quot;images/pedestrian&quot;, and &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00845&amp;sa=D&amp;ust=1465142038644000&amp;usg=AFQjCNGI9DBX1hfrQwYtO--uX468ID3GBw">http://arxiv.org/abs/1603.00845</a></span><span class="c1">&nbsp;[abs] end to end supervised saliency prediction with cnns. should go in &quot;images/saliency&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00622&amp;sa=D&amp;ust=1465142038645000&amp;usg=AFQjCNHbPVfBMeGzkje3b8WY4oXC7I67fQ">http://arxiv.org/abs/1603.00622</a></span><span class="c1">&nbsp;[abs] supervised reinforcement learning by trying to imitate &quot;model-predictive control&quot;. used to enforce safety in dangerous RL systems. should go in &quot;safety&quot;, &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00748&amp;sa=D&amp;ust=1465142038646000&amp;usg=AFQjCNFubeffbMbbG8UEBkBoHT2nbEiXSQ">http://arxiv.org/abs/1603.00748</a></span><span class="c1">&nbsp;[abs] continuous q learning via a maybe-new approach, named as &quot;normalized adantage functions&quot;. (is that a typo?). apparently much faster learning. should go in &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00751&amp;sa=D&amp;ust=1465142038647000&amp;usg=AFQjCNHhp9GNNO-thlXyQfTAsxjHVgp0Rw">http://arxiv.org/abs/1603.00751</a></span><span class="c1">&nbsp;[abs] company price prediction over one year. has rather low accuracy at 75%. should go in &quot;finance&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00788&amp;sa=D&amp;ust=1465142038647000&amp;usg=AFQjCNFEKEftOgA86MFniHvVGHBrS_tPSA">http://arxiv.org/abs/1603.00788</a></span><span class="c1">&nbsp;[abs] automatic differentiation based variational inference! should go in &quot;training algorithms&quot;, &quot;libraries&quot;. reference implementation in stan.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00806&amp;sa=D&amp;ust=1465142038648000&amp;usg=AFQjCNGo2ZaswrljDK09M5Jhmt6gwkCp1A">http://arxiv.org/abs/1603.00806</a></span><span class="c1">&nbsp;[abs] neural network based &quot;collaborative filtering&quot; - which I think is the &quot;you&#39;re like these users&quot; algorithm. should go in &quot;recommendation&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00856&amp;sa=D&amp;ust=1465142038649000&amp;usg=AFQjCNFKyQyBXK2br8kZ7Js1AnfuXkTCTQ">http://arxiv.org/abs/1603.00856</a></span><span class="c1">&nbsp;[abs] learning from undirected graphs for learning how molecules work. should go in &quot;applications/misc&quot;. maybe also relevant for &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.01722&amp;sa=D&amp;ust=1465142038650000&amp;usg=AFQjCNHQV5OpldRjsvrNVElEmjhWkOKfYg">http://arxiv.org/abs/1508.01722</a></span><span class="c1">&nbsp;[abs] face verification with convnets from &quot;IARPA Janus Benchmark A (IJB-A)&quot; dataset. should go in &quot;images/faces&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06238&amp;sa=D&amp;ust=1465142038650000&amp;usg=AFQjCNGcLZotu34xEqJl7D3nQm5kqi-Xew">http://arxiv.org/abs/1511.06238</a></span><span class="c1">&nbsp;[abs] some sort of multimodal sparse coding... thing. &quot;Multimodal sparse representation learning and applications&quot;. should go in &quot;sparsity&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.05561&amp;sa=D&amp;ust=1465142038651000&amp;usg=AFQjCNFFVpzYoMqoQP84wnUT2V_13EQwnQ">http://arxiv.org/abs/1505.05561</a></span><span class="c1">&nbsp;[abs] analysis of why autoencoders learn sparse representations, finds that &quot;both regularization and activation function that play an important role in encouraging sparsity&quot;. should go in &quot;regularization&quot;, &quot;sparsity&quot;, &quot;theory&quot;, &quot;representation learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00892&amp;sa=D&amp;ust=1465142038652000&amp;usg=AFQjCNGF5mPJz0u99QwcuTGcJ6mkdczmKw">http://arxiv.org/abs/1603.00892</a></span><span class="c1">&nbsp;[abs] word vector refinement thing that takes a corpus of word vectors, does something to them, and then they work better. &quot;a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations&quot;. should go in &quot;language/embeddings&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00957&amp;sa=D&amp;ust=1465142038653000&amp;usg=AFQjCNFv6u802AIyzBYYZgnksXwUagKY7w">http://arxiv.org/abs/1603.00957</a></span><span class="c1">&nbsp;[abs] question answering system that uses freebase and wikipedia and consistency checking nets to validate answers; should go in &quot;language/qa&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00968&amp;sa=D&amp;ust=1465142038654000&amp;usg=AFQjCNFJLpnsdYkz8cpguNNk-zR7jXbh5A">http://arxiv.org/abs/1603.00968</a></span><span class="c1">&nbsp;[abs] thing that uses multiple kinds of word vectors for the same task. should go in &quot;language/embeddings&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01232&amp;sa=D&amp;ust=1465142038654000&amp;usg=AFQjCNEAswNbHcM0FHwNmzu2t462wlyp8w">http://arxiv.org/abs/1603.01232</a></span><span class="c1">&nbsp;[abs] language generation system that first learns to generate language, then adds a small number of examples in a target domain. should go in &quot;language/talking computers&quot;, &quot;data efficiency&quot;/&quot;transfer learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00944&amp;sa=D&amp;ust=1465142038655000&amp;usg=AFQjCNElWAngiT5MnDYiU7gVTpPdT2svnw">http://arxiv.org/abs/1603.00944</a></span><span class="c1">&nbsp;[abs] analysis of an alternate deep model, &quot;PCAnet&quot;. should go in &quot;alternate deep methods&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00993&amp;sa=D&amp;ust=1465142038656000&amp;usg=AFQjCNGBQvVGdMolnctegk9SzSldP7tJfg">http://arxiv.org/abs/1603.00993</a></span><span class="c1">&nbsp;[abs] robot self-location detection based on visual features. apparently improves on a cnn approach. should go in &quot;applications/robots&quot; I guess. maybe &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01006&amp;sa=D&amp;ust=1465142038657000&amp;usg=AFQjCNHkrPzUenlGp854fEOzwncgvnYpyw">http://arxiv.org/abs/1603.01006</a></span><span class="c1">&nbsp;[abs] person identification from gait with resolution 8x lower just by using cnns. another field revolutionized. should go in &quot;images/video&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01068&amp;sa=D&amp;ust=1465142038658000&amp;usg=AFQjCNENLbGWZWYUbFFw0rZgNAJZpEgx3A">http://arxiv.org/abs/1603.01068</a></span><span class="c1">&nbsp;[abs] convnets for identifying what camera took a picture. should go in &quot;images&quot;. also they use it for security, do they not know about the adverserial examples problem?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01076&amp;sa=D&amp;ust=1465142038659000&amp;usg=AFQjCNEXj-06oi6uuiEzGsnGUGGjzlMqRQ">http://arxiv.org/abs/1603.01076</a></span><span class="c1">&nbsp;[abs] convnets are better for embedding when there is no domain shift relative to the training data, awful when there is. should go in &quot;images/embeddings&quot;. how does it work with the stuff on forcing generalization via unseen-class estimation and such?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01249&amp;sa=D&amp;ust=1465142038660000&amp;usg=AFQjCNG9jPiJ_J5LkbG3u3v8gmpzsvft0w">http://arxiv.org/abs/1603.01249</a></span><span class="c1">&nbsp;[abs] &quot;HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition&quot;. should go in &quot;images/faces&quot;, &quot;multimodal models&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01250&amp;sa=D&amp;ust=1465142038660000&amp;usg=AFQjCNHhpooMlS3ecTciPl0UnjCum8IvTA">http://arxiv.org/abs/1603.01250</a></span><span class="c1">&nbsp;[abs] hybrid of cnns and decision forests, via a technique that blends between them (cool!) should go in &quot;alternate deep techniques&quot;, &quot;images&quot;, &quot;classification&quot;. </span><span class="c9 c1 c10">works just as well as sota (so they say) on imagenet (!) and yet with many fewer parameters (!).</span></p><p class="c3"><span class="c9 c1 c10"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00988&amp;sa=D&amp;ust=1465142038661000&amp;usg=AFQjCNG4NAGun486KAdOEMmxbCdt7Eyong">http://arxiv.org/abs/1603.00988</a></span><span class="c1">&nbsp;[abs] analysis of depth of nns, when deep is better (compositional functions), etc. should go in &quot;theory&quot;. &quot;We also discuss connections between our results and learnability of sparse Boolean functions, settling an old conjecture by Bengio.&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01121&amp;sa=D&amp;ust=1465142038662000&amp;usg=AFQjCNEzzTCnvzhwHj8eH8-w3c4pAx-p2Q">http://arxiv.org/abs/1603.01121</a></span><span class="c1">&nbsp;[abs] finding nash equilibria with reinforcement learners - correctly finds the nash equilibrium for a known game, and performs similarly to humans and SOTA in another game (a poker thing). should go in &quot;rl&quot;. &quot;Deep Reinforcement Learning from Self-Play in Imperfect-Information Games&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00930&amp;sa=D&amp;ust=1465142038662000&amp;usg=AFQjCNFQRcBjh2VeEvSbrCQzCNZniHLw9A">http://arxiv.org/abs/1603.00930</a></span><span class="c1">&nbsp;[abs] application of lstm to procedural world gen by training it to generate super mario levels. doesn&#39;t do a good job of getting global structure. should go in &quot;generative&quot;, &quot;applications/procedural gen&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01025&amp;sa=D&amp;ust=1465142038663000&amp;usg=AFQjCNGpsygy9pX-zhERr0JqT6lU7PQDsQ">http://arxiv.org/abs/1603.01025</a></span><span class="c1">&nbsp;[abs] training a compressed model with 5 bits of weight and such, by using a log representation which better matches the distribution. should go in &quot;model compression&quot;, &quot;speed&quot;. they can use addition for multiplication, because it&#39;s in log space, I think? they also find it&#39;s good &quot;regularization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.00982&amp;sa=D&amp;ust=1465142038664000&amp;usg=AFQjCNEL1VLfq19l-Tp-i3ozHnFNt7uNcQ">http://arxiv.org/abs/1603.00982</a></span><span class="c1">&nbsp;[abs] autoencoders of sequence-to-sequence rnns - interesting, it actually works quite well. used for speech. should go in &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf&amp;sa=D&amp;ust=1465142038665000&amp;usg=AFQjCNGkDC5PBQGwLwitmHqixlO5ZQLHFQ">http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf</a></span><span class="c1">&nbsp;[full read] simple method for how to read research papers usefully. should go in &quot;basics&quot;. (&quot;how to read papers&quot;)</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.04280&amp;sa=D&amp;ust=1465142038665000&amp;usg=AFQjCNGiN_TZ6iXqj7IBd_VJzXMtGR131g">http://arxiv.org/abs/1512.04280</a></span><span class="c1">&nbsp;[abs] highway networks for speech recognition - narrow and deep nets. consistently outperform the non-highway nets. so they do work. should go in &quot;architecture&quot;, &quot;speech&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01312&amp;sa=D&amp;ust=1465142038666000&amp;usg=AFQjCNHDSecpwgzz7rrufKD63dzZUnMRzA">http://arxiv.org/abs/1603.01312</a></span><span class="c1">&nbsp;[abs] predicting the outcome of physical simulations in unity3d. comes with extensions to torch to allow it to run inside unity3d - very relevant for making game AIs! should go in &quot;rl&quot;, &quot;images/video&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01360&amp;sa=D&amp;ust=1465142038667000&amp;usg=AFQjCNExa8ZAI0TA-r4cVQbJYYbMQZXxmw">http://arxiv.org/abs/1603.01360</a></span><span class="c1">&nbsp;[abs] a neural+crf and a non-neural approach to &quot;named entity recognition&quot;. should go in &quot;language/parsing&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01547&amp;sa=D&amp;ust=1465142038668000&amp;usg=AFQjCNFHlYNp4Cdn1IgFk19wDVIr5LciZg">http://arxiv.org/abs/1603.01547</a></span><span class="c1">&nbsp;[abs] model that solves question answering by simply returning the result of an attentional query. should go in &quot;language/qa&quot;. run on cnn+dailymail and children&#39;s books datasets.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01597&amp;sa=D&amp;ust=1465142038669000&amp;usg=AFQjCNFs_kc1jXdbYbPl8Dca1A8j3a2fyw">http://arxiv.org/abs/1603.01597</a></span><span class="c1">&nbsp;[abs:0] application of nns to POS tagging medieval latin. should go in &quot;language/misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01354&amp;sa=D&amp;ust=1465142038670000&amp;usg=AFQjCNGQO-blP_1LzQDdcm7DdUJwYt4VbA">http://arxiv.org/abs/1603.01354</a></span><span class="c1">&nbsp;[abs:1] end to end sequence labeling with a combination cnn+lstm+crf model. uses word tokens and character tokens as input. should go in &quot;language/language models&quot; or something like that.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01359&amp;sa=D&amp;ust=1465142038671000&amp;usg=AFQjCNHsq1LUwq_k-0vYmC6hjcIKv_2yrg">http://arxiv.org/abs/1603.01359</a></span><span class="c1">&nbsp;[abs:3] multimodal model - learns single representation of objects across different input encodings. should go in &quot;multimodal models&quot;, &quot;images&quot;, &quot;classification&quot; (I guess).</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.03608&amp;sa=D&amp;ust=1465142038671000&amp;usg=AFQjCNGfOZh2Z3F6iBwp6p4d824GcQ3XCA">http://arxiv.org/abs/1510.03608</a></span><span class="c1">&nbsp;[abs:1] convnets for pedestrian detection application. should go in &quot;images/pedestrian&quot; or &quot;images/detection&quot; or &quot;images/cars&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05065&amp;sa=D&amp;ust=1465142038672000&amp;usg=AFQjCNH66WwB_EUM0YuGkkrzbupoblKQ7A">http://arxiv.org/abs/1511.05065</a></span><span class="c1">&nbsp;[abs:0] &quot;We introduce a new dataset that can be used to evaluate both general scene flow techniques and region-based approaches such as proposal flow&quot;. should go in &quot;datasets&quot; and I guess &quot;images/detection&quot; if that&#39;s what it is.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06433&amp;sa=D&amp;ust=1465142038673000&amp;usg=AFQjCNGqEX6qTNyVQF4444eeWi-HBFm3Jg">http://arxiv.org/abs/1511.06433</a></span><span class="c1">&nbsp;[abs:2] model compression back and forth between cnns and lstms, to allow them to learn from each other&#39;s biases. should go in &quot;model compression&quot;, &quot;convolution&quot;, &quot;recurrence&quot;. they try it on &quot;speech&quot; as well.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1405.4604&amp;sa=D&amp;ust=1465142038674000&amp;usg=AFQjCNEbFCrSPmEVI5SFpyBDyfJ8nYEvMQ">http://arxiv.org/abs/1405.4604</a></span><span class="c1">&nbsp;[abs:2] paper on saddle point problem. should go in &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08141&amp;sa=D&amp;ust=1465142038674000&amp;usg=AFQjCNF9ObhtoXXlS3PgXCTyrh9zDTtgWQ">http://arxiv.org/abs/1602.08141</a></span><span class="c1">&nbsp;[abs:1] safe urban uav navigation based on safety weighting of a search path. should go in &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08225&amp;sa=D&amp;ust=1465142038675000&amp;usg=AFQjCNGiWXibQmzqP3s0x_pw7pHltEf18A">http://arxiv.org/abs/1602.08225</a></span><span class="c1">&nbsp;[abs:3] multimodal emotion recognition. should go in &quot;images/emotion&quot;, &quot;multimodal&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08124&amp;sa=D&amp;ust=1465142038676000&amp;usg=AFQjCNEmJcSTjiYKMbQT0l28TNUK-UChWA">http://arxiv.org/abs/1602.08124</a></span><span class="c1">&nbsp;[abs:2] software approach for minimizing transfers between cpu and gpu. should go in &quot;speed&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.07247&amp;sa=D&amp;ust=1465142038677000&amp;usg=AFQjCNG0yUlMPa5rbAIqwXfzyxDqepy4cg">http://arxiv.org/abs/1511.07247</a></span><span class="c1">&nbsp;[abs:1] some place recognition thing using &quot;VLAD&quot; - &quot;Vector of Locally Aggregated Descriptors&quot; - image representation. should go in &quot;images/classification&quot; or maybe &quot;images/location&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.03662&amp;sa=D&amp;ust=1465142038677000&amp;usg=AFQjCNHqFeT1HMqbiUjEovhBH1odF1S9lQ">http://arxiv.org/abs/1506.03662</a></span><span class="c1">&nbsp;[abs:2] model of why variance-reduction sgd doesn&#39;t help as much as hoped: because this doesn&#39;t happen until a few epochs in. this is a method for speeding up training earlier in the process. should go in &quot;training algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08571&amp;sa=D&amp;ust=1465142038678000&amp;usg=AFQjCNEffbQ3nS-sYO3Aee2Cmc7Xh6ANzw">http://arxiv.org/abs/1602.08571</a></span><span class="c1">&nbsp;[abs:0] &quot;neural dna&quot; - neural knowledge representation thing. mentions being built out of &quot;four things&quot;, just like dna; I suspect this is a low quality paper. should go in &quot;representation learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06306&amp;sa=D&amp;ust=1465142038679000&amp;usg=AFQjCNHO_0IXzdlXn9m01JQhYS1Ufu4GSg">http://arxiv.org/abs/1511.06306</a></span><span class="c1">&nbsp;[abs:3] convnet that is regularized with noise is robust to adverserial examples problem (!!). should go in &quot;regularization&quot;, &quot;adverserial examples&quot;. says it outperforms other approaches, and that it does so even more so on difficult classification problems. this is very interesting.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08332&amp;sa=D&amp;ust=1465142038680000&amp;usg=AFQjCNEH1zpG0mDuaddCILimV6jcH1_c-A">http://arxiv.org/abs/1602.08332</a></span><span class="c1">&nbsp;[abs:3] application of the &quot;bounded rationality&quot; framework to neural networks to create a training rule that has built-in regularization and they claim it has SOTA on mnist (...lol what a challenge). sounds really cool, though. might be a big step towards theoretically optimal regularization. should go in &quot;training algorithms&quot;, &quot;regularization&quot;, &quot;bayesian neural networks&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08127&amp;sa=D&amp;ust=1465142038680000&amp;usg=AFQjCNFOVgp00o4wNVCpe8jUmkNldPVOaw">http://arxiv.org/abs/1602.08127</a></span><span class="c1">&nbsp;[abs:1] dimensionality reduction with an autoencoder for nearest neighbor indexing. should go in &quot;representation learning&quot; or &quot;applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08405&amp;sa=D&amp;ust=1465142038681000&amp;usg=AFQjCNHMUzZ86b51fkVK2PwAd426PUyclw">http://arxiv.org/abs/1602.08405</a></span><span class="c1">&nbsp;[abs:1] training object detectors with human verification of outputs instead of bounding boxes. should go in &quot;human interaction&quot;/&quot;data collection&quot;, &quot;images/detection&quot;. 6x-9x less labeling effort.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08465&amp;sa=D&amp;ust=1465142038682000&amp;usg=AFQjCNFBuHWe9Frv3BH6bcdnzQeqXScpsA">http://arxiv.org/abs/1602.08465</a></span><span class="c1">&nbsp;[abs:0] use temporally adjacent frame&#39;s object detection results to boost a difficult frame&#39;s results. should go in &quot;images/video&quot;, &quot;images/detection&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08151&amp;sa=D&amp;ust=1465142038683000&amp;usg=AFQjCNF52o096Iub9DSsPBNBCHryckSQ-w">http://arxiv.org/abs/1602.08151</a></span><span class="c1">&nbsp;[abs:1] another paper on abstaining from prediction when risky, and how to manage the tradeoff. should go in &quot;classification&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08191&amp;sa=D&amp;ust=1465142038684000&amp;usg=AFQjCNHprnafXN5VXYI0Et0cGnOVhpsZTw">http://arxiv.org/abs/1602.08191</a></span><span class="c1">&nbsp;[abs:0] spark parallelization. should go in &quot;parallelization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08323&amp;sa=D&amp;ust=1465142038684000&amp;usg=AFQjCNH1AqPb00pBVFecTmtVkAZCwNGLrQ">http://arxiv.org/abs/1602.08323</a></span><span class="c1">&nbsp;[abs:0] conversion of the typical relu network to a spiking network&hellip; with floating point spikes. wat. why is this even a thing. should go in &quot;spiking&quot;, and I guess it could be useful for &quot;parallelization&quot;. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04984&amp;sa=D&amp;ust=1465142038685000&amp;usg=AFQjCNGMGLqP7L1b04_-zEh96zlKK0wtEw">http://arxiv.org/abs/1602.04984</a></span><span class="c1">&nbsp;[abs:1] weakly supervised feature learning with deconvnets. (wait, what?) should go in &quot;representation learning&quot; (I guess?) and &quot;bio&quot; (because they apply it to medical data). they also mention &quot;tied weights&quot;, I&#39;m not sure which weights are tied, but it might be that the deconvolution is recursive.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04938&amp;sa=D&amp;ust=1465142038686000&amp;usg=AFQjCNFOb7ddxL91S1p09vnC-iyEFmiScQ">http://arxiv.org/abs/1602.04938</a></span><span class="c1">&nbsp;[abs:1] strange approach to explaining predictions: &quot;by learning an interpretable model locally around the prediction&quot;. should go in &quot;debugging&quot; and &quot;safety&quot;, though it also has an emphasis on getting users to trust it.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05179&amp;sa=D&amp;ust=1465142038687000&amp;usg=AFQjCNG3oaCLUmZ2nb-7TkUYZx9LRBt5Lg">http://arxiv.org/abs/1602.05179</a></span><span class="c1">&nbsp;[abs:1] bengio&#39;s crazy stdp-based approach to implementing backprop. works on &quot;permutation invariant&quot; (bag of pixels) mnist with three layers, gets the training error to 0% (but uh, does it generalize?) - should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04983&amp;sa=D&amp;ust=1465142038688000&amp;usg=AFQjCNEb8keypPUitexSF-B3BQpoyIGQCQ">http://arxiv.org/abs/1602.04983</a></span><span class="c1">&nbsp;[abs:2] learning to retrieve media based on natural language queries about the location and content. should go in &quot;images/retrieval&quot;, &quot;language/text understanding&quot;. they also do a user study and have interesting results from that.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.2620&amp;sa=D&amp;ust=1465142038689000&amp;usg=AFQjCNEMUYUl5MdXDZz4KHLnm70J2LNFbg">http://arxiv.org/abs/1412.2620</a></span><span class="c1">&nbsp;[abs:0] multidimensional lstm for handwriting recognition. honestly, just use recurrent convnets, geez&hellip; they also modify it to work better due to instability of lstm when used this way (though their explanation that dimensionality is the problem seems suspicious to me). should go in &quot;recurrence&quot;, &quot;images/classification&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06791&amp;sa=D&amp;ust=1465142038690000&amp;usg=AFQjCNE6j8YlLo2r9yPUe7mV9nVj2puazA">http://arxiv.org/abs/1509.06791</a></span><span class="c1">&nbsp;[abs:2] learning to fly a simulated quadcopter with &quot;model predictive control&quot; at training time for safety, and just a neural network at test time for performance. should go in &quot;rl&quot;. from pieter abbeel&#39;s group. </span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06241&amp;sa=D&amp;ust=1465142038690000&amp;usg=AFQjCNE24NO_hL8zw9m1Fg_b0gGoFQuCBQ">http://arxiv.org/abs/1511.06241</a></span><span class="c1">&nbsp;[abs:3] partially supervised learning by using k-means somehow - not super clear on how, sounds interesting. is it k-means on the output layer? k-means on the filters? I suspect the latter. should go in &quot;semi-supervised&quot;. they report good results. not sure if they cite the semi-supervised ladder network paper, though.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05292&amp;sa=D&amp;ust=1465142038691000&amp;usg=AFQjCNGdyLiO26PacCLVxf42PQQ2hEYNYA">http://arxiv.org/abs/1602.05292</a></span><span class="c1">&nbsp;[abs:2] author classification using a feedforward language model (feed forward? not recurrent?) comes with a dataset and code. should go in &quot;language/text analysis&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05531&amp;sa=D&amp;ust=1465142038692000&amp;usg=AFQjCNF2HZbGVqh_qcEV_TWcYxQbWa7Lxg">http://arxiv.org/abs/1602.05531</a></span><span class="c1">&nbsp;[abs:1] cnn features and an SVM regression for predicting human quality assessment of images. correlates much more strongly with human preferences than other algorithms. I think this is shallow, though. should go in &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05568&amp;sa=D&amp;ust=1465142038693000&amp;usg=AFQjCNFNfTuSqK5LpRZi2cmLKBOx5YU1QA">http://arxiv.org/abs/1602.05568</a></span><span class="c1">&nbsp;[abs:1] medical concept embedding with a custom algorithm. they test it on a large dataset of medical data. cool. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05220&amp;sa=D&amp;ust=1465142038694000&amp;usg=AFQjCNHLGGxk74hJOhIzANDO4EgeKGkr2Q">http://arxiv.org/abs/1602.05220</a></span><span class="c1">&nbsp;[abs:0] &quot;BioSpaun: A large-scale behaving brain model with complex neurons&quot; - this is that super-neuromorphic model. not really interesting for ML, but hella interesting for predicting the effects of drugs. should go in &quot;neuromorphic&quot;. note: would be really good to use ANN model compression to allow this to run faster, I&#39;ll bet you&#39;d see thousands of times speedup.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05473&amp;sa=D&amp;ust=1465142038694000&amp;usg=AFQjCNFarm2B4uuGQcQnSdYZLlUT6L559g">http://arxiv.org/abs/1602.05473</a></span><span class="c1">&nbsp;[abs:1] some sort of odd generative/semi-supervised learning thing - &quot;We extend deep generative models with auxiliary variables which improves the variational approximation.&quot;. should go in &quot;semi-supervised&quot; and &quot;generative&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05285&amp;sa=D&amp;ust=1465142038695000&amp;usg=AFQjCNGO3rpOVUU4-WHIr7vUZvzMQwY1QQ">http://arxiv.org/abs/1602.05285</a></span><span class="c1">&nbsp;[abs:1] approach to learning to rank &lt;things&gt; - eliminate the least worthy. uses highway networks. should go in &quot;applications&quot;. mentions a yahoo rankings dataset which might be public so also &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.08230&amp;sa=D&amp;ust=1465142038696000&amp;usg=AFQjCNFWSEkEt5sAK_kx3YwbCv-ZmEZu3A">http://arxiv.org/abs/1506.08230</a></span><span class="c1">&nbsp;[abs:1] custom designed alternate output layer for convnets where you&#39;d use logistic regression. (what problem framing makes you want logistic regression? what even is that again?) should go in &quot;output layers&quot;/&quot;activation functions&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05931&amp;sa=D&amp;ust=1465142038697000&amp;usg=AFQjCNGUQrsp4WvCVl91kNjgRFOkiZmbCA">http://arxiv.org/abs/1602.05931</a></span><span class="c1">&nbsp;[abs:3] throw away conv filters where the gradient norm of its weights isn&#39;t large enough. note: this seems like a lie; I suspect that what you really want is to use weight normalization, and monitor the magnitude of the weights that depend on that output. anyway, it improves error by a fair bit. </span><span class="c9 c1 c10">very cool.</span><span class="c1">&nbsp;should go in &quot;regularization&quot;. also is effectively &quot;model compression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05629&amp;sa=D&amp;ust=1465142038698000&amp;usg=AFQjCNHmZtPMjrYqOawzpDnW1EzdliI9Kg">http://arxiv.org/abs/1602.05629</a></span><span class="c1">&nbsp;[abs:2] distributed training of neural networks from private data on the devices that host the private data, rather than shipping it elsewhere. cooool. should go in &quot;parallelization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05908&amp;sa=D&amp;ust=1465142038699000&amp;usg=AFQjCNHV12P_SOkT4mik6VOlG3rwdbTAxg">http://arxiv.org/abs/1602.05908</a></span><span class="c1">&nbsp;[abs:1] third-order training approach for nns. y r u even. should go in &quot;training algorithms&quot;. mentions saddle points as something it helps with. maybe break into this if stuck in another training algorithm??</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05875&amp;sa=D&amp;ust=1465142038700000&amp;usg=AFQjCNGloCZSYNnGxroIHwNo7Gv43fsQ5Q">http://arxiv.org/abs/1602.05875</a></span><span class="c1">&nbsp;[abs:2] another interesting way to hybridize convolution and rnns: use multi-level nesting of rnns feeding rnns. should go in &quot;convolution&quot;, &quot;recurrence&quot;. used on &quot;speech&quot; classification and it works quite well. interesting for use with &quot;language/language modeling&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05682&amp;sa=D&amp;ust=1465142038701000&amp;usg=AFQjCNESZ7yPC4ZDMc_lSkMDJrYDFoOYmw">http://arxiv.org/abs/1602.05682</a></span><span class="c1">&nbsp;[abs:0] shitty paper trying to classify audio based on what recording device recorded it. should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.07427&amp;sa=D&amp;ust=1465142038702000&amp;usg=AFQjCNGI7WIPJ57hSHHNdXpEbGCPFgCK9g">http://arxiv.org/abs/1505.07427</a></span><span class="c1">&nbsp;[abs:3] posenet: predict 6DOF location of the camera given the image! cool! gets good results, and comes with a dataset! should go in &quot;datasets&quot;, &quot;images/3d&quot;. this is very very cool. note: how do they represent camera location, though?</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.01750&amp;sa=D&amp;ust=1465142038703000&amp;usg=AFQjCNH4fAQxiSf5uqkMA7rA_qTD0zcrUA">http://arxiv.org/abs/1601.01750</a></span><span class="c1">&nbsp;[abs:0] reducing errors in data from time of flight cameras. should go in &quot;3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.04798&amp;sa=D&amp;ust=1465142038704000&amp;usg=AFQjCNE9FZmoz12SkHdOcEvr1xfjLRsACQ">http://arxiv.org/abs/1601.04798</a></span><span class="c1">&nbsp;[abs:0] per-pixel &quot;scale-aware&quot; object localization. should go in &quot;images/localization&quot; or &quot;images/per-pixel prediction&quot; or whatever.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.3121&amp;sa=D&amp;ust=1465142038704000&amp;usg=AFQjCNHIyYx5CseUgK7uKWxdX9GwlC2Dfw">http://arxiv.org/abs/1412.3121</a></span><span class="c1">&nbsp;[abs:2] audio/video cross-modality transfer learning by training to output the same hidden states. should go in &quot;multi-modal models&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06023&amp;sa=D&amp;ust=1465142038705000&amp;usg=AFQjCNHN9qzOotlLbefB8IT6UECToCrvTw">http://arxiv.org/abs/1602.06023</a></span><span class="c1">&nbsp;[abs:1] summarization thing, outperforms previous thing on gigaword dataset. should go in &quot;language/summarization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06064&amp;sa=D&amp;ust=1465142038706000&amp;usg=AFQjCNFRoYxJFUiouVcPqwDeHa9jt4Dq_g">http://arxiv.org/abs/1602.06064</a></span><span class="c1">&nbsp;[abs:1] training bi-directional lstm with noise contrastive estimation instead of maximum likelihood. doesn&#39;t outperform one-way lstm, they admit this. cool, a negative result! should go in &quot;language/language modeling&quot;, and maybe also &quot;recurrence&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06149&amp;sa=D&amp;ust=1465142038707000&amp;usg=AFQjCNHG83c0Ar2mPrALLGDpuiNnR9-OPA">http://arxiv.org/abs/1602.06149</a></span><span class="c1">&nbsp;[abs:0] strange thing about face verification of &quot;large age gap&quot; face images. should go in &quot;images/faces&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05980&amp;sa=D&amp;ust=1465142038707000&amp;usg=AFQjCNEy_elulD7ogpzJ4-X6Bzu5yxklRA">http://arxiv.org/abs/1602.05980</a></span><span class="c1">&nbsp;[abs:3] leaky tanh and rescaled sigmoid activation functions. both work reliably and outperform relu nets. what a surprise - saturation is useful! seems like everyone has been trying to find a way to get saturation back, so we have actual nonlinearity again. should go in &quot;activation functions&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06183&amp;sa=D&amp;ust=1465142038708000&amp;usg=AFQjCNG8RkOo8H34BZiPlpfSD-cMYqh8HQ">http://arxiv.org/abs/1602.06183</a></span><span class="c1">&nbsp;[abs:2] per-*node* training of a deep network. huh. I wonder how this even works. I don&#39;t expect it to work well, but it sounds like an interesting read. should go in &quot;training algorithms&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.05996&amp;sa=D&amp;ust=1465142038709000&amp;usg=AFQjCNHwM4YviCkOrKCMkU43ftC7kWZQdw">http://arxiv.org/abs/1602.05996</a></span><span class="c1">&nbsp;[abs:0] some neuromorphic generative thing. should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06057&amp;sa=D&amp;ust=1465142038710000&amp;usg=AFQjCNGqb1Eciypm7dMDc4-m8aYHfth_8A">http://arxiv.org/abs/1602.06057</a></span><span class="c1">&nbsp;[abs:0] some white matter thing. should go in &quot;neuromorphic&quot;/&quot;neuroscience&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.06838&amp;sa=D&amp;ust=1465142038711000&amp;usg=AFQjCNEfxOdrOcH9uRTMQWv-pKCNqAmvxg">http://arxiv.org/abs/1507.06838</a></span><span class="c1">&nbsp;[abs:1] sex classification. combines cnn and something else for better results. should go in &quot;images/faces&quot;. comes with a dataset, should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06830&amp;sa=D&amp;ust=1465142038712000&amp;usg=AFQjCNGJidZEPrKjjARJG1VZbbVaWMXVPQ">http://arxiv.org/abs/1511.06830</a></span><span class="c1">&nbsp;[abs:2] interesting new dataset for separating general features from fine-grained detail: pascal detail and fashionista detail. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06462&amp;sa=D&amp;ust=1465142038713000&amp;usg=AFQjCNHruuuvOuLf6gHoHa43TYG-n5REjg">http://arxiv.org/abs/1602.06462</a></span><span class="c1">&nbsp;[abs:1] paper on whether the singularity will happen. should go in &quot;safety&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06484&amp;sa=D&amp;ust=1465142038713000&amp;usg=AFQjCNF5bKAWM8ln6Z7zs463FpVrJ38wjg">http://arxiv.org/abs/1602.06484</a></span><span class="c1">&nbsp;[abs:2] paper on the roadmap towards machines being able to understand and react in pro-social ways to stories. should go in &quot;safety&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06566&amp;sa=D&amp;ust=1465142038714000&amp;usg=AFQjCNE7k8UvGdChCpxqpnvMPaKVvcpVhA">http://arxiv.org/abs/1602.06566</a></span><span class="c1">&nbsp;[abs:0] non-neural &quot;storytelling algorithms&quot; thing. should go in &quot;datasets&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06359&amp;sa=D&amp;ust=1465142038715000&amp;usg=AFQjCNHRjc8enJmMFqjqhJvEjjtmi-VRtg">http://arxiv.org/abs/1602.06359</a></span><span class="c1">&nbsp;[abs:1] text similarity detection with convnets. should have just used a siamese network, idiots. should go in &quot;language/misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06797&amp;sa=D&amp;ust=1465142038716000&amp;usg=AFQjCNEAvlVO5z5YaoQLekIm9Fve3SMMpw">http://arxiv.org/abs/1602.06797</a></span><span class="c1">&nbsp;[abs:3] neural-network based clustering with a small amount of training data and k-means. should go in &quot;unsupervised/clustering&quot; and &quot;semi-supervised&quot;. they use it to learn representation for text snippets.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06564&amp;sa=D&amp;ust=1465142038717000&amp;usg=AFQjCNF3qooHAZ9W-XbP1HTHsPcJw36Qyg">http://arxiv.org/abs/1602.06564</a></span><span class="c1">&nbsp;[abs:1] convnets to extract buildings from aerial images. should go in &quot;images&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06697&amp;sa=D&amp;ust=1465142038718000&amp;usg=AFQjCNF62_bd8MEQPVKbq6eaAmwqjQWTmQ">http://arxiv.org/abs/1602.06697</a></span><span class="c1">&nbsp;[abs:0] some image retrieval thing. has hashing. whatever. should go in &quot;images/retrieval&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06667&amp;sa=D&amp;ust=1465142038719000&amp;usg=AFQjCNGIkAibjmb04U2BK2Qvcsz0--_T1g">http://arxiv.org/abs/1602.06667</a></span><span class="c1">&nbsp;[abs:2] non-neural, &quot;Motion Planning Strategies for Autonomously Mapping 3D Structures&quot;. should go in &quot;rl&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06727&amp;sa=D&amp;ust=1465142038720000&amp;usg=AFQjCNE11OMEz4KMCC8fzJPwUpVcFt_c7A">http://arxiv.org/abs/1602.06727</a></span><span class="c1">&nbsp;[abs:3] some thing that improves the quality of speech synthesis with &quot;Stacked Bottleneck Features and Minimum Trajectory Error Training&quot;. should go in &quot;speech&quot;. the other things sound cool too.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.03789&amp;sa=D&amp;ust=1465142038721000&amp;usg=AFQjCNHPD0qJQBOA-_XxJtFvPNIUdRuzKg">http://arxiv.org/abs/1509.03789</a></span><span class="c1">&nbsp;[abs:0] some neuromorphic thing: &quot;Bio-Inspired Human Action Recognition using Hybrid Max-Product Neuro-Fuzzy Classifier and Quantum-Behaved PSO&quot;. should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03962&amp;sa=D&amp;ust=1465142038722000&amp;usg=AFQjCNHpcKNxGPA5jIEDAQmm2cCn-PTfrQ">http://arxiv.org/abs/1511.03962</a></span><span class="c1">&nbsp;[abs:1] language models that have multiple scales in order to keep long-term structure: &quot;Document Context Language Models&quot;. slightly better predictive quality, much better at assessing document coherence. should go in &quot;language/language models&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.00949&amp;sa=D&amp;ust=1465142038722000&amp;usg=AFQjCNE8JLfhhIeCleWRq0cp4cHoldxwjw">http://arxiv.org/abs/1503.00949</a></span><span class="c1">&nbsp;[abs:0] object localization without specifying locations, contributes &quot;multi fold&quot; thing for avoiding overconfidence. should go in &quot;images/object localization&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.03929&amp;sa=D&amp;ust=1465142038724000&amp;usg=AFQjCNFJD8G6Z-5Yfomb_s59EaRGKsYsWQ">http://arxiv.org/abs/1508.03929</a></span><span class="c1">&nbsp;[abs:1] comparison of humans to deep network representations. should go in &quot;neuromorphic&quot;, &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.01277&amp;sa=D&amp;ust=1465142038725000&amp;usg=AFQjCNFiMYHoPr9MR8vf5ZlqHd_iM8ukfw">http://arxiv.org/abs/1509.01277</a></span><span class="c1">&nbsp;[abs:1] dataset of rgbd pose data from warehouses. should go in &quot;images/3d&quot;, &quot;datasets&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06522&amp;sa=D&amp;ust=1465142038725000&amp;usg=AFQjCNE7wQxWAexe9g9r16NrS6cjPsEkZQ">http://arxiv.org/abs/1511.06522</a></span><span class="c1">&nbsp;[abs:1] approach and dataset for classifying materials, or something. uses an odd extraction of convnet features. should go in &quot;applications&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.04438&amp;sa=D&amp;ust=1465142038726000&amp;usg=AFQjCNGLfJzftz89_HuzBQs9Q5Qr5vXibg">http://arxiv.org/abs/1509.04438</a></span><span class="c1">&nbsp;[abs:1] regex based decoder of ctc output. whatever. should go in &quot;images/ocr&quot;, used on handwriting recognition.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07032&amp;sa=D&amp;ust=1465142038727000&amp;usg=AFQjCNGbaIsAGONjA7AD6rUcdPACkqY0eA">http://arxiv.org/abs/1602.07032</a></span><span class="c1">&nbsp;[abs:0] a formal model of spaced repetition. should go in &quot;misc&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.06979&amp;sa=D&amp;ust=1465142038728000&amp;usg=AFQjCNFOjmGaPGUKcVQOUugX0ZRO3SmlRA">http://arxiv.org/abs/1602.06979</a></span><span class="c1">&nbsp;[abs:2] text topic augmented manual clustering with crowdsourcing for verification. should go in &quot;data collection&quot;, &quot;language/word embeddings&quot;, &quot;language/topic modeling&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07019&amp;sa=D&amp;ust=1465142038729000&amp;usg=AFQjCNFrNKnFMtGfWGzBSrG3N8PB3u8DKQ">http://arxiv.org/abs/1602.07019</a></span><span class="c1">&nbsp;[abs:2] answer selection and paraphrase detection with sentence vectors and cnns on them. should go in &quot;language/qa&quot;, &quot;language/word embeddings&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07017&amp;sa=D&amp;ust=1465142038730000&amp;usg=AFQjCNFbHqLhFvsxNHkL6K7xJlyfRvV79Q">http://arxiv.org/abs/1602.07017</a></span><span class="c1">&nbsp;[abs:2] survey of the sparse coding world. should go in &quot;sparsity&quot;, maaaaaybe &quot;basics&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07119&amp;sa=D&amp;ust=1465142038731000&amp;usg=AFQjCNE2zEzmSLeFylbyuw2hVcXMBQcj5w">http://arxiv.org/abs/1602.07119</a></span><span class="c1">&nbsp;[abs:3] training on all of imagenet as a pretraining for video event detection. they use a reorganized hierarchy of labels, and have custom models published to use it. should go in &quot;images&quot;, &quot;convolution&quot;, &quot;transfer learning&quot;, &quot;classification&quot;, &quot;architectures&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07188&amp;sa=D&amp;ust=1465142038732000&amp;usg=AFQjCNHS6x-sdTmuUNwYS_qIN-qm2v2LTA">http://arxiv.org/abs/1602.07188</a></span><span class="c1">&nbsp;[abs:2] &quot;Exploring the Neural Algorithm of Artistic Style&quot;. goes through some different ways of using it and tries it on some interesting things. should go in &quot;applications/art&quot;, &quot;debugging&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07029&amp;sa=D&amp;ust=1465142038733000&amp;usg=AFQjCNEYW8zjjdxP4Lcf1_DH8TbWHkS6Lg">http://arxiv.org/abs/1602.07029</a></span><span class="c1">&nbsp;[abs:1] some stuff on embedding student skill representations and predicting student success. should go in &quot;applications/teaching&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07393&amp;sa=D&amp;ust=1465142038734000&amp;usg=AFQjCNEC5VDmcTDsnY6vSLXYosAZhgGXtg">http://arxiv.org/abs/1602.07393</a></span><span class="c1">&nbsp;[abs:2] authorship classification. improvement over non-neural SOTA. should go in &quot;language/author classification&quot; or &quot;language/language models&quot;. comes with code: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://github.com/zge/authorship-attribution/&amp;sa=D&amp;ust=1465142038734000&amp;usg=AFQjCNGYUDmr5HiUBIuG8ASp0E7I89KYCQ">https://github.com/zge/authorship-attribution/</a></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07563&amp;sa=D&amp;ust=1465142038735000&amp;usg=AFQjCNHt0GLJ6HGSRs4qfjVFyDVdDZMgcg">http://arxiv.org/abs/1602.07563</a></span><span class="c1">&nbsp;[abs:1] analysis of sentiment classification on twitter: they find that it&#39;s basically solved, except that humans can&#39;t agree. should go in &quot;language/sentiment&quot; or &quot;language/text analysis&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07572&amp;sa=D&amp;ust=1465142038736000&amp;usg=AFQjCNGZwuTDgNrx7_IjKFe88SWQvAQK6Q">http://arxiv.org/abs/1602.07572</a></span><span class="c1">&nbsp;[abs:2] extreme dimensionality reduction of word vectors. should go in &quot;language/embeddings&quot;. they find that it doesn&#39;t hurt performance of the sentiment analysis thing they try (!)</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07618&amp;sa=D&amp;ust=1465142038736000&amp;usg=AFQjCNE3kEXG6nExVHmiRoxpbFVWQUnF_g">http://arxiv.org/abs/1602.07618</a></span><span class="c1">&nbsp;[abs:1] philosophy of science (I think that&#39;s what it&#39;s called) paper on reductionism being an oversimplification of a world where different things are interconnected. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07335&amp;sa=D&amp;ust=1465142038737000&amp;usg=AFQjCNHzrVsXRwPcEQ__d_6fCOf71NXG7Q">http://arxiv.org/abs/1602.07335</a></span><span class="c1">&nbsp;[abs:0] fake image detection algorithm. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07377&amp;sa=D&amp;ust=1465142038738000&amp;usg=AFQjCNECg6hZQejafwF4jRlFTtVokQ0cQA">http://arxiv.org/abs/1602.07377</a></span><span class="c1">&nbsp;[abs:0] recurrent convnets outdo rnns with hand-engineered features, what a surprise. should go in &quot;images/emotion&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07475&amp;sa=D&amp;ust=1465142038739000&amp;usg=AFQjCNF8AlvWtF6GzqjWozrHxTQ1rDlx4g">http://arxiv.org/abs/1602.07475</a></span><span class="c1">&nbsp;[abs:1] scene text detection and script detection with convnet features and a naive bayes classifier. should go in &quot;images/ocr&quot;, and &quot;datasets&quot; because it has a dataset of labeled scene text/script data.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07480&amp;sa=D&amp;ust=1465142038740000&amp;usg=AFQjCNFIHVnPHm5DeSm7w2FKvpt6ZpOT8w">http://arxiv.org/abs/1602.07480</a></span><span class="c1">&nbsp;[abs:1] another scene text thing. uses cnns with a &quot;patch based&quot; thing, because they think cnns don&#39;t work well when you have large variety in your aspect ratio. should go in &quot;images/ocr&quot;, and &quot;datasets&quot; because it also has one of those.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07320&amp;sa=D&amp;ust=1465142038741000&amp;usg=AFQjCNGaF1-PwLw7jrajHoN3GW5Mm2K4AA">http://arxiv.org/abs/1602.07320</a></span><span class="c1">&nbsp;[abs:3] analysis of loss space - they find that it&#39;s not even saddle points that are the problem, exactly, but flat regions (vanishing gradient?). &quot;</span><span class="c1 c21">evidence that apparent convergence of loss does not correspond to weights arriving at critical points, but instead to large movements through flat regions of weight space. While it&#39;s trivial to show that neural network error surfaces are globally non-convex, we show that error surfaces are also locally non-convex, even after breaking symmetry with a random initialization and also after partial training.</span><span class="c1">&quot;. should go in &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07495&amp;sa=D&amp;ust=1465142038742000&amp;usg=AFQjCNEfuUNmufgVrrpZPgHTiKpTiApFFQ">http://arxiv.org/abs/1602.07495</a></span><span class="c1">&nbsp;[abs:3] active learning approach with positive/unlabeled examples. should go in &quot;data collection&quot; or &quot;active learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07373&amp;sa=D&amp;ust=1465142038743000&amp;usg=AFQjCNEzTx1T0VaRvpr9LYl_LDlc17Ci-Q">http://arxiv.org/abs/1602.07373</a></span><span class="c1">&nbsp;[abs:0] binarized nn thing. should go in &quot;binary&quot;. rejected conference submission (!).</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07362&amp;sa=D&amp;ust=1465142038744000&amp;usg=AFQjCNHmFfka3nG07NiuVbn8hyr8gqEbWQ">http://arxiv.org/abs/1602.07362</a></span><span class="c1">&nbsp;[abs:1] analysis of differential privacy of prediction markets. shold go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07388&amp;sa=D&amp;ust=1465142038745000&amp;usg=AFQjCNGpFpmfhUuow6okSmdgSj5U30_elQ">http://arxiv.org/abs/1602.07388</a></span><span class="c1">&nbsp;[abs:1] analysis of when people select interesting things on stack exchange. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07721&amp;sa=D&amp;ust=1465142038745000&amp;usg=AFQjCNERwZwYhkfO7cX_a2o029A2gBICuA">http://arxiv.org/abs/1602.07721</a></span><span class="c1">&nbsp;[abs:2] game level design via watching gameplay videos and modeling them. cool. should go in &quot;generative&quot;, &quot;applications/procedural gen&quot;. Resubmission: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02738&amp;sa=D&amp;ust=1465142038746000&amp;usg=AFQjCNEcvMBJQ88tR1ALr-7-OBt_pXIRdg">http://arxiv.org/abs/1603.02738</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07764&amp;sa=D&amp;ust=1465142038747000&amp;usg=AFQjCNEf6Uu-LTddVIUFd6-CpNze6-gjWg">http://arxiv.org/abs/1602.07764</a></span><span class="c1">&nbsp;[abs:0] &quot;Reinforcement Learning of POMDP&#39;s using Spectral Methods&quot; &nbsp;should go in &quot;rl&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07749&amp;sa=D&amp;ust=1465142038748000&amp;usg=AFQjCNHAupD5BydEoOs4EnhGai3dpgssfw">http://arxiv.org/abs/1602.07749</a></span><span class="c1">&nbsp;[abs:1] using rnns to detect mentions of things. should go in &quot;language/language understanding&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07776&amp;sa=D&amp;ust=1465142038748000&amp;usg=AFQjCNG_a05MHtiyOR95oDumTuilhJ9Slg">http://arxiv.org/abs/1602.07776</a></span><span class="c1">&nbsp;[abs:3] &quot;We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. Experiments show that they provide better parsing in English than published supervised generative model and better language modeling than state-of-the-art RNNs.&quot; &nbsp;should go in &quot;language/parsing&quot;, &quot;language/language models&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07873&amp;sa=D&amp;ust=1465142038749000&amp;usg=AFQjCNEahNImNgu_hWlwY08AzR6TLnqe8g">http://arxiv.org/abs/1602.07873</a></span><span class="c1">&nbsp;[abs:0] application of cnns to deblurring license plates. should go in &quot;images/misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07714&amp;sa=D&amp;ust=1465142038751000&amp;usg=AFQjCNEhf5owBHrekkazRijT5GZdnW2izA">http://arxiv.org/abs/1602.07714</a></span><span class="c1">&nbsp;[abs:3] &quot;Learning functions across many orders of magnitudes&quot; - approach for regression of nns. they use it on dqns for atari and it&#39;s able to learn without the sign() on the score change. should go in &quot;rl&quot;, &quot;regression&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08007&amp;sa=D&amp;ust=1465142038752000&amp;usg=AFQjCNFQc_rPftB26aVVgjfD9kt2-Y3B6Q">http://arxiv.org/abs/1602.08007</a></span><span class="c1">&nbsp;[abs:3] application of a new training system, &quot;Riemannian Neural Networks&quot;, to real datasets; gets good results, faster convergence. should go in &quot;training algorithms&quot;, &quot;weights/constraints&quot; (I think?).</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.6071&amp;sa=D&amp;ust=1465142038753000&amp;usg=AFQjCNGVHw5vOt5z0suFcOCu8Q4vwZ_WJg">http://arxiv.org/abs/1412.6071</a></span><span class="c1">&nbsp;[abs:3] fractional max-pooling - allows maxpooling with non-integer values, allowing more control over the speed of information loss. should go in &quot;pooling&quot; or &quot;activation functions&quot; or &quot;architecture&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.09249&amp;sa=D&amp;ust=1465142038754000&amp;usg=AFQjCNGhKRXFztzPFqPOUl-6F-zcWq6cPA">http://arxiv.org/abs/1511.09249</a></span><span class="c1">&nbsp;[headings skim:3] juergen schmidhuber&#39;s world modeling thing. I remember this being posted to reddit and getting sarcasm, might be reinventing wheels. -&gt; not what people were saying! their thoughts were more like mine: &quot;cool, sounds well designed&quot;. is effectively a massive lit review plus a little. </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/3uycc2/_/&amp;sa=D&amp;ust=1465142038755000&amp;usg=AFQjCNFD4iAw1zgDu0BAPfwG45IshR7FYw">https://www.reddit.com/r/MachineLearning/comments/3uycc2/_/</a></span><span class="c1">&nbsp;should go in &quot;rl&quot;. No results in paper, from the look of it.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.6597&amp;sa=D&amp;ust=1465142038756000&amp;usg=AFQjCNFOjZ7eyJtRTcXdlh-8pg-oa15PZQ">http://arxiv.org/abs/1412.6597</a></span><span class="c1">&nbsp;[abs:1][awty] analysis of when unsupervised learning as pretraining is useful, considering it&#39;s going unused a lot. they conclude it&#39;s useful when you have a lot of unlabeled examples - and is detrimental otherwise. should go in &quot;unsupervised&quot; or &quot;semi-supervised&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://jmlr.csail.mit.edu/proceedings/papers/v28/goodfellow13.pdf&amp;sa=D&amp;ust=1465142038757000&amp;usg=AFQjCNHDyDFSRTHtdNqIHmFQv3L4vkhA4A">http://jmlr.csail.mit.edu/proceedings/papers/v28/goodfellow13.pdf</a></span><span class="c1">&nbsp;[abs++:1] maxout. should go in &quot;activations&quot;. designed to go well with dropout, should go next to it in &quot;regularization&quot;. works well, see &quot;maxout network in network&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02583&amp;sa=D&amp;ust=1465142038758000&amp;usg=AFQjCNHEXjLfPIB7GEqJKNG3oqaqn9uU6A">http://arxiv.org/abs/1511.02583</a></span><span class="c1">&nbsp;[abs:2][awty] maxout network in network: combination application. should go in &quot;aggregation&quot;, &quot;architecture&quot;. relates to maxout in &quot;activations&quot;, network in network in &quot;architecture&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.08985&amp;sa=D&amp;ust=1465142038759000&amp;usg=AFQjCNE4Er4Ho7kweU04DKA-1RT6B4Xsyw">http://arxiv.org/abs/1509.08985</a></span><span class="c1">&nbsp;[abs:1][awty] more powerful pooling: learned pooling structure. &quot;Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree&quot; - patented. should go in &quot;architecture/pooling&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.00330&amp;sa=D&amp;ust=1465142038760000&amp;usg=AFQjCNGNtGQdNngOhp2UHrH0xtq-9ZQ3rA">http://arxiv.org/abs/1508.00330</a></span><span class="c1">&nbsp;[abs:1][awty] knockoff of batch norm. doesn&#39;t appear to know it exists? lol. should go in &quot;regularization&quot; and &quot;architecture/layers&quot; anyway.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05635&amp;sa=D&amp;ust=1465142038761000&amp;usg=AFQjCNE0I0A-0MzWVsrpcSZ-NtYq9q0UIA">http://arxiv.org/abs/1511.05635</a></span><span class="c1">&nbsp;[abs:2][awty] &quot;maxout inception&quot;, basically. competition between filter sizes between the different inception sizes. should go in &quot;architectures&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02351&amp;sa=D&amp;ust=1465142038762000&amp;usg=AFQjCNEbFg_PR83Xgbr3Odfnub7YRARE9g">http://arxiv.org/abs/1506.02351</a></span><span class="c1">&nbsp;[abs:3][awty] stacked what-where autoencoders: has pooling layers that output &quot;what&quot; to more abstract layers, and &quot;where&quot; to decoder layers. has objective for decoder to be similar to encoder. this sounds more general than ladder networks. does it not work as well?? should go in &quot;autoencoders&quot;, &quot;semi-supervised&quot;, &quot;architecture&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.6830&amp;sa=D&amp;ust=1465142038763000&amp;usg=AFQjCNGNSixjwu4sM9ZbPzYV22dxD6NcjQ">http://arxiv.org/abs/1412.6830</a></span><span class="c1">&nbsp;[abs:1][awty] piecewise learned activation functions or something. sota on several datasets, it says. should go in &quot;activations&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1409.6070&amp;sa=D&amp;ust=1465142038764000&amp;usg=AFQjCNFIn0HCnipciIojFD5c4YZ908eUVA">http://arxiv.org/abs/1409.6070</a></span><span class="c1">&nbsp;[abs:2][awty] spatial sparsity for convnets - much greater efficiency by making a very high resolution, oversampled image. should go in &quot;architectures&quot;, &quot;sparsity&quot;. cool.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.6806&amp;sa=D&amp;ust=1465142038765000&amp;usg=AFQjCNFgusxBsgIh2nCt7ssGUaVTi4j8kg">http://arxiv.org/abs/1412.6806</a></span><span class="c1">&nbsp;[abs:4][awty] the all-convolutional net: no maxpooling, no fully connected layers. whoa. should go in &quot;pooling&quot;, &quot;convolution&quot;, &quot;architectures&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.04557&amp;sa=D&amp;ust=1465142038766000&amp;usg=AFQjCNFEoE6ESman7eeS38hZxnCyrQGQgw">http://arxiv.org/abs/1506.04557</a></span><span class="c1">&nbsp;[abs:3] &quot;Learning Deep Generative Models with Doubly Stochastic MCMC&quot; - should go in &quot;bayesian neural networks&quot; and/or &quot;training algorithms&quot;. claims new sota.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04484&amp;sa=D&amp;ust=1465142038767000&amp;usg=AFQjCNFTRlhbXB4H39TgUVXhYrohOkPS0A">http://arxiv.org/abs/1602.04484</a></span><span class="c1">&nbsp;[abs:3] analysis of inductive bias of dropout vs weight decay. has demonstrations of what they can&#39;t learn via noise sources they can&#39;t model. should go in &quot;regularization&quot;, &quot;theory&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02041&amp;sa=D&amp;ust=1465142038768000&amp;usg=AFQjCNHwI6QEClKp6P8oXZKXIvoYC6GF1w">http://arxiv.org/abs/1603.02041</a></span><span class="c1">&nbsp;[abs:2] multi-task reinforcement learning with swapped policies and value functions but the same environment representation. should go in &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.0035&amp;sa=D&amp;ust=1465142038769000&amp;usg=AFQjCNET3vB3oA_0XRJwHx3CxIJPMjHwng">http://arxiv.org/abs/1412.0035</a></span><span class="c1">&nbsp;[abs:1] analysis of what different image features retain about the image, via learning an inversion of the model. should go in &quot;debugging&quot;, &quot;generative&quot;, &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01913&amp;sa=D&amp;ust=1465142038769000&amp;usg=AFQjCNGYTLa3HP-C2OseY8p8_ujBKgShtQ">http://arxiv.org/abs/1603.01913</a></span><span class="c1">&nbsp;[abs:2] &quot;A Latent Variable Recurrent Neural Network for Discourse Relation Language Models&quot; - per-sentence prediction (I think?) via a &quot;discourse model&quot; that predicts words from meaning vectors. should go in &quot;language/language models&quot;, &quot;language/text understanding&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01801&amp;sa=D&amp;ust=1465142038770000&amp;usg=AFQjCNGjkbonQXvLGL_oRqzDAnxQq9KWoQ">http://arxiv.org/abs/1603.01801</a></span><span class="c1">&nbsp;[abs:1] interesting generative thing that uses a &quot;deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from the attributes alone to be &#39;close&#39; to the joint representation obtained from both face and attributes&quot;. should go in &quot;unsupervised&quot;, &quot;alternate deep techniques&quot;, &quot;bayesian neural networks&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01976&amp;sa=D&amp;ust=1465142038771000&amp;usg=AFQjCNHM36aAsjOaZvkuH7HP_ZbtCeAgLQ">http://arxiv.org/abs/1603.01976</a></span><span class="c1">&nbsp;[abs:0] &quot;Deep Contrast Learning for Salient Object Detection&quot;. should go in &quot;images/salience&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02003&amp;sa=D&amp;ust=1465142038772000&amp;usg=AFQjCNEGh0v8I9icGF1TKi6OUjyyU23OTA">http://arxiv.org/abs/1603.02003</a></span><span class="c1">&nbsp;[abs:1] &quot;Our network is a modified variational autoencoder (VAE) that supports supervised training of single-image analogies and in-network evaluation of outputs with a structured similarity objective that captures pixel covariances.&quot;. should go in &quot;unsupervised&quot;, &quot;applications/art&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02211&amp;sa=D&amp;ust=1465142038773000&amp;usg=AFQjCNGsN693o5ZjyzAn_Eu_22t79-_vvQ">http://arxiv.org/abs/1603.02211</a></span><span class="c1">&nbsp;[abs:0] amusing: authenticating based on arm accelerometer patterns. should go in &quot;misc&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01670&amp;sa=D&amp;ust=1465142038774000&amp;usg=AFQjCNGF4shrUGeredA93McJfvWgu7voDQ">http://arxiv.org/abs/1603.01670</a></span><span class="c1">&nbsp;[abs:3] &quot;Network Morphism&quot;: very high-power transfer learning: converting from one network directly to another. should go in &quot;transfer learning&quot;, maybe &quot;architecture&quot; I guess lol doesn&#39;t really fit in</span></p><p class="c3"><span class="c1"></span></p><h5 class="c5" id="h.konpcyxvrwat"><span>Mar 10</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02078&amp;sa=D&amp;ust=1465142038775000&amp;usg=AFQjCNEIpWkihnFQPe1Xdkxcy7ZPlcaimg">http://arxiv.org/abs/1603.02078</a></span><span class="c1">&nbsp;[abs:0] temporal pooling things for hand-designed features. should go in &quot;images/video&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02199&amp;sa=D&amp;ust=1465142038776000&amp;usg=AFQjCNGCQjmGhDkDqs7l--CDTK89KZwXKg">http://arxiv.org/abs/1603.02199</a></span><span class="c1">&nbsp;[abs:1] &quot;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&quot;. should go in &quot;applications/robots&quot; or &quot;rl/robots&quot; or &quot;robots&quot; or something. kinda meh, no dataset. Comes with a blog post: </span><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://googleresearch.blogspot.ca/2016/03/deep-learning-for-robots-learning-from.html&amp;sa=D&amp;ust=1465142038777000&amp;usg=AFQjCNHWgGv5R9UVQKxRagRMgRbWeQda_Q">http://googleresearch.blogspot.ca/2016/03/deep-learning-for-robots-learning-from.html</a></span><span class="c1">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02010&amp;sa=D&amp;ust=1465142038778000&amp;usg=AFQjCNEgpqXOmuWfvRB9R-S43w18tRLeMA">http://arxiv.org/abs/1603.02010</a></span><span class="c1">&nbsp;[abs:2] &quot;We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples.&quot;. should go in &quot;security&quot;/&quot;privacy&quot;, &quot;rl&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01770&amp;sa=D&amp;ust=1465142038779000&amp;usg=AFQjCNEaDQFwuS6g0ViMLn20wpeuKgTXtg">http://arxiv.org/abs/1603.01770</a></span><span class="c1">&nbsp;[abs:0] procedural music gen thing. cool. not neural. should go in &quot;misc&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02238&amp;sa=D&amp;ust=1465142038780000&amp;usg=AFQjCNEUtt-R5PsVoUoQwlb4ZGZQ-tnAtA">http://arxiv.org/abs/1603.02238</a></span><span class="c1">&nbsp;[abs:0] strange thing comparing synapses to functional programming. should go in &quot;neuroscience&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.00468&amp;sa=D&amp;ust=1465142038781000&amp;usg=AFQjCNE_uXc2SQhCzI8uWF0QT6riD_Vy3A">http://arxiv.org/abs/1505.00468</a></span><span class="c1">&nbsp;[abs:2] visual q/a dataset. should go in &quot;datasets&quot;, &quot;images/qa&quot;, &quot;language/qa&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02514&amp;sa=D&amp;ust=1465142038781000&amp;usg=AFQjCNFybLfYMOHerzFJvG4c2_zx4L9AmQ">http://arxiv.org/abs/1603.02514</a></span><span class="c1">&nbsp;[abs:2] semi-supervised learning on sequences. either applies or improves on &quot;generative modeling for semi-supervised learning&quot; or whatever it&#39;s called. should go in &quot;unsupervised/partially supervised training&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.06579&amp;sa=D&amp;ust=1465142038782000&amp;usg=AFQjCNFdALp7v4SDh0uNglrUsOv1ypRi8w">http://arxiv.org/abs/1506.06579</a></span><span class="c1">&nbsp;[abs:0][cited] application of deepdream to visualizing individual filters, is this literally just deepdream? should go in &quot;debugging&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02518&amp;sa=D&amp;ust=1465142038783000&amp;usg=AFQjCNEt7PGB6UPQfzUN8Ndqqru6hXRipQ">http://arxiv.org/abs/1603.02518</a></span><span class="c1">&nbsp;[skim:4] debugging tool that can point to what parts of an input were used or unused. should go in &quot;debugging&quot; and &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02636&amp;sa=D&amp;ust=1465142038785000&amp;usg=AFQjCNGljj6uwouWHg53xcJFP9rQIKE2Ig">http://arxiv.org/abs/1603.02636</a></span><span class="c1">&nbsp;[abs:1] depth-data image recognition with convnets. should go in &quot;images/3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.07947&amp;sa=D&amp;ust=1465142038786000&amp;usg=AFQjCNECbZOI7ANFQlgo3lpR_dywoIlZ-A">http://arxiv.org/abs/1504.07947</a></span><span class="c1">&nbsp;[abs:0] patch-based cnns for analyzing medical data that&#39;s too big to fit in single training shot. should go in &quot;bio&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.6387&amp;sa=D&amp;ust=1465142038787000&amp;usg=AFQjCNE0fqsFNinGEVpd-LfF1_3h4SkQQg">http://arxiv.org/abs/1411.6387</a></span><span class="c1">&nbsp;[abs:1] estimating depth maps from only one image. whoa! it actually mostly works! should go in &quot;images/3d&quot;.</span></p><p class="c3"><span class="c1"></span></p><h5 class="c5" id="h.jr3ys7re05xx"><span>Mar 11</span></h5><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://www.di.unipi.it/~ruggieri/Papers/ker.pdf&amp;sa=D&amp;ust=1465142038789000&amp;usg=AFQjCNFXEHn6PttnS_mQSns-GrlqQ48P3A">http://www.di.unipi.it/~ruggieri/Papers/ker.pdf</a></span><span class="c1">&nbsp;[abs:1][recommended] paper on discrimination analysis. Kind of tangentially related, should go in &quot;safety&quot;. DOI: 10.1017/S000000000000000</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://dl.acm.org/citation.cfm?id%3D2090255&amp;sa=D&amp;ust=1465142038790000&amp;usg=AFQjCNFPg9byRzhKF_FO8ScBbIht9xsbKA">http://dl.acm.org/citation.cfm?id=2090255</a></span><span class="c1">&nbsp;[abs:2][recommended] paper on how to train a classifier without being discriminatory. Should go in &quot;safety&quot;. &quot;Fairness through awareness&quot;, ITCS &#39;12 Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, Pages 214-226, ACM, ISBN: 978-1-4503-1115-1 doi&gt;10.1145/2090236.2090255</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber%3D4909197&amp;sa=D&amp;ust=1465142038790000&amp;usg=AFQjCNGYL6XaJCmB_x4P7QLkv58AlX0VDA">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4909197</a></span><span class="c1">&nbsp;[title:0][recommended] &quot;</span></p><p class="c6"><span class="c1">Classifying without discriminating&quot; - should go in &quot;safety&quot;. DOI:10.1109/IC4.2009.4909197</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c4 c1"><a class="c0" href="https://www.google.com/url?q=https://www.semanticscholar.org/paper/Measuring-Discrimination-in-Socially-Sensitive-Pedreschi-Ruggieri/895ae730ca9466eede99a910800efae7c09ddb36&amp;sa=D&amp;ust=1465142038792000&amp;usg=AFQjCNFO5Ar-E74LjFktgy6zodV4PCwX2A">https://www.semanticscholar.org/paper/Measuring-Discrimination-in-Socially-Sensitive-Pedreschi-Ruggieri/895ae730ca9466eede99a910800efae7c09ddb36</a></span><span class="c1">&nbsp;[title:0][recommended] &quot;Measuring Discrimination in Socially-Sensitive Decision Records&quot;, should go in &quot;safety&quot;</span></p><p class="c3"><span class="c1"></span></p><h5 class="c5" id="h.j92j2otlxvl2"><span>Mar 12</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03116&amp;sa=D&amp;ust=1465142038793000&amp;usg=AFQjCNEqZqJAcd7sTTLj6MiehmcYKSVZPQ">http://arxiv.org/abs/1603.03116</a></span><span>&nbsp;[abs:3] weight representation thing - &quot;Low-rank passthrough neural networks&quot;; based on the observation that time skipping is important but that this doesn&#39;t help with scaling memory, they use a low-rank representation of the memory update phases so you can make it bigger for free. Should go in &quot;weight&quot;, &quot;recurrence&quot;, &quot;memory&quot; - reddit: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/49yjry/_/&amp;sa=D&amp;ust=1465142038794000&amp;usg=AFQjCNFLbsl0urXZ_m4fffPDaY5lS84M-A">https://www.reddit.com/r/MachineLearning/comments/49yjry/_/</a></span><span>&nbsp;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/zomux/deepy&amp;sa=D&amp;ust=1465142038794000&amp;usg=AFQjCNFMBpzOgGqXHdQ5M6hvdsDbTlFFyA">https://github.com/zomux/deepy</a></span><span>&nbsp;[readme:1] library for theano that looks pretty easy to use. Should go in &quot;libraries&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1112.5309&amp;sa=D&amp;ust=1465142038795000&amp;usg=AFQjCNHnstJnQfxkZ53mfH2TdgZm-tmJEw">http://arxiv.org/abs/1112.5309</a></span><span>&nbsp;[abs:0] juergen schmidhuber&#39;s active learning thing that searches for unsolveable problems. Should go in &quot;data collection&quot;/&quot;active learning&quot;. Was laughed at on reddit.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1312.6034&amp;sa=D&amp;ust=1465142038796000&amp;usg=AFQjCNFNG_oFx3oK6bp5kAu75o16II3a1Q">http://arxiv.org/abs/1312.6034</a></span><span>&nbsp;[abs:0] &quot;Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps&quot; - some vis stuff based on gradients; weakly supervised object segmentation via the same; theoretical connection between gradient-based vis and inverted convolution (&quot;deconvolution&quot;). Should go in &quot;debugging&quot;. Note: old.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1405.1380&amp;sa=D&amp;ust=1465142038797000&amp;usg=AFQjCNEPs2BC9zquxdmxcJA91DPLykOBCw">http://arxiv.org/abs/1405.1380</a></span><span>&nbsp;[abs:0] &quot;Is Joint Training Better for Deep Auto-Encoders?&quot; - one of the papers that moved us from layerwise training to joint training. Jointly trains stacked autoencoders, which effectively means it&#39;s a deep autoencoder with per-layer forward/backward similarity regularization. Should go in &quot;unsupervised&quot; maybe, or &quot;historical&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.04343&amp;sa=D&amp;ust=1465142038798000&amp;usg=AFQjCNFnnL2E8nt3aewtEMAttnoWZ0MMyw">http://arxiv.org/abs/1504.04343</a></span><span>&nbsp;[abs:0] caffe con troll - shitty &quot;speeding up caffe&quot; paper. It kinda works, I guess. They claim that a haswell intel cpu has 1.2 TFLOPS?? Should go in &quot;speed&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.05508&amp;sa=D&amp;ust=1465142038798000&amp;usg=AFQjCNFNnDGv-vwcYEak1SEtq1Qk8rC7Rg">http://arxiv.org/abs/1508.05508</a></span><span>&nbsp;[abs:2] &quot;Towards Neural Network-based Reasoning&quot; - paper on reasoning systems, with some sort of multi-fact lookup and pooling between it or something. Should go in &quot;reasoning&quot; or &quot;logic&quot; or &quot;symbolic&quot;, maybe &quot;generality&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.05128&amp;sa=D&amp;ust=1465142038799000&amp;usg=AFQjCNEgCb6z91tWj7n1jgWqhEfuC6tPSg">http://arxiv.org/abs/1508.05128</a></span><span>&nbsp;[partial first pass:4] building &quot;lifted&quot; neural networks out of logical relationship statements. &quot;Lifted Relational Neural Networks&quot;. Should go in &quot;generality&quot;, &quot;reasoning&quot;/&quot;logic&quot;, &quot;model sub-selection&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.06615&amp;sa=D&amp;ust=1465142038800000&amp;usg=AFQjCNHT_TNSbTXz4dVgXj_zhwAq_7_h6w">http://arxiv.org/abs/1508.06615</a></span><span>&nbsp;[abs:2] building language models out of character-level input but word-level output. Architecture is a cnn over character embeddings (I think - maybe it&#39;s a cnn over the one-hot vectors themselves?) and a highway network (presumably in parallel?) that feed to an lstm (that presumably is allowed to not give output sometimes or something?). Gets a reduction in model parameters but not in error. Should go in &quot;language/language models&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.00657&amp;sa=D&amp;ust=1465142038802000&amp;usg=AFQjCNG7MUUr-NVPzyjZ6yExK6UKSRYGmQ">http://arxiv.org/abs/1508.00657</a></span><span>&nbsp;[abs:0] some thing about parsing complex languages with rnns. Should go in &quot;language/text analysis&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1402.3337&amp;sa=D&amp;ust=1465142038802000&amp;usg=AFQjCNFq5ccQ_gBzn4f_T4sTpuzd1KP0dg">http://arxiv.org/abs/1402.3337</a></span><span>&nbsp;[abs:1] paper on sparsity in autoencoders interfering with the learning of high-dimensionality hidden representations. They suggest an alternate activation function that makes the sparsity easier (I think?) thereby making the distribution more reasonable. Should go in &quot;unsupervised&quot;. Note: old.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.07096&amp;sa=D&amp;ust=1465142038803000&amp;usg=AFQjCNFOTpiIKvVU6HLZAzZ5YgpKNumoOQ">http://arxiv.org/abs/1508.07096</a></span><span>&nbsp;[abs:0] meh; unsupervised parallel training thing with dropout. Claims SOTA on mnist, lol. Should go in &quot;parallelization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.7028&amp;sa=D&amp;ust=1465142038804000&amp;usg=AFQjCNGxbi-O8iaglksioBE8El_-koi-1g">http://arxiv.org/abs/1412.7028</a></span><span>&nbsp;[abs:1] old paper on parsing with rnns. &quot;Joint RNN-Based Greedy Parsing and Word Composition&quot; - should go in &quot;language/text analysis&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.06585&amp;sa=D&amp;ust=1465142038805000&amp;usg=AFQjCNFVNtlO8LAMTD0YqAqEPFn8eRdcAQ">http://arxiv.org/abs/1508.06585</a></span><span>&nbsp;[abs:1] paper on a new type of unsupervised/semi-supervised learning: &quot;gibbs machines&quot; - a kind of variational autoencoder that is conducive to incrementally learning moving targets. Should go in &quot;unsupervised&quot; and/or &quot;semi-supervised&quot;, and &quot;transfer learning&quot;/&quot;knowledge sharing&quot;. Looks a bit meh maybe, they report mnist results.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.04156&amp;sa=D&amp;ust=1465142038805000&amp;usg=AFQjCNFv2XoWe-jD_8ppL8mZl4pVGYruuA">http://arxiv.org/abs/1502.04156</a></span><span>&nbsp;[abs:0] bengio&#39;s older paper on bio backprop. Should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1406.1831&amp;sa=D&amp;ust=1465142038806000&amp;usg=AFQjCNG31iQWGUxQ011ILF6-CzNhr55mCg">http://arxiv.org/abs/1406.1831</a></span><span>&nbsp;[abs:1] analysis of different kinds of noise in nns. Starts with single layer networks, works its way towards deep ones; compares dropout to different kinds of noise you can inject. Finds that it helps, of course, at regularizing - monte carlo bayesian neural networks, anyone? Should go in &quot;theory&quot;/&quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/paper/5551-deep-recursive-neural-networks-for-compositionality-in-language.pdf&amp;sa=D&amp;ust=1465142038807000&amp;usg=AFQjCNGXVmoK_6acnYnkHOaysbZ81Budrw">http://papers.nips.cc/paper/5551-deep-recursive-neural-networks-for-compositionality-in-language.pdf</a></span><span>&nbsp;[abs:0][2014] recursive networks work better when deep. Should go in &quot;obvious&quot;, &quot;recurrence&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.01745&amp;sa=D&amp;ust=1465142038808000&amp;usg=AFQjCNFQ7MHpDKMoPIIa7bhB-gtK1Iudsg">http://arxiv.org/abs/1508.01745</a></span><span>&nbsp;[abs:3] cool! Semantically conditioned language generation - given a meaning, generate a sentence with it. Greatly outperforms the non-nn-based approaches to this. Should go in &quot;language/talking computers&quot;. If the dataset in this is available, then it&#39;s a great single-component transfer learning part. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1410.0718&amp;sa=D&amp;ust=1465142038809000&amp;usg=AFQjCNE0mTxqXwdcq2uGZD6Si45zDgh0DQ">http://arxiv.org/abs/1410.0718</a></span><span>&nbsp;[abs:1] paper on the quality of word embeddings - apparently the embeddings learned by translation systems are dramatically higher quality than the ones learned by same-language text prediction. This is interesting for human learning too - to understand a language, learn another one. Should go in &quot;language/embeddings&quot;. Interesting, even thought it&#39;s old.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.3714&amp;sa=D&amp;ust=1465142038810000&amp;usg=AFQjCNHJpkbDFG1ZCxHTvZsdMn85joL-FA">http://arxiv.org/abs/1412.3714</a></span><span>&nbsp;[abs:0] recursive neural networks learn something like attention, naturally. Old and fail. Should go in &quot;misc/dunno where&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.08941&amp;sa=D&amp;ust=1465142038810000&amp;usg=AFQjCNGny_SIgFREutAB_SzSKl6ULaTLAA">http://arxiv.org/abs/1506.08941</a></span><span>&nbsp;[abs:1] learning to interact with text-based adventure games with reinforcement learning, would be interesting to use a search-free approach with this on IRC. should go in &quot;rl&quot;, &quot;language/text understanding&quot;, &quot;language/talking computers&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.phontron.com/paper/neubig15wat.pdf&amp;sa=D&amp;ust=1465142038811000&amp;usg=AFQjCNGKL7BWGe1M9yMXFBi9_HOE1KUWLA">http://www.phontron.com/paper/neubig15wat.pdf</a></span><span>&nbsp;[abs:0] neural machine translation paper - neural networks make it better, or something. Should go in &quot;language/machine translation&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.00814&amp;sa=D&amp;ust=1465142038812000&amp;usg=AFQjCNERhk78ZoQsw3CL-Q6Yb8a70uoF6w">http://arxiv.org/abs/1507.00814</a></span><span>&nbsp;[abs:3] exploration in reinforcement learners! Cool! Should go in &quot;rl&quot;. They mention what they think is wrong with just using the bayesian approach to exploration, and then they go into using another neural network with a reward for exploration. Sounds like my ideas.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.01193&amp;sa=D&amp;ust=1465142038813000&amp;usg=AFQjCNFli6k6AA9UvAMJkMwCBb51-JFyFA">http://arxiv.org/abs/1507.01193</a></span><span>&nbsp;[abs:2] reading words in &quot;dependency order&quot; in a language model. Should go in &quot;language/text understanding&quot;. Note: probably attention plus variable compute time will do this, but better. This is a model assembled out of pieces. By deepmind.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://indico.io/blog/getting-started-with-mxnet/&amp;sa=D&amp;ust=1465142038814000&amp;usg=AFQjCNFiQA1W6u8-tEEwnkn3BmxRaKGsTw">https://indico.io/blog/getting-started-with-mxnet/</a></span><span>&nbsp;[started:3] intro to mxnet - this looks awesome! Should go in &quot;libraries&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.sbfm9hohj8el"><span>Mar 19</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/20wh21/how_does_regularization_affect_dropout_while/cg9afo4?context%3D10&amp;sa=D&amp;ust=1465142038816000&amp;usg=AFQjCNHmvolYTiA00PcC8YqmLrNqfZIwNg">https://www.reddit.com/r/MachineLearning/comments/20wh21/how_does_regularization_affect_dropout_while/cg9afo4?context=10</a></span><span>&nbsp;[read:1]</span><span>&nbsp;interesting history about autoencoders, and dropout on them. Should go in &quot;history&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.05016&amp;sa=D&amp;ust=1465142038816000&amp;usg=AFQjCNGiyP6rSpUpi2Up13OkfXHQdaqsUg">http://arxiv.org/abs/1509.05016</a></span><span>&nbsp;[abs:2] multi-input-type model for driver action prediction; tries to predict things as early as it can, so the objective sounds interesting too. Should go in &quot;objectives&quot;, &quot;multi-modal models&quot;, &quot;cars&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/asheshjain399/ICCV2015_Brain4Cars&amp;sa=D&amp;ust=1465142038817000&amp;usg=AFQjCNEiboBsTzKunGD7n3VKiR_3_r2TLQ">https://github.com/asheshjain399/ICCV2015_Brain4Cars</a></span><span>&nbsp;[skim:4] dataset by some folks who have been doing road prediction stuff. They only license the dataset for non-commercial use, but it sounds like an awesome one to have. Doesn&#39;t require signup! Is 16gb. Should go in &quot;datasets&quot;, &quot;cars&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.00726&amp;sa=D&amp;ust=1465142038818000&amp;usg=AFQjCNETwySlX3XshYKgCaYMxh-w9PqIUg">http://arxiv.org/abs/1510.00726</a></span><span>&nbsp;[abs:1] literature survey for natural language processing in neural networks. Should go in &quot;language&quot;, &quot;basics&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1406.3284&amp;sa=D&amp;ust=1465142038819000&amp;usg=AFQjCNF7BpOVTxa6jyoojw4-WrTNFk1ZVQ">http://arxiv.org/abs/1406.3284</a></span><span>&nbsp;[abs:0] empirical research on the effectiveness of neural networks vs primate brains. Looks like absolutely horrible methodology, they can&#39;t really properly measure a primate brain to get a good result. Should go in &quot;misc&quot; or &quot;neuromorphic&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.05767&amp;sa=D&amp;ust=1465142038820000&amp;usg=AFQjCNGiA1Ahozfy9FYLeYrXSXic5H_wFA">http://arxiv.org/abs/1502.05767</a></span><span>&nbsp;[abs:0] paper that argues automatic differentiation has been incorrectly overlooked by the machine learning community, and goes into detail about how to use it. Not sure where this should go. &quot;Misc&quot;, I guess?</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.08660&amp;sa=D&amp;ust=1465142038820000&amp;usg=AFQjCNGzlLcJJJ2kEkoJCSm6rWkkm5TF3Q">http://arxiv.org/abs/1510.08660</a></span><span>&nbsp;[abs:0] simple visual tracking with attention in artificial video data. Should go in &quot;attention&quot;, &quot;video&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.1897&amp;sa=D&amp;ust=1465142038821000&amp;usg=AFQjCNFQreZ7WsAN-HImj-wImVz4TuEcOA">http://arxiv.org/abs/1412.1897</a></span><span>&nbsp;[abs:1] one of the original papers on adverserial examples. Should go in &quot;adverserial examples&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://webscope.sandbox.yahoo.com/catalog.php?datatype%3Di%26did%3D67&amp;sa=D&amp;ust=1465142038823000&amp;usg=AFQjCNGqJ8Tl-pO_gy-N8-1SpCbnx8AJXQ">https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&amp;did=67</a></span><span>&nbsp;yahoo creative commons dataset. Might be easier to get than the others? -&gt; yes! </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://aws.amazon.com/public-data-sets/multimedia-commons/&amp;sa=D&amp;ust=1465142038823000&amp;usg=AFQjCNHDLQokEgSHOGIRjIq15R_kjnRsRA">http://aws.amazon.com/public-data-sets/multimedia-commons/</a></span><span>&nbsp;- Should go in &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://jxieeducation.com/2016-03-13/Evaluating-Word-Embeddings/&amp;sa=D&amp;ust=1465142038824000&amp;usg=AFQjCNHLC-2QmRGUV155eXjpvn8QuvjYPg">http://jxieeducation.com/2016-03-13/Evaluating-Word-Embeddings/</a></span><span>&nbsp;some notes on how to evaluate word embeddings. Should go in &quot;language/embeddings&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04119&amp;sa=D&amp;ust=1465142038825000&amp;usg=AFQjCNG3Ca7xrrYlpxXhzFjrAV0kHwkeXg">http://arxiv.org/abs/1603.04119</a></span><span>&nbsp;[abs:3] whoa! Reinforcement learning to train agents to climb hills in minecraft! Not as interesting as what I wanted to do, but still interesting. Interesting framing, too. Should go in &quot;rl&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/4abfs6/yann_lecuns_comment_on_alphago_and_true_ai/d0z2qdm&amp;sa=D&amp;ust=1465142038826000&amp;usg=AFQjCNEzn2dm5PcSPlSSH615nFGE0qW5BQ">https://www.reddit.com/r/MachineLearning/comments/4abfs6/yann_lecuns_comment_on_alphago_and_true_ai/d0z2qdm</a></span><span>&nbsp;[read:3] comment proposing a structure for an RL system that can deal with the natural world. Should go in &quot;rl&quot;. Gets youagain&#39;s rnnai paper as a reply.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.cs.cmu.edu/~mfaruqui/papers/acl14-vecdemo.pdf&amp;sa=D&amp;ust=1465142038826000&amp;usg=AFQjCNEKLbsaRB-OJkw2M4T4F85ngsv3KA">http://www.cs.cmu.edu/~mfaruqui/papers/acl14-vecdemo.pdf</a></span><span>&nbsp;[abs:2] word vector black market. Should go in &quot;datasets&quot;, &quot;language/embeddings&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1211.5063&amp;sa=D&amp;ust=1465142038827000&amp;usg=AFQjCNGRmn-C8tLVQOhSenxl5RFLIrYqug">http://arxiv.org/abs/1211.5063</a></span><span>&nbsp;[abs:1] paper that proposes gradient clipping, currently taken for granted. Old, from bengio. Should go in &quot;recurrence&quot;, &quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.danielslater.net/2016/03/deep-q-learning-pong-with-tensorflow.html&amp;sa=D&amp;ust=1465142038828000&amp;usg=AFQjCNFf4n8EkUgrVJJ3mXO5jPmP0GyFxw">http://www.danielslater.net/2016/03/deep-q-learning-pong-with-tensorflow.html</a></span><span>&nbsp;[skim:3] tutorial on making a reinforcement learner on pong with python, pygame, and tensorflow. Should go in &quot;rl&quot;, &quot;basics&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03417&amp;sa=D&amp;ust=1465142038829000&amp;usg=AFQjCNHpVrKyL5U8L7q_UFuPfNnMplVpJQ">http://arxiv.org/abs/1603.03417</a></span><span>&nbsp;[abs and results:3] paper with a sort-of-autoencoder for learning to generalize a texture. Given a texture, find a representation that allows generating samples of textures like it. Should go in &quot;representation learning/generative&quot;. Reddit thread: </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/4a4itu/_/&amp;sa=D&amp;ust=1465142038829000&amp;usg=AFQjCNGZkKkiJ1Vf-2l-ALxCr3GAjQv3gA">https://www.reddit.com/r/MachineLearning/comments/4a4itu/_/</a></span><span>&nbsp;- people are commenting that they aren&#39;t producing high quality results. COMES WITH PRETRAINED MODELS! </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://github.com/DmitryUlyanov/texture_nets&amp;sa=D&amp;ust=1465142038830000&amp;usg=AFQjCNFw3C2wKoLviLu1jrh2gbBq19WQ_A">https://github.com/DmitryUlyanov/texture_nets</a></span><span>&nbsp;- should also go in &quot;pretrained&quot;. They&#39;re blowing dcgan out of the water, too.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.03790&amp;sa=D&amp;ust=1465142038831000&amp;usg=AFQjCNH2M2nl17ZGRHfPR0kpCVorfbqnIg">http://arxiv.org/abs/1508.03790</a></span><span>&nbsp;[abs:2] improved language modeling and machine translation via lstms that have gates going towards deeper layers, in addition to future ones. Should go in &quot;recurrence&quot;, &quot;language/language modeling&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.01549&amp;sa=D&amp;ust=1465142038831000&amp;usg=AFQjCNEKtak6IFl3fBITSsa9F3mJwqfOyA">http://arxiv.org/abs/1509.01549</a></span><span>&nbsp;[abs:1] machine learning to play chess - performs comparably to SOTA chess engines! Should go in &quot;rl&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1408.5882&amp;sa=D&amp;ust=1465142038832000&amp;usg=AFQjCNEOjCOkxW2h3-ZlkgE66ZVz3I-OkQ">http://arxiv.org/abs/1408.5882</a></span><span>&nbsp;[abs:2] one of the earliest examples of using convnets on word vectors. Should go in &quot;language models&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf&amp;sa=D&amp;ust=1465142038833000&amp;usg=AFQjCNEg3yXoYXrk8PyYpdtuuEpPVPs-9g">http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf</a></span><span>&nbsp;[abs:2] paper on random hyperparameter search actually being better than grid search. Should go in &quot;hyperparameters&quot;, &quot;meta&quot;. This is still true, and is pretty important.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.04788&amp;sa=D&amp;ust=1465142038834000&amp;usg=AFQjCNF_oC7tK6iK3HCAT6LoUdAOKUF-mA">http://arxiv.org/abs/1504.04788</a></span><span>&nbsp;[abs:3] interesting training-time approach for reducing the memory use of an nn: randomly share weights between different connections. Slightly reduces performance, but is a good tradeoff. Should go in &quot;speed&quot; and &quot;model compression&quot;, as well as &quot;weight representation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.7753&amp;sa=D&amp;ust=1465142038834000&amp;usg=AFQjCNFqBDR27XhCisOqQ-s8r-u78lyoUw">http://arxiv.org/abs/1412.7753</a></span><span>&nbsp;[abs and headings:3] older paper on compressing the weight representation for rnns to make them store memory better; closely related to clockwork rnns. Should go in &quot;recurrence&quot;, &quot;model compression&quot;, &quot;weight representation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.07583&amp;sa=D&amp;ust=1465142038835000&amp;usg=AFQjCNHdN9jjjaqE1sMYl5rVuK-Ney86Xw">http://arxiv.org/abs/1507.07583</a></span><span>&nbsp;[abs:2] another paper relating cnns to tree techniques - random forests. Should go in &quot;alternate deep techniques&quot;, &quot;images/per-pixel labeling&quot;, &quot;classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06825&amp;sa=D&amp;ust=1465142038837000&amp;usg=AFQjCNFcRPSK570vs57RsjcLttzZ9q3l-g">http://arxiv.org/abs/1509.06825</a></span><span>&nbsp;[abs:2] robot thing where they do data collection to learn to grasp objects. They build a pretty big dataset this way (though I&#39;d worry that it wouldn&#39;t vary over important natural dimensions such as lighting and background). Should go in &quot;robots&quot;, &quot;rl&quot;, maybe &quot;images&quot;, &quot;data collection&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1409.4842.pdf&amp;sa=D&amp;ust=1465142038837000&amp;usg=AFQjCNHyj1WwBHAHzl0X7mOSIn0qou9Tlg">http://arxiv.org/pdf/1409.4842.pdf</a></span><span>&nbsp;[abs:1] original googlenet paper. Should go in &quot;images/classification&quot; and &quot;architectures&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1409.1556.pdf&amp;sa=D&amp;ust=1465142038838000&amp;usg=AFQjCNEivUXhg6L7DDZQBVOPmkqK9oA1aA">http://arxiv.org/pdf/1409.1556.pdf</a></span><span>&nbsp;[abs:1] original vgg paper. Should go in &quot;images/classification&quot; and &quot;architectures&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&amp;sa=D&amp;ust=1465142038839000&amp;usg=AFQjCNHbEzueDdexrBiMNwGAgMYrqRGwQA">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></span><span>&nbsp;[abs:0] original alexnet paper. Should go in &quot;history&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.7wt53mgame35"><span>Mar 22</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05279&amp;sa=D&amp;ust=1465142038840000&amp;usg=AFQjCNGbq4K9P33XjW_OQZwHwIpXvsd_zA">http://arxiv.org/abs/1603.05279</a></span><span>&nbsp;[started:4] absolutely massive speedup via binary weights. Approximately 60x faster forward pass, and it looks like it might be faster in training, too. And it gets within 2% of SOTA on imagenet!!! This is the real thing, folks! Should go in &quot;speed&quot;, &quot;model compression&quot;, &quot;binary&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1412.1123.pdf&amp;sa=D&amp;ust=1465142038841000&amp;usg=AFQjCNHDrsqltaWVBJ6TD6paCCvHlccZpA">http://arxiv.org/pdf/1412.1123.pdf</a></span><span>&nbsp;[skim:0] stupid paper about edge detection. Should go in &quot;images/per-pixel&quot;, or something.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1506.06981.pdf&amp;sa=D&amp;ust=1465142038842000&amp;usg=AFQjCNEz-P-gtCM0H-OE3_QD4LmWlySTVA">http://arxiv.org/pdf/1506.06981.pdf</a></span><span>&nbsp;[abs:1] one of the localization-without-iterative-region-proposal papers. Should go in &quot;images/detection&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.3128&amp;sa=D&amp;ust=1465142038842000&amp;usg=AFQjCNGFdxD6BXyNtGqyjuYCDHtt0FiTQQ">http://arxiv.org/abs/1412.3128</a></span><span>&nbsp;[abs:1] grasp detection by predicting human-labeled results. Should go in &quot;robots&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.waejkiqq03ia"><span>Mar 23</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06571&amp;sa=D&amp;ust=1465142038844000&amp;usg=AFQjCNF72YQtet1xST9O00S-oyUzOgnaRA">http://arxiv.org/abs/1603.06571</a></span><span>&nbsp;[abs:1] bayesian/variational version of word2vec, appears to work equally well. Should go in &quot;language/word embeddings&quot;, &quot;bayesian neural networks&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06679&amp;sa=D&amp;ust=1465142038844000&amp;usg=AFQjCNFa4ZGlanw5Sd8gyFxsqXENbu7jew">http://arxiv.org/abs/1603.06679</a></span><span>&nbsp;[abs:1] sentiment analysis with recursive neural networks and conditional random fields. Should go in &quot;conditional random fields&quot;, &quot;recursive&quot;, &quot;language/sentiment analysis&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06744&amp;sa=D&amp;ust=1465142038845000&amp;usg=AFQjCNHJrjS87QfC-wSVxslIYAurnd1Ilg">http://arxiv.org/abs/1603.06744</a></span><span>&nbsp;[abs:2] not really clear quite what this is doing, but it&#39;s some sort of language/code generation thing, they generate code from mixed natural language and formal specification. Should go in &quot;language&quot;, &quot;code&quot;/&quot;learning algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>&nbsp;</span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06807&amp;sa=D&amp;ust=1465142038846000&amp;usg=AFQjCNEl9HE13ADehiLCWfgDVTdDD-QWsw">http://arxiv.org/abs/1603.06807</a></span><span>&nbsp;[abs:2] generating questions from facts, from factoids from freebase. They call it a dataset, but they generated it, so that&#39;s confusing. Should go in &quot;datasets&quot;, &quot;language/question answering&quot; or &quot;language/talking computers&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06759&amp;sa=D&amp;ust=1465142038847000&amp;usg=AFQjCNGz_TxEFIYbtWADUn3Cme3XjDxDgA">http://arxiv.org/abs/1603.06759</a></span><span>&nbsp;[abs:3] non-shared convolution over filters as an alternative to network-in-network 1x1 filters. Facepalm, I totally thought of this, and then I find someone else did at the same time&hellip; should go in &quot;architectures&quot;, &quot;convolution&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06765&amp;sa=D&amp;ust=1465142038848000&amp;usg=AFQjCNEXBpuGatdHx0DsGDLe4OpiHvnsxQ">http://arxiv.org/abs/1603.06765</a></span><span>&nbsp;[abs:2] fine-grained recognition approach that uses reinforcement learning to train fully convolutional networks on image recognition. Should go in &quot;RL&quot;, &quot;images/per-pixel prediction&quot;, &quot;attention&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.pjy8eog74cj"><span>Mar 28</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07965&amp;sa=D&amp;ust=1465142038849000&amp;usg=AFQjCNEHuTPSPgEOi6_XGt904UXAPymFPQ">http://arxiv.org/abs/1603.07965</a></span><span>&nbsp;[abs:1] medical image labeling by iterating between clustering and fine-tuning on the clustered targets. Should go in &quot;unsupervised/clustering&quot;, &quot;bio&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07998&amp;sa=D&amp;ust=1465142038850000&amp;usg=AFQjCNH5HMk4gYeWbaADeHJ5fhkBcZbkfQ">http://arxiv.org/abs/1603.07998</a></span><span>&nbsp;[abs:0] paper on predicting material friction properties using cnn features in some other algorithm. Should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07846&amp;sa=D&amp;ust=1465142038851000&amp;usg=AFQjCNFtdNMYxMEMpEaDXOTKFjKjYp9UpA">http://arxiv.org/abs/1603.07846</a></span><span>&nbsp;[abs:1] new framework, &quot;signa&quot;, targetted at scalability and stuff for multimedia. Should go in &quot;frameworks&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07866&amp;sa=D&amp;ust=1465142038852000&amp;usg=AFQjCNGCiCqn3L_hFY3K7cRXQIvjbVjowA">http://arxiv.org/abs/1603.07866</a></span><span>&nbsp;[abs:1] some thing or other on &quot;echo state neural networks&quot;. Should go in &quot;alternate deep techniques&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07893&amp;sa=D&amp;ust=1465142038853000&amp;usg=AFQjCNF9xfr0qJeYS6nFYpp4MlMcHsS2nA">http://arxiv.org/abs/1603.07893</a></span><span>&nbsp;[abs:2] paper on using lstms to predict stock prices. Should go in &quot;finance&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.08308&amp;sa=D&amp;ust=1465142038854000&amp;usg=AFQjCNHma3cssQUzJjWhVtMJ58rzb1cl0A">http://arxiv.org/abs/1511.08308</a></span><span>&nbsp;[abs:2] paper on using bilstm+cnn models for named entity recognition. Outdoes non-neural SOTA. should go in &quot;language/text understanding&quot;. Sounds like they&#39;re using it to get embeddings or something?</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.01197&amp;sa=D&amp;ust=1465142038855000&amp;usg=AFQjCNFd1XYenLt0s07Izo6zuxPjpUFclQ">http://arxiv.org/abs/1505.01197</a></span><span>&nbsp;[abs:1] action classification+localization and people attributes prediction, using a version of rcnn that gets global information too. Should go in &quot;images/localization&quot;, &quot;images/video&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.03283&amp;sa=D&amp;ust=1465142038856000&amp;usg=AFQjCNGDx0UtAtBv2ZXS4edJCVC4CABETg">http://arxiv.org/abs/1510.03283</a></span><span>&nbsp;[abs:2] attentional scene text recognition, with assistance from multi-target supervised attention objectives. Should go in &quot;images/ocr&quot;, &quot;attention&quot;, &quot;architectures/multi-target&quot; or &quot;multi-modal models&quot; or something.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07695&amp;sa=D&amp;ust=1465142038857000&amp;usg=AFQjCNHLVwFD_y3kXFhB9uH-xowO_V_vtA">http://arxiv.org/abs/1603.07695</a></span><span>&nbsp;[abs:1] word vector learning that uses part of speech supervised targets (I think?) and part of speech inference (pretty sure?) to help get good vectors. Should go in &quot;language/word vectors&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07388&amp;sa=D&amp;ust=1465142038858000&amp;usg=AFQjCNH8ahRyuUSOmnSd-nVUafMXh3jA-A">http://arxiv.org/abs/1603.07388</a></span><span>&nbsp;[abs:1] face recognition using multiple inputs from different poses. They also say they use 3d rendering to render the different poses? Wat? Should go in &quot;images/faces&quot;, &quot;images/3d&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07396&amp;sa=D&amp;ust=1465142038859000&amp;usg=AFQjCNF0IymPTTg6oeSLuCliuxr-x945uw">http://arxiv.org/abs/1603.07396</a></span><span>&nbsp;[abs:1] dataset of diagrams and parse graphs of them, and an application of lstm to read the images and output parses. Should go in &quot;graphs&quot;, &quot;images&quot;, I guess.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07415&amp;sa=D&amp;ust=1465142038859000&amp;usg=AFQjCNE1US19q8k8PSi-G6dxyMFsG42M3g">http://arxiv.org/abs/1603.07415</a></span><span>&nbsp;[abs:1] odd attention paper. They use something that sounds rather a lot like spatial transformer networks, with a recurrent attention portion and a region-cnn portion. Should go in &quot;images/localization&quot;, &quot;attention&quot;, &quot;convolution&quot;, &quot;architectures&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07442&amp;sa=D&amp;ust=1465142038860000&amp;usg=AFQjCNEcUFsI5IZ2n6UGR0r4DtuLyL33MQ">http://arxiv.org/abs/1603.07442</a></span><span>&nbsp;[abs:2] generative modeling/transformation paper. This is probably the right direction to do my old voice-imitation idea. They use a real/fake loss as in GAN, and a supervised relevance target. Their example is generating pictures of clothes from pictures of people wearing them, and they mention sharing the dataset. Should go in &quot;datasets&quot;, &quot;generative&quot;, &quot;multi-modal models&quot;, &quot;machine translation&quot; maybe?, &quot;images&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07485&amp;sa=D&amp;ust=1465142038861000&amp;usg=AFQjCNFPOvGVRKTH7NzGqpT25PXvHXO_UQ">http://arxiv.org/abs/1603.07485</a></span><span>&nbsp;[abs:0] shitty-looking weak-supervision paper. Uses bounding boxes as input, has really bad english in the abstract, but claims large improvement. Should go in &quot;images/localization&quot;, &quot;semi-supervised&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07341&amp;sa=D&amp;ust=1465142038862000&amp;usg=AFQjCNHznFZneuoCMUbctkBNt7nTEo8sJA">http://arxiv.org/abs/1603.07341</a></span><span>&nbsp;[abs:1] another memristor-based devices paper. Claims a 30,000x speedup, without having constructed and tested the device. However, that&#39;s a big number, and even if it underperforms by 100x that&#39;d be a big deal, if they&#39;re running the same algorithms otherwise. Should go in &quot;hardware&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07400&amp;sa=D&amp;ust=1465142038862000&amp;usg=AFQjCNF8bVAtl2IcrUdje1Dl-Tu3DRk0-Q">http://arxiv.org/abs/1603.07400</a></span><span>&nbsp;[abs:1] another low-power, high speed memristor device. Quality of the paper unclear, bad english in the abstract but it might be from israel, where intel&#39;s dev teams are, and if it&#39;s from them then it&#39;s serious news. Should go in &quot;hardware&quot;. They claim &quot;four to six orders of magnitude&quot; should be expected. </span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07454&amp;sa=D&amp;ust=1465142038863000&amp;usg=AFQjCNHnSwJCXSfpZ20vekcAihQr8l4o9w">http://arxiv.org/abs/1603.07454</a></span><span>&nbsp;[abs:0] some ensemble thing used in physics. Looks kinda cool, mentions using some sort of training algorithm controlled by a neural net. Should go in &quot;architectures&quot;, &quot;applications/physics&quot;, &quot;training algorithms/nn-controlled&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07646&amp;sa=D&amp;ust=1465142038864000&amp;usg=AFQjCNEL0jK0fRq0eTNiuCnxMnAFpEXulQ">http://arxiv.org/abs/1603.07646</a></span><span>&nbsp;[abs:1] recursive network for learning vector representations of tag data, which is pretty sparse. Should go in &quot;language/embeddings&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07771&amp;sa=D&amp;ust=1465142038864000&amp;usg=AFQjCNGinEeIKUoTbkQt3eI4ed-sDK3adw">http://arxiv.org/abs/1603.07771</a></span><span>&nbsp;[abs:3] condition language generation that scales is their tagline. they generate biographies, conditional on some sort of metadata. Massively outperforms the non-neural benchmark. Also comes with a dataset sub-selected from wikipedia. Should go in &quot;datasets&quot;, &quot;language/talking computers&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07954&amp;sa=D&amp;ust=1465142038865000&amp;usg=AFQjCNGtZkzm-SJT41gwvf7_1agwkiA3fg">http://arxiv.org/abs/1603.07954</a></span><span>&nbsp;[abs:4] reinforcement-learning-based knowledge extraction from documents, where the learner is allowed to search externally!! Tried on a database of shooting incidents (which is public). Should go in &quot;datasets&quot;, &quot;rl&quot;. Marked 4 because this is going in the direction of being the kind of rl application that is very dangerous.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07763&amp;sa=D&amp;ust=1465142038866000&amp;usg=AFQjCNFlhFovAgujyDom_t6Z3yo8zucnAg">http://arxiv.org/abs/1603.07763</a></span><span>&nbsp;[abs:0] graphics thing that predicts pose non-neurally. Notable because it outperforms neural networks. Should go in &quot;non-neural&quot;, maybe &quot;images/3d&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07772&amp;sa=D&amp;ust=1465142038867000&amp;usg=AFQjCNF-xpVDWS-Nv9kOywWjz0nrL-G1Zg">http://arxiv.org/abs/1603.07772</a></span><span>&nbsp;[abs:1] lstm-based action recognition given skeleton pose information as input. Not sure how they get that from video, though, probably some other model. Should go in &quot;images/video&quot;, &quot;recurrence&quot;. They also mention regularization, so &quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07810&amp;sa=D&amp;ust=1465142038868000&amp;usg=AFQjCNFDdl645RzGarhbT66q19a1R7Fwhw">http://arxiv.org/abs/1603.07810</a></span><span>&nbsp;[abs:2] confusing - sounds like a paper from a subfield I don&#39;t know about maybe - uses &quot;triplet networks&quot; to learn multiple subspaces of concepts, and then uses attention over these to solve some problem or other. Should go in &quot;images/embeddings&quot;, &quot;multi-modal models&quot;, &quot;graphical models&quot; (maybe?), &quot;unsupervised&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07823&amp;sa=D&amp;ust=1465142038868000&amp;usg=AFQjCNH8gdvFeg1ReRD4DVyx5qsllSFgTw">http://arxiv.org/abs/1603.07823</a></span><span>&nbsp;[abs:1] non-neural sketch face recognition, technique looks boring. Comes with a dataset of face sketches, though, which is cool. Should go in &quot;datasets&quot;. I downloaded this dataset to my laptop, it&#39;s 100mb.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07834&amp;sa=D&amp;ust=1465142038869000&amp;usg=AFQjCNHINwlin-FnlK92qlOG_ruIQ5syKQ">http://arxiv.org/abs/1603.07834</a></span><span>&nbsp;[abs:1] attentional autoencoder thing? &quot;Selective autoencoder&quot;. Looks rather interesting. Focused on detecting pathogens in microscope images (for pest control, heh). Should go in &quot;bio&quot;, &quot;semi-supervised&quot;, &quot;autoencoders&quot;, &quot;attention&quot;. Looks like an odd architecture, might be interesting.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07839&amp;sa=D&amp;ust=1465142038870000&amp;usg=AFQjCNEmAIunmKwPKGVUpOIBFLz2YAUoqQ">http://arxiv.org/abs/1603.07839</a></span><span>&nbsp;[abs:1] another &quot;selective autoencoder&quot; thingy. Used to predict explosions before they happen using eddies in the air around them. Should go in &quot;images/video&quot;, &quot;unsupervised&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07886&amp;sa=D&amp;ust=1465142038871000&amp;usg=AFQjCNFOu7Z76sNR_GtU5hNoatO4j2e0Jw">http://arxiv.org/abs/1603.07886</a></span><span>&nbsp;[abs:0] shitty neuromorphic thing. Should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07704&amp;sa=D&amp;ust=1465142038872000&amp;usg=AFQjCNGIzUzVuTKDf4UKunh6uvBZlOZHHw">http://arxiv.org/abs/1603.07704</a></span><span>&nbsp;[abs:2] reasoning thing, using an architecture that gets two events as input and predicts probability of other events (I think? Or maybe it has an output for every possible event.) they try both a feedforward and a recurrent version on language and reasoning tasks, and it works well, and even transfer learns quickly. Should go in &quot;generality&quot;, &quot;recurrence&quot;, &quot;reasoning&quot;/&quot;logic&quot;/&quot;symbolic&quot;. They use the datasets WordNet, ConceptNet, and FreeBase.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07012&amp;sa=D&amp;ust=1465142038873000&amp;usg=AFQjCNFNHTD_9XEPl044yzkIQbaSDxu4Ag">http://arxiv.org/abs/1603.07012</a></span><span>&nbsp;[abs:1] word sense disambiguation using rnns with a dictionary to look at. Should go in &quot;language/word sense&quot; or &quot;language/embeddings&quot; or something.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07044&amp;sa=D&amp;ust=1465142038875000&amp;usg=AFQjCNFi0lGY7W6mOUF0jpva3ZO9_GRirA">http://arxiv.org/abs/1603.07044</a></span><span>&nbsp;[abs:1] question answering thing with attention. Applied to an interesting sounding community questions dataset. Should go in &quot;datasets&quot;, &quot;attention&quot;, &quot;reasoning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07252&amp;sa=D&amp;ust=1465142038876000&amp;usg=AFQjCNFEnbLZLNvhVs0MALy3kOsIYqlkMg">http://arxiv.org/abs/1603.07252</a></span><span>&nbsp;[abs:1] summarization by extracting, with attention on what to extract. Not as interesting as the encoder/decoder version, but cool I guess. Should go in &quot;language/summarization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07253&amp;sa=D&amp;ust=1465142038877000&amp;usg=AFQjCNEsppAkAbMmOoOKcaCMqR2OBREtxQ">http://arxiv.org/abs/1603.07253</a></span><span>&nbsp;[abs:2] dataset of similarity ratings by humans. Should go in &quot;datasets&quot;. Looks small.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06995&amp;sa=D&amp;ust=1465142038877000&amp;usg=AFQjCNGerL1gMUccbd8lFHl2f2bTCMYfSA">http://arxiv.org/abs/1603.06995</a></span><span>&nbsp;[abs:1] time series classification using some sort of multi-scale models or other. Should go in &quot;convolution&quot;, &quot;classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07027&amp;sa=D&amp;ust=1465142038878000&amp;usg=AFQjCNGbhVBrIRVZIpNhdH08eT-StTh4wA">http://arxiv.org/abs/1603.07027</a></span><span>&nbsp;[abs:2] multi-objective approach that deals with unbalanced labels. Should go in &quot;objectives/multi-objective&quot;, &quot;faces&quot;. Maybe &quot;knowledge sharing&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07054&amp;sa=D&amp;ust=1465142038879000&amp;usg=AFQjCNGDrUaqU1s5z3itLMnDhCumaRG0KA">http://arxiv.org/abs/1603.07054</a></span><span>&nbsp;[abs:2]</span><span class="c21">&nbsp;&quot;41,585 pedestrian samples, each of which is annotated with 72 attributes as well as viewpoints, occlusions, body parts information.&quot;</span><span>&nbsp;Should go in &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07057&amp;sa=D&amp;ust=1465142038881000&amp;usg=AFQjCNEMHzAaIMTC0xSuE562rYxE8mBqFw">http://arxiv.org/abs/1603.07057</a></span><span>&nbsp;[abs:1] face-specific data augmentation. Should go in &quot;images/faces&quot;, &quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07063&amp;sa=D&amp;ust=1465142038882000&amp;usg=AFQjCNFc3QNtC0wQtNP-4dC9OQQS7c9VPw">http://arxiv.org/abs/1603.07063</a></span><span>&nbsp;[abs:1] odd technique, lstm over graphs of image boxes, or something. Should go in &quot;graphs&quot;, &quot;recurrence&quot;, &quot;images/object parsing&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07076&amp;sa=D&amp;ust=1465142038882000&amp;usg=AFQjCNHDlGbVl0kSbXM3O_bh5KxDZVtWVA">http://arxiv.org/abs/1603.07076</a></span><span>&nbsp;[abs:2] view-invariant pose recognition; near sota on frontal, way above sota on unusual angles. Should go in &quot;images/3d&quot;, &quot;images/video&quot;, &quot;representation learning&quot;. Also mentions collecting a dataset, which might be available, so it should also go in &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07120&amp;sa=D&amp;ust=1465142038883000&amp;usg=AFQjCNGclIGMSbdtPkNgVhd2TLkhDt4wXw">http://arxiv.org/abs/1603.07120</a></span><span>&nbsp;[abs:1] rgbd action recognition. Uses autoencoders and some funky interesting stuff. Should go in &quot;autoencoders&quot;, &quot;images/video&quot;, &quot;images/3d&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07141&amp;sa=D&amp;ust=1465142038884000&amp;usg=AFQjCNFZQsG6t3OJ-UJHfL1QIDFvC8bo2w">http://arxiv.org/abs/1603.07141</a></span><span>&nbsp;[abs:2] dataset of images, text, captions, gps, popularity, etc. they also try a technique on it, with a multi-modal model that shares most of its network between the tasks. Should go in &quot;images&quot;, &quot;language/talking computers&quot;, &quot;datasets&quot;, &quot;multi-modal models&quot;, &quot;transfer learning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07188&amp;sa=D&amp;ust=1465142038885000&amp;usg=AFQjCNGvmL_CTYGC-NsgznnRrs5uow--hw">http://arxiv.org/abs/1603.07188</a></span><span>&nbsp;[abs:2] weakly supervised fully convolutional networks, trying to solve a problem with them needing a lot of information to obtain reasonable performance. They use video motion data as the target. Should go in &quot;images/video&quot;, &quot;semi-supervised&quot;, &quot;images/localization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07234&amp;sa=D&amp;ust=1465142038886000&amp;usg=AFQjCNFpFNHtzLtiijn9nOs6EjvCAePEAA">http://arxiv.org/abs/1603.07234</a></span><span>&nbsp;[abs:4] very fast and data-efficient transfer learning technique, by reconstructing filters that perform badly between domains. Should go in &quot;transfer learning&quot;, &quot;convolution&quot;, &quot;theory&quot;. Cool!</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07235&amp;sa=D&amp;ust=1465142038886000&amp;usg=AFQjCNHU7ipx3JMc5CWhQMYSqJ_bwa4ShA">http://arxiv.org/abs/1603.07235</a></span><span>&nbsp;[abs:2] generative-ish super-resolution technique for faces. Should go in &quot;images/superresolution&quot;, &quot;images/faces&quot;, &quot;generative&quot;, &quot;adverserial&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07292&amp;sa=D&amp;ust=1465142038887000&amp;usg=AFQjCNEm7W15x_uVNCxWBPFYhFgKSDwTyw">http://arxiv.org/abs/1603.07292</a></span><span>&nbsp;[abs:3] debugging machine learning from the perspective of treating bugs as errors in the data. Should go in &quot;debugging&quot;. Non-neural, but useful for neural models, probably.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07294&amp;sa=D&amp;ust=1465142038888000&amp;usg=AFQjCNG6AtbgqOoBUczhp-8Sf8xhLYHP5w">http://arxiv.org/abs/1603.07294</a></span><span>&nbsp;[abs:1] privacy-preservation analysis thing. Should go in &quot;privacy&quot;. Non-neural, is for bayesian models.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.07249&amp;sa=D&amp;ust=1465142038889000&amp;usg=AFQjCNH31rqAIfV1nKzCI6mZW2RMIFQ3DQ">http://arxiv.org/abs/1603.07249</a></span><span>&nbsp;[abs:0] super outdated looking tutorial in deep learning stuff. Mentions deep belief networks as the way to train multi layer models. Should go in &quot;basics&quot; or &quot;history&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.06821&amp;sa=D&amp;ust=1465142038889000&amp;usg=AFQjCNHYMw8voQws0Wyn0QtbWkYjpSAYUg">http://arxiv.org/abs/1505.06821</a></span><span>&nbsp;[abs:0] some person re-identification paper. Should go in &quot;images/similarity&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.02995&amp;sa=D&amp;ust=1465142038890000&amp;usg=AFQjCNEG2TKinoXXGQkd0TsLVHN4LNSemg">http://arxiv.org/abs/1602.02995</a></span><span>&nbsp;[abs:1] fine-grained action classification with a cnn that is spatial in the early layers, temporal in later ones. Should go in &quot;images/video&quot;. Sounds cool I guess. Claims a large improvement in performance.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.06904&amp;sa=D&amp;ust=1465142038891000&amp;usg=AFQjCNF75EcQRnMammrKOXa6ZejchGC-rA">http://arxiv.org/abs/1508.06904</a></span><span>&nbsp;[abs:3] theory paper on making convnets that can be trusted to do a signal processing thing. Should go in &quot;theory&quot;, &quot;safety&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06430&amp;sa=D&amp;ust=1465142038891000&amp;usg=AFQjCNEgZylX_-juir11n4modqhRWWQX5Q">http://arxiv.org/abs/1603.06430</a></span><span>&nbsp;[abs:2] overview of deep learning in bio. Should go in &quot;bio&quot;, maybe in &quot;basics&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.08248&amp;sa=D&amp;ust=1465142038892000&amp;usg=AFQjCNF6VJwM_N-xFBf1p0hOwJvZknukGQ">http://arxiv.org/abs/1503.08248</a></span><span>&nbsp;[abs:1] I don&#39;t even know how to categorize this: &quot;Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement and Retrieval&quot;. Should go in &quot;images/retrieval&quot; I guess? It&#39;s a literature survey&hellip;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.07109&amp;sa=D&amp;ust=1465142038893000&amp;usg=AFQjCNHwq33fRCF1X0ZK5CYU6sNzcONLbw">http://arxiv.org/abs/1602.07109</a></span><span>&nbsp;[abs:1] variational recurrent anomaly detection. Should go in &quot;bayesian neural networks&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.vko4n89p94lu"><span>Mar 30</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06777&amp;sa=D&amp;ust=1465142038895000&amp;usg=AFQjCNES5a--7HQd_gaTlTXZfkX2spg39Q">http://arxiv.org/abs/1603.06777</a></span><span>&nbsp;[abs:2] approximation scheme for more energy-efficient computing that is 30x faster at equal accuracy and 100x faster at 99% accuracy. Should go in &quot;model compression&quot;, &quot;speed&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.01881&amp;sa=D&amp;ust=1465142038896000&amp;usg=AFQjCNGEhzumHehM5NG-SZhrZGKnH-yFfw">http://arxiv.org/abs/1512.01881</a></span><span>&nbsp;[abs:1] video feeds from cameras on hands to predict hand motion. Kinda cool. Should go in &quot;images/video&quot;. Uses a small dataset, so they also do interesting &quot;transfer learning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06829&amp;sa=D&amp;ust=1465142038897000&amp;usg=AFQjCNFSwo_C3GLHUaeSvmlTV9McIMWPWw">http://arxiv.org/abs/1603.06829</a></span><span>&nbsp;[abs:1] some sort of thing about learning from multiple velocities in video. Should go in &quot;images/video&quot;. They also mention &quot;semi-supervised&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06937&amp;sa=D&amp;ust=1465142038898000&amp;usg=AFQjCNG_WWo19fzV5eb_cOMS2QOPyHAJ6Q">http://arxiv.org/abs/1603.06937</a></span><span>&nbsp;[abs:2] convolutional architecture to predict human pose. They use upsampling to get multiple spatial scales. interesting. Should go in &quot;architecture&quot;, &quot;convolution&quot;, &quot;images/pose&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04031&amp;sa=D&amp;ust=1465142038899000&amp;usg=AFQjCNHPT8pDYCE16ZJf4iCkNloW2gGc1Q">http://arxiv.org/abs/1511.04031</a></span><span>&nbsp;[abs:1] face landmark detection. Should go in &quot;images/faces&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.09231&amp;sa=D&amp;ust=1465142038900000&amp;usg=AFQjCNEeHVrADYwENX-HXm-gvlGcByP0Wg">http://arxiv.org/abs/1511.09231</a></span><span>&nbsp;[abs:2] some empirical work using debugging techniques to improve the shapes of conv filters. Interesting. Should go in &quot;convolution&quot;, &quot;debugging&quot;, &quot;images/classification&quot;. Claims to improve over a baseline on imagenet!</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.06110&amp;sa=D&amp;ust=1465142038901000&amp;usg=AFQjCNHdBAqeaw16kbN2MZJacSj1088ltw">http://arxiv.org/abs/1512.06110</a></span><span>&nbsp;[abs:1] character sequence to sequence inflection generation. Should go in &quot;language/speech&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06624&amp;sa=D&amp;ust=1465142038902000&amp;usg=AFQjCNFluxMhUoGAseaAVdGjVKZPvDSq7Q">http://arxiv.org/abs/1603.06624</a></span><span>&nbsp;[abs:1] &quot;Variational Autoencoders for Feature Detection of Magnetic Resonance Imaging Data&quot;. Should go in &quot;bio&quot;, &quot;autoencoders&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06653&amp;sa=D&amp;ust=1465142038902000&amp;usg=AFQjCNH7V5IYcZCq0UqdDST8ovdZdtR84w">http://arxiv.org/abs/1603.06653</a></span><span>&nbsp;[abs:1] alternative to variational bayes as the regularization scheme for autoencoders, based on &quot;information theoretic learning&quot;. Interesting! Paper looks probably badly written though. Suspect this is uninteresting. Should go in &quot;regularization&quot;, &quot;autoencoders&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06708&amp;sa=D&amp;ust=1465142038904000&amp;usg=AFQjCNG6gZAIUk7IBulK5N0LbwO1aYWi9g">http://arxiv.org/abs/1603.06708</a></span><span>&nbsp;[abs:2] multi-label learning scheme that is regularized by a &quot;self-paced&quot; curriculum learning thing. Sounds interesting. Should go in &quot;regularization&quot;, &quot;classification/multi-label&quot; or whatever I called it.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06861&amp;sa=D&amp;ust=1465142038904000&amp;usg=AFQjCNFF7o805FM_zl0w-jP9lbylzRc8eQ">http://arxiv.org/abs/1603.06861</a></span><span>&nbsp;[abs:3] approximate variance-reduced SGD, with near linear convergence. Should go in &quot;training algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.00548&amp;sa=D&amp;ust=1465142038905000&amp;usg=AFQjCNFHIGjxm23lGZEcnCNJT8sxNDIvMw">http://arxiv.org/abs/1504.00548</a></span><span>&nbsp;[abs:1] learning word embedding from autoencoding word definitions, or something like that. Should go in &quot;language/embedding&quot;. Maps phrases to (lexical) representations of the words.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05234&amp;sa=D&amp;ust=1465142038906000&amp;usg=AFQjCNFgcH9VrLDc8OZ1wButyv2D0notbQ">http://arxiv.org/abs/1511.05234</a></span><span>&nbsp;[abs:3] visual question answering with a recurrent attentional memory network. Looks very cool. Should go in &quot;recurrence&quot;, &quot;attention&quot;, &quot;memory&quot;, &quot;images/question answering&quot;, &quot;language/question answering&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.05936&amp;sa=D&amp;ust=1465142038907000&amp;usg=AFQjCNHbjQPeHbgcJ23fmQfpo1n6yYw0pg">http://arxiv.org/abs/1509.05936</a></span><span>&nbsp;[abs:1] another of yoshua bengio&#39;s STDP papers. Should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06220&amp;sa=D&amp;ust=1465142038908000&amp;usg=AFQjCNE8a9OF76B4CqR44neOyCtriLjiNQ">http://arxiv.org/abs/1603.06220</a></span><span>&nbsp;[abs:1] theory paper based on information theory. Should go in &quot;theory&quot;. Looks kinda meh, but claims to find limits.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05739&amp;sa=D&amp;ust=1465142038909000&amp;usg=AFQjCNEzOcwnhPBtq6pGykDD50fTvT48kw">http://arxiv.org/abs/1603.05739</a></span><span>&nbsp;[abs:0] paper on readability analysis of presidential speeches. Not neural, but potentially interesting for the reference to REAP reading-level analysis. Should go in &quot;language/language analysis&quot;, &quot;objectives&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05772&amp;sa=D&amp;ust=1465142038910000&amp;usg=AFQjCNFJVtziajq1HFLfY-Fo8GDT38cgfA">http://arxiv.org/abs/1603.05772</a></span><span>&nbsp;[abs:2] learned search over reconstruction problems, to be more efficient via jumping ahead. Should go in &quot;generative&quot;, &quot;images&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05959&amp;sa=D&amp;ust=1465142038911000&amp;usg=AFQjCNFH2WLBMbOOciUzqhCuRfXBf7sTXA">http://arxiv.org/abs/1603.05959</a></span><span>&nbsp;[abs:1] multi-scale 3-spatial-dimensional cnn for brain lesion segmentation. Also uses conditional random field for something. Should go in &quot;bio&quot;, &quot;convolution&quot;, &quot;crf&quot;, &quot;datasets&quot; (their datasets are public), &quot;examples&quot; (their source code is public)</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05800&amp;sa=D&amp;ust=1465142038912000&amp;usg=AFQjCNHotXsX3aYLf6Omn3nv2bNrsRQeJQ">http://arxiv.org/abs/1603.05800</a></span><span>&nbsp;[abs:1] comparison of cnns and kernel models for speech recognition. They propose a model selection technique they call &quot;emtropy regularized perplexity&quot;. Should go in &quot;regularization&quot;, &quot;model selection&quot; (maybe? Do I have that?) and &quot;speech&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.05914&amp;sa=D&amp;ust=1465142038912000&amp;usg=AFQjCNHdDdINU_5-S3PoFYHVXD3wgZ8SPA">http://arxiv.org/abs/1505.05914</a></span><span>&nbsp;[abs:2] fully convolutional video version of neuraltalk. Cool. should go in &quot;images/captioning&quot;, &quot;images/video&quot;, &quot;talking computers&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05962&amp;sa=D&amp;ust=1465142038913000&amp;usg=AFQjCNGWqidyn5WOw0nRNgCGuHyhHeiCxw">http://arxiv.org/abs/1603.05962</a></span><span>&nbsp;[abs:3] deep topic modeling/unsupervised document representation learning. Should go in &quot;unsupervised&quot;, &quot;language/embedding&quot;, &quot;text analysis&quot;, &quot;topic modeling&quot;. Also makes neural language models much more effective via good context embeddings.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05824&amp;sa=D&amp;ust=1465142038914000&amp;usg=AFQjCNG0Qiu6mOMhdn4NE_yYEDsTOMfymA">http://arxiv.org/abs/1603.05824</a></span><span>&nbsp;[abs:1] acoustic recognition, investigating whether it works better in the frequency domain or not. They find it works better in the frequency domain. Should go in &quot;audio&quot;, &quot;language/speech&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.04862&amp;sa=D&amp;ust=1465142038914000&amp;usg=AFQjCNGy7ecnsTD-O9ivEZwAbTqmlB20oQ">http://arxiv.org/abs/1510.04862</a></span><span>&nbsp;[abs:1] unsupervised/imitation-based learning to extract videos of interaction to give to other humans to help them learn quickly. Should go in &quot;unsupervised&quot;, &quot;applications/teaching&quot;, &quot;video&quot;. Looks boring.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06169&amp;sa=D&amp;ust=1465142038915000&amp;usg=AFQjCNHvan6kaigLlMo_5xA7sQi6YVkz0g">http://arxiv.org/abs/1603.06169</a></span><span>&nbsp;[abs:1] predicting animal species in data from animal traps. Should go in &quot;images/classification&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06180&amp;sa=D&amp;ust=1465142038916000&amp;usg=AFQjCNFxPqAcEXQi7TEzIr_8o4QnL9Pw5w">http://arxiv.org/abs/1603.06180</a></span><span>&nbsp;[abs:3] whoa, cool! Segmentation conditional on an input sentence. Should go in &quot;images/segmentation&quot; or &quot;images/pixel-wise prediction&quot;, &quot;language/language understanding&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06182&amp;sa=D&amp;ust=1465142038917000&amp;usg=AFQjCNE7H1_GNffmDKM3v2nw9gwo-3FCfw">http://arxiv.org/abs/1603.06182</a></span><span>&nbsp;[abs:2] fourier transform features for emotion recognition combined with a cnn. Unclear if the fft is over learned features, it&#39;d be cool if it was. Should go in &quot;images/faces&quot;, &quot;datasets&quot; (they mention the videoemotion-8 dataset).</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06208&amp;sa=D&amp;ust=1465142038918000&amp;usg=AFQjCNF_pclE7ykxjuK6hvJMg4f0Nc15Hg">http://arxiv.org/abs/1603.06208</a></span><span>&nbsp;[abs:1] 3d object classification, with active learning in the sense that it gets to decide how to rotate it in order to perform best. Should go in &quot;images/3d&quot;, &quot;robots&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06668&amp;sa=D&amp;ust=1465142038919000&amp;usg=AFQjCNG0AugJb70FsHVrdktkNB_PuzZTJA">http://arxiv.org/abs/1603.06668</a></span><span>&nbsp;[abs:1] image colorization, predicting per-pixel color histograms. </span><span class="c9">&quot;Significantly outperforms existing methods&quot;</span><span>. Should go in &quot;images/per-pixel prediction&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06327&amp;sa=D&amp;ust=1465142038920000&amp;usg=AFQjCNGAQ7F36KQJddhQ6Q5DWLgSM2UdYg">http://arxiv.org/abs/1603.06327</a></span><span>&nbsp;[abs:2] odd - non-learning deep architecture for detecting self-similarity, or something. Sounds probably useless on its own but maybe very cool combined with learning. Should go in &quot;images&quot;. They also mention cross-modality things.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06359&amp;sa=D&amp;ust=1465142038921000&amp;usg=AFQjCNEyqZ7fPkq5-M7ygl7MPgXSTEwEsA">http://arxiv.org/abs/1603.06359</a></span><span>&nbsp;[abs:1] recursive convnet with random fields for predicting image depth maps. Apparently beats SOTA. should go in &quot;images/3d&quot;, &quot;crf&quot;, &quot;convolution&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06432&amp;sa=D&amp;ust=1465142038922000&amp;usg=AFQjCNEDJyv3Z8WQmLyFNiBAN09B3A_bcw">http://arxiv.org/abs/1603.06432</a></span><span>&nbsp;[abs:2] transfer learning between domains without sharing weights - not sure how they do that? Sounds interesting. Should go in &quot;transfer learning&quot;, &quot;architectures&quot;. &quot;we introduce a two-stream architecture, one of which operates in the source domain and the other in the target domain. In contrast to other approaches, the weights in corresponding layers are related but not shared to account for differences between the two domains.&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06470&amp;sa=D&amp;ust=1465142038923000&amp;usg=AFQjCNEWx3KAzTOqDjZSkHXX0wkmVQLjmQ">http://arxiv.org/abs/1603.06470</a></span><span>&nbsp;[abs:1] small-dataset learning of face recognition by involving another modality of face recognition. Should go in &quot;images/faces&quot;, &quot;transfer learning&quot;, &quot;small datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06531&amp;sa=D&amp;ust=1465142038924000&amp;usg=AFQjCNEC1vrac7LkCYTHD3Y_dDc6yCnlOw">http://arxiv.org/abs/1603.06531</a></span><span>&nbsp;[abs:2] semi-supervised technique for multi-scale video gesture recognition. Should go in &quot;autoencoders&quot;, &quot;semi-supervised&quot;, &quot;convolution&quot;, &quot;images/video&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06554&amp;sa=D&amp;ust=1465142038924000&amp;usg=AFQjCNGhS2maU1xIacgdXrdeZPP8UD3Yng">http://arxiv.org/abs/1603.06554</a></span><span>&nbsp;[abs:1] conditional restricted boltzmann machines to predict affect based on facial expressions and body movements (I think?), also used for generation. Should go in &quot;images/video&quot;, &quot;semi-supervised&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06318&amp;sa=D&amp;ust=1465142038925000&amp;usg=AFQjCNHZVmo1WUWQo9UZt4rXJu9RBRpCzg">http://arxiv.org/abs/1603.06318</a></span><span>&nbsp;[abs:4] neural networks augmented with logic rules - transferring the logic-based knowledge into the neural network dramatically improves its performance. Tried on named entity recognition with an rnn and sentiment analysis with a cnn. Should go in &quot;symbolic&quot;, &quot;language/understanding&quot;, &quot;generalization/strong generalization&quot;, &quot;regularization&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06430&amp;sa=D&amp;ust=1465142038926000&amp;usg=AFQjCNFx9HOkEHCvHlCzvKvXsTuPnuNaow">http://arxiv.org/abs/1603.06430</a></span><span>&nbsp;[abs:1] overview of deep learning in bio. Should go in &quot;bio&quot;, &quot;basics&quot;. &quot;To provide a big picture, we categorized the research by both bioinformatics domains - omics, biomedical imaging, biomedical signal processing - and deep learning architectures - deep neural network, convolutional neural network, recurrent neural network, modified neural network - as well as present brief descriptions of each work.&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06353&amp;sa=D&amp;ust=1465142038927000&amp;usg=AFQjCNH8XuvTh-GG3B2bXKBEUT9jlo5nEA">http://arxiv.org/abs/1603.06353</a></span><span>&nbsp;[abs:1] some bio-modeling, possibly neuromorphic, thing, using non-negative sparse modeling stuff. Should go in &quot;sparsity&quot;, &quot;bio&quot;, &quot;neuromorphic&quot;. The application is predicting olfactory system&hellip; stuff.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06129&amp;sa=D&amp;ust=1465142038928000&amp;usg=AFQjCNEP3J1efZ5jao-WeL5j04hAWx2FeA">http://arxiv.org/abs/1603.06129</a></span><span>&nbsp;[abs:4] syntax error autocorrect! Also mentions a code dataset, &quot;big code&quot;. Repairs 30% of submissions completely. Should go in &quot;language/programming&quot;, &quot;datasets&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06143&amp;sa=D&amp;ust=1465142038929000&amp;usg=AFQjCNFaefB-BnJx3h8V7EHsJneN8YNemA">http://arxiv.org/abs/1603.06143</a></span><span>&nbsp;[abs:2] neurally-accelerated procedural models. Also mentions &quot;sequential monte carlo&quot;. Should go in &quot;generative&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06078&amp;sa=D&amp;ust=1465142038929000&amp;usg=AFQjCNGJb6KC4d1GZUcOIP7gV5PvwrfMBQ">http://arxiv.org/abs/1603.06078</a></span><span>&nbsp;[abs:3] learned image shading - given normals and such, predict, per-pixel, the shading. Wow. should go in &quot;images/3d&quot;, &quot;images/per-pixel prediction&quot;, &quot;generative&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06060&amp;sa=D&amp;ust=1465142038930000&amp;usg=AFQjCNFvKRV1NbR2Rt5N0kBzmIh4nhSIVg">http://arxiv.org/abs/1603.06060</a></span><span>&nbsp;[abs:0] stacked autoencoders for retinal vessel segmentation. Boooring. Should go in &quot;bio&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05691&amp;sa=D&amp;ust=1465142038931000&amp;usg=AFQjCNE8VCUAID5PSQhyca1n9YwZh0QOOw">http://arxiv.org/abs/1603.05691</a></span><span>&nbsp;[abs:2] paper on the limits of model compression: deep networks do, in fact, need to be deep, and training single-layer models will usually not work. Should go in &quot;model compression&quot;. Note: does not invalidate previous results, but does find that they weren&#39;t as promising as presented.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06125&amp;sa=D&amp;ust=1465142038932000&amp;usg=AFQjCNH2FQ1710IQMcSB9G75zTXM9cb-0A">http://arxiv.org/abs/1603.06125</a></span><span>&nbsp;[abs:2] dynamic bayesian networks as a recurrent, turing-complete learned computation model. Apparently performs quite well? Should go in &quot;bayesian neural networks&quot;, even though it&#39;s a non-neural bayesian technique. Are they using variational inference? </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06021&amp;sa=D&amp;ust=1465142038933000&amp;usg=AFQjCNGQD-4T-wyWALNAFdVokg8JUxRaSA">http://arxiv.org/abs/1603.06021</a></span><span>&nbsp;[abs:3] stack-augmented sentence parser and interpreter, which can be batched, unlike normal tree-structured nns; &quot;significantly outperforms&quot; other sentence models. Should go in &quot;language/language understanding&quot;, &quot;memory&quot;, &quot;recurrence&quot;. This looks really awesome!</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.gzl1z7yz32hh"><span>Mar 31</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06042&amp;sa=D&amp;ust=1465142038934000&amp;usg=AFQjCNFRF5HAPnEHMi8fIKMxreXZ_yxXfQ">http://arxiv.org/abs/1603.06042</a></span><span>&nbsp;[abs:1] strange sounding &quot;transition based model&quot;. Should go in &quot;architectures&quot;, &quot;language/tagging&quot; (used on part of speech tagging). Also mentions global normalization. Not clear from the abstract how these all go together.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06059&amp;sa=D&amp;ust=1465142038935000&amp;usg=AFQjCNHKp1ILKUe_mYoZJkpSj24G5BNVOQ">http://arxiv.org/abs/1603.06059</a></span><span>&nbsp;[abs:3] &quot;visual question generation&quot; datasets. Should go in &quot;datasets&quot;, &quot;images/question answering&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06075&amp;sa=D&amp;ust=1465142038936000&amp;usg=AFQjCNFw3llF6-oFntPI9Nk__KsYngRCpA">http://arxiv.org/abs/1603.06075</a></span><span>&nbsp;[abs:2] tree-structured sequence to sequence NMT. should go in &quot;architectures/recursive&quot;, &quot;attention&quot;, &quot;machine translation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06111&amp;sa=D&amp;ust=1465142038937000&amp;usg=AFQjCNHCCYjlR5Sk8eVFaKeObFGnm3IEAA">http://arxiv.org/abs/1603.06111</a></span><span>&nbsp;[abs:2] analysis of knowledge transferrability in language stuff. Should go in &quot;transfer learning&quot;, &quot;theory&quot;, &quot;language&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06098&amp;sa=D&amp;ust=1465142038938000&amp;usg=AFQjCNE55qcqRGKfOh4-IE1tbNcGYgxgmw">http://arxiv.org/abs/1603.06098</a></span><span>&nbsp;[abs:1] loss function for training weakly supervised segmentation. Should go in &quot;images/segmentation&quot;, &quot;loss functions&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06127&amp;sa=D&amp;ust=1465142038938000&amp;usg=AFQjCNFGewvDx7S_C0aSVaBr0T0jaAi00A">http://arxiv.org/abs/1603.06127</a></span><span>&nbsp;[abs:2] comparisons across different sentence pair scoring datasets and approaches. Claims the models are similar. They offer a framework. Should go in &quot;libraries&quot;, &quot;language/text understanding&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06147&amp;sa=D&amp;ust=1465142038939000&amp;usg=AFQjCNFQ1P3f1m4pNAbgCHbo6qCdSUsxwg">http://arxiv.org/abs/1603.06147</a></span><span>&nbsp;[abs:1] per-character decoding for neural machine translation. Yes, it helps. What a surprise, it gets more time to think. Should go in &quot;neural machine translation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06270&amp;sa=D&amp;ust=1465142038940000&amp;usg=AFQjCNFaI5SK6qhsCu0acBmAqfBbZkFn6A">http://arxiv.org/abs/1603.06270</a></span><span>&nbsp;[abs:1] heirarchical recurrent network for sequence tagging. Uses both character and word level inputs, and a conditional random field on the output (damn that&#39;s popular these days). Should go in &quot;crf&quot;, &quot;architectures&quot;, &quot;language/tagging&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06393&amp;sa=D&amp;ust=1465142038940000&amp;usg=AFQjCNFTUe0PpOt2NtvSQRBEccS4oyzAwA">http://arxiv.org/abs/1603.06393</a></span><span>&nbsp;[abs:2] sequence-to-sequence learning often requires explicitly copying part of the input; they propose a model that does this without learning that part. Should go in &quot;architectures&quot;, &quot;recurrence&quot;, &quot;memory&quot;, &quot;language models&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06036&amp;sa=D&amp;ust=1465142038941000&amp;usg=AFQjCNFYqSM7R_ia4SIB5wRTWd21IfqGgQ">http://arxiv.org/abs/1603.06036</a></span><span>&nbsp;[abs:1] fractal something something blah blah. Should go in &quot;applications/physics&quot;. Maybe also &quot;convolution&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.06041&amp;sa=D&amp;ust=1465142038942000&amp;usg=AFQjCNH_9N0P9UWDEL98UT1HjoVtFuq-uQ">http://arxiv.org/abs/1603.06041</a></span><span>&nbsp;[abs:2] learning to match images with just by watching video and doing unsupervised similarity learning. Should go in &quot;unsupervised&quot;, &quot;representation learning&quot;, &quot;weakly supervised&quot;. Has &quot;surprising performance&quot;, and they say promises more generality.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04833&amp;sa=D&amp;ust=1465142038942000&amp;usg=AFQjCNEV3ctr93021y_RLpcaeM0motvWnQ">http://arxiv.org/abs/1603.04833</a></span><span>&nbsp;[abs:1] application of convnets for detecting retinal vessels. Should go in &quot;bio&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04467&amp;sa=D&amp;ust=1465142038943000&amp;usg=AFQjCNFxxKy2y0_yfdlJ_zYsrHboAVUXUw">http://arxiv.org/abs/1603.04467</a></span><span>&nbsp;[abs:0] tensorflow original paper. Should go in &quot;libraries&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04733&amp;sa=D&amp;ust=1465142038944000&amp;usg=AFQjCNFVnFoZDe6Vv92ovdaXq-Xapuww8w">http://arxiv.org/abs/1603.04733</a></span><span>&nbsp;[abs:4] variational bayesian neural networks with a compressed factorized prior, that they then show is equivalent to a gaussian process. They then implement this efficiently with variational inference. Holy crap. Should go in &quot;bayesian neural networks&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.04395&amp;sa=D&amp;ust=1465142038945000&amp;usg=AFQjCNGkD4sFdQEqCX7TluqZmG4lhvxLOQ">http://arxiv.org/abs/1508.04395</a></span><span>&nbsp;[abs:1] large-vocabulary speech recognition with attention over the input. Has attention speedup stuff. Should go in &quot;language/speech&quot;, &quot;attention&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06824&amp;sa=D&amp;ust=1465142038945000&amp;usg=AFQjCNFBBl67t1OB-ieO8N76Buivpj5-6Q">http://arxiv.org/abs/1509.06824</a></span><span>&nbsp;[abs:1] model-based rl thing. Should go in &quot;rl&quot;. Non-neural.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.08228&amp;sa=D&amp;ust=1465142038946000&amp;usg=AFQjCNFkDnfrvTgXL0E5R1qEG-9tKqy2Hw">http://arxiv.org/abs/1511.08228</a></span><span>&nbsp;[partially implemented:3] neural gpu paper. (for people I share this with: recurrent nets convolutional over the hidden state. Should go in &quot;recurrence&quot;, &quot;memory&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.05259&amp;sa=D&amp;ust=1465142038947000&amp;usg=AFQjCNFIIr4P7TvGaxPsY2bp2N6rEEKGQw">http://arxiv.org/abs/1507.05259</a></span><span>&nbsp;[abs:1] fairness in classifiers. Should go in &quot;safety&quot; or such, also &quot;classifiers&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05118&amp;sa=D&amp;ust=1465142038948000&amp;usg=AFQjCNGBJcF2-iEu-y1Mvp376EzF6dflMA">http://arxiv.org/abs/1603.05118</a></span><span>&nbsp;[abs:3] recurrent dropout that actually helps. Cool! Should go in &quot;regularization&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05157&amp;sa=D&amp;ust=1465142038949000&amp;usg=AFQjCNExXsP6ZMKTShjX6JFnIEyWXbocWg">http://arxiv.org/abs/1603.05157</a></span><span>&nbsp;[abs:1] comparison of convnets to traditional (rule based?) approaches for for language fill-the-blank inference. Should go in &quot;language/text understanding&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04871&amp;sa=D&amp;ust=1465142038949000&amp;usg=AFQjCNHw6lTUvqbDETM2PRPj5tQotSFTMQ">http://arxiv.org/abs/1603.04871</a></span><span>&nbsp;[abs:1] hybrid of convolution and recurrence for semantic segmentation. Interesting. Should go in &quot;recurrence&quot; and &quot;convolution&quot;. They use spatial recurrence, and this is apparently more efficient. Should go in &quot;images/per-pixel prediction&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06709&amp;sa=D&amp;ust=1465142038950000&amp;usg=AFQjCNFzvIicTwci5Q7AGPHOhZU_HbiYeQ">http://arxiv.org/abs/1511.06709</a></span><span>&nbsp;[abs:1] training NMT models on multiple targets, to also predict monolingual data in the target language. Has large improvements. Should go in &quot;machine translation&quot;, &quot;objectives/multi-target&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05670&amp;sa=D&amp;ust=1465142038951000&amp;usg=AFQjCNE6zzPtdYmDS8Lp5JE2KDSMhr4h9g">http://arxiv.org/abs/1603.05670</a></span><span>&nbsp;[abs:2] analyzing news with unsupervised learning, and summarizing it. Should go in &quot;language/summarization&quot;, &quot;unsupervised&quot;, &quot;finance&quot; (applied to financial news).</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05673&amp;sa=D&amp;ust=1465142038951000&amp;usg=AFQjCNGQgZX_1Bh7IwHqKzVZQ82IeYrePw">http://arxiv.org/abs/1603.05673</a></span><span>&nbsp;[abs:0] predicting health inspection results from the yelp dataset. Shallow. Should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.07909&amp;sa=D&amp;ust=1465142038952000&amp;usg=AFQjCNEWAOU0QOXARFoXnGTuRWX0DH1DxA">http://arxiv.org/abs/1508.07909</a></span><span>&nbsp;[abs:1] machine translation of rare words by chunking words and trying to predict partial word tokens. Explores different word tokenizations, shows empirical improvement. Woo hoo. Should go in &quot;neural machine translation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04908&amp;sa=D&amp;ust=1465142038953000&amp;usg=AFQjCNHZORvweTjx8ta1q8cNIz7pFaIfGw">http://arxiv.org/abs/1603.04908</a></span><span>&nbsp;[abs:2] RGB-D data collected from a headmounted camera to predict actions and objects. Cool I guess. Should go in &quot;images/3d&quot;, &quot;images/action recognition&quot;/&quot;images/video&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04922&amp;sa=D&amp;ust=1465142038954000&amp;usg=AFQjCNFPeCyXMZ1N_Xk2SjlA1k7-jeAHjA">http://arxiv.org/abs/1603.04922</a></span><span>&nbsp;[abs:2] 3d global context architecture for convnets. Sounds cool. Should go in &quot;architectures&quot;, &quot;convolution&quot;, &quot;images/3d&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02917&amp;sa=D&amp;ust=1465142038954000&amp;usg=AFQjCNFOBUv0aOT1tI7RH1fv5Viyx2pxjQ">http://arxiv.org/abs/1511.02917</a></span><span>&nbsp;[abs:1] multi-person action detection in videos. Should go in &quot;attention&quot;, &quot;images/video&quot;, &quot;datasets&quot; (they collect a basketball dataset).</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04930&amp;sa=D&amp;ust=1465142038955000&amp;usg=AFQjCNH5fLSCFInxc6eG7cSiRbh1j-nbYA">http://arxiv.org/abs/1603.04930</a></span><span>&nbsp;[abs:1] deep compressive sensing on video. Should go in &quot;alternate deep techniques&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04992&amp;sa=D&amp;ust=1465142038956000&amp;usg=AFQjCNHuySP_Q9-gBiVzOJ0v3fSsEuGZxg">http://arxiv.org/abs/1603.04992</a></span><span>&nbsp;[abs:3] inferring depth using weakly supervised training and view translation data. Should go in &quot;images/3d&quot;, &quot;unsupervised&quot;. Sounds pretty cool.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05027&amp;sa=D&amp;ust=1465142038957000&amp;usg=AFQjCNEQZP-yxgrUA68iaD0S5NgMagtpuw">http://arxiv.org/abs/1603.05027</a></span><span>&nbsp;[abs:4] analysis of resnets to figure out what&#39;s actually important (ablation experiments). They propose a better residual block. result: 1000 layer resnet on cifar-100 and 200-layer on imagenet. Hooooly shit. Should go in &quot;theory&quot;, &quot;architectures&quot;, &quot;images/classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05145&amp;sa=D&amp;ust=1465142038958000&amp;usg=AFQjCNFWQ43RM4i1cewh1BXsm3_GPGAcCg">http://arxiv.org/abs/1603.05145</a></span><span>&nbsp;[abs:3] activation function that deals with adverserial examples via decreasing activation if it gets too large. Should go in &quot;activation functions&quot;, &quot;adverserial examples&quot;, &quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.02476&amp;sa=D&amp;ust=1465142038959000&amp;usg=AFQjCNHel0YfhpHJtq8Bgabwh1cDk9UT7w">http://arxiv.org/abs/1502.02476</a></span><span>&nbsp;[abs:1] adaptively-sized RBM. should go in like, &quot;hyperparameters&quot; (in the sense that it&#39;s runtime adjustment of them), and &quot;unsupervised/rbm&quot;. (do rbms still do useful things? Why did they die? Are they still a good unsupervised learning approach, or do we have much better now?)</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05201&amp;sa=D&amp;ust=1465142038959000&amp;usg=AFQjCNHEfxN1wCi0w0yz2KrN-t0_uFs2oA">http://arxiv.org/abs/1603.05201</a></span><span>&nbsp;[abs:2] based on some analysis of CNN layers, they introduce an activation function that simplifies early layers by compressing equivalent but inverse filters. Should go in &quot;convolution&quot;, &quot;activations&quot;, &quot;images/classification&quot;. (the activation function is called CReLU).</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05189&amp;sa=D&amp;ust=1465142038960000&amp;usg=AFQjCNFFRw87CdD8a10ezG90oUAyWACxjw">http://arxiv.org/abs/1603.05189</a></span><span>&nbsp;[abs:1] application of anns to predict vehicle performance without actually running the vehicle. Should go in &quot;applications/cars&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05106&amp;sa=D&amp;ust=1465142038961000&amp;usg=AFQjCNF9Z_gLoeQuu-YhrWG3oDZFpVNaqw">http://arxiv.org/abs/1603.05106</a></span><span>&nbsp;[abs:3] one-shot learning with bayesian neural generative models, using recurrence and attention. Should go in &quot;generality&quot;, &quot;recurrence&quot;, &quot;learning algorithms&quot;, &quot;bayesian neural networks&quot;, &quot;unsupervised learning&quot;, &quot;small datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05474&amp;sa=D&amp;ust=1465142038962000&amp;usg=AFQjCNGesa7J8x2whfHbYE6F4AKmRV8w-A">http://arxiv.org/abs/1603.05474</a></span><span>&nbsp;[abs:2] predicting a representation of a person given an arbitrary number of pictures of them. Should go in &quot;recurrence&quot;, &quot;representation learning&quot;, &quot;attention&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05600&amp;sa=D&amp;ust=1465142038962000&amp;usg=AFQjCNGGR6ztuc66nhQEV7zgER6wslejBA">http://arxiv.org/abs/1603.05600</a></span><span>&nbsp;[abs:3] dataset and model for predicting the effects of forces on images - should go in &quot;images/3d&quot;, &quot;robotics&quot;, &quot;RL&quot;, &quot;datasets&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05631&amp;sa=D&amp;ust=1465142038963000&amp;usg=AFQjCNGIzO1xz2VAoE4epSH2yILUoi7PhQ">http://arxiv.org/abs/1603.05631</a></span><span>&nbsp;[abs:2] advancement to generative image modeling: style and structure separately learned, with a generator that generates the structure, and then another network that takes this and fills it in. interesting multi-modal training. Should go in &quot;multi-modal models&quot;, &quot;generative/adverserial&quot;, maybe also &quot;3d&quot; because of the use of normals.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.4a9w4rq067eh"><span>Apr 1</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05544&amp;sa=D&amp;ust=1465142038965000&amp;usg=AFQjCNEyVkDPTfFNYzQD0DqeSgIabbZeTg">http://arxiv.org/abs/1603.05544</a></span><span>&nbsp;[abs:2] statistical process control based improvements to SGD, in order to make it focus effort better. Should go in &quot;training algorithms&quot;, &quot;prioritization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.05191&amp;sa=D&amp;ust=1465142038965000&amp;usg=AFQjCNEWtoikoAvVH0m8_hDpuAfyUkL87Q">http://arxiv.org/abs/1603.05191</a></span><span>&nbsp;[abs:1] approximating newton methods stuff. Should go in &quot;training algorithms&quot;. They mention it worked on a 273gb dataset or something.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06409&amp;sa=D&amp;ust=1465142038966000&amp;usg=AFQjCNF7NbWk36yHTLZ8bcTG0wKWkX3VBg">http://arxiv.org/abs/1511.06409</a></span><span>&nbsp;[abs:3] perceptual similarity objective based on human perceptual similarity research. Cool! The objective is called &quot;MS-SSIM&quot;, &quot;multiscale structure similarity&quot;. (loss on a wavelet decomposed version?) this should be effectively what adverserial networks learn plus some, but it&#39;s still interesting for when you can&#39;t do that or want to bootstrap. Should go in &quot;objectives&quot;, &quot;generative/adverserial&quot;. They also do human user testing of the results.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04513&amp;sa=D&amp;ust=1465142038967000&amp;usg=AFQjCNGpxemnAesw-kOJcD4WxfR35FACAQ">http://arxiv.org/abs/1603.04513</a></span><span>&nbsp;[abs:1] variable size convolution over text. Only works with unsupervised pretraining, apparently. Should go in &quot;language/classification&quot;, &quot;convolution&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04747&amp;sa=D&amp;ust=1465142038968000&amp;usg=AFQjCNGunDMreBpPF-72_yzefQuNkPQ36w">http://arxiv.org/abs/1603.04747</a></span><span>&nbsp;[abs:2] topic modeling algorithm based on word vectors; outperforms LDA. should go in &quot;language/topic modeling&quot;, &quot;unsupervised&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06841&amp;sa=D&amp;ust=1465142038968000&amp;usg=AFQjCNHGK3zlLNoKPAYaoHgk6vqV0bP1_g">http://arxiv.org/abs/1509.06841</a></span><span>&nbsp;[abs:2] model-based reinforcement learning to do one-shot learning, or something. From peter abbeel&#39;s group, looks cool. Should go in &quot;RL&quot;, &quot;generality&quot;, &quot;small datasets&quot;, &quot;robotics&quot;. Claims good performance.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04525&amp;sa=D&amp;ust=1465142038969000&amp;usg=AFQjCNGMtR0l8tvxXTQjAfgW_ESMWHibhQ">http://arxiv.org/abs/1603.04525</a></span><span>&nbsp;[abs:1] pedestrian detection: &quot;pushing the limits&quot; paper that combines the best available techniques, rather than trying to be inventive. Uses convolutional feature maps as input for boosted decision models, and improve best loss by &quot;11.7% &nbsp;to 8.9%, a relative improvement of 24%&quot;. Should go in &quot;images/people&quot; or &quot;images/pedestrian&quot; or &quot;images/localization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.09016&amp;sa=D&amp;ust=1465142038970000&amp;usg=AFQjCNFZvgzGlYC1kNONzYkYd3eN6ij4Rg">http://arxiv.org/abs/1506.09016</a></span><span>&nbsp;[abs:1] looks like one of the early papers on variance-reduced sgd? &quot;online learning to sample&quot;, &quot;the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples&quot;. Should go in &quot;training algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04530&amp;sa=D&amp;ust=1465142038971000&amp;usg=AFQjCNGgyj63jGfdpKyT5SddKh6yT15dIg">http://arxiv.org/abs/1603.04530</a></span><span>&nbsp;[abs:2] deep edge and contour detection via a weakly supervised learning approach that is then used as the basis for better object segmentation. Should go in &quot;images/per-pixel prediction&quot;, I guess.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04531&amp;sa=D&amp;ust=1465142038972000&amp;usg=AFQjCNHH66gYTYlrCYGywBIIc9fhFMmDeA">http://arxiv.org/abs/1603.04531</a></span><span>&nbsp;[abs:0] analysis of photos based on the *results* of deep learning models. Not actually deep itself. Should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04595&amp;sa=D&amp;ust=1465142038973000&amp;usg=AFQjCNFbNRz8Xtvbi-yJYOwC9KCOUOXGxQ">http://arxiv.org/abs/1603.04595</a></span><span>&nbsp;[abs:1] compact binary hashes for image retrieval, uses restricted boltzmann machines. Should go in &quot;unsupervised/representation learning&quot; or &quot;unsupervised/rbms&quot;, &quot;images/retrieval&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04713&amp;sa=D&amp;ust=1465142038973000&amp;usg=AFQjCNGu3tkOMkfasSVag3PQyeyFXycb6w">http://arxiv.org/abs/1603.04713</a></span><span>&nbsp;[abs:2] recurrent similarity detection with recurrent siamese networks. Predicts a classification per-timestep I think? Should go in &quot;recurrence&quot;, &quot;similarity&quot;, &quot;classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04771&amp;sa=D&amp;ust=1465142038974000&amp;usg=AFQjCNEkB09Ig7EpBtl7oYNXpyZqe4gLIg">http://arxiv.org/abs/1603.04771</a></span><span>&nbsp;[abs:1] deblurring motion-blurred images by using a neural network to predict a deconvolution filter (in the traditional sense of deconvolution, as used in image processing). Should go in &quot;images/misc&quot;. Cool!</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04779&amp;sa=D&amp;ust=1465142038975000&amp;usg=AFQjCNF3NkXesVeOP_SHU7Shrb06n6Oktg">http://arxiv.org/abs/1603.04779</a></span><span>&nbsp;[abs:1 or 2?] &quot;adaptive&quot; batch normalization to increase generalizability across domains. Should go in &quot;transfer learning&quot;, &quot;regularization&quot; - looks maybe kind of crappy though? Mentions that you usually need fine tuning for transfer learning. Interesting factoid, if true. Bad english, from chinese authors.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.02377&amp;sa=D&amp;ust=1465142038976000&amp;usg=AFQjCNFqBZBVFihkNlKBcujG-Ufnza8tMw">http://arxiv.org/abs/1502.02377</a></span><span>&nbsp;[abs:1] &quot;Sparse Coding with Earth Mover&#39;s Distance for Multi-Instance Histogram Representation&quot;. Should go in &quot;sparsity&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06382&amp;sa=D&amp;ust=1465142038977000&amp;usg=AFQjCNEEFh5rECssJgF-rKCWrI61NQDDsw">http://arxiv.org/abs/1511.06382</a></span><span>&nbsp;[abs:0] training directed belief networks with variational inference. Should go in &quot;bayesian neural networks&quot;, &quot;unsupervised&quot;, I guess.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04042&amp;sa=D&amp;ust=1465142038977000&amp;usg=AFQjCNGIe6gngExptCCXLlX6nefpMarl6w">http://arxiv.org/abs/1603.04042</a></span><span>&nbsp;[abs:2] interactive object selection via using transfer learning to significantly reduce the size of the needed dataset. Should go in &quot;data augmentation&quot;, &quot;transfer learning&quot;, &quot;data collection&quot;/&quot;active learning&quot;/&quot;human interaction&quot;, &quot;images/per-pixel prediction&quot;. Also mentions objectness, which is interesting.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04245&amp;sa=D&amp;ust=1465142038978000&amp;usg=AFQjCNGMpknRNvZQco_V2-9VyVdbaXbZWw">http://arxiv.org/abs/1603.04245</a></span><span>&nbsp;[abs:1] analysis of accelerated gradient techniques (such as nesterov&#39;s momentum) from the perspective of variational stuffs, and &quot;continuous time&quot; (wat? How is this related?). Should go in &quot;bayesian neural networks&quot; (I guess? I don&#39;t think that&#39;s what it is?), &quot;learning algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04508&amp;sa=D&amp;ust=1465142038979000&amp;usg=AFQjCNHgqeDsZ0XkdOumYCFmZ1AURGThzQ">http://arxiv.org/abs/1511.04508</a></span><span>&nbsp;[abs:3] knowledge distillation as a regularization technique against adverserial examples, with a focus on being good enough to be secure against them (ha, isn&#39;t this proven impossible? Predict it&#39;s either impossible or equivalent to the halting problem in the general case.) anyway, they say they get very good results. Should go in &quot;regularization&quot;, &quot;adverserial examples problem&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.03745&amp;sa=D&amp;ust=1465142038980000&amp;usg=AFQjCNEcFiCg0iFMnKSR7hWLJP6-ZDZ6Ng">http://arxiv.org/abs/1511.03745</a></span><span>&nbsp;[abs:3] unsupervised learning to localize sentences in images - by using an attention mechanism as the choke point for an autoencoder! Holy shit, cool! Should go in &quot;unsupervised/autoencoders&quot;, &quot;attention&quot;, &quot;language/talking computers&quot;, &quot;images/captioning&quot;. Can also be used supervised, via a target for the attention map.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03793&amp;sa=D&amp;ust=1465142038980000&amp;usg=AFQjCNHp5SX4mXayS5Rbj5qo9YFS9e8fuw">http://arxiv.org/abs/1603.03793</a></span><span>&nbsp;[abs:2] stack-lstm parser that uses an approach that accounts for recursive model prediction at training time &quot;rather than assuming an error free action history&quot;, like the idea behind what I was trying to do with input skipping. Uses &quot;dynamic oracles&quot; (which has a citation) to do this. They call it &quot;training with exploration&quot;. Should go in &quot;language/parsing&quot;, &quot;language models&quot;, &quot;attention&quot; (? it looks like it uses attention - it&#39;s a dependency parser ?), &quot;rl&quot;, &quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03827&amp;sa=D&amp;ust=1465142038981000&amp;usg=AFQjCNGvvP9xDyf4iiD6IPmXephZSLPfvQ">http://arxiv.org/abs/1603.03827</a></span><span>&nbsp;[abs:1] multi-scale recurrent/convolutional text processing for classifying short texts, such as sentences. Should go in &quot;language/classification&quot;. Claims to get SOTA. should also in &quot;recurrence&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03873&amp;sa=D&amp;ust=1465142038982000&amp;usg=AFQjCNEKUoyFgUxYwp1PsU64c-LzbFIv6w">http://arxiv.org/abs/1603.03873</a></span><span>&nbsp;[abs:2] funky semantic memory thing to allow the model to learn representations of facts, and then use them for parsing and comparing discourse. Looks like a low quality paper, but maybe an interesting idea anyway. Should go in &quot;memory&quot;, &quot;language/text understanding&quot;. Also looks like maybe it uses a multilevel memory system - multiple layers of deep networks each performing different tasks, which is a major part of &quot;generality&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03876&amp;sa=D&amp;ust=1465142038983000&amp;usg=AFQjCNENvCQ7A4bEqR5XQ47ffTvZDHrtIg">http://arxiv.org/abs/1603.03876</a></span><span>&nbsp;[abs:1] generative models of discourse, with neural networks involved somehow. Unclear whether the generative model is neural. Should go in &quot;language/text understanding&quot;, &quot;generative&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04351&amp;sa=D&amp;ust=1465142038983000&amp;usg=AFQjCNGM4h_ZsLUYZrCdPHXCJJD00UoEqw">http://arxiv.org/abs/1603.04351</a></span><span>&nbsp;[abs:1] dependency parsing thing that uses bidirectional LSTMs. Apparently beats SOTA with few tricks. Wat? Should go in &quot;language/text parsing&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03915&amp;sa=D&amp;ust=1465142038984000&amp;usg=AFQjCNHCKulPJSl3cLyBfKqmuctS_Ueyng">http://arxiv.org/abs/1603.03915</a></span><span>&nbsp;[abs:1] scene text recognition via spatial transformer networks and some sequence thing they call a &quot;sequence recognition network&quot;, or an SRN (??). Trained end to end with only images and target text. Should go in &quot;images/ocr&quot;, &quot;architectures&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03925&amp;sa=D&amp;ust=1465142038985000&amp;usg=AFQjCNFn-1xn7KGLCU9nHKS1O6kH3jdPeg">http://arxiv.org/abs/1603.03925</a></span><span>&nbsp;[abs:1] another image-captioning-with-attention thing. Claims to be SOTA. should go in &quot;attention&quot;, &quot;images/captioning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03958&amp;sa=D&amp;ust=1465142038986000&amp;usg=AFQjCNE04v8o0kM4BrP-XQcbi-ThWoIt1g">http://arxiv.org/abs/1603.03958</a></span><span>&nbsp;[abs:1] combining deep networks with linear svms apparently outperforms SOTA by a large margin on face identification. Also mentions a datset, should go in &quot;datasets&quot;, &quot;images/faces&quot;, &quot;alternate deep techniques&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04000&amp;sa=D&amp;ust=1465142038986000&amp;usg=AFQjCNGVK9KRBGneoyUteDgpYCEe3bLnWw">http://arxiv.org/abs/1603.04000</a></span><span>&nbsp;[abs:2] learning a representation of fonts via predicting other font characters given four examples of a font, and via predicting whether a fifth letter is of the same font. Should go in &quot;images/fonts&quot; I guess.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04146&amp;sa=D&amp;ust=1465142038987000&amp;usg=AFQjCNHthRICmgGbsT2VHHggknrMGcO5KQ">http://arxiv.org/abs/1603.04146</a></span><span>&nbsp;[abs:1] improvement for object detection via using saliency detection, which I think is also known as &quot;class activation mapping&quot; (I usually label this as a debugging technique or per-pixel classification) to refine the attention. Should go in &quot;images/per-pixel prediction&quot;/&quot;images/localization&quot;, &quot;debugging&quot; (not because it belongs there, because other things there are closely related.)</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04186&amp;sa=D&amp;ust=1465142038988000&amp;usg=AFQjCNGFZlVVjN2l9mS-_Vg4aOO0m9mhvg">http://arxiv.org/abs/1603.04186</a></span><span>&nbsp;[abs:2] sort of &quot;attentional&quot; system that uses alternating phases of attentional classification and &quot;introspection&quot; of class activation mapping to recognize. Beats a lot of other approaches with less supervision. Should go in &quot;small datasets&quot;, &quot;attention&quot;, &quot;debugging/class activation mapping&quot;, &quot;images/classification&quot;, &quot;classification&quot;, &quot;generality/introspection&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03768&amp;sa=D&amp;ust=1465142038989000&amp;usg=AFQjCNHZX7bQkLwlGS7bdAsmTu1HJvc-AQ">http://arxiv.org/abs/1603.03768</a></span><span>&nbsp;[abs:1] image analysis thing that compares a known algorithm for predicting light scattering from semitransparent objects to neural networks, and pulls ideas from neural networks over. Should go in &quot;alternate deep techniques&quot;.</span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04259&amp;sa=D&amp;ust=1465142038989000&amp;usg=AFQjCNGrf6xupkaPbgT0uglSiyvzp3gPCA">http://arxiv.org/abs/1603.04259</a></span><span>&nbsp;[abs:1] item2vec - applying the word2vec algorithm to collaborative filtering. should go in &quot;recommendation/collaborative filtering&quot;, &quot;language/word embeddings&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.04080&amp;sa=D&amp;ust=1465142038990000&amp;usg=AFQjCNEWNWB0dMFKiYI7OjEBj-qFP_fOjQ">http://arxiv.org/abs/1603.04080</a></span><span>&nbsp;[abs:0] neuromorphic stuff - implementation of STDP. doesn&#39;t mention performance on anything, and lecun has a rule about those things. Should go in &quot;neuromorphic&quot;, &quot;hardware&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.04888&amp;sa=D&amp;ust=1465142038991000&amp;usg=AFQjCNEF_vroef1HOBeLUcm46lRFjqSUlg">http://arxiv.org/abs/1507.04888</a></span><span>&nbsp;[abs:1] using a neural network to approximate the reward function in inverse reinforcement learning. Uses simplified convolution - just a bunch of 1x1 convolutions; ie, just a bunch of feed forward networks, one per pixel. Lol. should go in &quot;rl&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03185&amp;sa=D&amp;ust=1465142038991000&amp;usg=AFQjCNE6vfUEGrg_Xs_QMeQVdBnCm_c8RQ">http://arxiv.org/abs/1603.03185</a></span><span>&nbsp;[abs:1] speech recognition that is fast, low latency, and did they mention fast? Runs faster than real time clientside on a nexus 5. Uses CTC-trained LSTM on phoneme targets, with very heavy model compression. Uses a non-neural language model that is shared between different tasks. Should go in &quot;language/speech&quot;, &quot;model compression&quot;, &quot;speed&quot;, and if I have it, &quot;phones&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03101&amp;sa=D&amp;ust=1465142038992000&amp;usg=AFQjCNEhCGQgRT9L68Bn8OaEGd6p7TQ1_g">http://arxiv.org/abs/1603.03101</a></span><span>&nbsp;[abs:2] end to end OCR with recursive convolution (cool, I was curious about that), a char-lm, and soft attention. Should go in &quot;images/ocr&quot;, &quot;architectures&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.02895&amp;sa=D&amp;ust=1465142038993000&amp;usg=AFQjCNG0Nsayg6E7NSvAj1dU31O819npkQ">http://arxiv.org/abs/1512.02895</a></span><span>&nbsp;[abs:1] multi-level fine-grained classification. Mentions datasets for this. Should go in &quot;images/fine grained classification&quot;, &quot;classification/structured classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.01013&amp;sa=D&amp;ust=1465142038994000&amp;usg=AFQjCNFW9k604Rqw3vTgBC-joidwKMtUZA">http://arxiv.org/abs/1504.01013</a></span><span>, resubmitted as </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03183&amp;sa=D&amp;ust=1465142038994000&amp;usg=AFQjCNEEWfDLVRadTJjfA8_UJxYMwlcRAA">http://arxiv.org/abs/1603.03183</a></span><span>&nbsp;[abs:0] very bad english abstract, something about CRF+CNN models and context something or other for semantic segmentation. Should go in &quot;images/segmentation&quot;. They also mention a long list of semantic segmentation datasets.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03234&amp;sa=D&amp;ust=1465142038995000&amp;usg=AFQjCNHwOcqzhrBbduyzCo359GiTi99wkA">http://arxiv.org/abs/1603.03234</a></span><span>&nbsp;[abs:0] some image hashing thing. Should go in &quot;images/retrieval&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03149&amp;sa=D&amp;ust=1465142038996000&amp;usg=AFQjCNGo5wh71x8aa1iu0fffhjr_oaMChQ">http://arxiv.org/abs/1603.03149</a></span><span>&nbsp;[abs:0] application of anns to error detection in metal arc welding. Should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03669&amp;sa=D&amp;ust=1465142038998000&amp;usg=AFQjCNFgZXu46FLNcA5i-y8H6Ow9NDsAbQ">http://arxiv.org/abs/1603.03669</a></span><span>&nbsp;[abs:1] predicting human saliency maps from viewing of rgbd videos on 2d screens. Uses a per-pixel attention map for something or other. Also presents a dataset of attention maps. Should go in &quot;images/3d&quot;, &quot;attention&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03657&amp;sa=D&amp;ust=1465142038999000&amp;usg=AFQjCNFRLciELHQj8DejoCcylBooI2bKEw">http://arxiv.org/abs/1603.03657</a></span><span>&nbsp;[abs:3] memoized convolution over time! About time this showed up in an actual goddamn arxiv paper, there&#39;s some shitty paper on it from a year ago. No mention of performance, though. This helps a lot with my implementing it, though. Should go in &quot;convolution&quot;, &quot;recurrence&quot;, &quot;architectures/timeseries&quot;. They call it &quot;deep shifting&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03713&amp;sa=D&amp;ust=1465142039000000&amp;usg=AFQjCNGd4gqzFGbXWHd_MHEvRkSZtqSibQ">http://arxiv.org/abs/1603.03713</a></span><span>&nbsp;[abs:1] cost-sensitive objective functions for advertising - relevant for rl when actions have predictable costs. Should go in &quot;rl&quot;, &quot;finance&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.06719&amp;sa=D&amp;ust=1465142039001000&amp;usg=AFQjCNHiFSC5PW19VsaizQFd8iX3IV1XfA">http://arxiv.org/abs/1601.06719</a></span><span>&nbsp;[abs:2] weird sounding object detection thing using a different architecture or something. I&#39;d say it was stupid, but it sounds kind of interesting. Should go in &quot;images/object detection&quot;, &quot;architectures&quot;, &quot;convolution. They call it an R2-cnn.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02776&amp;sa=D&amp;ust=1465142039002000&amp;usg=AFQjCNESBzc0MOjkyRdr5kXX8rvaynBKSQ">http://arxiv.org/abs/1603.02776</a></span><span>&nbsp;[abs:0] discourse convnets wat wat thing thing. Should go in &quot;language/text understanding&quot;. Same people as all the other papers that have &quot;discourse&quot; in their descriptions.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02729&amp;sa=D&amp;ust=1465142039003000&amp;usg=AFQjCNHG6popBw-3SodS3ABw_F6BGOgYkw">http://arxiv.org/abs/1603.02729</a></span><span>&nbsp;[abs:3] review of active perception from the olden days, with the goal of summarizing the relevant parts for the modern world. Should go in &quot;data collection&quot;/&quot;active learning&quot;, &quot;robotics&quot;, &quot;reviews&quot;, &quot;basics&quot;, &quot;rl&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02814&amp;sa=D&amp;ust=1465142039004000&amp;usg=AFQjCNEHndELRqv1vVEnWHBY-xJWlg243Q">http://arxiv.org/abs/1603.02814</a></span><span>&nbsp;[abs:1] image captioning and visual question answering with a model that gets high level context as additional input. Should go in &quot;multi-input models&quot; (where did I put that?), &quot;images/captioning&quot;. Continuation of the research in </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.01144&amp;sa=D&amp;ust=1465142039004000&amp;usg=AFQjCNGsMsme040m1mv4Wrai36ddSGpAkA">http://arxiv.org/abs/1506.01144</a></span><span>&nbsp;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.04164&amp;sa=D&amp;ust=1465142039005000&amp;usg=AFQjCNG2y7UVXJvxFOIHLd8kPU-KWQFCGQ">http://arxiv.org/abs/1511.04164</a></span><span>&nbsp;[abs:1] localization of objects given a description. Should go in &quot;language/text understanding&quot;, &quot;images/localization&quot;, &quot;recurrence&quot;, maybe &quot;global context&quot; or something.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02844&amp;sa=D&amp;ust=1465142039006000&amp;usg=AFQjCNHJqX2didfv4EU6K24wP92U4x74QA">http://arxiv.org/abs/1603.02844</a></span><span>&nbsp;[abs:2] &quot;triplet loss&quot; .. &quot;has been shown to be most effective for ranking problems&quot;. Interesting. They present a fast algorithm for mapping images to binary codes with a triplet loss. Should go in &quot;images/retrieval&quot;, &quot;images/similarity&quot; or &quot;similarity&quot;, &quot;representation learning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02836&amp;sa=D&amp;ust=1465142039008000&amp;usg=AFQjCNHDKziEIfwodKQ41v5GXTknrg_b9g">http://arxiv.org/abs/1603.02836</a></span><span>&nbsp;[abs:0] going back in time: layerwise pretraining&hellip; but faster. Should go in &quot;parallelization&quot;, &quot;unsupervised learning/old stuff&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.02839&amp;sa=D&amp;ust=1465142039009000&amp;usg=AFQjCNHlBtr4j6DqyGJWbWD6Wfvd8YsO1A">http://arxiv.org/abs/1603.02839</a></span><span>&nbsp;[abs:1] for non-neural learning, &quot;Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show -- theoretically and empirically -- how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an n-sample in 2n, instead of nlogn steps.&quot;. Should go in &quot;training algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.03007&amp;sa=D&amp;ust=1465142039009000&amp;usg=AFQjCNF0kiYtMfGolmL4mIgpUyHoAjKJNQ">http://arxiv.org/abs/1603.03007</a></span><span>&nbsp;[abs:0] lol. Emotional biologically plausible neural networks. Should go in &quot;neuromorphic&quot;. I guess some people still hold the belief that we need computers to have emotions in order for them to do useful things?</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02300&amp;sa=D&amp;ust=1465142039010000&amp;usg=AFQjCNFBUQ3vcgwBmVwx_umR7mghXmbcIg">http://arxiv.org/abs/1511.02300</a></span><span>&nbsp;[abs:2] RGB-D volumetric localization. Cooool. Comes with code and pretrained models, outperforms SOTA by a lot, and is 200x faster. Takes a volumetric input for a 3d convnet. Should go in &quot;images/3d&quot;, &quot;images/localization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012&amp;sa=D&amp;ust=1465142039011000&amp;usg=AFQjCNGRCiaOnhumXafle343jJVCOrq7kQ">http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012</a></span><span>&nbsp;[unread] list of papers from nips 2012. Should go in &quot;historical&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1312.6229&amp;sa=D&amp;ust=1465142039012000&amp;usg=AFQjCNHVI4yyUn4MqXhdZJqg6WP6fD1ELQ">http://arxiv.org/abs/1312.6229</a></span><span>&nbsp;[awesome-deep-vision list] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks should go in &quot;historical&quot;, &quot;images/localization&quot;, &quot;images/classification&quot;, &quot;architectures&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1311.2524&amp;sa=D&amp;ust=1465142039013000&amp;usg=AFQjCNGEKGoUZKoNpso1ixPty0oldBMgsg">http://arxiv.org/abs/1311.2524</a></span><span>&nbsp;[awesome-deep-vision list] Rich feature hierarchies for accurate object detection and semantic segmentation should go in &quot;historical&quot;, &quot;images/localization&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1406.4729&amp;sa=D&amp;ust=1465142039014000&amp;usg=AFQjCNGgOrxLUI2XOBs7qxS1QSo8fuEBCw">http://arxiv.org/abs/1406.4729</a></span><span>&nbsp;[awesome-deep-vision list] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition - variable sized pooling. Should go in &quot;pooling&quot;, &quot;images/classification&quot;, &quot;architectures&quot;, &quot;historical&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.08083&amp;sa=D&amp;ust=1465142039015000&amp;usg=AFQjCNFtHJQxzBE1av6NR10wLK8NFAfc8w">http://arxiv.org/abs/1504.08083</a></span><span>&nbsp;[awesome-deep-vision list] Fast R-CNN - should go in &quot;images/localization&quot;, &quot;architectures&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.04878&amp;sa=D&amp;ust=1465142039016000&amp;usg=AFQjCNHRtqrdLSmuiPXczOjD3WDQEuhOJg">http://arxiv.org/abs/1506.04878</a></span><span>&nbsp;[awesome-deep-vision list] people detection. Should go in &quot;images/localization&quot;, &quot;recurrent&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02640&amp;sa=D&amp;ust=1465142039017000&amp;usg=AFQjCNG3Vgtec-I0lW16NC45W-ebwWLiGg">http://arxiv.org/abs/1506.02640</a></span><span>&nbsp;[YOLO][awesome-deep-vision list] you only look once YOLO should go in &quot;YOLO&quot;, &quot;you only live once&quot;, &quot;images/localization&quot;, &quot;architectures&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.04143&amp;sa=D&amp;ust=1465142039017000&amp;usg=AFQjCNER_1_CTsZi8r9967v-8MxSPzT9PA">http://arxiv.org/abs/1512.04143</a></span><span>&nbsp;[awesome-deep-vision list; abs:3] Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks - goes into tricks of the trade, and presents algorithms for multi-scale information extraction via a layer skipping thingy. &quot;Skip pooling&quot;, whatever that is. Should go in &quot;images/localization&quot;, &quot;architectures&quot;, &quot;pooling&quot;, &quot;convolution&quot;, &quot;basics/tricks&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.06796&amp;sa=D&amp;ust=1465142039018000&amp;usg=AFQjCNEK1snSsCZD6xCy7rrIJLp9Yg5ouw">http://arxiv.org/abs/1502.06796</a></span><span>&nbsp;[awesome-deep-vision list] Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network - should go in &quot;images/tracking&quot;, &quot;transfer learning&quot;, &quot;shallow and deep combined&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1501.00092&amp;sa=D&amp;ust=1465142039019000&amp;usg=AFQjCNFeK1NXAK-kY8G02egyQk2caZnKoA">http://arxiv.org/abs/1501.00092</a></span><span>&nbsp;[awesome-deep-vision list, abs:1] Image Super-Resolution Using Deep Convolutional Networks should go in &quot;images/superresolution&quot;, &quot;generative&quot;, &quot;sparsity&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.06993&amp;sa=D&amp;ust=1465142039020000&amp;usg=AFQjCNEIDTbpieZlJ1l8EIWBZQbZsCeVjA">http://arxiv.org/abs/1504.06993</a></span><span>&nbsp;[awesome-deep-vision list] Compression Artifacts Reduction by a Deep Convolutional Network - should go in &quot;generative&quot;/&quot;representation&quot;/&quot;unsupervised&quot;/I&#39;m not exactly sure, but it&#39;s sort of unsupervised. Also &quot;images/superresolution&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1406.7444&amp;sa=D&amp;ust=1465142039021000&amp;usg=AFQjCNG2R_rmgkAavdd9el41Cn3oayaghw">http://arxiv.org/abs/1406.7444</a></span><span>&nbsp;[awesome-deep-vision list] Learning To Deblur - should go in &quot;images/misc processing&quot;, next to the other blur paper. Also &quot;historical&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.00593&amp;sa=D&amp;ust=1465142039022000&amp;usg=AFQjCNEwQix9HaRhs7mIWA8la-7kOZpIHw">http://arxiv.org/abs/1503.00593</a></span><span>&nbsp;[awesome-deep-vision list] Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal - should go in &quot;images/misc processing&quot;, next to the other blur papers. Also &quot;historical&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.06375&amp;sa=D&amp;ust=1465142039023000&amp;usg=AFQjCNH2zrfKq95FvZTrCiRFneBYVsVqXg">http://arxiv.org/abs/1504.06375</a></span><span>&nbsp;[awesome-deep-vision list] Holistically-Nested Edge Detection should go in &quot;images/whereever the hell I&#39;m putting edge detection papers I mean really&quot;, &quot;images/per pixel prediction&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.02108&amp;sa=D&amp;ust=1465142039024000&amp;usg=AFQjCNH2XWpGhhuncTeAFKHM74JZdZOvzA">http://arxiv.org/abs/1506.02108</a></span><span>&nbsp;[awesome-deep-vision list, abs:1] Deeply Learning the Messages in Message Passing Inference - &quot;We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs)&quot; ; should go in &quot;CRF&quot;, &quot;images/semantic segmentation&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.02634&amp;sa=D&amp;ust=1465142039024000&amp;usg=AFQjCNFKgldHQsPgkPaeTb9djHh_FdDObg">http://arxiv.org/abs/1509.02634</a></span><span>&nbsp;[awesome-deep-vision list, abs:1] Semantic Image Segmentation via Deep Parsing Network - should go in &quot;images/semantic segmentation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.01640&amp;sa=D&amp;ust=1465142039025000&amp;usg=AFQjCNEcJ4GZr-0wqucDfahho1ws7xE8kQ">http://arxiv.org/abs/1503.01640</a></span><span>&nbsp;[awesome-deep-vision list] BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation - should go in &quot;images/semantic segmentation&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.04366&amp;sa=D&amp;ust=1465142039026000&amp;usg=AFQjCNGwlLAQK4g3CjknCCbqKYU3CaAQNQ">http://arxiv.org/abs/1505.04366</a></span><span>&nbsp;[awesome-deep-vision list, abs:1] Learning Deconvolution Network for Semantic Segmentation - should go in &quot;images/per-pixel prediction&quot;, &quot;images/semantic segmentation&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.04924&amp;sa=D&amp;ust=1465142039027000&amp;usg=AFQjCNGvpq-U8s02nbSAMK9eR5_fKEUf1g">http://arxiv.org/abs/1506.04924</a></span><span>&nbsp;[awesome-deep-vision list] Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation - should go in &quot;images/semantic segmentation&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.03240&amp;sa=D&amp;ust=1465142039028000&amp;usg=AFQjCNG-FUWy3ThXfQJT7m09hbl7YfnXJw">http://arxiv.org/abs/1502.03240</a></span><span>&nbsp;[awesome-deep-vision list] Conditional Random Fields as Recurrent Neural Networks - should go in &quot;images/semantic segmentation&quot;, &quot;crf&quot;, &quot;recurrence&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.02734&amp;sa=D&amp;ust=1465142039028000&amp;usg=AFQjCNGzBUYBcjkr-lzQtN_as-ee4e_K9g">http://arxiv.org/abs/1502.02734</a></span><span>&nbsp;[awesome-deep-vision list] Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation - should go in &quot;images/semantic segmentation&quot;, &quot;semi-supervised learning&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.01581&amp;sa=D&amp;ust=1465142039029000&amp;usg=AFQjCNEl2QnHm2JdMsSYYI67AMB4VLx2Gg">http://arxiv.org/abs/1507.01581</a></span><span>&nbsp;[awesome-deep-vision list] not even sure how to summarize. &quot;(1) Objects occur at multiple scales and therefore we should use regions at multiple scales. However, these regions are overlapping which creates conflicting class predictions at the pixel-level. (2) Class frequencies are highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to a single class, which creates competition between classes. We address all three problems with a joint calibration method which optimizes a multi-class loss defined over the final pixel-level output labeling, as opposed to simply region classification.&quot; - should go in &quot;images/per-pixel prediction&quot;, &quot;images/semantic segmentation&quot;, &quot;objective functions&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://mi.eng.cam.ac.uk/projects/segnet/&amp;sa=D&amp;ust=1465142039030000&amp;usg=AFQjCNF6BCgIgR8O-32OtMW78IQ7N49BTg">http://mi.eng.cam.ac.uk/projects/segnet/</a></span><span>&nbsp;and </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.07293&amp;sa=D&amp;ust=1465142039031000&amp;usg=AFQjCNFqJqgGBUTOa8M3eSYhFH-cYgA5Lw">http://arxiv.org/abs/1505.07293</a></span><span>&nbsp;and </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.00561&amp;sa=D&amp;ust=1465142039031000&amp;usg=AFQjCNHhedBiN_lW0VOWCshwHKlRM0hq9g">http://arxiv.org/abs/1511.00561</a></span><span>&nbsp;and </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.02680&amp;sa=D&amp;ust=1465142039031000&amp;usg=AFQjCNFYAuvE-jyifnGGpGz5JrczXko-SA">http://arxiv.org/abs/1511.02680</a></span><span>&nbsp;[awesome-deep-vision list] SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling;; SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation;; Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding - should go in &quot;bayesian neural networks&quot;, &quot;semantic image segmentation&quot;, &quot;images/per-pixel prediction&quot;, &quot;autoencoders&quot; (...sort of&hellip;)</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.7755&amp;sa=D&amp;ust=1465142039032000&amp;usg=AFQjCNGwYWpD5cWi0QolciYzlO6rj2c0oQ">http://arxiv.org/abs/1412.7755</a></span><span>&nbsp;[awesome-deep-vision list] Multiple Object Recognition with Visual Attention - should go in &quot;attention&quot;, &quot;RL&quot;, &quot;images/classification&quot;, &quot;recurrence&quot;, &quot;historical&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.6856&amp;sa=D&amp;ust=1465142039033000&amp;usg=AFQjCNH651bLSioHHE6g4FZ1UirHWC7wNA">http://arxiv.org/abs/1412.6856</a></span><span>&nbsp;[awesome-deep-vision list] Object Detectors Emerge in Deep Scene CNNs - should go in &quot;historical&quot;, &quot;theory&quot;, &quot;images/semantic segmentation&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1410.1090&amp;sa=D&amp;ust=1465142039034000&amp;usg=AFQjCNG46qu8pHJgNKnpui8wK8AnfTo8-A">http://arxiv.org/abs/1410.1090</a></span><span>&nbsp;[awesome-deep-vision list] Explain Images with Multimodal Recurrent Neural Networks - should go in &quot;recurrence&quot;, &quot;images/captioning&quot;, &quot;historical&quot;, &quot;multi-modal models&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.2539&amp;sa=D&amp;ust=1465142039035000&amp;usg=AFQjCNHvV6-pzw7x6czB846PIToSPdHC0Q">http://arxiv.org/abs/1411.2539</a></span><span>&nbsp;[awesome-deep-vision list] Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models - should go in &quot;recurrence&quot;, &quot;images/captioning&quot;, &quot;historical&quot;, &quot;multi-modal models&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.4389&amp;sa=D&amp;ust=1465142039035000&amp;usg=AFQjCNFwQovtdxbEXAfNNYYTEK0ZacKtMA">http://arxiv.org/abs/1411.4389</a></span><span>&nbsp;[awesome-deep-vision list] Long-term Recurrent Convolutional Networks for Visual Recognition and Description - should go in &quot;recurrence&quot;, &quot;images/captioning&quot;, &quot;historical&quot;, &quot;multi-modal models&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://cs.stanford.edu/people/karpathy/cvpr2015.pdf&amp;sa=D&amp;ust=1465142039036000&amp;usg=AFQjCNGgawBNgYTY1MWOz0tIieet9CtthA">http://cs.stanford.edu/people/karpathy/cvpr2015.pdf</a></span><span>&nbsp;[awesome-deep-vision list] Deep Visual-Semantic Alignments for Generating Image Descriptions - also known as &quot;neuraltalk&quot;. Should go in &quot;images/captioning&quot;, &quot;recurrence&quot;, &quot;multi-modal models&quot;, &quot;images/localization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1412.4729&amp;sa=D&amp;ust=1465142039037000&amp;usg=AFQjCNGYPSUKkoEWO11n7WzqvN2vA-JIfg">http://arxiv.org/abs/1412.4729</a></span><span>&nbsp;[awesome-deep-vision list] Translating Videos to Natural Language Using Deep Recurrent Neural Networks - should go in &quot;historical&quot;, &quot;images/captioning&quot;, &quot;recurrence&quot;, &quot;images/video&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.5654&amp;sa=D&amp;ust=1465142039038000&amp;usg=AFQjCNE3ulQ5tQujXf5VcdDj6JJOrGnTOA">http://arxiv.org/abs/1411.5654</a></span><span>&nbsp;[awesome-deep-vision list] Learning a Recurrent Visual Representation for Image Caption Generation - should go in &quot;memory&quot;, &quot;recurrence&quot;, &quot;images/embeddings&quot;, &quot;images/captioning&quot;, &quot;historical&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1411.4952&amp;sa=D&amp;ust=1465142039039000&amp;usg=AFQjCNGeddD3EeNDCj2FQIlxzFPuV-m_eg">http://arxiv.org/abs/1411.4952</a></span><span>&nbsp;[awesome-deep-vision list] From Captions to Visual Concepts and Back &quot;images/captioning&quot;, &quot;recurrence&quot;, &quot;multi-modal models&quot;, &quot;images/localization&quot;. This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.03671&amp;sa=D&amp;ust=1465142039040000&amp;usg=AFQjCNFiVmPakMUqqLSzkHaPQ3_H3xFtVQ">http://arxiv.org/abs/1502.03671</a></span><span>&nbsp;[awesome-deep-vision list] Phrase-based Image Captioning - should go in &quot;images/captioning&quot;. Non neural.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.06692&amp;sa=D&amp;ust=1465142039041000&amp;usg=AFQjCNEXgLymyVPacRRwHZM-LN8RbS1QzQ">http://arxiv.org/abs/1504.06692</a></span><span>&nbsp;[awesome-deep-vision list] Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images - should go in &quot;small datasets&quot;, &quot;online learned learning&quot;/&quot;one=shot learning&quot;, &quot;images/captioning&quot;, &quot;weight sharing&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.04467&amp;sa=D&amp;ust=1465142039042000&amp;usg=AFQjCNGZJMBBgg3K39gPc5rnY3oLDfgCmg">http://arxiv.org/abs/1505.04467</a></span><span>&nbsp;[awesome-deep-vision list] Exploring Nearest Neighbor Approaches for Image Captioning - should go in &quot;images/captioning&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.01809&amp;sa=D&amp;ust=1465142039043000&amp;usg=AFQjCNGpPm48Ql7Gh-SLizRONzq7GHm9NQ">http://arxiv.org/abs/1505.01809</a></span><span>&nbsp;[awesome-deep-vision list] Language Models for Image Captioning: The Quirks and What Works - should go in &quot;basics/reviews&quot;, &quot;images/captioning&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.03694&amp;sa=D&amp;ust=1465142039043000&amp;usg=AFQjCNG8I1fd10zaPor8ceYI0pGUaaZwBg">http://arxiv.org/abs/1506.03694</a></span><span>&nbsp;[awesome-deep-vision list; title:3] Learning language through pictures - should go in &quot;images/captioning&quot;, &quot;language/language models&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.01053&amp;sa=D&amp;ust=1465142039045000&amp;usg=AFQjCNE6LsYtnAArZ07AcKaLJXWtixvGVQ">http://arxiv.org/abs/1507.01053</a></span><span>&nbsp;[awesome-deep-vision list] Describing Multimedia Content using Attention-based Encoder--Decoder Networks - should go in &quot;images/captioning&quot;, &quot;images/video&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.02091&amp;sa=D&amp;ust=1465142039046000&amp;usg=AFQjCNHNe0zZsitK3RC_DrkuOZyeySri4A">http://arxiv.org/abs/1508.02091</a></span><span>&nbsp;[awesome-deep-vision list] Image Representations and New Domains in Neural Image Captioning - should go in &quot;images/captioning&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.00487&amp;sa=D&amp;ust=1465142039047000&amp;usg=AFQjCNF4OunBRsMh3IMmj-cO0lw30wV6OA">http://arxiv.org/abs/1505.00487</a></span><span>&nbsp;[awesome-deep-vision list] Sequence to Sequence -- Video to Text - should go in &quot;recurrence&quot;, &quot;images/captioning&quot;, &quot;images/video&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.08029&amp;sa=D&amp;ust=1465142039047000&amp;usg=AFQjCNECS8w-_xXDt8YBKihkIAm74slg2Q">http://arxiv.org/abs/1502.08029</a></span><span>&nbsp;[awesome-deep-vision list] Describing Videos by Exploiting Temporal Structure - should go in &quot;images/captioning&quot;, &quot;images/video&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.01698&amp;sa=D&amp;ust=1465142039048000&amp;usg=AFQjCNEmvygkRP7GQ7Pk1ivgZH7RaYg0_A">http://arxiv.org/abs/1506.01698</a></span><span>&nbsp;[awesome-deep-vision list] The Long-Short Story of Movie Description - should go in &quot;images/captioning&quot;, &quot;images/video&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.06724&amp;sa=D&amp;ust=1465142039049000&amp;usg=AFQjCNHDQmcOPOmeY57NBwPU2pHOQErBZA">http://arxiv.org/abs/1506.06724</a></span><span>&nbsp;[awesome-deep-vision list:2] Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books - should go in &quot;multi-modal models&quot;, &quot;images/captioning&quot;, &quot;images/video&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.01121&amp;sa=D&amp;ust=1465142039049000&amp;usg=AFQjCNEkPWF8bsV9ToBBgnRwDm7Mt-6XmA">http://arxiv.org/abs/1505.01121</a></span><span>&nbsp;[awesome-deep-vision list] Ask Your Neurons: A Neural-based Approach to Answering Questions about Images - should go in &quot;images/question answering&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.02074&amp;sa=D&amp;ust=1465142039050000&amp;usg=AFQjCNHmsqEDYpMFbWn2YsksZ7lNip2fWQ">http://arxiv.org/abs/1505.02074</a></span><span>&nbsp;[awesome-deep-vision list] Exploring Models and Data for Image Question Answering - should go in &quot;images/question answering&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.05612&amp;sa=D&amp;ust=1465142039051000&amp;usg=AFQjCNFgSDExhARQ-qY0TJ6YlyMJy0FsfA">http://arxiv.org/abs/1505.05612</a></span><span>&nbsp;[awesome-deep-vision list] Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering - should go in &quot;machine translation&quot;, &quot;images/question answering&quot;, &quot;turing test&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05756&amp;sa=D&amp;ust=1465142039053000&amp;usg=AFQjCNG1rshZ9GC6P8xPmvGB2M09zmBQLQ">http://arxiv.org/abs/1511.05756</a></span><span>&nbsp;[awesome-deep-vision list] Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction - should go in &quot;weights&quot;, &quot;neural nets making neural nets, how perverse&quot;, &quot;images/question answering&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.00873&amp;sa=D&amp;ust=1465142039054000&amp;usg=AFQjCNEGcgW61o2PAjl53e2M5xQsbGvszw">http://arxiv.org/abs/1502.00873</a></span><span>&nbsp;[awesome-deep-vision list] DeepID3: Face Recognition with Very Deep Neural Networks - should go in &quot;images/faces&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.03832&amp;sa=D&amp;ust=1465142039055000&amp;usg=AFQjCNFIocOsQWka1E-Gif0MCuBwH1ljDg">http://arxiv.org/abs/1503.03832</a></span><span>&nbsp;[awesome-deep-vision list] FaceNet: A Unified Embedding for Face Recognition and Clustering - should go in &quot;images/faces&quot;</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.cf7nxmq8hsef"><span>Apr 2</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.02206&amp;sa=D&amp;ust=1465142039057000&amp;usg=AFQjCNESmp30cki9-X5WpNcI6QXxpBHLgA">http://arxiv.org/abs/1505.02206</a></span><span>&nbsp;[abs:2] images plus the context of what actions created them produce learning of better features than just the images. They mention datasets, and good transfer to other domains. Should go in &quot;transfer learning&quot;, &quot;multi-type input&quot;/&quot;multi-modal input&quot;/&quot;what did I even call this&quot;, &quot;images/classification&quot;, &quot;robotics&quot;, &quot;cars&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.01342&amp;sa=D&amp;ust=1465142039058000&amp;usg=AFQjCNHn9JsytyFM5gwth7hFq11SolYUYw">http://arxiv.org/abs/1506.01342</a></span><span>&nbsp;[abs:3] face recognition with a &quot;bilinear CNN&quot;, which apparently gets unusually good performance for fine-grained recognition; they have a citation for it. Should go in &quot;images/fine-grained&quot;, &quot;images/faces&quot;, &quot;transfer learning&quot; - they also mention public datasets for faces, so &quot;datasets&quot;, and pre-trained models, so &quot;examples&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06881&amp;sa=D&amp;ust=1465142039059000&amp;usg=AFQjCNHJ17QyLvPnChvLDai3LZYTs8zwZg">http://arxiv.org/abs/1511.06881</a></span><span>&nbsp;[abs:1] &quot;Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net&quot; - should go in &quot;images/localization&quot;, &quot;architectures&quot;, &quot;attention/hard attention&quot; (I think that&#39;s what it is, anyway.)</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.03059&amp;sa=D&amp;ust=1465142039060000&amp;usg=AFQjCNGBlWR85YaTb-igidTOKkrsMMIBUw">http://arxiv.org/abs/1506.03059</a></span><span>&nbsp;[abs:3] architecture that generalizes convnets and gives them more representation power with less compute; finds them to produce better results with limited compute, and comparable results with large amounts of compute. Should go in &quot;convolution&quot;, &quot;architectures/weights&quot;, &quot;architectures/activations&quot;, &quot;architectures/pooling&quot;, &quot;speed&quot;, &quot;model compression&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.05724&amp;sa=D&amp;ust=1465142039061000&amp;usg=AFQjCNExi-jftTlAqrecAVdcFDBMsWjJqg">http://arxiv.org/abs/1503.05724</a></span><span>&nbsp;[abs:2] architecture that allows smoothly varying between addition and multiplication (of the different inputs?), with an efficient and backprop-able combination function. Should go in &quot;architectures/weights&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08869&amp;sa=D&amp;ust=1465142039062000&amp;usg=AFQjCNEDX63HrjEQRXY4dOpD1cKsvVClsw">http://arxiv.org/abs/1603.08869</a></span><span>&nbsp;[abs:2] hierarchical non-neural Q learning. Looks cool, I wonder how well it works for neural things? I&#39;ve been wondering about temporally hierarchical reinforcement learning. Should go in &quot;rl&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08832&amp;sa=D&amp;ust=1465142039063000&amp;usg=AFQjCNG7Wfi2OxmJ1pnn-iMkuThvLW4TyQ">http://arxiv.org/abs/1603.08832</a></span><span>&nbsp;[abs:1] non-neural, but interesting analysis of gender bias in fiction written on wattpad. Also, they apparently scrape a dataset; can has? Should go in &quot;datasets&quot;, &quot;applications/misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08575&amp;sa=D&amp;ust=1465142039064000&amp;usg=AFQjCNHi0ua30ZMk2hvDr0Jvydxaot2guQ">http://arxiv.org/abs/1603.08575</a></span><span>&nbsp;[abs:4] attentional generative models? A bit confusing, not sure if it&#39;s attention in a generative model or what. Has variable-length computation, as determined by an rnn; should go in &quot;recurrence&quot;, &quot;images/3d&quot;, &quot;images/rendering&quot;, &quot;generative/autoencoders&quot;, &quot;bayesian neural networks&quot;. Hinton&#39;s the last author, I think it was from his group.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08631&amp;sa=D&amp;ust=1465142039065000&amp;usg=AFQjCNGBXl3FLY9wexqxxTmAUPjLVTZK9A">http://arxiv.org/abs/1603.08631</a></span><span>&nbsp;[title] Classification of Alzheimer&#39;s Disease using fMRI Data and Deep Learning Convolutional Neural Networks - should go in &quot;bio&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08637&amp;sa=D&amp;ust=1465142039065000&amp;usg=AFQjCNEy_uERNYRDa7k4NieH4x4uyNXYWg">http://arxiv.org/abs/1603.08637</a></span><span>&nbsp;[abs:4] generative modeling of 3d images paired with inferring the generative representation from a 2d image. Should go in &quot;images/3d&quot;, &quot;generative/autoencoders&quot;. Very cool looking. Mentions voxels.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08367&amp;sa=D&amp;ust=1465142039066000&amp;usg=AFQjCNEFMNlEzwK_zmPX571frPylBII77Q">http://arxiv.org/abs/1603.08367</a></span><span>&nbsp;[abs:1] sparsity approach of some kind that can be used as a regularizer on a normal neural network. Should go in &quot;sparsity&quot;, &quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08155&amp;sa=D&amp;ust=1465142039067000&amp;usg=AFQjCNGf83g4OzsVRvVXKh01giespush5Q">http://arxiv.org/abs/1603.08155</a></span><span>&nbsp;[abs:3] image transformation, such as superresolution or style transfer, with both human-perceptual and per-pixel losses. Claims to get similar qualitative results to the original style transfer papers. Should go in &quot;applications/art&quot;, &quot;images&quot;, &quot;generative&quot;, &quot;loss functions&quot;, maybe also &quot;adverserial&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08152&amp;sa=D&amp;ust=1465142039068000&amp;usg=AFQjCNFCkRAKprt4UIwk-LxLtLtLueHNRg">http://arxiv.org/abs/1603.08152</a></span><span>&nbsp;[abs:2] bootstrapping car viewpoint prediction by using generated datasets from renderers. Can use much smaller natural datasets, and they find that realism is important. Should go in &quot;applications/cars&quot;, &quot;images/3d&quot;, &quot;images/rendering&quot;, &quot;data augmentation&quot;, &quot;data collection&quot;, &quot;rl&quot;. Maybe also &quot;transfer learning&quot; but that&#39;s a lot of places.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08105&amp;sa=D&amp;ust=1465142039069000&amp;usg=AFQjCNFW4clTmea1iSWIHzhkNwjvSKl3Kg">http://arxiv.org/abs/1603.08105</a></span><span>&nbsp;[abs:1] Unsupervised Domain Adaptation in the Wild: Dealing with Asymmetric Label Sets - automatically selecting a subset of labels to use for a different dataset. Should go in &quot;unsupervised&quot;, &quot;transfer learning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08079&amp;sa=D&amp;ust=1465142039070000&amp;usg=AFQjCNEN4RDk8RwvxqV1u3m_pqVjozjIRQ">http://arxiv.org/abs/1603.08079</a></span><span>&nbsp;[abs:2] disambiguation of a sentence, given a scene that specifies which possible interpretation to use. A dataset for this task, and a demonstration of a model on it. Should go in &quot;images/captioning&quot;, &quot;images/video&quot;, &quot;datasets&quot;, &quot;language/text understanding&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08474&amp;sa=D&amp;ust=1465142039071000&amp;usg=AFQjCNER9Bs7JSvQpfJQfFq5djzIIwFVsg">http://arxiv.org/abs/1603.08474</a></span><span>&nbsp;[abs:2] embedding of object relationships via an embedding model with the task of predicting object relationships in an image given a textual description. They don&#39;t mention its performance in the abstract, suspicious. Cool idea, though, so I&#39;m marking it interesting. Should go in &quot;images/embedding&quot;, I guess. Maybe also &quot;language/embedding&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08458&amp;sa=D&amp;ust=1465142039072000&amp;usg=AFQjCNGfdBERw1vjnjVXXpV41EDHpW8wIg">http://arxiv.org/abs/1603.08458</a></span><span>&nbsp;[abs:1] &quot;Longitudinal Analysis of Discussion Topics in an Online Breast Cancer Community using Convolutional Neural Networks&quot; - should go in &quot;topic modeling&quot;, &quot;datasets&quot;, &quot;applications&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08148&amp;sa=D&amp;ust=1465142039073000&amp;usg=AFQjCNGbLD0oUYtKSvE1stHjgUVsqTJZcg">http://arxiv.org/abs/1603.08148</a></span><span>&nbsp;[abs:2] language model that predicts both word copying and vocabulary words, and a weighting between which one to use. Should go in &quot;language/language models&quot;, &quot;recurrence&quot;, &quot;machine translation&quot;. Cool!</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08042&amp;sa=D&amp;ust=1465142039074000&amp;usg=AFQjCNFa3lPAqD7kTPb9I2_w8LZG2Jeizg">http://arxiv.org/abs/1603.08042</a></span><span>&nbsp;[abs:1] &quot;On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition&quot; - third of the size with negligible loss in accuracy. Should go in &quot;model compression&quot;, &quot;recurrence&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08023&amp;sa=D&amp;ust=1465142039075000&amp;usg=AFQjCNHEvNREonxT2swwqSjDYXKPsglLEg">http://arxiv.org/abs/1603.08023</a></span><span>&nbsp;[abs:2] &quot;How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation&quot; - should go in &quot;objectives&quot;, &quot;language/talking computers&quot;. &quot;these metrics correlate very weakly or not at all with human judgements. We provide recommendations for future development of better automatic evaluation metrics for dialogue systems.&quot; </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08262&amp;sa=D&amp;ust=1465142039076000&amp;usg=AFQjCNEa4xgVbTnU43JnXbiUiMRIK1wPlQ">http://arxiv.org/abs/1603.08262</a></span><span>&nbsp;[abs:0] review? &quot;Towards machine intelligence&quot;. Should go in &quot;generality&quot;, but I think it&#39;s probably a crackpot paper.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08253&amp;sa=D&amp;ust=1465142039077000&amp;usg=AFQjCNEakOGQmB-6c3xJWXDcpm2xkTYP8g">http://arxiv.org/abs/1603.08253</a></span><span>&nbsp;[abs:2] regression using only negative examples! Using negative learning rates - which I&#39;m pretty sure is equivalent to a negative loss function? Seems like you&#39;d want to be negative per-example with a squashing, so single examples don&#39;t dominate. Something like: avg(-log(x) for x in per_example_losses) should go in &quot;loss functions&quot;, &quot;regression&quot;... I want to put it somewhere relevant to the negative examples, but I&#39;m not sure where. I guess &quot;RL&quot; is relevant.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08029&amp;sa=D&amp;ust=1465142039078000&amp;usg=AFQjCNHdhDwJ_1T0OZZ4u-Yhc_dhFICxRw">http://arxiv.org/abs/1603.08029</a></span><span>&nbsp;[abs:3] Resnet in resnet - should go in &quot;architectures&quot;. New SOTA on CIFAR-100. Not surprising, given the general rule that we seem to have found: more thinking time is more betterer.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08511&amp;sa=D&amp;ust=1465142039079000&amp;usg=AFQjCNHUgzikAlY9j64pP4P6CbEl0t5Z_w">http://arxiv.org/abs/1603.08511</a></span><span>&nbsp;[abs:2] colorization of images with CNNs, with a user test that finds that this passes the turing test 20% of the time. Should go in &quot;images/colorization&quot;. Uses heavy &quot;data augmentation&quot; via class-rebalancing something something blah blah. Should also go in &quot;applications/art&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08507&amp;sa=D&amp;ust=1465142039080000&amp;usg=AFQjCNH2mMPCElE2W_X4d4NjabGFBAGfTg">http://arxiv.org/abs/1603.08507</a></span><span>&nbsp;[abs:3] generating text that describes a classification of an image, via an approach that trains this portion unsupervised (I think?), using some sort of reinforcement learning to maximize the sentence&#39;s languageyness (I think). Should go in &quot;language/talking computers&quot;, &quot;safety&quot;, &quot;debugging&quot;, &quot;images/captioning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08486&amp;sa=D&amp;ust=1465142039081000&amp;usg=AFQjCNFra8VJ1Cv1ySm5sp02vkle22ot5A">http://arxiv.org/abs/1603.08486</a></span><span>&nbsp;[title:1] Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation - should go in &quot;bio&quot;, &quot;image captioning&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08358&amp;sa=D&amp;ust=1465142039082000&amp;usg=AFQjCNGcKpBt1glc74Ma2vWIRCegXBKtQA">http://arxiv.org/abs/1603.08358</a></span><span>&nbsp;[abs:3] semantic image segmentation with deep gaussian conditional random fields. Should go in &quot;CRF&quot;, &quot;images/semantic segmentation&quot;, &quot;architectures&quot;. Claims to outperform baselines by a lot. Are the baselines strong enough? This isn&#39;t just appending a small thing, this is a totally different approach; it&#39;s not as obviously comparable.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08321&amp;sa=D&amp;ust=1465142039082000&amp;usg=AFQjCNGxtuRmqH3bCgw1eMcIyeiFuLTs9w">http://arxiv.org/abs/1603.08321</a></span><span>&nbsp;[abs:1] emotion recognition in video+audio. Should go in &quot;images/video&quot;, &quot;language/speech&quot;, &quot;audio&quot;, &quot;datasets&quot;, &quot;multi-type models&quot; or whatever.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08240&amp;sa=D&amp;ust=1465142039083000&amp;usg=AFQjCNGhbpgfFKzERWQEHskCJ73caUps3w">http://arxiv.org/abs/1603.08240</a></span><span>&nbsp;[abs:1] cnn to predict rendering parameters (textures, materials, etc) and light maps to recreate an image, initially trained on synthetic data. Cooool. Should go in &quot;generative&quot;, &quot;images/rendering&quot;, &quot;images/3d&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08212&amp;sa=D&amp;ust=1465142039084000&amp;usg=AFQjCNHdSEwKzUpT5HJep3rtPyVJniDZtg">http://arxiv.org/abs/1603.08212</a></span><span>&nbsp;[abs:1] predicting human pose via a voting based aggregation across a fully convolutional net (but they don&#39;t call it that). Should go in &quot;images/pose&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08199&amp;sa=D&amp;ust=1465142039085000&amp;usg=AFQjCNHr-n-h_ca7w8XEBlhg83QW92Z4Cw">http://arxiv.org/abs/1603.08199</a></span><span>&nbsp;[abs:2] predicting human attention for use as the attention for attentional models on video data. They use &quot;mixture density networks&quot; in the architecture, which also involves recurrence and 3d convolution. Should go in &quot;images/video&quot;, &quot;images/saliency&quot;, and they mention &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.06720&amp;sa=D&amp;ust=1465142039086000&amp;usg=AFQjCNG_zI8vkWLJGiZpTQmk0LzATT3cag">http://arxiv.org/abs/1509.06720</a></span><span>&nbsp;[abs:1] pose estimation using multiple training sets of different types. Should go in &quot;images/pose&quot;, &quot;transfer learning&quot;, &quot;multi-input models&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08270&amp;sa=D&amp;ust=1465142039087000&amp;usg=AFQjCNHZ-EC2gHGNOIsaSjFH3XlFGisq9w">http://arxiv.org/abs/1603.08270</a></span><span>&nbsp;[skim:2] neuromorphic computing chips can apparently run standard models, fast, and get good results. Should go in &quot;speed&quot;, &quot;hardware&quot;, &quot;neuromorphic&quot;. Very very very fast and power efficient feedforward; we&#39;ll probably see these in phones pretty soon. Not sure I&#39;m looking forward. Training still doesn&#39;t work, or it&#39;d be a major jump towards agi, but this is serious news for deployment.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08146&amp;sa=D&amp;ust=1465142039087000&amp;usg=AFQjCNGXIejUm9umwMLzHu5qu-aehRdJGA">http://arxiv.org/abs/1603.08146</a></span><span>&nbsp;[abs:0] spiking neural networks stuff. Should go in &quot;neuromorphic/spiking&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.07385&amp;sa=D&amp;ust=1465142039088000&amp;usg=AFQjCNGwku4D670j_QjjiwOSYcd99X0peA">http://arxiv.org/abs/1509.07385</a></span><span>&nbsp;[abs:1] kinda crappy paper, but looks interesting. &quot;Provable approximation properties for deep neural networks&quot;. Should go in &quot;theory&quot;, &quot;safety&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08551&amp;sa=D&amp;ust=1465142039089000&amp;usg=AFQjCNGHwxM5Tt_v2KQPaMQb_u4_WufMMw">http://arxiv.org/abs/1603.08551</a></span><span>&nbsp;[abs:0] genetic algorithm based nns for 3d shape gen. Should go in &quot;misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08604&amp;sa=D&amp;ust=1465142039090000&amp;usg=AFQjCNF9HwDcYrgLpijUWcZfl9U-jX7E3g">http://arxiv.org/abs/1603.08604</a></span><span>&nbsp;[abs:4] open source financial market prediction thing. Should go in &quot;finance&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08907&amp;sa=D&amp;ust=1465142039090000&amp;usg=AFQjCNGoZLIoz7hmLSpS2LpHB_KUf-B-2g">http://arxiv.org/abs/1603.08907</a></span><span>&nbsp;[abs:1] Cross-modal Supervision for Learning Active Speaker Detection in Video - should go in &quot;language/speech&quot;, &quot;images/video&quot;, &quot;multi-input-type models&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08895&amp;sa=D&amp;ust=1465142039091000&amp;usg=AFQjCNH_MTdd4OjcykiCrOsC7UkYZuLMPQ">http://arxiv.org/abs/1603.08895</a></span><span>&nbsp;[abs:?] Latent Embeddings for Zero-shot Classification - should go in &quot;generality&quot;, &quot;small datasets&quot;, &quot;images/embedding&quot;. Also mentions stuff relevant to &quot;debugging&quot;, because interpretability.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08754&amp;sa=D&amp;ust=1465142039092000&amp;usg=AFQjCNH4gmt452PFVAk3uMYxnERuA59g7g">http://arxiv.org/abs/1603.08754</a></span><span>&nbsp;[abs:?] Multi-Cue Zero-Shot Learning with Strong Supervision - should go in &quot;generality&quot;, &quot;small datasets&quot;, &quot;language/embedding&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08695&amp;sa=D&amp;ust=1465142039093000&amp;usg=AFQjCNFF4PFopMDkqJGA2NBUItu4NxVPUw">http://arxiv.org/abs/1603.08695</a></span><span>&nbsp;[abs:?] Learning to Refine Object Segments - should go in &quot;images/segmentation&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08678&amp;sa=D&amp;ust=1465142039093000&amp;usg=AFQjCNFh0lz7_eChlDoVIIMinMnPDTTXsg">http://arxiv.org/abs/1603.08678</a></span><span>&nbsp;[abs:?] Instance-sensitive Fully Convolutional Networks - should go in &quot;images/segmentation&quot;</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.eeft3bnht5k"><span>Apr 3</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09037&amp;sa=D&amp;ust=1465142039094000&amp;usg=AFQjCNFQxXZzCFnUJ64JRjXkJulOdV7aPQ">http://arxiv.org/abs/1603.09037</a></span><span>&nbsp;[abs:3] speedups by doing learning in a compressed space: &quot;pyramid vector quantization&quot; (haven&#39;t heard of it). They don&#39;t mention performance, but this is very interesting direction. Should go in &quot;speed&quot;, &quot;hardware&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09188&amp;sa=D&amp;ust=1465142039095000&amp;usg=AFQjCNFHHi__aQU-SQk-V396agCkzIuqcA">http://arxiv.org/abs/1603.09188</a></span><span>&nbsp;[abs:2] new task and new dataset for visual &quot;sense disambiguation&quot; - looks like a kinda meh dataset, additional labels for coco and tuhi. Should go in &quot;datasets&quot;, &quot;images/captioning&quot;, &quot;language/word grounding&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09170&amp;sa=D&amp;ust=1465142039096000&amp;usg=AFQjCNFGQPDTEbMNZMrCC9GyRI74NTXnUw">http://arxiv.org/abs/1603.09170</a></span><span>&nbsp;[abs:1] language models using some sort of random field things. Should go in &quot;language/language models&quot;. Non neural, but gets similar performance? Should also go in &quot;alternate deep techniques&quot;, &quot;crf&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09128&amp;sa=D&amp;ust=1465142039097000&amp;usg=AFQjCNEfUXAar52xw4jTaXv4UebQdna72Q">http://arxiv.org/abs/1603.09128</a></span><span>&nbsp;[abs:3] learning multi-sense embeddings with an autoencoder that encodes context, and must select a single sense embedding to use to represent the concept. Sounds pretty cool, and the concept seems like it has interesting implications for how to create unsupervised learning that can learn more complicated structural concepts. Should go in &quot;autoencoders&quot;, &quot;language/word embeddings&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08988&amp;sa=D&amp;ust=1465142039098000&amp;usg=AFQjCNFhGCaqOiZ2ydndozf6EcJJP_rR4w">http://arxiv.org/abs/1603.08988</a></span><span>&nbsp;[abs:1] non-neural bayesian methods: more efficient approximate black-box inference in limited compute budget. Should go in &quot;bayesian&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.08988&amp;sa=D&amp;ust=1465142039098000&amp;usg=AFQjCNFhGCaqOiZ2ydndozf6EcJJP_rR4w">http://arxiv.org/abs/1603.08988</a></span><span>&nbsp;[abs:0] non-learned convnet, with predefined wavelet transforms, for feature extraction for palmprint recognition. Should go in &quot;misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09016&amp;sa=D&amp;ust=1465142039099000&amp;usg=AFQjCNErj7iZk_BD4g1a3By-qIy54QXhXg">http://arxiv.org/abs/1603.09016</a></span><span>&nbsp;[intro:1] image captioning improvements: faster, higher quality, more generalization. Uses entity recognition on well-known things, seems to be its main feature. Should go in &quot;images/captioning&quot;, &quot;talking computers&quot;, &quot;multi-modal models&quot;. </span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09129&amp;sa=D&amp;ust=1465142039100000&amp;usg=AFQjCNFbQu5IQLKsEXKhuhswJ8-t7ZTUrw">http://arxiv.org/abs/1603.09129</a></span><span>&nbsp;[abs:0] non-neural facial landmark based emotion detection. Should go in &quot;images/faces&quot;. Maybe interesting as a target for model &quot;compression&quot;?</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09065&amp;sa=D&amp;ust=1465142039101000&amp;usg=AFQjCNFhyxGrFPE_k2sVU6aZZ4YE_2jUCg">http://arxiv.org/abs/1603.09065</a></span><span>&nbsp;[abs:1] human pose estimation that doesn&#39;t use a convnet, but outperforms convnet baselines anyway. Should go in &quot;images/pose&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09114&amp;sa=D&amp;ust=1465142039101000&amp;usg=AFQjCNEIudhHMCaGgIyY063Ag6cazE_z_A">http://arxiv.org/abs/1603.09114</a></span><span>&nbsp;[headings:1] alternative to SIFT that uses deep networks to infer notable features in images. Should go in &quot;images/localization&quot;, &quot;images/similarity&quot;, &quot;convolution&quot;. Uses STNs and rotation matching and stuff too. Maybe also &quot;architectures&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09056&amp;sa=D&amp;ust=1465142039102000&amp;usg=AFQjCNFEYbn1cT8v0Vd9lJ6ueNSr_dhfxA">http://arxiv.org/abs/1603.09056</a></span><span>&nbsp;[abs:1] image denoising with skip connections, like ladder networks have. Should go in &quot;images/superresolution&quot;, &quot;unsupervised/denoising autoencoders&quot;, &quot;architectures/very deep&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09046&amp;sa=D&amp;ust=1465142039103000&amp;usg=AFQjCNExF1rf_ECSgX3jJfqsO6Y_yfuKMw">http://arxiv.org/abs/1603.09046</a></span><span>&nbsp;[abs:3] feature representation architecture for feeding to an image captioning lstm - uses a &quot;spatial pyramid VLAD&quot; coding, apparently has many fewer dimensions in the feature vectors (or maybe in the parameters?) and produces good results. Should go in &quot;images/captioning&quot;, &quot;attention&quot; (I guess?? Sort of?), &quot;convolution&quot;. Maybe &quot;representation&quot;? Maybe &quot;architectures&quot;? Maybe &quot;memory&quot;?</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09002&amp;sa=D&amp;ust=1465142039104000&amp;usg=AFQjCNHCA1mNM7-GeUWEH190eOIs9gV1ZQ">http://arxiv.org/abs/1603.09002</a></span><span>&nbsp;[abs:0.5] suspicious-sounding &quot;dataflow matrix machines&quot; as a generalization of RNNs. should go in &quot;recurrence&quot;, &quot;weights&quot;. Maybe &quot;architectures&quot;</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09123&amp;sa=D&amp;ust=1465142039105000&amp;usg=AFQjCNFZ0wJkZX5Fhr9JSHxd1jLIDXaapA">http://arxiv.org/abs/1603.09123</a></span><span>&nbsp;[abs:1] &quot;deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks&quot; - should go in &quot;bio&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09050&amp;sa=D&amp;ust=1465142039106000&amp;usg=AFQjCNH5cgHWociI4siBXTFKY0aiNo7qug">http://arxiv.org/abs/1603.09050</a></span><span>&nbsp;[abs:2] analysis of active learning non-neural bayesian stuff, for bad priors: &quot;Robustness of Bayesian Pool-based Active Learning Against Prior Misspecification&quot; - should go in &quot;bayesian&quot;, &quot;active learning&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09035&amp;sa=D&amp;ust=1465142039107000&amp;usg=AFQjCNHcCn0Q0QwpR57oa65PiTGwLl9SsQ">http://arxiv.org/abs/1603.09035</a></span><span>&nbsp;[abs:2] geo-distributed machine learning - parallelization of training across high-latency links. Sometimes works, sometimes works better. Should go in &quot;parallelization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09025&amp;sa=D&amp;ust=1465142039108000&amp;usg=AFQjCNEIcJPzrUKtmxuSenfcM5GNHr7PCg">http://arxiv.org/abs/1603.09025</a></span><span>&nbsp;[abs:3] batch normalization for recurrent networks. (did it not already work?) - should go in &quot;recurrence&quot;, &quot;regularization&quot;. Looks pretty awesome. Not literally world-changing, but a pretty important regularization, considering how well batch norm works.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09246&amp;sa=D&amp;ust=1465142039108000&amp;usg=AFQjCNEFUZoRQY3vJpVV_Zq_zwgUFAWvYQ">http://arxiv.org/abs/1603.09246</a></span><span>&nbsp;[abs:2] interesting sort of denoising unsupervised learning - correcting large structural errors in the data, in the form of solving jigsaw puzzles generated from images. They use a smaller alexnet, or something. Should go in &quot;unsupervised&quot;. Gets very good performance with only unsupervised training. What does this imply about unsupervised objectives? Surely there&#39;s a generalization&hellip;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.05328&amp;sa=D&amp;ust=1465142039109000&amp;usg=AFQjCNH1AKEgF5t7MlkX_G0PcbtMy3bJNg">http://arxiv.org/abs/1508.05328</a></span><span>&nbsp;[abs:1] non-neural RL algorithm: Multi-agent Reinforcement Learning with Sparse Interactions by Negotiation and Knowledge Transfer ; should go in &quot;RL&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.08119&amp;sa=D&amp;ust=1465142039110000&amp;usg=AFQjCNGUc1VN9SAwy4FezikbrvL1JqyBJw">http://arxiv.org/abs/1511.08119</a></span><span>&nbsp;[abs:1] semantic segmentation with better priors. Also, apparently conditional random fields are used primarily for semantic segmentation. &quot;Higher Order Conditional Random Fields in Deep Neural Networks&quot;, should go in &quot;images/semantic segmentation&quot;, &quot;crf&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1601.00248&amp;sa=D&amp;ust=1465142039111000&amp;usg=AFQjCNG0HNRecfvdS6pijuXGNdJsggXMMg">http://arxiv.org/abs/1601.00248</a></span><span>&nbsp;[abs:2] alternative to perplexity for evaluating language models; allows comparison between different vocabularies or models with unusual structure such as sentence-level ones, which perplexity apparently doesn&#39;t allow for. Should go in &quot;language/language models&quot;, &quot;objectives&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09638&amp;sa=D&amp;ust=1465142039112000&amp;usg=AFQjCNFrWDPcsA9BWRfYI_FXzvByZmQB-w">http://arxiv.org/abs/1603.09638</a></span><span>&nbsp;[abs:1] non-neural (I think?) malware detection with machine learning. Regularizes with extra features available only at training time, which is independently an interesting idea. Should go &quot;security&quot;, &quot;objectives&quot;/&quot;regularization&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09420&amp;sa=D&amp;ust=1465142039112000&amp;usg=AFQjCNEkqeLEqo69me1nhVH6c-1iQ0L1mw">http://arxiv.org/abs/1603.09420</a></span><span>&nbsp;[abs:2] new alternative to GRU/LSTM, designed for simplicity - has fewer trainable parameters and fewer ops, and claims to compete with GRU (which is not as good as LSTM). I&#39;m curious how it performs on long-term gradient tests, where it has only to remember something through many layers of nothing else happening. Should go in &quot;recurrence&quot;, &quot;architectures&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09441&amp;sa=D&amp;ust=1465142039113000&amp;usg=AFQjCNGhP-VU4kA-xjvSZkwUhu-BLFeN3g">http://arxiv.org/abs/1603.09441</a></span><span>&nbsp;[abs:2] a couple of functions that are hard to find optima for, designed to be similar to hyperparameter optimization, but much much faster to evaluate. Should go in &quot;hyperparameters/optimization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09381&amp;sa=D&amp;ust=1465142039114000&amp;usg=AFQjCNG7DF9ixaco_wjVx5WohVs5CoDkBw">http://arxiv.org/abs/1603.09381</a></span><span>&nbsp;[abs:0] Clinical Information Extraction via Convolutional Neural Network - application; boring. Should go in &quot;bio&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09742&amp;sa=D&amp;ust=1465142039115000&amp;usg=AFQjCNHl_Qa3MISbGJJ0bctKs1kNFTJYHQ">http://arxiv.org/abs/1603.09742</a></span><span>&nbsp;[abs:1] fully convolutional networks with an extra prior about object boundaries. Should go in &quot;images/semantic segmentation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09687&amp;sa=D&amp;ust=1465142039116000&amp;usg=AFQjCNE-YTCRDJ9OCE407fmGmCEiQd8UmA">http://arxiv.org/abs/1603.09687</a></span><span>&nbsp;[abs:2] content-based search of images in large scale indexed search system lucene. Should go in &quot;images/retrieval&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09439&amp;sa=D&amp;ust=1465142039117000&amp;usg=AFQjCNE16nJlgUF1F110d_qtAWZHq0Ygrw">http://arxiv.org/abs/1603.09439</a></span><span>&nbsp;[abs:2] dataset of vine videos with some sort of labels or other. Should go in &quot;datasets&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09423&amp;sa=D&amp;ust=1465142039117000&amp;usg=AFQjCNGDtSVCrLCPdmqx85OpKZamGM4Y8g">http://arxiv.org/abs/1603.09423</a></span><span>&nbsp;[skim abs:1] localization of text in an image. Uses a custom architecture for the purposes, there&#39;s something or other special about it beyond just localization. Should go in &quot;images/ocr&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09727&amp;sa=D&amp;ust=1465142039118000&amp;usg=AFQjCNHXugOna-FLBeLe6fmS_tGO4jxUgw">http://arxiv.org/abs/1603.09727</a></span><span>&nbsp;[abs:2] cool! Correcting typos in language and such. Should go in &quot;language/language models&quot;, &quot;language/text understanding&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09643&amp;sa=D&amp;ust=1465142039119000&amp;usg=AFQjCNHR-jqpw4hgPC3Q-4IUWb7rPGwELg">http://arxiv.org/abs/1603.09643</a></span><span>&nbsp;[abs:1] speech recognition and speaker recognition as one task. Should go in &quot;language/speech&quot;. Also uses &quot;recurrence&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09630&amp;sa=D&amp;ust=1465142039120000&amp;usg=AFQjCNHdrUTBcKauempRZ2_srG9bdLzkWA">http://arxiv.org/abs/1603.09630</a></span><span>&nbsp;[abs:1] parameterized pooling operations for something or other. Used for acoustic models, not completely clear what the objective is. Should go in &quot;audio&quot;, &quot;language/speech&quot;, &quot;architecture/pooling&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09509&amp;sa=D&amp;ust=1465142039121000&amp;usg=AFQjCNFo-lMKBaQukiD5zikMGLNr8OsRBQ">http://arxiv.org/abs/1603.09509</a></span><span>&nbsp;[abs:2] audio recognition works better if you convolve at high resolution over the waveforms than if you convolve spectrograms; this seems unsurprising. It does take more compute, but oh well. Seems like fft convolutions imply interesting things about speeding this up - like, shouldn&#39;t you be able to apply the transformation this implies to frequency space, such that you can still take advantage of frequency space things? Should go in &quot;audio&quot;, &quot;language/speech recognition&quot;, &quot;architecture/architectures&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09460&amp;sa=D&amp;ust=1465142039122000&amp;usg=AFQjCNHXhofsGBekKt5gp6nL_4cTqjLKZQ">http://arxiv.org/abs/1603.09460</a></span><span>&nbsp;[abs:1] something something speech recognition better with nns combined with other things. &quot;System Combination for Short Utterance Speaker Recognition&quot; - should go in &quot;language/speech recognition&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09457&amp;sa=D&amp;ust=1465142039123000&amp;usg=AFQjCNFZEN7pv_rjrGBJlM8iduZ6yWsWmw">http://arxiv.org/abs/1603.09457</a></span><span>&nbsp;[abs:3] LSTM language models on irc data benefit from speaker embeddings. What a surprise. (the irc data is from #ubuntu, it&#39;s that ubuntu dataset popping up again.) should go in &quot;language/language models&quot;, &quot;unsupervised/embeddings&quot; (?? I want to put it somewhere about embeddings, but it doesn&#39;t fit anywhere. &quot;Embedding types&quot; maybe?)</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.09405&amp;sa=D&amp;ust=1465142039124000&amp;usg=AFQjCNG0BxkBjmeggSHLPW5B7UbVVs6Bfw">http://arxiv.org/abs/1603.09405</a></span><span>&nbsp;[abs:2] multi-input-type bilstm: word embeddings and one-hot characters; works better for sentence relationship modeling. Should go in &quot;language/language models&quot; (I think?), &quot;multi-input-type models&quot;</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.1glzhyqaxy7w"><span>Apr 4</span></h5><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00187&amp;sa=D&amp;ust=1465142039125000&amp;usg=AFQjCNFjApom53yiuIp13Czo1SwmnsODPw">http://arxiv.org/abs/1604.00187</a></span><span>&nbsp;[abs:1] &quot;PHOC&quot; representation of CNN inputs? Of cnn features? Weights? Anyway, applied to &quot;word spotting&quot;, which appears to be some sort of ocr thing. Should go in &quot;images/ocr&quot; (was it &quot;language/ocr&quot;?), &quot;convolution&quot;, and tentatively &quot;weight/representation&quot; and &quot;architectures&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00125&amp;sa=D&amp;ust=1465142039126000&amp;usg=AFQjCNGo5dOzkPGKBemVvndaUTRV8K8YbA">http://arxiv.org/abs/1604.00125</a></span><span>&nbsp;[abs:1] attentional document summarization/ranking, for use with search results - eg, google. Should go in &quot;language/text understanding&quot;, maybe &quot;language/summarization&quot; or &quot;language/embedding&quot; or &quot;language/retrieval&quot;, &quot;attention&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.04450&amp;sa=D&amp;ust=1465142039127000&amp;usg=AFQjCNFoPknW8lDbZR2UXGLeZkMv7GbZLQ">http://arxiv.org/abs/1602.04450</a></span><span>&nbsp;[abs:1] bayesian optimization with constraints that are dangerous to violate. Should go in &quot;safety&quot;, &quot;hyperparameters&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00279&amp;sa=D&amp;ust=1465142039128000&amp;usg=AFQjCNGgA8zASkgrr3l-hGxXjJHDNWzfvQ">http://arxiv.org/abs/1604.00279</a></span><span>&nbsp;[abs:0] application of rnns to do something related to quantum stuff. Should go in &quot;applications/physics&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00289&amp;sa=D&amp;ust=1465142039129000&amp;usg=AFQjCNG2GFB1IRnPkmAAPao8L92lDqME6Q">http://arxiv.org/abs/1604.00289</a></span><span>&nbsp;[abs:2] AI-directions paper, focusing on what architectural problems there are with current neural networks before we can do more human-like things. Should go in &quot;reviews&quot;, &quot;generality&quot;. Also related to &quot;safety&quot;, I suppose.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00077&amp;sa=D&amp;ust=1465142039130000&amp;usg=AFQjCNHcd1s4Uq4pCUnjTWuhi5Zv7a3LGg">http://arxiv.org/abs/1604.00077</a></span><span>&nbsp;[abs:1] sequence classification with attentional rnns, and an application of this. Should go in &quot;language/classification&quot;, &quot;classification&quot;, &quot;recurrence&quot;, &quot;attention&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00100&amp;sa=D&amp;ust=1465142039130000&amp;usg=AFQjCNG2tSkT7K2N08xm-TNyYdVaR1XNVA">http://arxiv.org/abs/1604.00100</a></span><span>&nbsp;[abs:3] language model that uses some sort of learned composition trees to discard the assumption of linearity used in next-word style models, and gets very dramatically better performance for it. Should go in &quot;language/language models&quot;, &quot;recurrence&quot;, &quot;graphs&quot;/&quot;trees&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00117&amp;sa=D&amp;ust=1465142039131000&amp;usg=AFQjCNFECcM8CJDNwPBpO6LiM5AmWjFnRg">http://arxiv.org/abs/1604.00117</a></span><span>&nbsp;[abs:2] multi-task language slot-filling model; designed to minimize difficulty of transfer learning. Should go in &quot;language/slot filling&quot; (maybe just &quot;language/language models&quot;?), &quot;datasets&quot; (they have a new dataset), and &quot;multi-objective models&quot;. Also &quot;transfer learning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00126&amp;sa=D&amp;ust=1465142039132000&amp;usg=AFQjCNEQ8FK5FuHbsdJEFTKQKFZhSDUnDQ">http://arxiv.org/abs/1604.00126</a></span><span>&nbsp;[abs:0] non-neural, but interesting new topic modeling approach: &quot;Nonparametric Spherical Topic Modeling with Word Embeddings&quot; - should go in &quot;language/topic modeling&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00317&amp;sa=D&amp;ust=1465142039133000&amp;usg=AFQjCNH-ueqne7_4vuLH7xfOA8sag39YNQ">http://arxiv.org/abs/1604.00317</a></span><span>&nbsp;[abs:1] &quot;A Semisupervised Approach for Language Identification based on Ladder Networks&quot; - should go in &quot;language/language identification&quot; and/or &quot;language/speech&quot;, &quot;semi-supervised&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00036&amp;sa=D&amp;ust=1465142039134000&amp;usg=AFQjCNGVNge_Jgfz8X67g7I0HCBx4lcZ0g">http://arxiv.org/abs/1604.00036</a></span><span>&nbsp;[abs:0] non-neural, I think? Uses convnet activations. &quot;Modeling Visual Compatibility through Hierarchical Mid-level Elements&quot; ; should go in &quot;images/similarity&quot;. Seems somewhat related to neural style, could also go in applications/art.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00066&amp;sa=D&amp;ust=1465142039134000&amp;usg=AFQjCNHFYdWcYtTzAf8hdmqqMNnI2zIGcw">http://arxiv.org/abs/1604.00066</a></span><span>&nbsp;[abs:0] predicting stability of wooden blocks with a non-neural physical simulation approach. Should go in &quot;misc&quot;, &quot;rl&quot;, &quot;robots&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00092&amp;sa=D&amp;ust=1465142039135000&amp;usg=AFQjCNGmipdIiXjxpGdKCjb-DKNJALhZGA">http://arxiv.org/abs/1604.00092</a></span><span>&nbsp;[abs:2] alternate convnet architecture based on a restricted architecture that allows very efficient inference. Should go in &quot;architectures&quot;, &quot;weight representation&quot;, &quot;alternate deep techniques&quot;, &quot;images/semantic segmentation&quot;. Mentions that it&#39;s closely related to &quot;CRF&quot;, and compares favorably to techniques that use actual CRFs.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00133&amp;sa=D&amp;ust=1465142039136000&amp;usg=AFQjCNGQ0Qj6w158Sgu-Cx6esn5wbBRX0w">http://arxiv.org/abs/1604.00133</a></span><span>&nbsp;[abs:0] &quot;Good Practice in CNN Feature Transfer&quot; - unclear what they&#39;re applying it to. Should go in &quot;images/similarity&quot;, I guess.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00136&amp;sa=D&amp;ust=1465142039137000&amp;usg=AFQjCNECqlae-TAvzLOIkZBUocesKoAVQQ">http://arxiv.org/abs/1604.00136</a></span><span>&nbsp;[abs:1] non-neural inference thingy for detecting motion in the environment even with ego-motion and similar interference. Introduces a new dataset for the purpose. Should go in &quot;images/segmentation&quot;, &quot;images/video&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.ub8jhlcgp0dc"><span>Apr 5</span></h5><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.06342&amp;sa=D&amp;ust=1465142039138000&amp;usg=AFQjCNGx-RaDvLfQZniL6TAlXW9uwo0Uag">http://arxiv.org/abs/1511.06342</a></span><span>&nbsp;[abs:4] model-compression-based generalization, in the context of RL - train multiple different networks on separate tasks, then compress them all into a single model. The compressed model will generalize dramatically better, and can successfully play some games with no experience(!). Should go in &quot;RL&quot;, &quot;multi-task&quot;, &quot;towards generality&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://shapenet.cs.stanford.edu/projects/JointEmbedding/&amp;sa=D&amp;ust=1465142039139000&amp;usg=AFQjCNFeBva6t88qYEa5unQ4kFgfbKyv1w">https://shapenet.cs.stanford.edu/projects/JointEmbedding/</a></span><span>&nbsp;and </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://shapenet.cs.stanford.edu/projects/JointEmbedding/JointEmbedding.pdf&amp;sa=D&amp;ust=1465142039140000&amp;usg=AFQjCNFbqQ_ZZBWkL50_ZbHtRDIgTmNSYQ">https://shapenet.cs.stanford.edu/projects/JointEmbedding/JointEmbedding.pdf</a></span><span>&nbsp;[abs:3] embedding of 3d shapes and images containing them into the same distributed space, via first embedding the models and then training a convnet to embed images of the models into the same space. Should go in &quot;images/embedding&quot;, &quot;images/3d&quot;. Is based on the shapenet dataset, which is freely available; should go in &quot;datasets&quot;. Comes with code, based on caffe. Could go in &quot;examples&quot;, though caffe sucks.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00861&amp;sa=D&amp;ust=1465142039141000&amp;usg=AFQjCNGMeRiRco5pFw8zFTYTNQY8gGQFDQ">http://arxiv.org/abs/1604.00861</a></span><span>&nbsp;[abs:1] sound event detection with only a bidirectional LSTM directly on audio, no convolution or such things. Should go in &quot;audio&quot;, &quot;classification&quot;, &quot;recurrence&quot;, &quot;data augmentation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.07679&amp;sa=D&amp;ust=1465142039142000&amp;usg=AFQjCNGj_d898iWXhjag0YrQAUt6cBScZw">http://arxiv.org/abs/1512.07679</a></span><span>&nbsp;[abs:2] reinforcement learning on tasks with very large but discrete and potentially finite action spaces. Uses an action embedding, and a non-neural lookup to select the action to take, based on approximate nearest neighbor search. Should go in &quot;rl&quot;, &quot;recommendation&quot;, maybe &quot;non-neural&quot; because of the funky ending layer. Related to thoughts I&#39;ve had of this type - using databases as nn layers.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.00103&amp;sa=D&amp;ust=1465142039143000&amp;usg=AFQjCNEt5QYscupHYeks4aWJIUW3tU_FYA">http://arxiv.org/abs/1512.00103</a></span><span>&nbsp;[abs:2] language processing that reads in bytes (not characters - utf8, I think), and outputs spans of bytes to use for&hellip; something. They use it on multiple languages without giving it explicit representations for any of them. Should go in &quot;language/text understanding&quot;, &quot;machine translation&quot;. Will probably generate very interesting embeddings, because of the sort of blindsight this will force - it won&#39;t be able to remember all of any of the languages, so it can&#39;t learn a vocabulary.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05726&amp;sa=D&amp;ust=1465142039144000&amp;usg=AFQjCNG4XILt1UmY7j8de7kBWp4SMgljnA">http://arxiv.org/abs/1512.05726</a></span><span>&nbsp;[abs:3] question retrieval for things like stack overflow, with semantic lookup based on semi-supervised training, done in multiple stages. First trains something based on generating a question from its title (I think? Maybe the other direction?), and then trains on labels of &quot;this is a duplicate of that&quot;. Gets good results. Should go in &quot;language/embeddings&quot;, &quot;architectures&quot;. Also should go in &quot;recurrence&quot; and &quot;convolution&quot;, because it introduces some sort of recurrent convolution model. Unclear how that compares to the CGRU, but it looks interesting.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05960&amp;sa=D&amp;ust=1465142039145000&amp;usg=AFQjCNEjQIjFzNbYek0r7PwQaiHP3JFA4Q">http://arxiv.org/abs/1511.05960</a></span><span>&nbsp;[abs:1] attentional model applied to visual question answering. Appears to be mostly uninteresting in terms of methods, but gets good results. Should go in &quot;images/question answering&quot;, &quot;language/talking computers&quot;. Maaaybe also &quot;attention&quot;, &quot;recurrence&quot;, &quot;convolution&quot; as those are the things it uses; it&#39;s always good to build up lists of &quot;this uses that&quot; so you can get tricks from application papers.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.05830&amp;sa=D&amp;ust=1465142039146000&amp;usg=AFQjCNEnyzR2es33n2QT2fQFiQLjUG8TMA">http://arxiv.org/abs/1512.05830</a></span><span>&nbsp;[visual skim:2] backprop structure that uses auxilary losses to train earlier layers, with the range of layers each loss trains overlapping. Allows training deeper networks, but nothing remotely like what resnets can do. Seems to be a similar idea, though. Should go in &quot;training algorithms&quot; (sort of), &quot;architectures&quot;. Applied to scene recognition, so it should also go in &quot;images/classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.01626&amp;sa=D&amp;ust=1465142039147000&amp;usg=AFQjCNHYqq-sGR3JC2r6r3QebTrBDIOKPg">http://arxiv.org/abs/1509.01626</a></span><span>&nbsp;[abs:2] per-character convnets on text; yann lecun is last author, maybe from his group. They also mention constructing datasets, which may be available. Should go in &quot;convolution&quot;, &quot;language/classification&quot; and/or &quot;language/per-character&quot;. Note: doesn&#39;t look recurrent. Should also go in &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00697&amp;sa=D&amp;ust=1465142039148000&amp;usg=AFQjCNEzuBvAgKijsLDq3CdvZMQuRVnAJg">http://arxiv.org/abs/1604.00697</a></span><span>&nbsp;[abs:0] more efficient implementations of stuff on truenorth. Should go in &quot;neuromorphic&quot;, &quot;hardware&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00644&amp;sa=D&amp;ust=1465142039149000&amp;usg=AFQjCNFzEv_2Sp-PTIpqdj1vYUCAz7Iu6Q">http://arxiv.org/abs/1604.00644</a></span><span>&nbsp;[abs:1] neuroevolution on games. Should go in &quot;training algorithms&quot;, &quot;rl&quot;. Still haven&#39;t seen neuroevolution head-to-head with a well-tuned recurrent q learner.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00449&amp;sa=D&amp;ust=1465142039150000&amp;usg=AFQjCNGZZRoW-lkAlcQfUwt_TTWST05mzQ">http://arxiv.org/abs/1604.00449</a></span><span>&nbsp;[abs:3] model voxelization, given a series of shots of the model, recurrently. Uses synthetic data, but not labels. Should go in &quot;images/3d&quot;. Performs much better, as expected.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00494&amp;sa=D&amp;ust=1465142039151000&amp;usg=AFQjCNHbXQgxX8GoxSeRHsfWbDQ_Vfte-A">http://arxiv.org/abs/1604.00494</a></span><span>&nbsp;[abs:1] &quot;A Fully Convolutional Neural Network for Cardiac Segmentation in Short-Axis MRI&quot; - should go in &quot;images/segmentation&quot;, &quot;bio&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00546&amp;sa=D&amp;ust=1465142039152000&amp;usg=AFQjCNHJJ4FZaBwbV1Lcf1XMeqUtPpsSng">http://arxiv.org/abs/1604.00546</a></span><span>&nbsp;[abs:1] non-neural - focus quality metrics. Interesting as a baseline, possibly interesting as an objective. Should go in &quot;images/misc&quot;, &quot;objectives&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00600&amp;sa=D&amp;ust=1465142039153000&amp;usg=AFQjCNHkkyo2cocoHdRUhkU51O-XWnMrMg">http://arxiv.org/abs/1604.00600</a></span><span>&nbsp;[abs:2] object localization thing that uses a deep feature extraction network first, and then does iterative processing on the feature map. (I think.) should go in &quot;images/localization&quot;, &quot;architectures&quot;. &quot;we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 5 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00676&amp;sa=D&amp;ust=1465142039153000&amp;usg=AFQjCNFK458uwa6DVdRwAChSxRLgOqxL1w">http://arxiv.org/abs/1604.00676</a></span><span>&nbsp;[abs:2] add multiple bias+activation outputs per linear (or conv) output, to allow selecting different magnitudes, which is apparently a common thing for filters to learn. Should go in &quot;architectures&quot;, &quot;model compression&quot;, &quot;images/classification&quot;, &quot;biases&quot;, maybe &quot;weights&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.ghslxx1vvof0"><span>Apr 6</span></h5><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00790&amp;sa=D&amp;ust=1465142039155000&amp;usg=AFQjCNGnC22NBwBh63E5267I6gPVH_1qvg">http://arxiv.org/abs/1604.00790</a></span><span>&nbsp;[abs:1] bilstm for image captioning. Comes with some interesting bonuses - data augmentation references, visualization of the internal states. Should go in &quot;images/captioning&quot;, &quot;language/talking computers&quot;, &quot;recurrence&quot;, &quot;data augmentation&quot;, &quot;debugging&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00825&amp;sa=D&amp;ust=1465142039156000&amp;usg=AFQjCNEbAcHXwaGVEX1DnjaZYm8dt7q_gg">http://arxiv.org/abs/1604.00825</a></span><span>&nbsp;[abs:2] input-relevance debugging technique, &quot;relevance propogation&quot;, for networks that include local response normalization. Should go in &quot;debugging&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00906&amp;sa=D&amp;ust=1465142039157000&amp;usg=AFQjCNELW5ytpFUEFKKP2Iy9Gsg7m62veg">http://arxiv.org/abs/1604.00906</a></span><span>&nbsp;[abs:1] dataset and approach for determining what someone was doing, given only video from their perspective, which may be missing a view of their hands, and is definitely missing a view of their eyes. Should go in &quot;images/video&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00974&amp;sa=D&amp;ust=1465142039158000&amp;usg=AFQjCNGUPDZOUrzOA1-zN_iH0cMWrWKVWg">http://arxiv.org/abs/1604.00974</a></span><span>&nbsp;[abs:1] handwriting verification thing. Application of cnns, uses writer-dependent classifiers. Mentions datasets for the topic. Should go in &quot;datasets&quot;, &quot;security&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00989&amp;sa=D&amp;ust=1465142039159000&amp;usg=AFQjCNHi9ZvBsapUCZwtdq9YY4FhZg0HdA">http://arxiv.org/abs/1604.00989</a></span><span>&nbsp;[abs:2] (non-neural contribution, &quot;rank ordering&quot;) - classifying faces by identity - unsupervised. Classifying, because it performs well empirically on a supervised classification problem without ever having seen the labels. Very large classification problem with a cluster to example ratio of 1:10, for 100 million images! Should go in &quot;images/faces&quot;, &quot;unsupervised/clustering&quot;, &quot;classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00999&amp;sa=D&amp;ust=1465142039160000&amp;usg=AFQjCNFeMICD5CkzFSNNGf5nVJwsCbTaOw">http://arxiv.org/abs/1604.00999</a></span><span>&nbsp;[abs:1] analysis of rgbd datasets usefulness and review and stuff. Should go in &quot;datasets&quot; and &quot;reviews&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00923&amp;sa=D&amp;ust=1465142039161000&amp;usg=AFQjCNEgwz1T-Wst3ZB82rfi5T-x4tZBGQ">http://arxiv.org/abs/1604.00923</a></span><span>&nbsp;[abs:2] more data-efficient RL off-policy learning algorithm. Should go in &quot;rl&quot;, maybe &quot;data efficiency&quot;. Intended for use with dangerous environments.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00981&amp;sa=D&amp;ust=1465142039162000&amp;usg=AFQjCNGGvM0tlF-hkFydk7s-J3HW_riP4Q">http://arxiv.org/abs/1604.00981</a></span><span>&nbsp;[abs:2] another paper that tries synchronous parallel sgd. Should go in &quot;parallelism&quot;, &quot;training algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00457&amp;sa=D&amp;ust=1465142039163000&amp;usg=AFQjCNEAk4OIgKR00GobLQgYsqR5X1RNHw">http://arxiv.org/abs/1604.00457</a></span><span>&nbsp;[abs:0] &quot;analytic neural networks&quot; something something blah blah. Should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00462&amp;sa=D&amp;ust=1465142039164000&amp;usg=AFQjCNHFIWKxj-I4GaT5vm8xEr1vvGTBTQ">http://arxiv.org/abs/1604.00462</a></span><span>&nbsp;[abs:0] some neuromorphic thing. Should go in &quot;neuromorphic&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00552&amp;sa=D&amp;ust=1465142039165000&amp;usg=AFQjCNF5C6UaEPFeQtTjnCFI7t9LHm5xXA">http://arxiv.org/abs/1604.00552</a></span><span>&nbsp;[abs:0] application of nns to ph prediction of drinking water. Should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00558&amp;sa=D&amp;ust=1465142039165000&amp;usg=AFQjCNGyJFlEpKhkPcVvYVV8CIuL9aM5yg">http://arxiv.org/abs/1604.00558</a></span><span>&nbsp;[abs:0] application of nns to some radio signal control thing. Should go in &quot;applications/misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00545&amp;sa=D&amp;ust=1465142039167000&amp;usg=AFQjCNGXV6YOBiVeP2tHtrg2ohzmAAkZIg">http://arxiv.org/abs/1604.00545</a></span><span>&nbsp;[abs:2] paper on the safety problem from james babcock and friends. Should go in &quot;safety&quot;. Note: just a review, doesn&#39;t look that interesting (sorry, james).</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00400&amp;sa=D&amp;ust=1465142039168000&amp;usg=AFQjCNElfGktk5vOzTZSMTVlOI6q4Dmo2Q">http://arxiv.org/abs/1604.00400</a></span><span>&nbsp;[abs:2] better evaluation metric for summary quality of scientific paper; apparently normal summary quality metrics normally used kinda suck. Should go in &quot;objectives&quot;, &quot;language/text understanding&quot;, &quot;language/summarization&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00425&amp;sa=D&amp;ust=1465142039169000&amp;usg=AFQjCNHza7QEVD80rjI_K4iGQzsHhgjYPQ">http://arxiv.org/abs/1604.00425</a></span><span>&nbsp;[abs:2] review of different approaches for learning word embeddings from cross-lingual data. (they almost always work much better.) Should go in &quot;language/word embeddings&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00503&amp;sa=D&amp;ust=1465142039170000&amp;usg=AFQjCNHKhRBfOII5qUNy15c9i3Sk7_JRnQ">http://arxiv.org/abs/1604.00503</a></span><span>&nbsp;[abs:1] ??? &quot;Discriminative Phrase Embedding for Paraphrase Identification&quot; - should go in &quot;language/word embeddings&quot;, &quot;language/paraphrase identification&quot;, &quot;unsupervised&quot;, &quot;representations and types&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00562&amp;sa=D&amp;ust=1465142039171000&amp;usg=AFQjCNF3T_qEEFwptTvlf3dl-XEboHo-ng">http://arxiv.org/abs/1604.00562</a></span><span>&nbsp;[abs:2] image captioning approach that uses a neural &quot;listener&quot; and a neural &quot;speaker&quot; - kind of an adverserial thing, I guess? Maximize the listener&#39;s understanding? Though this might be speech synthesis. Should go in &quot;language/speech&quot;, &quot;language/talking computers&quot;, &quot;images/captioning&quot;. Also related to &quot;rl&quot;, &quot;unsupervised/adverserial&quot;, &quot;search&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00727&amp;sa=D&amp;ust=1465142039171000&amp;usg=AFQjCNGix3QO7V12P5wGbohG_X44kYk_nQ">http://arxiv.org/abs/1604.00727</a></span><span>&nbsp;[abs:1] character-level attentional model on a textual question answering dataset. Works much better, 16x parameters. Significantly less data used, and training/testing overlap is 1%. Should go in &quot;language/question answering&quot;, &quot;attention&quot;, &quot;language/per-character&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00734&amp;sa=D&amp;ust=1465142039172000&amp;usg=AFQjCNHSENP3kwXr7lJqcu9Dv5EN7EPEuw">http://arxiv.org/abs/1604.00734</a></span><span>&nbsp;[abs:1] convnets on words to do entity reference resolution on text. Should go in &quot;language/tagging&quot;, I guess.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00788&amp;sa=D&amp;ust=1465142039173000&amp;usg=AFQjCNG0bnUOEwOxPbY_xGZMkMvP5OqP3g">http://arxiv.org/abs/1604.00788</a></span><span>&nbsp;[abs:2] neural machine translation that is primarily word-based, but drops down to the output of a character-based model if the word-based one is missing a word. Should go in &quot;language/neural machine translation&quot;, &quot;multi-modal models&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.00433&amp;sa=D&amp;ust=1465142039174000&amp;usg=AFQjCNHWp-Q9WVXVA4jufd5MC4f8j8Edzg">http://arxiv.org/abs/1604.00433</a></span><span>&nbsp;[abs:2] regularization/model compression technique that, in addition to compressing the model, also compresses the input: the compressed model is trained with reduced-quality inputs, and demonstrates improved performance on them for it. Should go in &quot;images/fine-grained classification&quot;, &quot;model compression&quot;, &quot;regularization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01146&amp;sa=D&amp;ust=1465142039175000&amp;usg=AFQjCNF3JiyJngYVgn9PqSMmzikSL-6aJw">http://arxiv.org/abs/1604.01146</a></span><span>&nbsp;[abs:2] generalizing from classifying text to classifying images, without having seen the images you want to classify at training time (ie, zero-shot learning). Apparently this has a noise problem; they add an objective based on norms to reduce noise of the textual representation. Should go in &quot;language/text understanding&quot;, &quot;unsupervised/zero-shot&quot;, &quot;objectives&quot;</span></p><p class="c6"><span>&nbsp;</span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01277%23&amp;sa=D&amp;ust=1465142039177000&amp;usg=AFQjCNFHPO1X4U-To8k5v1I4gw8T6v01Gw">http://arxiv.org/abs/1604.01277</a></span><span>&nbsp;[abs:1] using planning to recognize plans. Should go in &quot;search/gofai&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 6 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01219&amp;sa=D&amp;ust=1465142039178000&amp;usg=AFQjCNHegpCMW6mWb6nZc4eGEvGeZ5-afQ">http://arxiv.org/abs/1604.01219</a></span><span>&nbsp;[abs:2] dataset, and boring-sounding approach, for generating posters of scientific papers given the papers. Should go in &quot;datasets&quot;, &quot;language/text understanding&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.mt6b1vxhakfq"><span>Apr 7</span></h5><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c6 c25"><span class="c8 c1">Apr 7 16 </span><span class="c43 c1 c29 c46"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01537&amp;sa=D&amp;ust=1465142039180000&amp;usg=AFQjCNGhOgS52FvFROFai7_865GlL0xddQ">https://arxiv.org/abs/1604.01537</a></span><span class="c8 c1">&nbsp;generating Chinese poems with seq2seq. Should go in &quot;language/talking computers&quot;.</span></p><p class="c6"><span>Apr 7 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01692&amp;sa=D&amp;ust=1465142039181000&amp;usg=AFQjCNHtYnU5Z3hxVX-RcSc7CEeNc7mAgQ">https://arxiv.org/abs/1604.01692</a></span><span>&nbsp;ensemble of good word embedding techniques produces better ones. Don&#39;t forget about ensembles. Should go in &quot;language/embeddings&quot;, &quot;unsupervised&quot;, &quot;architectures&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 7 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01696&amp;sa=D&amp;ust=1465142039182000&amp;usg=AFQjCNG_JLKSrsjUysfkvl1Wtk4yB26ENg">https://arxiv.org/abs/1604.01696</a></span><span>&nbsp;five-sentence story dataset, with false alternate endings as a multiple choice training problem. Should go in &quot;language/text understanding&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.s2b8ry7zytr0"><span>Apr 8</span></h5><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1511.05950&amp;sa=D&amp;ust=1465142039184000&amp;usg=AFQjCNH-2bu8K0kPBqK0vQYW3RzvCZDnKQ">http://arxiv.org/abs/1511.05950</a></span><span>&nbsp;[abs:2] &quot;we propose a variant of the ASGD algorithm in which the learning rate is modulated according to the gradient staleness and provide theoretical guarantees for convergence of this algorithm&quot; - competes with synchronous sgd again. Should go in &quot;parallelization&quot;, &quot;training algorithms&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1507.06120&amp;sa=D&amp;ust=1465142039185000&amp;usg=AFQjCNEy-LzRt7CRMQsjgP6pNQaMpdeZnQ">http://arxiv.org/abs/1507.06120</a></span><span>&nbsp;[abs:1] review of analysis of visual lifelogging data (ie, long-term egocentric video). Should go in &quot;images/video&quot;, &quot;reviews&quot;, maybe &quot;unsupervised&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01348&amp;sa=D&amp;ust=1465142039186000&amp;usg=AFQjCNGSLHp7QGH5LQrTHmb4FEbjyuDbfg">http://arxiv.org/abs/1604.01348</a></span><span>&nbsp;[abs:1] &quot;a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling.&quot; Should go in &quot;hyperparameters/optimization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01252&amp;sa=D&amp;ust=1465142039187000&amp;usg=AFQjCNGsbDicgXgPu1eC9cW4A5iqBjx-UQ">http://arxiv.org/abs/1604.01252</a></span><span>&nbsp;[abs:2] image recommendation system that uses some sort of comparison-based objective to learn an embedding space. Should go in &quot;images/similarity&quot;, &quot;images/embeddings&quot;, &quot;recommendation&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01325&amp;sa=D&amp;ust=1465142039188000&amp;usg=AFQjCNGVyFpIu5rcfgfI1S4mz_uW_a53Dg">http://arxiv.org/abs/1604.01325</a></span><span>&nbsp;[abs:1] image retrieval thing that uses a sort of region-proposal based attention to generate an image representation. Should go in &quot;images/retrieval&quot;, &quot;images/embeddings&quot;, &quot;images/localization&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01335&amp;sa=D&amp;ust=1465142039189000&amp;usg=AFQjCNEK9PsaENJNIpqmKxwuHRZAGLBIvw">http://arxiv.org/abs/1604.01335</a></span><span>&nbsp;[abs:5] cross-task residual blocks for multi-task/multi-modal models. Should go in &quot;knowledge sharing/multi-task&quot;, &quot;generality&quot;, &quot;architectures/very deep&quot;, &quot;multi-modal models&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01345&amp;sa=D&amp;ust=1465142039190000&amp;usg=AFQjCNGXhxoDYWVy8eHONLQK9D3Id3_GCg">http://arxiv.org/abs/1604.01345</a></span><span>&nbsp;[abs:1] weak supervision to improve material recognition or something. Should go in &quot;images/textures&quot;, or something. Maybe &quot;images/classification&quot;? They also examine the internal features and find them to be similar to human opinions, which implies &quot;neuromorphic&quot; relevance, and &quot;debugging&quot; relevance.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01347&amp;sa=D&amp;ust=1465142039191000&amp;usg=AFQjCNGurGcBd4uoTjE4wn4m3-6_o5bV9g">http://arxiv.org/abs/1604.01347</a></span><span>&nbsp;[abs:2] predicting surface normals given an input image, and then model retrieval with something about this. Should go in &quot;images/3d&quot;, &quot;images/retrieval&quot;. Pretty cool.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01360&amp;sa=D&amp;ust=1465142039192000&amp;usg=AFQjCNEA_Nen3rLAilUCcO3kNzQW2aGoDA">http://arxiv.org/abs/1604.01360</a></span><span>&nbsp;[abs:1] robot-based &quot;unsupervised&quot; rl to learn visual representations. Should go in &quot;RL&quot;, &quot;robots&quot;, &quot;data collection&quot;, &quot;unsupervised&quot;. Produces better representations than training on imagenet apparently.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 *this document* is actually a meaningfully interesting dataset. Text classification and regression techniques would be very interesting to predict my descriptions, classifications, and labels. This note should go in &quot;datasets&quot;, &quot;language/text classification&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01178&amp;sa=D&amp;ust=1465142039193000&amp;usg=AFQjCNF8caKq8oveegj4CXbSPwHIUQli3w">http://arxiv.org/abs/1604.01178</a></span><span>&nbsp;[abs:1] simple embedding-learning network that embeds relationships between two sentences, and uses this for learning answer selection for question answering. Should go in &quot;language/question answering&quot;, &quot;language/embeddings&quot;, &quot;representations&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01221&amp;sa=D&amp;ust=1465142039194000&amp;usg=AFQjCNFI5wj-zLdNVIrP5WdmERlj5z_PAQ">http://arxiv.org/abs/1604.01221</a></span><span>&nbsp;[abs:0.5] multilingual news monitoring: story segmentation. Should go in &quot;language/machine translation&quot;, I guess. </span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1604.01272&amp;sa=D&amp;ust=1465142039195000&amp;usg=AFQjCNFzH0l7D6gSHKXx66CJZjqUlax-CQ">http://arxiv.org/abs/1604.01272</a></span><span>&nbsp;[abs:1] paragraph representations with nns, and then LDA on the paragraph representations? Cool. should go in &quot;language/embeddings&quot;, &quot;unsupervised/clustering&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 8 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01729&amp;sa=D&amp;ust=1465142039196000&amp;usg=AFQjCNHCoGdMy4RYwqPdhLKeTJyfuqNJvw">https://arxiv.org/abs/1604.01729</a></span><span>&nbsp;[abs:1] transfer learning from text corpora helps with video description. Should go in &quot;knowledge sharing/transfer learning&quot;, &quot;language/talking computers&quot;, &quot;images/captioning&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.8krv9evoguit"><span>Apr 11</span></h5><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02426&amp;sa=D&amp;ust=1465142039197000&amp;usg=AFQjCNFWnrHHxRMJ_vgzJH19ZO5VYSWDmA">https://arxiv.org/abs/1604.02426</a></span><span>&nbsp;[abs:1] image retrieval, automated with transfer learning. Should go in &quot;transfer learning&quot;, &quot;images/retrieval&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02313&amp;sa=D&amp;ust=1465142039198000&amp;usg=AFQjCNG0PC2I7TbZLESfJCkLHJ2AChL_Aw">https://arxiv.org/abs/1604.02313</a></span><span>&nbsp;[abs:2] norm-preserving activation function. Bad english and grammar in the abstract, but might be cool. Should go in &quot;architectures/very deep&quot;, &quot;activation functions&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.00466&amp;sa=D&amp;ust=1465142039199000&amp;usg=AFQjCNHeJIHMeT5MV3UjqN1Uplc270cuAw">https://arxiv.org/abs/1604.00466</a></span><span>&nbsp;[abs:0] data collection somehow, without machine learning? Should go in &quot;data collection&quot;. They build a massive database of visual facts, but the ground truth annotations sound pretty low quality.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1511.02853&amp;sa=D&amp;ust=1465142039200000&amp;usg=AFQjCNHfjmh8VqTcxwb4epqCU0CJeCFecw">https://arxiv.org/abs/1511.02853</a></span><span>&nbsp;[abs:1] weakly supervised object detection. Should go in &quot;images/localization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1511.05616&amp;sa=D&amp;ust=1465142039200000&amp;usg=AFQjCNFuhS9Al6ybuj10QpRH49sldPFqsA">https://arxiv.org/abs/1511.05616</a></span><span>&nbsp;[abs:2] multi-scale target hierarchy. Sounds cool. Should go in &quot;architectures&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1509.01851&amp;sa=D&amp;ust=1465142039201000&amp;usg=AFQjCNHOJUlOAkoYUXWwCOxiTY2eoaQvHg">https://arxiv.org/abs/1509.01851</a></span><span>&nbsp;[abs:0] replaced by </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01952&amp;sa=D&amp;ust=1465142039202000&amp;usg=AFQjCNFv3MGSN6Kb5DoNOL5Gc61-tphNiw">https://arxiv.org/abs/1604.01952</a></span><span>&nbsp; </span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01952&amp;sa=D&amp;ust=1465142039203000&amp;usg=AFQjCNE5E1WqaFqql7w53d6B19e5c34YUg">https://arxiv.org/abs/1604.01952</a></span><span>&nbsp;[abs:3] analysis of the convergence of gradient descent on neural networks, with a theoretical basis for why, based on an equivalence proof. Applies to actually-interesting networks. Should go in &quot;theory&quot;, &quot;learning algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02336&amp;sa=D&amp;ust=1465142039204000&amp;usg=AFQjCNHmWMONX7oxfqC0U2M9BOySoTlfxg">https://arxiv.org/abs/1604.02336</a></span><span>&nbsp;[abs:0] bayesian approach to knowledge tracing outperforms rnns. Should go in &quot;bayesian&quot;, &quot;applications/teaching&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02416&amp;sa=D&amp;ust=1465142039205000&amp;usg=AFQjCNFKmHHE1SsCbIU-ljPxV7bjPXAg0w">https://arxiv.org/abs/1604.02416</a></span><span>&nbsp;[abs:0] analysis of how much there is or isn&#39;t to gain from deep learning on knowledge tracing. Should go in &quot;applications/teaching&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02201&amp;sa=D&amp;ust=1465142039206000&amp;usg=AFQjCNGS6RJ8H4buAN4tSuDote8gOeOFvA">https://arxiv.org/abs/1604.02201</a></span><span>&nbsp;[abs:1] transfer learning to initialize neural machine translation between low-data language pairs. Should go in &quot;machine translation&quot;, &quot;transfer learning&quot;, &quot;small datasets&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02182&amp;sa=D&amp;ust=1465142039207000&amp;usg=AFQjCNFNgir69sfmzISh-c27in2LcBo58A">https://arxiv.org/abs/1604.02182</a></span><span>&nbsp;[abs:2] kinship recognition task and dataset. Should go in &quot;images/similarity&quot;, &quot;datasets&quot;, &quot;images/faces&quot;. Already superhuman, with feature extraction with a pretrained net.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02245&amp;sa=D&amp;ust=1465142039208000&amp;usg=AFQjCNHFtvA8fr2RkePsdrwpYVka4P9jdA">https://arxiv.org/abs/1604.02245</a></span><span>&nbsp;[abs:1] colorization of infrared images with convnets. Should go in &quot;images/per-pixel prediction&quot;. They don&#39;t seem to share their dataset, but it has a 1:1 mapping between input and target pixels, so that&#39;s nice.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02271&amp;sa=D&amp;ust=1465142039209000&amp;usg=AFQjCNGHdsxsxdbo0NrpW_otsVCclbehCQ">https://arxiv.org/abs/1604.02271</a></span><span>&nbsp;[abs:2] parsing images to produce trees of objects that follow the same structure as the language parse of their descriptions. Uses conv and rnn, unclear targets. Should go in &quot;images/object detection&quot;, &quot;images/captioning&quot;, &quot;language/talking computers&quot;, &quot;graphs&quot;, &quot;transfer learning&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02316&amp;sa=D&amp;ust=1465142039210000&amp;usg=AFQjCNGuFVQ1KmpAy6YvfSNqKP3esFdemQ">https://arxiv.org/abs/1604.02316</a></span><span>&nbsp;[abs:2] dataset and fully convolutional approach to predicting available movement space (I think?) for cars, from images. Should go in &quot;applications/cars&quot;, &quot;images/per-pixel prediction&quot;, &quot;datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 11 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.02388&amp;sa=D&amp;ust=1465142039211000&amp;usg=AFQjCNFsZTv3OoxYL2b5ckZJh2pE0TXIlg">https://arxiv.org/abs/1604.02388</a></span><span>&nbsp;[abs:3] adaptive pooling and upsampling for fully convolutional networks, such that it deals with boundaries better. Should go in &quot;pooling&quot;, &quot;architectures&quot; (maybe), &quot;images/segmentation&quot;. They apply it to rgbd data, so should also go in &quot;images/3d&quot;.</span></p><p class="c3"><span></span></p><h5 class="c5" id="h.ohgmlasvk10m"><span>Apr 12</span></h5><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/4biap8/_/&amp;sa=D&amp;ust=1465142039213000&amp;usg=AFQjCNEPk6ztKPXrdULcV19Pl45_PLTwuQ">https://www.reddit.com/r/MachineLearning/comments/4biap8/_/</a></span><span>&nbsp;[skim:1] interesting comments that actor-critic, policy gradients, and q learning can all be combined. Should go in &quot;rl&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=http://quod.lib.umich.edu/cgi/p/pod/dod-idx/beyond-the-beat-towards-metre-rhythm-and-melody-modelling.pdf?c%3Dicmc;idno%3Dbbp2372.2014.075&amp;sa=D&amp;ust=1465142039214000&amp;usg=AFQjCNGRN2zsFSBqIH-yOPp4I9XvhNfXfw">http://quod.lib.umich.edu/cgi/p/pod/dod-idx/beyond-the-beat-towards-metre-rhythm-and-melody-modelling.pdf?c=icmc;idno=bbp2372.2014.075</a></span><span>&nbsp;[abs:3] very cool architecture for audio processing: oscillator/matchers that give signal as to the similarity of the input to a wave, and then an rnn over these. Might be cool to give the rnn the ability to output what wave it wants to look for. Should go in &quot;audio&quot;, &quot;architectures&quot;. From </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://www.reddit.com/r/MachineLearning/comments/4ae69a/audio_deepdream/&amp;sa=D&amp;ust=1465142039215000&amp;usg=AFQjCNG5grWh9ImAtXXRnbHdnKmYjh94lg">https://www.reddit.com/r/MachineLearning/comments/4ae69a/audio_deepdream/</a></span><span>&nbsp;</span></p><p class="c3"><span></span></p><p class="c6"><span>(new window)</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01485&amp;sa=D&amp;ust=1465142039216000&amp;usg=AFQjCNFfMuoA0oUioBKtDIbTR2HRoBqRAQ">https://arxiv.org/abs/1604.01485</a></span><span>&nbsp;[abs:2] attentional question-answering model that uses question-object detection as the attention system. Should go in &quot;images/object detection&quot;, &quot;images/question answering&quot;, &quot;attention&quot;. Uses recurrence.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01545&amp;sa=D&amp;ust=1465142039218000&amp;usg=AFQjCNEjvf2AVJGHPbXUl8zKrKuQzZ0p-A">https://arxiv.org/abs/1604.01545</a></span><span>&nbsp;[abs:3] dataset: &quot;Multi-Domain Road Scene Semantic Segmentation&quot;; and a transfer-learning/model-compression based technique for making high-quality and fast networks to run on embedded devices. Should go in &quot;applications/cars&quot;, &quot;datasets&quot;, &quot;model compression&quot;, &quot;transfer learning&quot;, &quot;small datasets&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01655&amp;sa=D&amp;ust=1465142039219000&amp;usg=AFQjCNGXagsmEBIW_GL4nMHCn7_Jd9N5FQ">https://arxiv.org/abs/1604.01655</a></span><span>&nbsp;[abs:0] deep network that sees both color and depth at once in rgbd. Lolwut? Is this new? Should go in &quot;images/3d&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01685&amp;sa=D&amp;ust=1465142039219000&amp;usg=AFQjCNEeA3Pa5WaxkeFVaCU_u1x6AIEbjA">https://arxiv.org/abs/1604.01685</a></span><span>&nbsp;[abs:3] urban scene understanding dataset - video sequences recorded from different cities, with 5000 images of pixel annotations and 20k coarse annotations. Should go in &quot;datasets&quot;, &quot;images/object detection&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01720&amp;sa=D&amp;ust=1465142039220000&amp;usg=AFQjCNGPti-p6HIltPThJ9j2rJRgT1uk0Q">https://arxiv.org/abs/1604.01720</a></span><span>&nbsp;[abs:1] not related to neural networks, but cool: encoding steganographic messages that human eyes can&#39;t detect, but cameras can. Should go in &quot;security&quot;, &quot;misc&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01753&amp;sa=D&amp;ust=1465142039222000&amp;usg=AFQjCNEycQwIK74bUy9BqOTozDsw0eSTvQ">https://arxiv.org/abs/1604.01753</a></span><span>&nbsp;[abs:2] new dataset, charades, from people recording video in homes. Crowdsourced. Annotated with free-text descriptions, action labels, action intervals, and classes of interacted objects. 24k descriptions, 37k localized intervals for 160 action classes, 24k labels for 40 object classes. Should go in &quot;datasets&quot;. Also comes with a baseline for &quot;images/action recognition&quot; and for &quot;images/captioning&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01474&amp;sa=D&amp;ust=1465142039223000&amp;usg=AFQjCNF2F5LYta48jOGP-2npJDdHIc-Rrg">https://arxiv.org/abs/1604.01474</a></span><span>&nbsp;[abs:0] &quot;self paced multi task learning&quot;. Looks really low quality. Should go in &quot;data prioritization&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01475&amp;sa=D&amp;ust=1465142039224000&amp;usg=AFQjCNFkHuALHxC5l_kH4AFnBFGzcLmyRQ">https://arxiv.org/abs/1604.01475</a></span><span>&nbsp;[abs:0] &quot;Learning A Deep &#8467;&infin; Encoder for Hashing&quot; - looks really low quality.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01416&amp;sa=D&amp;ust=1465142039226000&amp;usg=AFQjCNE_nAYdR4ON6rBTaIBlCQGQcbITYw">https://arxiv.org/abs/1604.01416</a></span><span>&nbsp;[abs:1] distributed math library, for use writing things like tensorflow, that is better at not transferring things between gpus. Should go in &quot;libraries&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01662&amp;sa=D&amp;ust=1465142039227000&amp;usg=AFQjCNGG4Wkw2RS6kFghv1kCqae046ZiCQ">https://arxiv.org/abs/1604.01662</a></span><span>&nbsp;[abs:2] overview of bayesian neural networks. Should go in &quot;bayesian neural networks&quot;, &quot;reviews&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.01444&amp;sa=D&amp;ust=1465142039229000&amp;usg=AFQjCNGejIrvzV7wrGYWcthRBCtU8RQ7SQ">https://arxiv.org/abs/1604.01444</a></span><span>&nbsp;[abs:0] &quot;A Convolutional Neural Network Neutrino Event Classifier&quot;, should go in &quot;applications/physics&quot;</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.08183&amp;sa=D&amp;ust=1465142039230000&amp;usg=AFQjCNHlrVDnNzVsiTFon5EXfgBs2cxJCQ">https://arxiv.org/abs/1512.08183</a></span><span>&nbsp;[abs:1] bag of ngrams still best for sentiment classification; modification of paragraph vector learning to predict ngram features as well as words, producing &quot;document vectors&quot;. Claims to outperform both other deep learning approaches and bag of ngrams. Should go in &quot;language/sentiment analysis&quot;, &quot;language/embeddings&quot;. Works well in an ensemble. Comes with source.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1603.01360&amp;sa=D&amp;ust=1465142039231000&amp;usg=AFQjCNHwEG1vZl-QAemdcR74o4iPIalWMA">https://arxiv.org/abs/1603.01360</a></span><span>&nbsp;[abs:2] multi-task/transfer learning semi-supervised approach, using bilstms and crfs, to do named entity recognition from larger datasets than are normally available. Should go in &quot;semi-supervised learning&quot;, &quot;multi-task&quot;, &quot;transfer learning&quot;, &quot;language/tagging&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1604.00727&amp;sa=D&amp;ust=1465142039232000&amp;usg=AFQjCNFV-P9GUG4oOo2sjbwOVU3I0bdqIg">https://arxiv.org/abs/1604.00727</a></span><span>&nbsp;[abs:3] attentional character-level question answering. 16x fewer parameters, uses less training data and augmentation, performs better. Should go in &quot;language/per-character&quot;, &quot;attention&quot;, &quot;small datasets&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1510.07712&amp;sa=D&amp;ust=1465142039233000&amp;usg=AFQjCNFxv3QKhx3SnNPM6GEGK6hh-CjSgQ">https://arxiv.org/abs/1510.07712</a></span><span>&nbsp;[abs:3] multi-level next-step prediction architecture for video captioning; an attentional sentence generator that predicts text given an embedding, and a paragraph generator that feeds the sentence generator with things to say, also attentional over the video. Should go in &quot;language/talking computers&quot;, &quot;towards generality&quot;, &quot;multi-modal models&quot;, &quot;architectures&quot;, &quot;images/captioning&quot;. Also mentions datasets for the problem.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1511.06040&amp;sa=D&amp;ust=1465142039234000&amp;usg=AFQjCNHgODfozE_RYUc0S3cepO_5Ql-2Jg">https://arxiv.org/abs/1511.06040</a></span><span>&nbsp;[abs:1] multi-level prediction for group activity classification, by representing each individual person&#39;s activity independently and then training on aggregating this. Should go in &quot;images/video&quot;, &quot;multi-modal models&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span>Apr 12 16 </span><span class="c4"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1511.08250&amp;sa=D&amp;ust=1465142039235000&amp;usg=AFQjCNEypZZreP5H5k6AFyyHgapRinhkfQ">https://arxiv.org/abs/1511.08250</a></span><span>&nbsp;[abs:3] approach for image segmentation that uses a recurrent network to predict segmentations one at a time, with a feedback loop informing it what&#39;s left to do. Should go in &quot;images/per-pixel prediction&quot;, &quot;images/segmentation&quot;, &quot;recurrence&quot;. Also feels more like an agent, maybe &quot;towards generality&quot;? Mentions cool &quot;datasets&quot; too.</span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><h5 class="c5" id="h.2llsq91n6nic"><span>Head</span></h5><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><hr style="page-break-before:always;display:none;"><p class="c3"><span class="c1"></span></p><h1 class="c5" id="h.v54ppv87huhk"><span>Rubbish Compactor</span></h1><p class="c3"><span></span></p><p class="c6"><span class="c11"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1512.04509&amp;sa=D&amp;ust=1465142039243000&amp;usg=AFQjCNEKZf6Q8blg7Evquj59GRAEzdegvQ">http://arxiv.org/abs/1512.04509</a></span><span class="c36">&nbsp;one shot classification learning dubious technique -&gt; this is just the perceptron training algorithm lol, fail</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c14 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1603.01772&amp;sa=D&amp;ust=1465142039244000&amp;usg=AFQjCNGKRGemrMY5OgpeTh8IkBDwZvFJ7A">http://arxiv.org/abs/1603.01772</a></span><span class="c1 c24">&nbsp;[abs:0] unclear what this even is. &quot;Fast calculation of correlations in recognition systems&quot; - should go in &quot;rubbish bin&quot;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c14 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.03820&amp;sa=D&amp;ust=1465142039245000&amp;usg=AFQjCNFRn-v_YQ-81MjOe0BnUImMI-u4uQ">http://arxiv.org/abs/1510.03820</a></span><span class="c1 c24">&nbsp;[abs:1] analysis of the sensitivity of convnets to different hyperparameters. only tests on one-layer ones. should go in &quot;hyperparameters&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c14 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.09046&amp;sa=D&amp;ust=1465142039247000&amp;usg=AFQjCNGEWjxn-XjOW6UHK9CtujngKGfJHg">http://arxiv.org/abs/1602.09046</a></span><span class="c1 c24">&nbsp;[abs] regularization using &quot;complex&quot; valued CNNs. they recognize that it&#39;s effectively just the same thing with double the parameters. this sounds stupid. relevant to josh&#39;s interests, though. should go in &quot;regularization&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1 c27 c29"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.08118&amp;sa=D&amp;ust=1465142039248000&amp;usg=AFQjCNFfXYGPZ0iS4xL3Gq_v62H7R0DUdw">http://arxiv.org/abs/1602.08118</a></span><span class="c1 c27">&nbsp;bs paper about rnns by Andrew J.R. Simpson. if he shows up in a paper, it&#39;s almost certainly nonsense. should go in &quot;rubbish bin&quot;. his papers have interesting names, though; if one passes muster, just mark it with a warning, rather than throwing it away. more of his papers to greylist (heh it&#39;s grey): </span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.05711&amp;sa=D&amp;ust=1465142039249000&amp;usg=AFQjCNGL9QdwyY59YaR54lxDPP7QM97asg">1510.05711</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.02442&amp;sa=D&amp;ust=1465142039249000&amp;usg=AFQjCNH_w402To1MGQzPZihXSnqseX3ZMQ">1510.02442</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1510.00797&amp;sa=D&amp;ust=1465142039250000&amp;usg=AFQjCNHP0Ldfm6ua00IQ2rXXBIy9Pjphgg">1510.00797</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.05765&amp;sa=D&amp;ust=1465142039250000&amp;usg=AFQjCNGRzTf3b7INtRzNaYCpBlq6G-gtWQ">1509.05765</a></span><span class="c9 c42">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.05173&amp;sa=D&amp;ust=1465142039251000&amp;usg=AFQjCNEDHYWKjs6pEKR0Gi8yRQIj9Ew08A">1509.05173</a></span><span class="c42 c9">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.03185&amp;sa=D&amp;ust=1465142039251000&amp;usg=AFQjCNHG8pK4x9HN3_Ew6tIgBWRNzyINzg">1509.03185</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1509.00913&amp;sa=D&amp;ust=1465142039252000&amp;usg=AFQjCNFhA9MkIVTUphmiosp-BKuTgrroIg">1509.00913</a></span><span class="c42 c9">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.07130&amp;sa=D&amp;ust=1465142039252000&amp;usg=AFQjCNGM8uUcThfuocHOx7QtEit-T5jEaw">1508.07130</a></span><span>&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1508.04826&amp;sa=D&amp;ust=1465142039253000&amp;usg=AFQjCNGwFROXE-H1UwUflvC42ujxEskAsQ">1508.04826</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.05972&amp;sa=D&amp;ust=1465142039253000&amp;usg=AFQjCNF-YM0ZKa6IrZCFtkzZxJ3qbGU2jQ">1505.05972</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1505.00289&amp;sa=D&amp;ust=1465142039254000&amp;usg=AFQjCNGWhCTbv7c8neiyoL7exKq4WUgDIw">1505.00289</a></span><span class="c1 c27">&nbsp;</span><span class="c9 c16"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.07372&amp;sa=D&amp;ust=1465142039254000&amp;usg=AFQjCNGSRzPi2IUdnMSobXrf1kpo9Ts03g">1504.07372</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.04658&amp;sa=D&amp;ust=1465142039255000&amp;usg=AFQjCNHC9iWxjVnxecTGFNOuR2fwB7UjMQ">1504.04658</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.02945&amp;sa=D&amp;ust=1465142039255000&amp;usg=AFQjCNHxQDfINDnVQDTrpSVeS6LbMZu6GA">1504.02945</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.06962&amp;sa=D&amp;ust=1465142039256000&amp;usg=AFQjCNEhyw5hwCA6xGW0FVw0e7hBMinXUQ">1503.06962</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.06046&amp;sa=D&amp;ust=1465142039256000&amp;usg=AFQjCNGUWhxCr95jfFwOUeGa6VeUJ2RESQ">1503.06046</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1503.05849&amp;sa=D&amp;ust=1465142039256000&amp;usg=AFQjCNFFGw0i0C_6QBd7TpqkSPRQALvGXw">1503.05849</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.04617&amp;sa=D&amp;ust=1465142039257000&amp;usg=AFQjCNG3JDwujp1rIoBNckJ3tLMCh0ut8A">1502.04617</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.04042&amp;sa=D&amp;ust=1465142039257000&amp;usg=AFQjCNFmGIgQ8RlU_Y4SfezUKMcuCUv-Bg">1502.04042</a></span><span class="c1 c27">&nbsp;</span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1502.03648&amp;sa=D&amp;ust=1465142039258000&amp;usg=AFQjCNEjbPi73NM9GNSumRxiRbotz7WYlQ">1502.03648</a></span><span class="c1 c27">&nbsp;</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c14 c1"><a class="c0" href="https://www.google.com/url?q=http://deeplearning.cs.cmu.edu/pdfs/1111/jmlr10_larochelle.pdf&amp;sa=D&amp;ust=1465142039259000&amp;usg=AFQjCNGNxdYRZdX1ziDyH7uLv2DDRe0Ibw">http://deeplearning.cs.cmu.edu/pdfs/1111/jmlr10_larochelle.pdf</a></span><span class="c1 c24">&nbsp;overview of what it takes to train nns as of 2009. </span></p><p class="c3"><span class="c1"></span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c14 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00828&amp;sa=D&amp;ust=1465142039260000&amp;usg=AFQjCNEp6cg3bbveIvJFZv8qmWJ7BODBVw">http://arxiv.org/abs/1602.00828</a></span><span class="c1 c24">&nbsp;convnets are good at discovering features for image retrieval. whoda thunk? should go in &quot;images/retrieval&quot; or &quot;representation learning&quot; or I dunno, this paper sounds stupid.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c1 c14"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1602.00417&amp;sa=D&amp;ust=1465142039261000&amp;usg=AFQjCNGD00qelnvdhbd6LXGhm8Zo95s7Hg">http://arxiv.org/abs/1602.00417</a></span><span class="c1 c24">&nbsp;(fluff?) paper on transfer learning using feature concat and then some funky classifier called AdaBoost.MH I&#39;ve never heard of. should go in &quot;transfer learning&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c19 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/pdf/1601.03805v1.pdf&amp;sa=D&amp;ust=1465142039262000&amp;usg=AFQjCNFULhjaeq2qsL2PXQ7o4UecKLQEqA">http://arxiv.org/pdf/1601.03805v1.pdf</a></span><span class="c1 c34">&nbsp;arxiv/ml comedy? this looks like a re-derivation of convnets, based on the objection that convnets don&#39;t take the matrix-ness of the input into account. ???? - might just be location connections in 2d. I didn&#39;t read in detail. pretty wat, though. they were complaining about how a vector loses information. because, you know, keeping it in a matrix shape does something different, even though it&#39;s still fully connected, right? anyway, should go in &quot;rubbish bin&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c19 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1504.08291&amp;sa=D&amp;ust=1465142039263000&amp;usg=AFQjCNHOMLizFfAChxpus80T_mdjhV8avw">http://arxiv.org/abs/1504.08291</a></span><span class="c1 c34">&nbsp;analysis of neural networks. formal proofs about why classifiers work. I strongly suspect that this has stupid assumptions in the proofs, due to language in the abstract. should go in &quot;theory&quot;.</span></p><p class="c3"><span class="c1"></span></p><p class="c6"><span class="c19 c1"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1506.08473&amp;sa=D&amp;ust=1465142039264000&amp;usg=AFQjCNEZ-RbnwOJpLema0MfzN9IkNsyVHQ">http://arxiv.org/abs/1506.08473</a></span><span class="c1 c34">&nbsp;provable algorithm for finding the best weights for a two-layer (one-hidden) neural network. looks probably useless, unless it generalizes to deeper networks, and I don&#39;t understand tensor decomposition yet, which it uses. should go in &quot;learning algorithms&quot;.</span></p><p class="c3"><span></span></p><p class="c6"><span class="c19"><a class="c0" href="https://www.google.com/url?q=http://mi.eng.cam.ac.uk/~sjy/papers/wgms15.pdf&amp;sa=D&amp;ust=1465142039265000&amp;usg=AFQjCNG-RE1CNGCgUHx4iBS7CeQJe4_3Bg">http://mi.eng.cam.ac.uk/~sjy/papers/wgms15.pdf</a></span><span class="c34">&nbsp;&quot;Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems&quot; - looks like yet another attempt at language modeling, cites a lot of really old stuff. doesn&#39;t stand out as likely to be interesting. Should go in &quot;rubbish bin&quot;</span></p></body></html>