{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "newlist = [\"1601.01887\", \"1511.06739\",\"1506.08350\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: chunk the newlist\n",
    "arxivurl = (\"http://export.arxiv.org/api/query?search_query=\" +\n",
    "         \" OR \".join([\"id:\" + x for x in newlist]) +\n",
    "         \"&start=0&max_results=\" + str(len(newlist)+2))\n",
    "\n",
    "r = requests.get(arxivurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "lesoup = BeautifulSoup(r.text, \"xml\")\n",
    "entries = lesoup.find_all(\"entry\")\n",
    "print(len(entries))\n",
    "\n",
    "# check for special entries\n",
    "for entry in entries:\n",
    "    for child in entry.descendants:\n",
    "        if child.name not in (None, \"id\", \"updated\", \"published\", \"title\", \"summary\", \"author\",\n",
    "                     \"name\", \"link\", \"category\", \"primary_category\", \"comment\", \"doi\"):\n",
    "            print(child.parent.name, child.name)\n",
    "            print(\"attrs\",child.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "abstracts = \n",
    "\n",
    "for entry in entries:\n",
    "    paper = {}\n",
    "    paper[\"link\"] = entry.id.string\n",
    "    paper[\"title\"] = entry.title.string\n",
    "    paper[\"summary\"] = entry.summary.string\n",
    "\n",
    "    paper[\"comment\"] = entry.comment.string if entry.comment else None\n",
    "    paper[\"doi\"] = entry.doi.string if entry.doi else None\n",
    "    paper[\"journalref\"] = entry.journal_ref.string if entry.journal_ref else None\n",
    "    \n",
    "    res = re.findall(\"([019][0-9][0-1][0-9]\\.[0-9]+)\",entry.id.string)\n",
    "    if len(res) != 1:\n",
    "        print(\"ERROR wrong match in \", entry)\n",
    "        break\n",
    "    elif res in papers.keys():\n",
    "        print(\"ERROR RES ALREADY IN HERE\", entry)\n",
    "        break\n",
    "    paper[\"arxivid\"] = res[0]\n",
    "    authorsaffil = []\n",
    "    for author in entry.find_all(\"author\"):\n",
    "        authorsaffil.append([author.find(\"name\").string, author.affiliation.string if author.affiliation else None])\n",
    "    paper[\"authorsaffil\"] = authorsaffil\n",
    "\n",
    "    temp = entry.find_all([\"category\", \"primary_category\"])\n",
    "    if temp[0][\"term\"] == temp[1][\"term\"]:\n",
    "        paper[\"categoryterms\"] = [x[\"term\"] for x in temp[1:]]\n",
    "    else:\n",
    "        paper[\"categoryterms\"] = [x[\"term\"] for x in temp]\n",
    "\n",
    "    paper[\"published\"] = entry.published.string\n",
    "    papers[res[0]] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['90C06', 'cs.LG', 'cs.NA', '90C', 'cs', '90'],\n",
       " ['I.2.10', 'I.2', 'I', 'I.2.6', 'cs', 'cs.CV'],\n",
       " ['cs.CL', 'cs', 'cs.DL']]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = [[authoraffil[0] for authoraffil in paper[\"authorsaffil\"]] for paper in papers.values()]\n",
    "\n",
    "lecategories = []\n",
    "for paper in papers.values():\n",
    "    results = {}\n",
    "    for categorystuff in paper[\"categoryterms\"]:\n",
    "        temp = re.findall(\"((([A-Z])\\.\\d*)(?:\\.[0-9m]*)?)(?:[.]\\w)?|\" +\n",
    "                         \"(((\\d\\d)[-A-Z])[0-9xX][0-9xX])|\" + \"(([-a-zA-Z]*)(?:\\.[-a-zA-Z]*)?)\",\n",
    "                         categorystuff.encode(\"UTF-8\"))\n",
    "        for matches in temp:\n",
    "            for match in matches:\n",
    "                if match != \"\":\n",
    "                    results[match] = match\n",
    "\n",
    "    lecategories.append(results.keys())\n",
    "                     #\"(?:([A-Z]\\.[0-9]*(?:\\.[0-9m]*)?(?:[.][0-9a-z])?)[;, ]?[;, ]?)*\"))\n",
    "                     #\"((([A-Z])[.][0-9]*)(?:[.][0-9m]*)?)(?:[.][0-9a-z])?|\" + \n",
    "                     #\"(((\\d\\d)[-A-Z])[0-9xX][0-9xX])|(([-a-zA-Z]*)(?:\\.[-a-zA-Z]*)?)\"))\n",
    "\n",
    "lecategories\n",
    "abstracts = [paper]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'arxivid': u'1506.08350',\n",
       "  'authorsaffil': [[u'Yadong Mu', None],\n",
       "   [u'Wei Liu', None],\n",
       "   [u'Wei Fan', None]],\n",
       "  'categoryterms': [u'cs.LG', u'cs.NA', u'90C06'],\n",
       "  'comment': u'14 pages, 9 figures',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1506.08350v2',\n",
       "  'published': u'2015-06-28T03:33:38Z',\n",
       "  'summary': u'  Stochastic gradient descent (SGD) holds as a classical method to build large\\nscale machine learning models over big data. A stochastic gradient is typically\\ncalculated from a limited number of samples (known as mini-batch), so it\\npotentially incurs a high variance and causes the estimated parameters bounce\\naround the optimal solution. To improve the stability of stochastic gradient,\\nrecent years have witnessed the proposal of several semi-stochastic gradient\\ndescent algorithms, which distinguish themselves from standard SGD by\\nincorporating global information into gradient computation. In this paper we\\ncontribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm\\nto this nascent research area, accelerating the optimization of a large family\\nof composite convex functions. Though theoretically converging faster, prior\\nsemi-stochastic algorithms are found to suffer from high iteration complexity,\\nwhich makes them even slower than SGD in practice on many datasets. In our\\nproposed S3GD, the semi-stochastic gradient is calculated based on efficient\\nmanifold propagation, which can be numerically accomplished by sparse matrix\\nmultiplications. This way S3GD is able to generate a highly-accurate estimate\\nof the exact gradient from each mini-batch with largely-reduced computational\\ncomplexity. Theoretic analysis reveals that the proposed S3GD elegantly\\nbalances the geometric algorithmic convergence rate against the space and time\\ncomplexities during the optimization. The efficacy of S3GD is also\\nexperimentally corroborated on several large-scale benchmark datasets.\\n',\n",
       "  'title': u'Stochastic Gradient Made Stable: A Manifold Propagation Approach for\\n  Large-Scale Optimization'},\n",
       " {'arxivid': u'1511.06739',\n",
       "  'authorsaffil': [[u'Raghudeep Gadde', None],\n",
       "   [u'Varun Jampani', None],\n",
       "   [u'Martin Kiefel', None],\n",
       "   [u'Peter V. Gehler', None]],\n",
       "  'categoryterms': [u'cs.CV', u'I.2.10; I.2.6'],\n",
       "  'comment': u'Conference track submission to ICLR-2016',\n",
       "  'doi': None,\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1511.06739v3',\n",
       "  'published': u'2015-11-20T19:58:38Z',\n",
       "  'summary': u'  In this paper we propose a CNN architecture for image segmentation. We\\nintroduce a new \"bilateral inception\" layer that is used on top of a\\nconvolutional architecture. The bilateral inception performs a filtering\\nbetween superpixels in an image. This addresses two problems that arise with\\nCNN segmentation architectures. First, this layer propagates information\\nbetween (super) pixels while respecting image edges, thus using the structured\\ninformation of the problem for improved results. Second, the layer recovers a\\nfull resolution segmentation result from the lower resolution solution of a\\nCNN. In the experiments we replace the deconvolution networks and Dense-CRF\\nthat have previously been proposed to address these problems with bilateral\\ninception layers. The reduction to superpixels reduces the amount of\\ncomputations and simplifies the network design. Further, we report better\\nempirical results by replacing De-convolutional and CNN+Dense-CRF steps in four\\ndifferent semantic segmentation CNN architecutres, even with-out re-training\\ntheir filter weights.\\n',\n",
       "  'title': u'Superpixel Convolutional Networks using Bilateral Inceptions'},\n",
       " {'arxivid': u'1601.01887',\n",
       "  'authorsaffil': [[u'Rustam Tagiew', None]],\n",
       "  'categoryterms': [u'cs.CL', u'cs.DL'],\n",
       "  'comment': u'5 pages, 2 figure',\n",
       "  'doi': u'10.13140/RG.2.1.1619.1847',\n",
       "  'journalref': None,\n",
       "  'link': u'http://arxiv.org/abs/1601.01887v1',\n",
       "  'published': u'2016-01-08T14:29:44Z',\n",
       "  'summary': u\"  The number of scientific papers grows exponentially in many disciplines. The\\nshare of online available papers grows as well. At the same time, the period of\\ntime for a paper to loose at chance to be cited anymore shortens. The decay of\\nthe citing rate shows similarity to ultradiffusional processes as for other\\nonline contents in social networks. The distribution of papers per author shows\\nsimilarity to the distribution of posts per user in social networks. The rate\\nof uncited papers for online available papers grows while some papers 'go\\nviral' in terms of being cited. Summarized, the practice of scientific\\npublishing moves towards the domain of social networks. The goal of this\\nproject is to create a text engineering tool, which can semi-automatically\\ncategorize a paper according to its type of contribution and extract\\nrelationships between them into an ontological database. Semi-automatic\\ncategorization means that the mistakes made by automatic pre-categorization and\\nrelationship-extraction will be corrected through a wikipedia-like front-end by\\nvolunteers from general public. This tool should not only help researchers and\\nthe general public to find relevant supplementary material and peers faster,\\nbut also provide more information for research funding agencies.\\n\",\n",
       "  'title': u'Research Project: Text Engineering Tool for Ontological Scientometry'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# transform abstracts to a vector\n",
    "# retrieve dictionary from savestuff\n",
    "# get corpus and tm objects\n",
    "# apply to get words matrix\n",
    "# create matrix input object\n",
    "    # get cbind object\n",
    "    # put together authors, categories, etc.\n",
    "# get predict object\n",
    "# use prediction object\n",
    "temp = robjects.r('''\n",
    "        load(\"savestuff.rdata\")\n",
    "        library(tm)\n",
    "        corpus <- Corpus(VectorSource(tolower(---------)))\n",
    "        corpus <- tm_map(corpus, removePunctuation)\n",
    "        corpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n",
    "\n",
    "        library(SnowballC)\n",
    "        corpus <- tm_map(corpus, stemDocument)\n",
    "        corpus <- tm_map(corpus, stripWhitespace)\n",
    "        ''')\n",
    "# [paper[\"categoryterms\"] for paper in papers.values()]\n",
    "papers.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_input\n",
      "categories\n",
      "authors\n",
      "frequencies.sparse\n",
      "caret_fit\n"
     ]
    }
   ],
   "source": [
    "for x in temp:\n",
    "    print x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
