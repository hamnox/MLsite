{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# code modified from http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "docfile = open(\"nn.html\", \"r\")\n",
    "\n",
    "docbuff = docfile.readlines()\n",
    "\n",
    "docfile.close()\n",
    "## was concerned this would be too long, seems fine tho. Only 1.1M\n",
    "# docbuff = \"nothing here yet\"\n",
    "# while docbuff != None:\n",
    "# print(docfile.tell())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "soup = BeautifulSoup(docbuff[0], \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories length 99\n",
      "categories now length 99\n",
      "\n",
      "How to Implement Cutting Edge Neural Networks\n",
      "Basics\n",
      "Libraries\n",
      "Debugging\n",
      "Classification\n",
      "Attention\n",
      "Initialization\n",
      "Activation functions\n",
      "Data collection\n",
      "In-training Dataset Labeling\n",
      "Human-compatibility\n",
      "RL\n",
      "Misc\n",
      "Q-learning\n",
      "Useful for RL\n",
      "AGI Safety/reliable reinforcement learners\n",
      "Unsupervised\n",
      "Automatic labeling\n",
      "Misc Representation Learning\n",
      "Generative modeling\n",
      "Partially supervised training\n",
      "Sparsity\n",
      "Architecture\n",
      "Skip-ahead connections\n",
      "Very Deep/Vanishing Gradients Problem\n",
      "Variable Compute\n",
      "Recurrence\n",
      "Attention\n",
      "Memory\n",
      "application: misc\n",
      "application: language modeling\n",
      "META\n",
      "Optimizers / Learning Algorithms\n",
      "Hyperparameters techniques\n",
      "Knowledge sharing\n",
      "Transfer learning\n",
      "Multi-task objectives and sharing\n",
      "Towards generality\n",
      "Efficiency and theory\n",
      "Data Prioritization\n",
      "Regularization\n",
      "Bayesian Neural Nets\n",
      "Theory about neural network effectiveness\n",
      "Adverserial Examples Problem\n",
      "Speed\n",
      "Model compression\n",
      "Hardware\n",
      "Parallelization\n",
      "Images\n",
      "100% Whole-Image Deep Learning\n",
      "Question answering\n",
      "Similarity detection\n",
      "Image Object Segmentation/Pixelwise classification\n",
      "3D\n",
      "Faces\n",
      "OCR\n",
      "Language\n",
      "Text analysis\n",
      "Talking Computers\n",
      "Speech\n",
      "Embeddings\n",
      "\"Language Models\" - next word prediction and etc\n",
      "Machine Translation\n",
      "Application\n",
      "Datasets\n",
      "Bio modeling (and bio context/pure bio papers)\n",
      "Finance\n",
      "Cars\n",
      "Misc applications\n",
      "My ideas\n",
      "Misc\n",
      "unsure how to classify/where it's useful\n",
      "Alternate/fringe deep techniques\n",
      "Misc non-ML or non-deep\n",
      "Neuromorphic\n",
      "Inbox\n",
      "Inbox and Uncategorized\n",
      "Inbox 2\n",
      "Mar 10\n",
      "Mar 11\n",
      "Mar 12\n",
      "Mar 19\n",
      "Mar 22\n",
      "Mar 23\n",
      "Mar 28\n",
      "Mar 30\n",
      "Mar 31\n",
      "Apr 1\n",
      "Apr 2\n",
      "Apr 3\n",
      "Apr 4\n",
      "Apr 5\n",
      "Apr 6\n",
      "Apr 7\n",
      "Apr 8\n",
      "Apr 11\n",
      "Apr 12\n",
      "Head\n",
      "Rubbish Compactor\n"
     ]
    }
   ],
   "source": [
    "categories = soup.body.find_all([\"h1\",\"h2\",\"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "print \"categories length\", len(categories)\n",
    "for category in categories:\n",
    "    if not category.text or len(item_string) < 1:\n",
    "        print category.contents, \"is empty\"\n",
    "        category.decompose()\n",
    "\n",
    "print \"categories now length\", len(categories)\n",
    "\n",
    "\n",
    "# print soup.body.prettify()[5000:10000]\n",
    "print\n",
    "category_strings = [category.text for category in categories]\n",
    "for item in category_strings:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import NavigableString\n",
    "\n",
    "all_strings = soup.body.find_all([\"li\",\"p\", \"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "\n",
    "\n",
    "# first and last item should be [category, last_linked]\n",
    "ambiguous_items = list()\n",
    "\n",
    "# [[category, next_category], [prev_linked, next_linked], [ambiguous_spans]]\n",
    "ambiguous_all = list()\n",
    "\n",
    "# [[category, first_link, text, all_links]]\n",
    "parsed = list()\n",
    "last_category = None\n",
    "last_linked = None\n",
    "\n",
    "\n",
    "for item in all_strings:\n",
    "    # ignore categories\n",
    "    item_string = item.text\n",
    "    if item_string == \"\":\n",
    "        continue\n",
    "    if item_string in category_strings:\n",
    "        # print \"category\", item_string, \"total\", len(parsed)\n",
    "        last_category = item_string\n",
    "        if ambiguous_items:\n",
    "            ambiguous_items[1] = last_category\n",
    "            ambiguous_all.append(ambiguous_items)\n",
    "            ambiguous_items = list()\n",
    "        continue\n",
    "    links = list()\n",
    "    for link in item.find_all(\"a\"):\n",
    "        if link.get(\"href\"):\n",
    "            links.append(link.get(\"href\"))\n",
    "    if links:\n",
    "        last_linked = item_string\n",
    "        if ambiguous_items:\n",
    "            ambiguous_items[1] = last_category\n",
    "            ambiguous_items[3] = last_linked\n",
    "            ambiguous_all.append(ambiguous_items)\n",
    "            ambiguous_items = list()\n",
    "        addon = [last_category, item_string]\n",
    "        addon.extend(links)\n",
    "        parsed.append(addon)\n",
    "    else:\n",
    "        if not ambiguous_items:\n",
    "            ambiguous_items = [last_category, None, last_linked, None]\n",
    "        ambiguous_items.append(item_string)\n",
    "\n",
    "print len(parsed)\n",
    "print len(all_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for item in parsed[100:120]:\n",
    "#     print item[2]\n",
    "\n",
    "# print len(ambiguous_all)\n",
    "\n",
    "# for item in ambiguous_all:\n",
    "#     print item[0]\n",
    "#     print item[1]\n",
    "#     if item[2]:\n",
    "#         print \"\".join(list(item[2].strings))\n",
    "#     else:\n",
    "#         print item[2]\n",
    "#     if item[3]:\n",
    "#         print \"\".join(list(item[3].strings))\n",
    "#     else:\n",
    "#         print item[3]\n",
    "#     for subitem in item[4:]:\n",
    "#         print \"----->\",subitem\n",
    "#     print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\n",
      "None\n",
      "asdf\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# help from https://docs.python.org/2/library/csv.html\n",
    "\n",
    "with open('doctable.csv', 'wb') as f:\n",
    "    writer = csv.writer(f, delimiter=\"|\")\n",
    "    for row in parsed:\n",
    "        writer.writerow([None if not s else s.encode(\"ascii\", errors=\"xmlcharrefreplace\") for s in row])\n",
    "\n",
    "with open('ambiguous.csv', 'wb') as f:\n",
    "    writer = csv.writer(f, delimiter=\"|\")\n",
    "    for row in ambiguous_all:\n",
    "        writer.writerow([None if not s else s.encode(\"ascii\", errors=\"xmlcharrefreplace\") for s in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "test = \"<body></body>\"\n",
    "test = BeautifulSoup(test, \"html.parser\")\n",
    "links = list()\n",
    "for link in test.find_all(\"a\"):\n",
    "    if link.get(\"href\"):\n",
    "        links.append(link.get(\"href\"))\n",
    "        \n",
    "if links:\n",
    "    print links\n",
    "else:\n",
    "    print \"hi!\"\n",
    "    \n",
    "test = [1, 2, 3, 4]\n",
    "test.extend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
